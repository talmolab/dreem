{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/talmolab/dreem/blob/docs/examples/microscopy-demo-simple.ipynb)\n",
    "\n",
    "## DREEM workflow for microscopy\n",
    "### From raw tiff stacks to tracked identities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk you through the typical workflow for microscopy identity tracking. We start with a raw tiff stack, pass it through an off-the-shelf detection model, and feed those detections into DREEM. \n",
    "\n",
    "This notebook uses a simple entrypoint into the tracking code. You only need to specify a configuration file, and a few lines of code!\n",
    "\n",
    "To run this demo, we have provided sample data and model checkpoints. A GPU is recommended if you run the CellPose segmentation step, otherwise the tracking will run on a CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install DREEM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install dreem-track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/main/dreem-docs/dreem/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download a pretrained model, configs and some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = \"./models\"\n",
    "config_save_dir = \"./configs\"\n",
    "os.makedirs(config_save_dir, exist_ok=True)\n",
    "os.makedirs(model_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = hf_hub_download(\n",
    "    repo_id=\"talmolab/microscopy-pretrained\",\n",
    "    filename=\"pretrained-microscopy.ckpt\",\n",
    "    local_dir=model_save_dir,\n",
    ")\n",
    "\n",
    "config_path = hf_hub_download(\n",
    "    repo_id=\"talmolab/microscopy-pretrained\",\n",
    "    filename=\"sample-microscopy-config.yaml\",\n",
    "    local_dir=config_save_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hf download talmolab/microscopy-demo --repo-type dataset --local-dir ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use CellPose to create segmentation masks for our instances. **If you want to skip this stage**, we have provided segmentation masks for the lysosomes dataset located at ./data/lysosomes, and you can go straight ahead to the \"Tracking\" section below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run CellPose segmentation with uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/dynamicnuclearnet/test_1\"\n",
    "segmented_path = \"./data/dynamicnuclearnet/test_1_GT/TRA\"\n",
    "os.makedirs(segmented_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the approximate diameter (in pixels) of the instances you want to segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "diam_px = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_flag = \"--gpu\" if torch.cuda.is_available() else \"--no-gpu\"\n",
    "\n",
    "print(\"Running CellPose segmentation with uv...\")\n",
    "result = subprocess.run(\n",
    "    [\n",
    "        \"uv\",\n",
    "        \"run\",\n",
    "        \"run_cellpose_segmentation.py\",\n",
    "        \"--data_path\",\n",
    "        data_path,\n",
    "        \"--output_path\",\n",
    "        segmented_path,\n",
    "        \"--diameter\",\n",
    "        str(diam_px),\n",
    "        gpu_flag,\n",
    "    ],\n",
    "    check=True,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Errors/Warnings:\", result.stderr)\n",
    "\n",
    "# Load the original stack and masks for visualization\n",
    "tiff_files = [\n",
    "    f for f in os.listdir(data_path) if f.endswith(\".tif\") or f.endswith(\".tiff\")\n",
    "]\n",
    "tiff_files.sort()  # Ensure consistent ordering\n",
    "first_img = tifffile.imread(os.path.join(data_path, tiff_files[0]))\n",
    "mask_path = os.path.join(segmented_path, f\"{os.path.splitext(tiff_files[0])[0]}.tif\")\n",
    "first_mask = tifffile.imread(mask_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the segmentation result and original image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1.imshow(first_mask)\n",
    "ax1.set_title(\"Segmentation Mask\")\n",
    "ax2.imshow(first_img)\n",
    "ax2.set_title(\"Original Image\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking\n",
    "Note that the segmented masks are saved to ./data/dynamicnuclearnet/test_1_GT/TRA; in general, any segmented masks are expected to be in a directory with the same name as the original data, with _GT/TRA appended to the end.\n",
    "\n",
    "The command below assumes you have run the CellPose segmentation step, and that the segmented masks are saved to ./data/dynamicnuclearnet/test_1_GT/TRA. If you have not run the segmentation step, you can use the following command to track the lysosome data that we have provided: \n",
    "\n",
    "```\n",
    "!dreem track ./data/lysosomes --checkpoint ./models/pretrained-microscopy.ckpt --output ./results-lyso --video-type tif --crop-size 22\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mConfiguration saved to: results-lyso/config.track.\u001b[0m\u001b[1;32m02\u001b[0m\u001b[32m-\u001b[0m\u001b[1;32m04\u001b[0m\u001b[32m-\u001b[0m\u001b[1;32m2026\u001b[0m\u001b[32m-\u001b[0m\u001b[1;32m14\u001b[0m\u001b[32m-\u001b[0m\u001b[1;32m40\u001b[0m\u001b[32m-\u001b[0m\u001b[1;32m21.\u001b[0m\u001b[32myaml\u001b[0m\n",
      "╭──────────────────────────── Track Configuration ─────────────────────────────╮\n",
      "│ \u001b[1;36mCheckpoint:\u001b[0m\u001b[1;36m  \u001b[0mmodels/pretrained-microscopy.ckpt                               │\n",
      "│ \u001b[1;36mOutput:    \u001b[0m\u001b[1;36m  \u001b[0mresults-lyso                                                    │\n",
      "│ \u001b[1;36mInput:     \u001b[0m\u001b[1;36m  \u001b[0mdata/lysosomes                                                  │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "\u001b[2KPredicting \u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 7/7 \u001b[2m0:00:50 • 0:00:00\u001b[0m \u001b[2;4m0.16it/s\u001b[0m [2;4m0.13it/s\u001b[0m \n",
      "\u001b[?25h\u001b[32mResults saved to results-lyso\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!dreem track ./data/dynamicnuclearnet --checkpoint ./models/pretrained-microscopy.ckpt --output ./results-dnn --video-type tif --crop-size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results\n",
    "To visualize the tracked tiff stacks, you can use tools like ImageJ, Fiji, or Napari plugins."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
