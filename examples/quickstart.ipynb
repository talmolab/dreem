{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/talmolab/dreem/blob/main/examples/quickstart.ipynb)\n",
        "\n",
        "# DREEM Quickstart: Fly Tracking Demo\n",
        "\n",
        "Welcome to DREEM! This notebook will guide you through tracking a social interaction between two flies using a pretrained model. By the end, you'll have hands-on experience with the core DREEM workflow.\n",
        "\n",
        "### What you'll learn:\n",
        "- How to download sample data and pretrained models\n",
        "- Running tracking inference with `dreem-track`\n",
        "- Evaluating tracking accuracy with `dreem-eval`\n",
        "- Visualizing tracking results\n",
        "\n",
        "### Requirements:\n",
        "- **Runtime**: ~5-10 minutes total\n",
        "- **Hardware**: CPU is sufficient (GPU optional but faster)\n",
        "- **Data**: We provide sample fly videos from the [SLEAP fly32 dataset](https://sleap.ai/datasets.html#fly32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 1: Install Dependencies\n",
        "\n",
        "First, we'll install DREEM and the required packages. This may take a few minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install DREEM and dependencies\n",
        "%pip install dreem-tracker huggingface_hub opencv-python-headless\n",
        "\n",
        "# Install ffmpeg for video visualization (Colab-specific)\n",
        "!apt-get install -y ffmpeg 2>/dev/null || echo \"ffmpeg already installed or not on Linux\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check available hardware\n",
        "\n",
        "Let's detect what compute device is available. DREEM works on CPU, CUDA (NVIDIA GPUs), or MPS (Apple Silicon).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "cuda_available = torch.cuda.is_available()\n",
        "print(f\"CUDA available: {cuda_available}\")\n",
        "\n",
        "if cuda_available:\n",
        "    accelerator = \"cuda\"\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    accelerator = \"mps\"\n",
        "    print(\"Using Apple Silicon (MPS)\")\n",
        "else:\n",
        "    accelerator = \"cpu\"\n",
        "    print(\"Using CPU (tracking will be slower but still works!)\")\n",
        "\n",
        "print(f\"\\nâœ“ Device selected: {accelerator}\")\n",
        "torch.set_float32_matmul_precision(\"medium\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 2: Download Sample Data\n",
        "\n",
        "We'll download a sample fly tracking dataset from Hugging Face. This includes:\n",
        "- Video files (`.mp4`) of fly interactions\n",
        "- Detection files (`.slp`) with pose keypoints from SLEAP\n",
        "- Configuration files for running inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!huggingface-cli download talmolab/sample-flies --repo-type dataset --local-dir ./data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Expected directory structure\n",
        "\n",
        "After downloading, your data folder should look like this:\n",
        "\n",
        "```\n",
        "./data\n",
        "    /test\n",
        "        190719_090330_wt_18159206_rig1.2@15000-17560.mp4\n",
        "        GT_190719_090330_wt_18159206_rig1.2@15000-17560.slp\n",
        "    /train\n",
        "        190612_110405_wt_18159111_rig2.2@4427.mp4\n",
        "        GT_190612_110405_wt_18159111_rig2.2@4427.slp\n",
        "    /val\n",
        "        two_flies.mp4\n",
        "        GT_two_flies.slp\n",
        "    /inference\n",
        "        190719_090330_wt_18159206_rig1.2@15000-17560.mp4\n",
        "        190719_090330_wt_18159206_rig1.2@15000-17560.slp\n",
        "    /configs\n",
        "        inference.yaml\n",
        "        base.yaml\n",
        "        eval.yaml\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify the data downloaded correctly\n",
        "import os\n",
        "\n",
        "expected_dirs = ['test', 'train', 'val', 'inference', 'configs']\n",
        "missing = [d for d in expected_dirs if not os.path.exists(f'./data/{d}')]\n",
        "\n",
        "if missing:\n",
        "    print(f\"âš  Missing directories: {missing}\")\n",
        "    print(\"Please re-run the download cell above.\")\n",
        "else:\n",
        "    print(\"âœ“ Data downloaded successfully!\")\n",
        "    print(\"\\nContents:\")\n",
        "    for d in expected_dirs:\n",
        "        files = os.listdir(f'./data/{d}')\n",
        "        print(f\"  ./data/{d}/: {len(files)} files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 3: Download Pretrained Model\n",
        "\n",
        "Now we'll download a pretrained DREEM model. This model was trained on various animal data (mice, flies, zebrafish) and generalizes well to new animal tracking tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!huggingface-cli download talmolab/animals-pretrained animals-pretrained.ckpt --local-dir=./models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify the model downloaded correctly\n",
        "model_path = \"./models/animals-pretrained.ckpt\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
        "    print(f\"âœ“ Model downloaded successfully!\")\n",
        "    print(f\"  Path: {model_path}\")\n",
        "    print(f\"  Size: {size_mb:.1f} MB\")\n",
        "else:\n",
        "    print(\"âš  Model not found. Please re-run the download cell above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 4: Run Tracking\n",
        "\n",
        "Now for the main event! We'll use `dreem-track` to run tracking inference on our fly video.\n",
        "\n",
        "**What happens during tracking:**\n",
        "1. DREEM loads the video and detections (pose keypoints)\n",
        "2. For each frame, it extracts visual features around each detection\n",
        "3. The transformer model associates detections across frames to form tracks\n",
        "4. Results are saved as a new `.slp` file with track IDs assigned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!dreem-track --config-dir=./data/configs --config-name=inference ckpt_path=./models/animals-pretrained.ckpt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check that results were generated\n",
        "results_dir = \"./results\"\n",
        "if os.path.exists(results_dir):\n",
        "    result_files = [f for f in os.listdir(results_dir) if f.endswith('.slp')]\n",
        "    if result_files:\n",
        "        print(\"âœ“ Tracking complete! Output files:\")\n",
        "        for f in result_files:\n",
        "            print(f\"  {results_dir}/{f}\")\n",
        "        # Store the latest result file path for later use\n",
        "        result_file = sorted(result_files)[-1]\n",
        "        result_path = os.path.join(results_dir, result_file)\n",
        "    else:\n",
        "        print(\"âš  No .slp files found in results directory\")\n",
        "else:\n",
        "    print(\"âš  Results directory not found. Check the tracking output above for errors.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 5: Evaluate Tracking Accuracy (Optional)\n",
        "\n",
        "If you have ground truth labels, you can use `dreem-eval` instead of `dreem-track`. This computes tracking metrics like:\n",
        "- **MOTA** (Multiple Object Tracking Accuracy)\n",
        "- **IDF1** (ID F1 Score)  \n",
        "- **Number of ID switches**\n",
        "\n",
        "The eval config points to test data with ground truth labels (files prefixed with `GT_`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!dreem-eval --config-dir=./data/configs --config-name=eval ckpt_path=./models/animals-pretrained.ckpt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 6: Visualize Results\n",
        "\n",
        "Let's visualize the tracking results! We'll create an animation showing the tracked flies with their assigned IDs.\n",
        "\n",
        "> **Note**: For the best visualization experience with full pose keypoints, you can open the `.slp` file in the SLEAP GUI locally:\n",
        "> ```bash\n",
        "> sleap-label results/<your_output_file>.slp\n",
        "> ```\n",
        "> The SLEAP GUI won't render in Colab, but the animation below gives you a quick preview.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sleap_io as sio\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib.patches import Circle\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML, display\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load tracking results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the latest tracking result\n",
        "results_dir = \"./results\"\n",
        "result_files = sorted([f for f in os.listdir(results_dir) if f.endswith('.slp') and 'dreem_inference' in f])\n",
        "result_path = os.path.join(results_dir, result_files[-1])\n",
        "print(f\"Loading results from: {result_path}\")\n",
        "\n",
        "# Load the predictions\n",
        "pred_slp = sio.load_slp(result_path)\n",
        "print(f\"Loaded {len(pred_slp)} frames with tracking results\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert predictions to a DataFrame for visualization\n",
        "list_frames = []\n",
        "for lf in pred_slp:\n",
        "    for instance in lf.instances:\n",
        "        centroid = np.nanmean(instance.numpy(), axis=0)\n",
        "        track_id = int(instance.track.name) if instance.track else -1\n",
        "        list_frames.append({\n",
        "            \"frame_id\": lf.frame_idx,\n",
        "            \"track_id\": track_id,\n",
        "            \"centroid\": centroid\n",
        "        })\n",
        "df = pd.DataFrame(list_frames)\n",
        "print(f\"Found {df['track_id'].nunique()} unique tracks across {df['frame_id'].nunique()} frames\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create tracking animation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "def create_tracking_animation(video_path, metadata_df, fps=15, marker_size=15, max_frames=200, display_width=600):\n",
        "    \"\"\"Create and display a tracking animation in the notebook.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Could not open video file: {video_path}\")\n",
        "    \n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    \n",
        "    # Create colormap for track IDs\n",
        "    unique_ids = metadata_df[\"track_id\"].unique()\n",
        "    cmap = cm.get_cmap(\"tab10\", len(unique_ids))\n",
        "    id_to_color = {id_val: cmap(i) for i, id_val in enumerate(unique_ids)}\n",
        "    \n",
        "    # Setup figure\n",
        "    fig_width = display_width / 100\n",
        "    fig_height = fig_width * (height / width)\n",
        "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
        "    \n",
        "    frame_img = ax.imshow(np.zeros((height, width, 3), dtype=np.uint8))\n",
        "    markers, texts = [], []\n",
        "    \n",
        "    frame_ids = sorted(metadata_df[\"frame_id\"].unique())\n",
        "    if max_frames and max_frames < len(frame_ids):\n",
        "        frame_ids = frame_ids[:max_frames]\n",
        "        print(f\"Showing first {max_frames} frames\")\n",
        "    \n",
        "    def update(frame_num):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            return []\n",
        "        \n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame_img.set_array(frame_rgb)\n",
        "        \n",
        "        for m in markers: m.remove()\n",
        "        for t in texts: t.remove()\n",
        "        markers.clear()\n",
        "        texts.clear()\n",
        "        \n",
        "        frame_data = metadata_df[metadata_df[\"frame_id\"] == frame_num]\n",
        "        for _, row in frame_data.iterrows():\n",
        "            x, y = row[\"centroid\"]\n",
        "            color = id_to_color[row[\"track_id\"]]\n",
        "            circle = Circle((x, y), marker_size, color=color, alpha=0.7)\n",
        "            markers.append(ax.add_patch(circle))\n",
        "            text = ax.text(x, y, str(row[\"track_id\"]), color=\"white\", fontsize=8, \n",
        "                          ha=\"center\", va=\"center\", fontweight=\"bold\")\n",
        "            texts.append(text)\n",
        "        \n",
        "        frame_text = ax.text(10, 20, f\"Frame: {frame_num}\", color=\"white\", \n",
        "                            fontsize=8, backgroundcolor=\"black\")\n",
        "        texts.append(frame_text)\n",
        "        return [frame_img] + markers + texts\n",
        "    \n",
        "    ax.set_xlim(0, width)\n",
        "    ax.set_ylim(height, 0)\n",
        "    ax.axis(\"off\")\n",
        "    \n",
        "    print(f\"Creating animation with {len(frame_ids)} frames...\")\n",
        "    anim = FuncAnimation(fig, update, frames=frame_ids, blit=True)\n",
        "    plt.close(fig)\n",
        "    \n",
        "    display(HTML(anim.to_html5_video()))\n",
        "    cap.release()\n",
        "    return anim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the video file used for inference\n",
        "video_dir = \"./data/inference\"\n",
        "video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
        "video_path = os.path.join(video_dir, video_files[0])\n",
        "print(f\"Video: {video_path}\")\n",
        "\n",
        "# Create the animation\n",
        "anim = create_tracking_animation(\n",
        "    video_path=video_path,\n",
        "    metadata_df=df,\n",
        "    fps=15,\n",
        "    marker_size=15,\n",
        "    max_frames=200\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Next Steps\n",
        "\n",
        "Congratulations! You've successfully run DREEM tracking on fly data. Here's where to go next:\n",
        "\n",
        "### Dive Deeper\n",
        "- **[End-to-End Demo](https://colab.research.google.com/github/talmolab/dreem/blob/main/examples/dreem-demo.ipynb)**: Train your own model, run inference, and evaluate results\n",
        "- **[Microscopy Demo](https://colab.research.google.com/github/talmolab/dreem/blob/main/examples/microscopy-demo-simple.ipynb)**: Track cells in microscopy data\n",
        "\n",
        "### Documentation\n",
        "- **[Usage Guide](https://dreem.sleap.ai/usage/)**: Full CLI reference and configuration options\n",
        "- **[Configuration Reference](https://dreem.sleap.ai/configs/)**: Customize training and inference parameters\n",
        "- **[API Reference](https://dreem.sleap.ai/reference/dreem/)**: Python API documentation\n",
        "\n",
        "### Use Your Own Data\n",
        "The pretrained animals model works with any SLEAP detections! Just:\n",
        "1. Generate pose predictions with [SLEAP](https://sleap.ai/)\n",
        "2. Update the config to point to your video and `.slp` files\n",
        "3. Run `dreem-track` as shown above\n",
        "\n",
        "Happy tracking! ðŸª°\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
