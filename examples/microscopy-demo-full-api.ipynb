{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DREEM workflow for microscopy - detailed API usage\n",
    "### From raw tiff stacks to tracked identities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk you through the typical workflow for microscopy identity tracking. We start with an off-the-shelf detection model, and feed those results into DREEM. \n",
    "Here, we'll use the API, but we also provide a CLI interface for convenience.\n",
    "\n",
    "To run this demo, we have provided sample data, model checkpoints, and configurations. The data used in this demo is small enough to be run on a single machine, though a GPU is recommended. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory structure (data, models and configs will be downloaded)\n",
    "```\n",
    "./data\n",
    "    /dynamicnuclearnet\n",
    "        /test_1\n",
    "        /mp4-for-visualization\n",
    "    /lysosomes\n",
    "        /7-2\n",
    "        /7-2_GT\n",
    "        /mp4-for-visualization\n",
    "./configs\n",
    "    eval.yaml\n",
    "./models\n",
    "    pretrained_microscopy.ckpt\n",
    " microscopy-demo-full-api.ipynb\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install huggingface hub to access models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf\n",
    "from dreem.io import Config\n",
    "from dreem.datasets import TrackingDataset\n",
    "from dreem.models import GTRRunner\n",
    "from dreem.inference import Tracker\n",
    "import sleap_io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download a pretrained model, configs and some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = \"./models\"\n",
    "config_save_dir = \"./configs\"\n",
    "data_save_dir = \"./data\"\n",
    "os.makedirs(config_save_dir, exist_ok=True)\n",
    "os.makedirs(data_save_dir, exist_ok=True)\n",
    "os.makedirs(model_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = hf_hub_download(repo_id=\"talmolab/microscopy-pretrained\", filename=\"pretrained-microscopy.ckpt\",\n",
    "local_dir=model_save_dir)\n",
    "\n",
    "config_path = hf_hub_download(repo_id=\"talmolab/microscopy-pretrained\", filename=\"sample-eval-microscopy.yaml\",\n",
    "local_dir=config_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download talmolab/microscopy-demo --repo-type dataset --local-dir ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify that the model loads properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GTRRunner.load_from_checkpoint(model_path, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if a GPU is available. For Apple silicon users, you can run on MPS, but ensure your version of PyTorch is compatible with MPS, and that you have installed the correct version of DREEM. You can also run without a GPU. The demo has been tested on an M3 Macbook Air running only on a CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "if cuda_available:\n",
    "    accelerator = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    accelerator = \"mps\"\n",
    "    devices = 1\n",
    "else:\n",
    "    accelerator = \"cpu\"\n",
    "print(\"Using device: \", accelerator)\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use CellPose to create segmentation masks for our instances. If you want to skip this stage, we have provided segmentation masks for the lysosomes dataset located at ./data/lysosomes. You can enter this path in the configuration file provided, under dataset.test_dataset.dir.path, and then skip straight ahead to the section labelled DREEM Inference below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install CellPose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://www.github.com/mouseland/cellpose.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "from cellpose import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/dynamicnuclearnet/test_1\"\n",
    "segmented_path = \"./data/dynamicnuclearnet/test_1_GT/TRA\"\n",
    "os.makedirs(segmented_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the approximate diameter (in pixels) of the instances you want to segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diam_px = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_files = [f for f in os.listdir(data_path) if f.endswith('.tif') or f.endswith('.tiff')]\n",
    "stack = np.stack([tifffile.imread(os.path.join(data_path, f)) for f in tiff_files])\n",
    "frames, Y, X = stack.shape\n",
    "\n",
    "channels = [0, 0]\n",
    "# use builtin latest model\n",
    "model = models.CellposeModel(gpu=True)\n",
    "all_masks = np.zeros_like(stack)\n",
    "for i, img in enumerate(stack):\n",
    "    masks, flows, styles = model.eval(\n",
    "        img,\n",
    "        diameter=diam_px,\n",
    "        cellprob_threshold=0.0,\n",
    "        channels=channels,\n",
    "        z_axis=None,\n",
    "    )\n",
    "    all_masks[i] = masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(segmented_path, exist_ok=True)\n",
    "for i, (mask, filename) in enumerate(zip(all_masks, tiff_files)):\n",
    "    new_tiff_path = os.path.join(segmented_path, f\"{os.path.splitext(filename)[0]}.tif\")\n",
    "    print(f\"exporting frame {i} to tiff at {new_tiff_path}\")\n",
    "    tifffile.imwrite(new_tiff_path, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the segmentation result and original image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1.imshow(all_masks[0])\n",
    "ax1.set_title('Segmentation Mask')\n",
    "ax2.imshow(stack[0])\n",
    "ax2.set_title('Original Image')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DREEM Inference\n",
    "In this section, we demonstrate the standard DREEM inference pipeline using the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GTRRunner.load_from_checkpoint(model_path, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup inference configs\n",
    "#### NOTE: We can only specify 1 directory at a time when running inference. To test a different dataset, just enter the path to the directory containing the dataset. See the config for an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cfg_path = \"./configs/sample-eval-microscopy.yaml\"\n",
    "# use OmegaConf to load the config\n",
    "pred_cfg = OmegaConf.load(pred_cfg_path)\n",
    "pred_cfg = Config(pred_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the tracker settings from the config and initialize the tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_cfg = pred_cfg.get_tracker_cfg()\n",
    "model.tracker_cfg = tracker_cfg\n",
    "model.tracker = Tracker(**model.tracker_cfg)\n",
    "trainer = pred_cfg.get_trainer()\n",
    "# inference results will be saved here\n",
    "outdir = \"./results\"\n",
    "os.makedirs(outdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_files, vid_files = pred_cfg.get_data_paths(mode=\"test\", data_cfg=pred_cfg.cfg.dataset.test_dataset)\n",
    "\n",
    "for label_file, vid_file in zip(labels_files, vid_files):\n",
    "    dataset = pred_cfg.get_dataset(\n",
    "        label_files=[label_file], vid_files=[vid_file], mode=\"test\"\n",
    "    )\n",
    "    dataloader = pred_cfg.get_dataloader(dataset, mode=\"test\")\n",
    "    \n",
    "    # the actual inference is done here\n",
    "    preds = trainer.predict(model, dataloader)\n",
    "\n",
    "    # convert the predictions to sleap format\n",
    "    pred_slp = []\n",
    "    tracks = {}\n",
    "    for batch in preds:\n",
    "        for frame in batch:\n",
    "            if frame.frame_id.item() == 0:\n",
    "                video = (\n",
    "                    sio.Video(frame.video)\n",
    "                    if isinstance(frame.video, str)\n",
    "                    else sio.Video\n",
    "                )\n",
    "            lf, tracks = frame.to_slp(tracks, video=video)\n",
    "            pred_slp.append(lf)\n",
    "    pred_slp = sio.Labels(pred_slp)\n",
    "    # save the predictions to disk (requires sleap-io)\n",
    "    if isinstance(vid_file, list):\n",
    "        save_file_name = vid_file[0].split(\"/\")[-2]\n",
    "    else:\n",
    "        save_file_name = vid_file\n",
    "    outpath = os.path.join(\n",
    "        outdir,\n",
    "        f\"{Path(save_file_name).stem}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.slp\",\n",
    "    )\n",
    "    pred_slp.save(outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.patches import Circle\n",
    "import matplotlib.cm as cm\n",
    "from IPython.display import HTML, display\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import Video\n",
    "\n",
    "def create_tracking_animation(video_path, metadata_df, \n",
    "                                             fps=30, text_size=8, marker_size=20,\n",
    "                                             max_frames=None, display_width=800):\n",
    "    \"\"\"\n",
    "    Create and display an animal tracking animation directly in the notebook.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    video_path : str\n",
    "        Path to the input MP4 video file\n",
    "    metadata_df : pandas.DataFrame\n",
    "        DataFrame with columns: frame_id, track_id, centroid\n",
    "    fps : int\n",
    "        Frames per second for the animation\n",
    "    text_size : int\n",
    "        Size of the ID text\n",
    "    marker_size : int\n",
    "        Size of the marker circle\n",
    "    max_frames : int, optional\n",
    "        Maximum number of frames to process (useful for previewing)\n",
    "    display_width : int\n",
    "        Width of the displayed animation in the notebook\n",
    "    \"\"\"\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Create a colormap for track IDs\n",
    "    unique_ids = metadata_df['track_id'].unique()\n",
    "    cmap = cm.get_cmap('viridis', len(unique_ids))  # Using 'hsv' for bright, distinct colors\n",
    "    id_to_color = {id_val: cmap(i) for i, id_val in enumerate(unique_ids)}\n",
    "    \n",
    "    # Set up the figure and axis with the correct aspect ratio\n",
    "    fig_width = display_width / 100  # Convert to inches (assuming 100 dpi)\n",
    "    fig_height = fig_width * (height / width)\n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "    \n",
    "    # Initialize the plot elements\n",
    "    frame_img = ax.imshow(np.zeros((height, width, 3), dtype=np.uint8))\n",
    "    markers = []\n",
    "    texts = []\n",
    "    \n",
    "    # Get the list of frame IDs from the metadata\n",
    "    frame_ids = sorted(metadata_df['frame_id'].unique())\n",
    "    \n",
    "    # Limit the number of frames if specified\n",
    "    if max_frames is not None and max_frames < len(frame_ids):\n",
    "        frame_ids = frame_ids[:max_frames]\n",
    "        print(f\"Limiting preview to {max_frames} frames\")\n",
    "    \n",
    "    # Function to update the animation for each frame\n",
    "    def update(frame_num):\n",
    "        # Read the frame from the video\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Failed to read frame {frame_num}\")\n",
    "            return []\n",
    "        \n",
    "        # Convert BGR to RGB (OpenCV uses BGR, matplotlib uses RGB)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_img.set_array(frame_rgb)\n",
    "        \n",
    "        # Clear previous markers and texts\n",
    "        for marker in markers:\n",
    "            marker.remove()\n",
    "        markers.clear()\n",
    "        \n",
    "        for text in texts:\n",
    "            text.remove()\n",
    "        texts.clear()\n",
    "        \n",
    "        # Get data for the current frame\n",
    "        frame_data = metadata_df[metadata_df['frame_id'] == frame_num]\n",
    "        \n",
    "        # Add markers and IDs for each animal in the current frame\n",
    "        for _, row in frame_data.iterrows():\n",
    "            track_id = row['track_id']\n",
    "            x, y = row['centroid']\n",
    "            color = id_to_color[track_id]\n",
    "            \n",
    "            # Add circle marker\n",
    "            circle = Circle((x, y), marker_size, color=color, alpha=0.3)\n",
    "            markers.append(ax.add_patch(circle))\n",
    "            \n",
    "            # Add ID text\n",
    "            text = ax.text(x, y, str(track_id), color='white', \n",
    "                          fontsize=text_size, ha='center', va='center', \n",
    "                          fontweight='bold')\n",
    "            texts.append(text)\n",
    "        \n",
    "        # Add frame number for reference\n",
    "        frame_text = ax.text(10, 20, f\"Frame: {frame_num}\", color='white', \n",
    "                            fontsize=text_size, backgroundcolor='black')\n",
    "        texts.append(frame_text)\n",
    "        \n",
    "        return [frame_img] + markers + texts\n",
    "    \n",
    "    # Set up the axis\n",
    "    ax.set_xlim(0, width)\n",
    "    ax.set_ylim(height, 0)  # Invert y-axis to match image coordinates\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create the animation\n",
    "    print(f\"Creating animation with {len(frame_ids)} frames...\")\n",
    "    anim = FuncAnimation(fig, update, frames=frame_ids, blit=True)\n",
    "    \n",
    "    # Display the animation in the notebook\n",
    "    plt.close(fig)  # Prevent duplicate display\n",
    "    \n",
    "    # Display as HTML5 video\n",
    "    html_video = HTML(anim.to_html5_video())\n",
    "    display(html_video)\n",
    "    \n",
    "    return anim\n",
    "\n",
    "# Option to save the animation to a file for later viewing\n",
    "def save_animation(anim, output_path, fps=10, dpi=100):\n",
    "    \"\"\"Save the animation to a file\"\"\"\n",
    "    anim.save(output_path, writer='ffmpeg', fps=fps, dpi=dpi)\n",
    "    print(f\"Animation saved to {output_path}\")\n",
    "    \n",
    "    # Display the saved video in the notebook\n",
    "    return Video(output_path, embed=True, width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the predictions into a dataframe to make an animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_frames = []\n",
    "for lf in pred_slp:\n",
    "    for instance in lf.instances:\n",
    "        centroid = np.nanmean(instance.numpy(), axis=0)\n",
    "        track_id = int(instance.track.name)\n",
    "        list_frames.append({'frame_id': lf.frame_idx, 'track_id': track_id, 'centroid': centroid})\n",
    "df = pd.DataFrame(list_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and display the animation in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(os.path.join(pred_cfg.cfg.dataset.test_dataset['dir']['path'], 'mp4-for-visualization')):\n",
    "    if file.endswith('.mp4'):\n",
    "        video_path = os.path.join(pred_cfg.cfg.dataset.test_dataset['dir']['path'], 'mp4-for-visualization', file)\n",
    "\n",
    "anim = create_tracking_animation(\n",
    "    video_path=video_path,\n",
    "    metadata_df=df,\n",
    "    fps=15,\n",
    "    text_size=5,\n",
    "    marker_size=8,\n",
    "    max_frames=300\n",
    ")\n",
    "\n",
    "# save the animation\n",
    "video = save_animation(anim, f\"./tracking_vis-{video_path.split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the tracking results\n",
    "In this section, we evaluate metrics on a ground truth labelled test set. Note that in this example, the test set we used to demonstrate the inference pipeline is the same as the one we use here. To verify that\n",
    "we do not in fact use any ground truth information during tracking, go to our full dreem-demo notebook, where we use de-labelled data to verify this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cfg_path = \"./configs/eval.yaml\"\n",
    "# use OmegaConf to load the config\n",
    "eval_cfg = OmegaConf.load(pred_cfg_path)\n",
    "eval_cfg = Config(eval_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics[\"test\"] = eval_cfg.get(\"metrics\", {}).get(\"test\", \"all\")\n",
    "model.test_results[\"save_path\"] = eval_cfg.get(\"outdir\", \"./eval\")\n",
    "os.makedirs(model.test_results[\"save_path\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run evaluation pipeline. Note how we use trainer.test() to run evaluation whereas earlier, we used trainer.predict() to run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_files, vid_files = eval_cfg.get_data_paths(mode=\"test\", data_cfg=eval_cfg.cfg.dataset.test_dataset)\n",
    "trainer = eval_cfg.get_trainer()\n",
    "for label_file, vid_file in zip(labels_files, vid_files):\n",
    "    dataset = eval_cfg.get_dataset(\n",
    "        label_files=[label_file], vid_files=[vid_file], mode=\"test\"\n",
    "    )\n",
    "    dataloader = eval_cfg.get_dataloader(dataset, mode=\"test\")\n",
    "    metrics = trainer.test(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the results and view key metrics\n",
    "The results get saved to an HDF5 file in the directory specified in the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(model.test_results[\"save_path\"]):\n",
    "    if file.endswith(\".h5\"):\n",
    "        h5_path = os.path.join(model.test_results[\"save_path\"], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vid_motmetrics = {}\n",
    "dict_vid_gta = {}\n",
    "dict_vid_switch_frame_crops = {}\n",
    "\n",
    "with h5py.File(h5_path, \"r\") as results_file:\n",
    "    # Iterate through all video groups\n",
    "    for vid_name in results_file.keys():\n",
    "        print(\"Extracting metrics and crops for video: \", vid_name)\n",
    "        vid_group = results_file[vid_name]\n",
    "        # Load MOT summary\n",
    "        if \"mot_summary\" in vid_group:\n",
    "            mot_summary_keys = list(vid_group[\"mot_summary\"].attrs)\n",
    "            mot_summary_values = [vid_group[\"mot_summary\"].attrs[key] for key in mot_summary_keys]\n",
    "            df_motmetrics = pd.DataFrame(list(zip(mot_summary_keys, mot_summary_values)), columns=[\"metric\", \"value\"])\n",
    "            dict_vid_motmetrics[vid_name] = df_motmetrics\n",
    "        # Load global tracking accuracy if available\n",
    "        if \"global_tracking_accuracy\" in vid_group:\n",
    "            gta_keys = list(vid_group[\"global_tracking_accuracy\"].attrs)\n",
    "            gta_values = [vid_group[\"global_tracking_accuracy\"].attrs[key] for key in gta_keys]\n",
    "            df_gta = pd.DataFrame(list(zip(gta_keys, gta_values)), columns=[\"metric\", \"value\"])\n",
    "            dict_vid_gta[vid_name] = df_gta\n",
    "        # Find all frames with switches and save the crops\n",
    "        frame_crop_dict = {}\n",
    "        for key in vid_group.keys():\n",
    "            if key.startswith(\"frame_\"):\n",
    "                frame = vid_group[key]\n",
    "                frame_id = frame.attrs[\"frame_id\"]\n",
    "                for key in frame.keys():\n",
    "                    if key.startswith(\"instance_\"):\n",
    "                        instance = frame[key]\n",
    "                        if \"crop\" in instance.keys():\n",
    "                            frame_crop_dict[frame_id] = instance[\"crop\"][:].squeeze().transpose(1,2,0)\n",
    "        dict_vid_switch_frame_crops[vid_name] = frame_crop_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the switch count (and other mot metrics) for the whole video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motmetrics = list(dict_vid_motmetrics.values())[0]\n",
    "# motmetrics.loc[motmetrics['metric'] == 'num_switches']\n",
    "motmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check global tracking accuracy. This represents the percentage of frames where the tracker correctly maintained identities for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gta = list(dict_vid_gta.values())[0]\n",
    "gta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
