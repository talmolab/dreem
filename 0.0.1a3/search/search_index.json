{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DREEM Relates Every Entities' Motion","text":"<p>Global Tracking Transformers for biological multi-object tracking.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#development","title":"Development","text":""},{"location":"#clone-the-repository","title":"Clone the repository:","text":"<pre><code>git clone https://github.com/talmolab/dreem &amp;&amp; cd dreem\n</code></pre>"},{"location":"#set-up-in-a-new-conda-environment","title":"Set up in a new conda environment:","text":""},{"location":"#linuxwindows","title":"Linux/Windows:","text":""},{"location":"#gpu-accelerated-requires-cudanvidia-gpu","title":"GPU-accelerated (requires CUDA/nvidia gpu)","text":"<pre><code>conda env create -y -f environment.yml &amp;&amp; conda activate dreem\n</code></pre>"},{"location":"#cpu","title":"CPU:","text":"<pre><code>conda env create -y -f environment_cpu.yml &amp;&amp; conda activate dreem\n</code></pre>"},{"location":"#osx-m-chip","title":"OSX (M chip)","text":"<pre><code>conda env create -y -f environment_osx-arm.yml &amp;&amp; conda activate dreem\n</code></pre>"},{"location":"#uninstalling","title":"Uninstalling","text":"<pre><code>conda env remove -n dreem\n</code></pre>"},{"location":"cli/","title":"Command-line Interface","text":"<p>DREEM provides several types of functionality accessible through a command prompt.</p>"},{"location":"cli/#training","title":"Training","text":""},{"location":"cli/#dreem-train","title":"<code>dreem-train</code>","text":"<p><code>dreem-train</code> is the command-line interface for training. Use this for training on a remote machine/cluster/colab notebook instead of using the API directly.</p>"},{"location":"cli/#usage","title":"Usage","text":"<pre><code>usage: dreem-train [-h] [--hydra-help] [--config-dir] [--config-name] [+params_config] [+batch_config]\n\npositional arguments:\n    --config-dir    Path to configs dir\n    --config-name   Name of the .yaml config file stored in config-dir without the .yaml extension\n\noptional arguments:\n    -h, --help      Shows the application's help and exit.\n    --hydra-help    Shows Hydra specific flags (recommended over -h) \n    +params_config  Path to .yaml file containing subset of params to override\n    +batch_config   Path to .csv file where each row indicates params to override for a single task in a batch job\n</code></pre> <p>See the usage guide for a more in-depth explanation on how to use <code>dreem-train</code> and see the training config walkthrough for all available parameters.</p>"},{"location":"cli/#eval","title":"Eval","text":""},{"location":"cli/#dreem-eval","title":"<code>dreem-eval</code>","text":"<p><code>dreem-eval</code> is the command-line interface for inference. Use this for evaluating a trained model using pymotmetrics on a remote machine/cluster/notebook instead of using the API directly.</p>"},{"location":"cli/#usage_1","title":"Usage","text":"<p><pre><code>usage: dreem-eval [-h] [--hydra-help] [--config-dir] [--config-name] [+params_config] [+batch_config]\n\npositional arguments:\n    --config-dir    Path to configs dir\n    --config-name   Name of the .yaml config file stored in config-dir without the .yaml extension\n\noptional arguments:\n    -h, --help      Shows the application's help and exit.\n    --hydra-help    Shows Hydra specific flags (recommended over -h) \n    +params_config  Path to .yaml file containing subset of params to override\n    +batch_config   Path to .csv file where each row indicates params to override for a single task in a batch job\n</code></pre> See the usage guide for a more in-depth explanation on how to use <code>dreem-track</code> and see the inference config walkthrough for all available parameters.</p>"},{"location":"cli/#inference","title":"Inference","text":""},{"location":"cli/#dreem-track","title":"<code>dreem-track</code>","text":"<p><code>dreem-track</code> is the command-line interface for inference. Use this for tracking using a pretrained model on a remote machine/cluster/notebook instead of using the API directly.</p>"},{"location":"cli/#usage_2","title":"Usage","text":"<p><pre><code>usage: dreem-track [-h] [--hydra-help] [--config-dir] [--config-name] [+params_config] [+batch_config]\n\npositional arguments:\n    --config-dir    Path to configs dir\n    --config-name   Name of the .yaml config file stored in config-dir without the .yaml extension\n\noptional arguments:\n    -h, --help      Shows the application's help and exit.\n    --hydra-help    Shows Hydra specific flags (recommended over -h) \n    +params_config  Path to .yaml file containing subset of params to override\n    +batch_config   Path to .csv file where each row indicates params to override for a single task in a batch job\n</code></pre> See the usage guide for a more in-depth explanation on how to use <code>dreem-track</code> and see the inference config walkthrough for all available parameters.</p>"},{"location":"cli/#dreem-visualize","title":"<code>dreem-visualize</code>","text":"<p><code>dreem-visualize</code> is the command-line utility for generating annotated videos based on DREEM output</p>"},{"location":"cli/#usage_3","title":"Usage","text":"<pre><code>usage: dreem-visualize [-h] [--hydra-help] [+labels_path] [+vid_path] [+save_path] [+annotate.key] [--config-path] [+annotate.color_palette=\"tab20\"] [+annotate.trails=2] [+annotate.boxes=64] [+annotate.names=True] [+annotate.track_scores=0.5] [+annotate.centroids=4] [+annotate.poses=False] [+annotate.fps=30] [+annotate.alpha=0.2]\n\npositional arguments:\n    +labels_path                The path to the dreem tracks as a `.csv` file containing the following keys:\n                                    - \"X\": the X position of the instance in pixels\n                                    - \"Y\": the Y position of the instance in pixels\n                                    - \"Frame_id\": The frame of the video in which the instance occurs\n                                    - \"Pred_track_id\": The track id output from DREEM\n                                    - \"Track_score\": The trajectory score output from DREEM\n                                    - (Optional) \"Gt_track_id\": the gt track id for the instance (mostly used for debugging)\n                                where each row is an instance. See `dreem.inference.track.export_trajectories` \n    +vid_path                   The path to the video file in an `imageio`-accepted format (See: https://imageio.readthedocs.io/en/v2.4.1/formats.html)\n    +save_path                  The path to where you want to store the annotated video (e.g \"./tracked_videos/dreem_output.mp4\")\n    +annotate.key               The key where labels are stored in the dataframe - mostly used for choosing whether to annotate based on pred or gt labels\n\noptional arguments\n    -h, --help                  Shows the application's help and exit.\n    --hydra-help                Shows Hydra specific flags (recommended over -h) \n    --config-path               The path to the config .yaml file containing params for annotating. We recommend using this file in lieu of any \"annotate.*\" arguments.\n    +annotate.color_palette     The matplotlib colorpalette to use for annotating the video. Defaults to `tab10`\n    +annotate.trails            The size of the track trails. If the trail size &lt;=0 or none then it is not used.\n    +annotate.boxes             The size of the bbox. If bbox size &lt;= 0 or None then it is not added\n    +annotate.names             Whether or not to annotate with name\n    +annotate.centroids         The size of the centroid. If centroid size &lt;=0 or None then it is not added.\n    +annotate.poses             Whether or not to annotate with poses\n    +annotate.fps               The frames-per-second of the annotated video\n    +annotate.alpha             The opacity of the centroid annotations.\n</code></pre> <p>As noted above, instead of specifying each individual <code>+annotate.*</code> param we recommend setting up a visualize config (e.g <code>./configs/visualize.yaml</code>) which looks like:</p> <pre><code>annotate:\n    color_palette: \"tab10\"\n    trails: 5\n    boxes: (64, 64)\n    names: true\n    centroids: 10\n    poses: false\n    fps: 30\n    alpha: 0.2\n</code></pre> <p>and then running:</p> <pre><code>dreem-visualize +labels_path=\"/path/to/labels.csv\" +vid_path=\"/path/to/vid.{VID EXTENSION} +annotate.key=\"Pred_track_id\" --config-path=\"/path/to/configs/visualize.yaml\"\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>Here we describe a basic workflow from setting up data thru training and running inference. Regardless if you're only interested in running inference, we recommend at least skimming thru the entire tutorial as there may be useful information that applies to both training and inference!</p>"},{"location":"usage/#background","title":"Background","text":"<p>This repo is built on 3 main packages</p> <ol> <li><code>hydra</code> for config handling</li> <li><code>pytorch</code> for model construction</li> <li><code>pytorch-lightning</code> for high-level training/eval/inference handling. </li> </ol> <p>Thus, we recommend skimming through their respective docs to get some familiarity but is not necessary.</p>"},{"location":"usage/#setup","title":"Setup","text":""},{"location":"usage/#step-1-clone-the-repository","title":"Step 1. Clone the repository:","text":"<p>In your terminal run: <pre><code>git clone https://github.com/talmolab/dreem &amp;&amp; cd dreem\n</code></pre> This will clone the dreem repo into your current working directory and then move you inside the <code>dreem</code> directory.</p>"},{"location":"usage/#step-2-set-up-in-a-new-conda-environment","title":"Step 2. Set up in a new conda environment:","text":"<p>Next run: <pre><code>conda env create -y -f environment.yml &amp;&amp; conda activate dreem\n</code></pre> This will create a conda environment called <code>dreem</code> which will have <code>dreem</code> installed as well as any other dependcies needed to run.</p>"},{"location":"usage/#step-3-activate-the-dreem-environment","title":"Step 3. Activate the <code>dreem</code> environment","text":"<p>Finally run: <pre><code>conda activate dreem &amp;&amp; cd ..\n</code></pre> This will activate the <code>dreem</code> repo and move you back to your original working directory.</p>"},{"location":"usage/#training","title":"Training","text":""},{"location":"usage/#step-1-generate-ground-truth-data","title":"Step 1: Generate Ground Truth Data","text":"<p>In order to train a model you need 2 things.</p> <ol> <li>A video.<ul> <li>For animal data see the <code>imageio</code> docs for supported file types.</li> <li>For microscopy data we currently support <code>.tif</code> files. Video formats supported by <code>imageio</code> coming soon.</li> </ul> </li> <li>A ground truth labels file. This labels file contains two main pieces of data:<ol> <li>Detections/instances (i.e. locations of the instances in each frame). This can come in the form of centroids, poses or segmentations</li> <li>Ground truth identities (also called tracks, or trajectories). These are temporally consistent labels that group detections through time.</li> </ol> </li> </ol>"},{"location":"usage/#step-11-get-initial-labels","title":"Step 1.1: Get Initial labels","text":"<p>To generate your initial labels we recommend a couple methods.</p> <ul> <li>For animal pose-estimation, we highly recommend heading over to SLEAP and running through their work flow. </li> <li>For microscopy tracking, check out TrackMate.</li> </ul> <p>This is because these methods will handle both the detection and tracking steps together. Furthermore, these are currently the two main label formats we support in our data pipelines for arbitrary animal and microscopy datasets. If you'd like to use a different method, (e.g DeepLabCut or ilastik etc), the easiest way to make your data compatible with <code>dreem</code> is to convert your labels to a <code>.slp</code> file and your video to an <code>imageio</code>-supported video format. See the next section for more information on how to do this. Alternatively, you can write a custom dataloader but that will take significantly more overhead.</p>"},{"location":"usage/#step-12-proofreading","title":"Step 1.2 Proofreading","text":"<p>Once you have your labels file containing the detections and tracks, you'll want to make sure to proofread your labels. This is because object-tracking is hard and the methods we recommend above may have made mistakes. The most important thing to train a good model is to have high quality data. In our case good quality means two things:</p> <ol> <li>No identity switches. This is essential for training our object tracking model. If there are identity switches (say an animal/cell/organelle in frame 1 has track label 0 but in frame 2 it has track label 1), this will basically teach the model to make mistakes and cause irrecoverable failures. See below for an example of an identity switch. //TODO add example id switch</li> <li>Good detections. Because the input to the model is a crop centered around each detection, we want to make sure the coordinates we crop around are as accurate as possible. This is a bit more arbitrary and up to the user to determine what is a \"good\" detection. A general rule of thumb is to avoid having detection coordinates be hard to distinguish. For instance, with animal pose estimation, we want to avoid having the key points on two instances. For segmentation, the boundaries should be as tight as possible. This may be unavoidable however, in cases of occlusion and overlap. See below for example cases of good and bad detections. //TODO add example good vs bad detection.</li> </ol> <p>We recommend using the <code>sleap-label</code> gui for proofreading. This is because SLEAP's in-built gui provides useful. functionality for visualizing detections, moving them, and reassigning tracks. It also provides some nice heuristics for flagging where switches may have occured. </p>"},{"location":"usage/#converting-data-to-a-sleap-compatible-format","title":"Converting data to a SLEAP compatible format.","text":"<p>In order to use the SLEAP gui you'll need to have your labels and videos in a SLEAP comptabile format. Check out the sleap-io docs for available formats. The easiest way to ensure your labels are compatible with sleap is to convert them to a <code>.slp</code> file. For animal tracking, if you used SLEAP, this is already how SLEAP saves your labesl so you can start proofreading right away. Otherwise if you used a different system (e.g DeepLabCut) check out <code>sleap.io.convert</code> for available converters. With microscopy, we highly recommend starting out with TrackMate and then proofread in SLEAP's gui. Here is a converter from trackmate's output to a <code>.slp</code> file. In general, you can use <code>sleap-io</code> to write a custom converter to <code>.slp</code> if you'd like to use the sleap-gui for proofreading. Once you've ensured that your labels files have no identity switches and your detections are as good as you're willing to make them, they're ready to use for training.</p>"},{"location":"usage/#step-13-organize-data","title":"Step 1.3 Organize data.","text":"<p>Although, our data loading does take paths to label files and video files directly so it's fairly flexible, we recommend organizing your data with a couple things in mind.</p> <ol> <li>Match video and labels file stems. Because our dataloaders just take in a list of video files and a list of corresponding label files, the order in which the corresponding files are passed must match. (e.g <code>[file1.slp, file2.slp], [vid1.mp4, vid2.mp4]</code>). In order to make programmatic file searching easy its best to save your labels and vid files with the same stem so that you can ensure the ordering will be consistent. Its also just best practice so you know which video a labels file corresponds to.</li> <li>Store corresponding videos and labels files in the same directory. This will again make searching for the files much easier. </li> </ol> <p>The best practice is to have your dataset organized as follows: <pre><code>dataset_name/\n    train/\n        vid_1.{VID_EXTENSION}\n        vid_1.{slp, csv, xml} #depends on labels source\n            .\n            .\n            .\n        vid_n.{VID_EXTENSION}\n        vid_n.{slp, csv, xml}\n    val/\n        vid_1.{VID_EXTENSION}\n        vid_1.{slp, csv, xml}\n            .\n            .\n            .\n        vid_n.{VID_EXTENSION}\n        vid_n.{slp, csv, xml}\n    test/\n        vid_1.{VID_EXTENSION}\n        vid_1.{slp, csv, xml}\n            .\n            .\n            .\n        vid_n.{slp, csv, xml}\n        vid_n.{slp, csv, xml}\n</code></pre></p>"},{"location":"usage/#step-2-training","title":"Step 2. Training","text":"<p>Now that you have your dataset set up, we can start training! In <code>dreem.training.train</code>, we provide a train script that allows you to train with just a <code>.yaml</code> file. It also serves as a good example for how to write your own train script if you're an advanced user and would like to have some additional flexibility!</p>"},{"location":"usage/#step-21-setup-config","title":"Step 2.1. Setup Config:","text":"<p>The input into our training script is a <code>.yaml</code> file that contains all the parameters needed for training. Please see here for a detailed description of all the parameters and how to set up the config. We also provide an example training config to give you an idea for how the config should look. In general, the best practice is to keep a single <code>base.yaml</code> file which has all the default arguments you'd like to use. Then you can have a second <code>.yaml</code> file which will override only those specific set of parameters when training (for an example see here).</p>"},{"location":"usage/#step-22-train-model","title":"Step 2.2 Train Model","text":"<p>Once you have your config file and dataset set up, training is as easy as running</p> <p><pre><code>dreem-train --config-base=[CONFIG_DIR] --config-name=[CONFIG_STEM]\n</code></pre> where <code>CONFIG_DIR</code> is the directory that <code>hydra</code> should search for the <code>config.yaml</code> and <code>CONFIG_STEM</code> is the name of the config without the <code>.yaml</code> extension.</p> <p>e.g. If I have a config file called <code>base.yaml</code> inside my <code>/home/aaprasad/dreem_configs</code> directory I can call <pre><code>dreem-train --config-base=/home/aaprasad/dreem_configs --config-name=base\n</code></pre></p> <p>Note: you can use relative paths as well but may be a bit riskier so we recommend absolute paths whenever possible.</p>"},{"location":"usage/#overriding-arguments","title":"Overriding Arguments","text":"<p>Instead of changing the <code>base.yaml</code> file every time you want to run a different config, <code>hydra</code> enables us to either</p> <ol> <li>provide another <code>.yaml</code> file with a subset of the parameters to overide</li> <li>provide the args to the cli directly</li> </ol>"},{"location":"usage/#file-based-override","title":"File-based override","text":"<p>For overriding specific params with a sub-config, you can specify a <code>params_config</code> key and path in your <code>config.yaml</code> or run</p> <pre><code>dreem-train --config-base=[CONFIG_DIR] --config-name=[BASE_CONFIG_STEM] ++params_config=\"/path/to/params.yaml\"\n</code></pre> <p>e.g. If I have a <code>params_to_override.yaml</code> file inside my <code>/home/aaprasad/dreem_configs</code> directory that contains a only a small selection of parameters that I'd like to override, I can run:</p> <pre><code>dreem-train --config-base=/home/aaprasad/dreem_configs --config-name=base ++params_config=/home/aaprasad/dreem_configs/params_to_override.yaml\n</code></pre>"},{"location":"usage/#cli-based-override","title":"CLI-based override","text":"<p>For directly overriding a specific param via the command line directly you can use the <code>section.param=key</code> syntax as so:</p> <pre><code>dreem-train --config-base=[CONFIG_DIR] --config-name=[BASE_CONFIG_STEM] section.param=value\n</code></pre> <p>e.g If now I want to override a couple parameters again, say change the number of attention heads and change the name of this run in my logger, I can pass <code>model.head=3</code> and <code>logger.name=\"test_nheads=3\"</code> into </p> <pre><code>dreem-train --config-base=/home/aaprasad/dreem_configs --config-name=base model.nhead=3 logger.name=\"test_nheads=3\"\n</code></pre> <p>Note: using the <code>section.param</code> syntax for CLI override will only override if the parameter exists in your config file otherwise an error will be thrown if you'd like add a new parameter you can add <code>++</code> to the front of <code>section.param</code> e.g <code>++model.nhead=3</code>, However in this case, if the parameter exists in the config it will throw an error. thus if you'd like to overwrite when the parameter exists, otherwise create a new one you can add a single <code>+</code> to the front of <code>section.param</code> e.g <code>+model.nhead=3</code>.  When using <code>+</code>, you'll want to make sure you've matched the param exactly, otherwise it will simply add a new parameter with your spelling and the original value won't change.  e.g doing <code>model.n_head=3</code> will cause the original <code>model.nhead</code> to stay the same and just add a <code>model.n_head</code> field.</p> <p>See here for more information on overriding params.</p> <p>Note: You will usually only need the direct override or the file-based override however you can technically do both via  <pre><code>dreem-train --config-base=[CONFIG_DIR] --config-name=[BASE_CONFIG_STEM] ++params_config=\"/path/to/params.yaml\" section.param=value\n</code></pre> e.g. alongside the <code>params_to_override.yaml</code> <pre><code>dreem-train --config-base=/home/aaprasad/dreem_configs --config-name=base ++params_config=/home/aaprasad/dreem_configs/params_to_override.yaml model.nhead=3 logger.name=\"test_nheads=3\"\n</code></pre> but the <code>--config-base=[CONFIG_DIR] --config-name=[BASE_CONFIG_STEM]</code> arguments are always required.  However, be careful to ensure there are no conflicts when combining CLI and file-based override syntax (e.g make sure <code>section.param</code> doesn't appear both in your override file and in the CLI).  This is because hydra does not make a distinction, it will first overwrite with the CLI value and then overwrite with the params file value. this means that it will always default to the params file value when there are conflicts.</p>"},{"location":"usage/#output","title":"Output","text":"<p>The output of the train script will be at least 1 <code>*.ckpt</code> file, assuming you've configured the <code>checkpointing</code> section of the config correctly and depending on the params you've used.</p>"},{"location":"usage/#eval","title":"Eval","text":"<p>Normally, testing will be done at the end of training assuming you passed in a test dataset. However, if you'd like to test your model on another dataset, we provide a way to do so here. Please see above for how to set up your evaluation data. Basically we recommend storing your test videos/labels files as so to make things easier: dataset_name/     test/         vid_1.{VID_EXTENSION}         vid_1.{slp, csv, xml}             .             .             .         vid_n.{slp, csv, xml}         vid_n.{slp, csv, xml} Once you have your dataset set up you can move on to setting up your config!</p>"},{"location":"usage/#step-2-set-up-config","title":"Step 2. Set up config","text":"<p>Similar to training, we need to set up a config file that specifies</p> <ol> <li>a <code>ckpt_path</code></li> <li>a <code>runner.save_path</code></li> <li>a list of the <code>pymotmetrics</code> to be computed</li> <li>a <code>Tracker</code> config</li> <li>a <code>dataset</code> config with a <code>test_dataset</code> subsection containing dataset params.</li> </ol> <p>Please see here for a walk through of the inference params as well as how to set up an inference conifg and see here for an example inference config file.</p>"},{"location":"usage/#step-3-run-evaluation","title":"Step 3 Run Evaluation","text":"<p>Just like training we can use the hydra syntax for specifying arguments via the cli. Thus you can run inference via:</p> <pre><code>dreem-eval --config-base=[CONFIG_DIR] --config-name=[CONFIG_STEM]\n</code></pre> <p>e.g. If I had an inference config called <code>track.yaml</code> inside <code>/home/aaprasad/dreem_configs</code> </p> <pre><code>dreem-eval --config-base=/home/aaprasad/dreem_configs --config-name=track\n</code></pre>"},{"location":"usage/#overriding-parameters","title":"Overriding Parameters.","text":"<p>Because there aren't as many parameters during inference as during training we recommend just using the cli-based override rather than the file based but you're more than welcome to do so.</p> <p>In order to override params via the CLI, we can use the same <code>hydra</code> <code>section.param</code> syntax:</p> <p><pre><code>dreem-eval --config-base=[CONFIG_DIR] --config-name=[CONFIG_STEM] section.param=[VALUE]\n</code></pre> e.g if I want to set the window size of the tracker to 32 instead of 8 through <code>tracker.window_size=32</code> and use a different model saved in <code>/home/aaprasad/models/new_best.ckpt</code> I can do: <pre><code>dreem-eval --config-base=/home/aaprasad/dreem_configs --config-name=track ckpt_path=\"/home/aaprasad/models/new_best.ckpt\" tracker.window_size=32`\n</code></pre></p>"},{"location":"usage/#output_1","title":"Output","text":"<p>This will run inference on the videos/detections you specified in the <code>dataset.test_dataset</code> section of the config and compute the pymotmetrics for the videos you specified. These results will be saved in an hdf5 file saved to the <code>runner.save_path</code> argument. It will be structured as so: <pre><code>results_h5/\n    vid_1_file_name/\n        clip_1/\n            frame_0/\n                instance_0/\n                instance_1/\n                ...\n                instance_n/\n                traj_scores\n                asso_output\n</code></pre> The evaluation metrics, and respective metadata etc will be stored in the <code>attrs</code> of the hdf5 file. If you aren't familiar with opening, reading and manipulating hdf5 files, we recommend checking out the docs for <code>h5py</code> We also store model inputs (e.g crops, visual encoder features, spatiotemporal embeddings) in failure cases (identity switches)</p>"},{"location":"usage/#inference","title":"Inference","text":""},{"location":"usage/#step-1-setup-data","title":"Step 1. Setup Data","text":""},{"location":"usage/#step-11-get-detections","title":"Step 1.1 Get detections","text":"<p>One of the main advantages of this system is that we decouple detection and tracking so you can use off-the-shelf high performant detectors/pose-estimators/segementors. This means that in order to run inference(tracking) with our model you need 3 things.</p> <ol> <li>A pretrained model saved as a <code>.ckpt</code> file.</li> <li>A video.<ul> <li>For animal data see the <code>imageio</code> docs for supported file types.</li> <li>For microscopy data we currently support <code>.tif</code> files. Video formats supported by <code>imageio</code> coming soon.</li> </ul> </li> <li>A labels file. This labels file is slightly different than in training because we only need detections for tracking since the tracks will of course come from our model predictions.</li> </ol> <p>We still recommend using SLEAP and TrackMate to generate these labels however this time you would only need to proofread your detections if you'd like to. For TrackMate we recommend using the \"spots table\" labels <code>.csv</code> instead of the \"tracks table\" in the case where trackmate misses tracks.</p>"},{"location":"usage/#step-12-organize-data","title":"Step 1.2 Organize Data","text":"<p>We recommend setting up your inference data with the same practices we used for training.</p> <ol> <li>Match video and labels file stems. Because our dataloaders just take in a list of video files and a list of corresponding label files, the order in which the corresponding files are passed must match. (e.g <code>[file1.slp, file2.slp], [vid1.mp4, vid2.mp4]</code>). In order to make programmatic file searching easy its best to save your labels and vid files with the same stem so that you can ensure the ordering will be consistent. Its also just best practice so you know which video a labels file corresponds to.</li> <li>Store corresponding videos and labels files in the same directory. This will again make searching for the files much easier. </li> </ol> <p>The best practice is to have your dataset organized as follows: <pre><code>dataset_name/\n    inference/\n        vid_1.{VID_EXTENSION}\n        vid_1.{slp, csv, xml} #extension depends on labels source, doesn't need tracks\n            .\n            .\n            .\n        vid_n.{slp, csv, xml}\n        vid_n.{slp, csv, xml}\n</code></pre></p>"},{"location":"usage/#step-2-set-up-config_1","title":"Step 2. Set up config","text":"<p>Similar to training, we need to set up a config file that specifies</p> <ol> <li>a <code>ckpt_path</code></li> <li>a <code>out_dir</code></li> <li>a <code>Tracker</code> config</li> <li>a <code>dataset</code> config with a <code>test_dataset</code> subsection containing dataset params.</li> </ol> <p>Please see here for a walk through of the inference params as well as how to set up an inference conifg and see here for an example inference config file.</p>"},{"location":"usage/#step-3-run-inference","title":"Step 3 Run Inference","text":"<p>Just like training we can use the hydra syntax for specifying arguments via the cli. Thus you can run inference via:</p> <pre><code>dreem-track --config-base=[CONFIG_DIR] --config-name=[CONFIG_STEM]\n</code></pre> <p>e.g. If I had an inference config called <code>track.yaml</code> inside <code>/home/aaprasad/dreem_configs</code> </p> <pre><code>dreem-track --config-base=/home/aaprasad/dreem_configs --config-name=track\n</code></pre>"},{"location":"usage/#overriding-parameters_1","title":"Overriding Parameters.","text":"<p>Because there aren't as many parameters during inference as during training we recommend just using the cli-based override rather than the file based but you're more than welcome to do so.</p> <p>In order to override params via the CLI, we can use the same <code>hydra</code> <code>section.param</code> syntax:</p> <p><pre><code>dreem-track --config-base=[CONFIG_DIR] --config-name=[CONFIG_STEM] section.param=[VALUE]\n</code></pre> e.g if I want to set the window size of the tracker to 32 instead of 8 through <code>tracker.window_size=32</code> and use a different model saved in <code>/home/aaprasad/models/new_best.ckpt</code> I can do: <pre><code>dreem-track --config-base=/home/aaprasad/dreem_configs --config-name=track ckpt_path=\"/home/aaprasad/models/new_best.ckpt\" tracker.window_size=32`\n</code></pre></p>"},{"location":"usage/#output_2","title":"Output","text":"<p>This will run inference on the videos/detections you specified in the <code>dataset.test_dataset</code> section of the config and save the tracks to individual <code>[VID_NAME].dreem_inference.slp</code> files. If an <code>outdir</code> is specified in the config it will save to  <code>./[OUTDIR]/[VID_NAME].dreem_inference.slp</code>, otherwise it will just save to <code>./results/[VID_NAME].dreem_inference.slp</code>. Now you can load the file with <code>sleap-io</code> and do what you please!</p>"},{"location":"configs/","title":"DREEM Config API","text":"<p>We utilize <code>.yaml</code> based configs with <code>hydra</code> and <code>omegaconf</code> for config parsing.</p>"},{"location":"configs/config/","title":"<code>Config</code> Parser","text":""},{"location":"configs/config/#dreem.io.Config","title":"<code>dreem.io.Config</code>","text":"<p>Class handling loading components based on config params.</p> Source code in <code>dreem/io/config.py</code> <pre><code>class Config:\n    \"\"\"Class handling loading components based on config params.\"\"\"\n\n    def __init__(self, cfg: DictConfig, params_cfg: DictConfig | None = None):\n        \"\"\"Initialize the class with config from hydra/omega conf.\n\n        First uses `base_param` file then overwrites with specific `params_config`.\n\n        Args:\n            cfg: The `DictConfig` containing all the hyperparameters needed for\n                training/evaluation.\n            params_cfg: The `DictConfig` containing subset of hyperparameters to override.\n                training/evaluation\n        \"\"\"\n        base_cfg = cfg\n        logger.info(f\"Base Config: {cfg}\")\n\n        if \"params_config\" in cfg:\n            params_cfg = OmegaConf.load(cfg.params_config)\n\n        if params_cfg:\n            logger.info(f\"Overwriting base config with {params_cfg}\")\n            with open_dict(base_cfg):\n                self.cfg = OmegaConf.merge(base_cfg, params_cfg)  # merge configs\n        else:\n            self.cfg = cfg\n\n    def __repr__(self):\n        \"\"\"Object representation of config class.\"\"\"\n        return f\"Config({self.cfg})\"\n\n    def __str__(self):\n        \"\"\"Return a string representation of config class.\"\"\"\n        return f\"Config({self.cfg})\"\n\n    @classmethod\n    def from_yaml(cls, base_cfg_path: str, params_cfg_path: str | None = None) -&gt; None:\n        \"\"\"Load config directly from yaml.\n\n        Args:\n            base_cfg_path: path to base config file.\n            params_cfg_path: path to override params.\n        \"\"\"\n        base_cfg = OmegaConf.load(base_cfg_path)\n        params_cfg = OmegaConf.load(params_cfg_path) if params_cfg else None\n        return cls(base_cfg, params_cfg)\n\n    def set_hparams(self, hparams: dict) -&gt; bool:\n        \"\"\"Setter function for overwriting specific hparams.\n\n        Useful for changing 1 or 2 hyperparameters such as dataset.\n\n        Args:\n            hparams: A dict containing the hyperparameter to be overwritten and\n                the value to be changed\n\n        Returns:\n            `True` if config is successfully updated, `False` otherwise\n        \"\"\"\n        if hparams == {} or hparams is None:\n            logger.warning(\"Nothing to update!\")\n            return False\n        for hparam, val in hparams.items():\n            try:\n                OmegaConf.update(self.cfg, hparam, val)\n            except Exception as e:\n                logger.exception(f\"Failed to update {hparam} to {val} due to {e}\")\n                return False\n        return True\n\n    def get_model(self) -&gt; \"GlobalTrackingTransformer\":\n        \"\"\"Getter for gtr model.\n\n        Returns:\n            A global tracking transformer with parameters indicated by cfg\n        \"\"\"\n        from dreem.models import GlobalTrackingTransformer\n\n        model_params = self.cfg.model\n        with open_dict(model_params):\n            ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n        if ckpt_path is not None and len(ckpt_path) &gt; 0:\n            return GTRRunner.load_from_checkpoint(ckpt_path).model\n\n        return GlobalTrackingTransformer(**model_params)\n\n    def get_tracker_cfg(self) -&gt; dict:\n        \"\"\"Getter for tracker config params.\n\n        Returns:\n            A dict containing the init params for `Tracker`.\n        \"\"\"\n        tracker_params = self.cfg.tracker\n        tracker_cfg = {}\n        for key, val in tracker_params.items():\n            tracker_cfg[key] = val\n        return tracker_cfg\n\n    def get_gtr_runner(self, ckpt_path=None) -&gt; \"GTRRunner\":\n        \"\"\"Get lightning module for training, validation, and inference.\n\n        Args:\n            ckpt_path: path to checkpoint for override\n\n        Returns:\n            a gtr runner model\n        \"\"\"\n        from dreem.models import GTRRunner\n\n        tracker_params = self.cfg.tracker\n        optimizer_params = self.cfg.optimizer\n        scheduler_params = self.cfg.scheduler\n        loss_params = self.cfg.loss\n        gtr_runner_params = self.cfg.runner\n        model_params = self.cfg.model\n\n        if ckpt_path is None:\n            with open_dict(model_params):\n                ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n        if ckpt_path is not None and ckpt_path != \"\":\n            model = GTRRunner.load_from_checkpoint(\n                ckpt_path,\n                tracker_cfg=tracker_params,\n                train_metrics=self.cfg.runner.metrics.train,\n                val_metrics=self.cfg.runner.metrics.val,\n                test_metrics=self.cfg.runner.metrics.test,\n                test_save_path=self.cfg.runner.save_path,\n            )\n\n        else:\n            model = GTRRunner(\n                model_params,\n                tracker_params,\n                loss_params,\n                optimizer_params,\n                scheduler_params,\n                **gtr_runner_params,\n            )\n\n        return model\n\n    def get_data_paths(self, data_cfg: dict) -&gt; tuple[list[str], list[str]]:\n        \"\"\"Get file paths from directory.\n\n        Args:\n            data_cfg: Config for the dataset containing \"dir\" key.\n\n        Returns:\n            lists of labels file paths and video file paths respectively\n        \"\"\"\n        with open_dict(data_cfg):\n            dir_cfg = data_cfg.pop(\"dir\", None)\n        label_files = vid_files = None\n        if dir_cfg:\n            labels_suff = dir_cfg.labels_suffix\n            vid_suff = dir_cfg.vid_suffix\n            labels_path = f\"{dir_cfg.path}/*{labels_suff}\"\n            vid_path = f\"{dir_cfg.path}/*{vid_suff}\"\n            logger.debug(f\"Searching for labels matching {labels_path}\")\n            label_files = glob.glob(labels_path)\n            logger.debug(f\"Searching for videos matching {vid_path}\")\n            vid_files = glob.glob(vid_path)\n            logger.debug(f\"Found {len(label_files)} labels and {len(vid_files)} videos\")\n\n        else:\n            if \"slp_files\" in data_cfg:\n                label_files = data_cfg.slp_files\n                vid_files = data_cfg.video_files\n            elif \"tracks\" in data_cfg or \"source\" in data_cfg:\n                label_files = data_cfg.tracks\n                vid_files = data_cfg.videos\n            elif \"raw_images\" in data_cfg:\n                label_files = data_cfg.gt_images\n                vid_files = data_cfg.raw_images\n\n        return label_files, vid_files\n\n    def get_dataset(\n        self,\n        mode: str,\n        label_files: list[str] | None = None,\n        vid_files: list[str | list[str]] = None,\n    ) -&gt; \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\":\n        \"\"\"Getter for datasets.\n\n        Args:\n            mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n                train, val, or test params for dataset\n            label_files: path to label_files for override\n            vid_files: path to vid_files for override\n\n        Returns:\n            Either a `SleapDataset` or `MicroscopyDataset` with params indicated by cfg\n        \"\"\"\n        from dreem.datasets import MicroscopyDataset, SleapDataset, CellTrackingDataset\n\n        if mode.lower() == \"train\":\n            dataset_params = self.cfg.dataset.train_dataset\n        elif mode.lower() == \"val\":\n            dataset_params = self.cfg.dataset.val_dataset\n        elif mode.lower() == \"test\":\n            dataset_params = self.cfg.dataset.test_dataset\n        else:\n            raise ValueError(\n                \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n            )\n        if label_files is None or vid_files is None:\n            with open_dict(dataset_params):\n                label_files, vid_files = self.get_data_paths(dataset_params)\n        # todo: handle this better\n        if \"slp_files\" in dataset_params:\n            if label_files is not None:\n                dataset_params.slp_files = label_files\n            if vid_files is not None:\n                dataset_params.video_files = vid_files\n            return SleapDataset(**dataset_params)\n\n        elif \"tracks\" in dataset_params or \"source\" in dataset_params:\n            if label_files is not None:\n                dataset_params.tracks = label_files\n            if vid_files is not None:\n                dataset_params.videos = vid_files\n            return MicroscopyDataset(**dataset_params)\n\n        elif \"raw_images\" in dataset_params:\n            if label_files is not None:\n                dataset_params.gt_images = label_files\n            if vid_files is not None:\n                dataset_params.raw_images = vid_files\n            return CellTrackingDataset(**dataset_params)\n\n        else:\n            raise ValueError(\n                \"Could not resolve dataset type from Config! Please include \\\n                either `slp_files` or `tracks`/`source`\"\n            )\n\n    def get_dataloader(\n        self,\n        dataset: \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\",\n        mode: str,\n    ) -&gt; torch.utils.data.DataLoader:\n        \"\"\"Getter for dataloader.\n\n        Args:\n            dataset: the Sleap or Microscopy Dataset used to initialize the dataloader\n            mode: either [\"train\", \"val\", or \"test\"] indicates which dataset\n                config to use\n\n        Returns:\n            A torch dataloader for `dataset` with parameters configured as specified\n        \"\"\"\n        if mode.lower() == \"train\":\n            dataloader_params = self.cfg.dataloader.train_dataloader\n        elif mode.lower() == \"val\":\n            dataloader_params = self.cfg.dataloader.val_dataloader\n        elif mode.lower() == \"test\":\n            dataloader_params = self.cfg.dataloader.test_dataloader\n        else:\n            raise ValueError(\n                \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n            )\n        if dataloader_params.num_workers &gt; 0:\n            # prevent too many open files error\n            pin_memory = True\n            torch.multiprocessing.set_sharing_strategy(\"file_system\")\n        else:\n            pin_memory = False\n\n        return torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size=1,\n            pin_memory=pin_memory,\n            collate_fn=dataset.no_batching_fn,\n            **dataloader_params,\n        )\n\n    def get_optimizer(self, params: Iterable) -&gt; torch.optim.Optimizer:\n        \"\"\"Getter for optimizer.\n\n        Args:\n            params: iterable of model parameters to optimize or dicts defining\n                parameter groups\n\n        Returns:\n            A torch Optimizer with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_optimizer\n\n        optimizer_params = self.cfg.optimizer\n\n        return init_optimizer(params, optimizer_params)\n\n    def get_scheduler(\n        self, optimizer: torch.optim.Optimizer\n    ) -&gt; torch.optim.lr_scheduler.LRScheduler:\n        \"\"\"Getter for lr scheduler.\n\n        Args:\n            optimizer: The optimizer to wrap the scheduler around\n\n        Returns:\n            A torch learning rate scheduler with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_scheduler\n\n        lr_scheduler_params = self.cfg.scheduler\n\n        return init_scheduler(optimizer, lr_scheduler_params)\n\n    def get_loss(self) -&gt; \"dreem.training.losses.AssoLoss\":\n        \"\"\"Getter for loss functions.\n\n        Returns:\n            An AssoLoss with specified params\n        \"\"\"\n        from dreem.training.losses import AssoLoss\n\n        loss_params = self.cfg.loss\n\n        return AssoLoss(**loss_params)\n\n    def get_logger(self) -&gt; pl.loggers.Logger:\n        \"\"\"Getter for logging callback.\n\n        Returns:\n            A Logger with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_logger\n\n        logger_params = OmegaConf.to_container(self.cfg.logging, resolve=True)\n\n        return init_logger(\n            logger_params, OmegaConf.to_container(self.cfg, resolve=True)\n        )\n\n    def get_early_stopping(self) -&gt; pl.callbacks.EarlyStopping:\n        \"\"\"Getter for lightning early stopping callback.\n\n        Returns:\n            A lightning early stopping callback with specified params\n        \"\"\"\n        early_stopping_params = self.cfg.early_stopping\n        return pl.callbacks.EarlyStopping(**early_stopping_params)\n\n    def get_checkpointing(self) -&gt; pl.callbacks.ModelCheckpoint:\n        \"\"\"Getter for lightning checkpointing callback.\n\n        Returns:\n            A lightning checkpointing callback with specified params\n        \"\"\"\n        # convert to dict to enable extracting/removing params\n        checkpoint_params = self.cfg.checkpointing\n        logging_params = self.cfg.logging\n        dirpath = checkpoint_params.pop(\"dirpath\", None)\n        if dirpath is None:\n            if \"group\" in logging_params:\n                dirpath = f\"./models/{logging_params.group}/{logging_params.name}\"\n            else:\n                dirpath = f\"./models/{logging_params.name}\"\n\n        dirpath = Path(dirpath).resolve()\n        if not Path(dirpath).exists():\n            try:\n                Path(dirpath).mkdir(parents=True, exist_ok=True)\n            except OSError as e:\n                logger.exception(\n                    f\"Cannot create a new folder. Check the permissions to the given Checkpoint directory. \\n {e}\"\n                )\n        with open_dict(checkpoint_params):\n            _ = checkpoint_params.pop(\"dirpath\", None)\n            monitor = checkpoint_params.pop(\"monitor\", [\"val_loss\"])\n        checkpointers = []\n\n        for metric in monitor:\n            checkpointer = pl.callbacks.ModelCheckpoint(\n                monitor=metric,\n                dirpath=dirpath,\n                filename=f\"{{epoch}}-{{{metric}}}\",\n                **checkpoint_params,\n            )\n            checkpointer.CHECKPOINT_NAME_LAST = f\"{{epoch}}-best-{{{metric}}}\"\n            checkpointers.append(checkpointer)\n        return checkpointers\n\n    def get_trainer(\n        self,\n        callbacks: list[pl.callbacks.Callback] | None = None,\n        logger: pl.loggers.WandbLogger | None = None,\n        devices: int = 1,\n        accelerator: str = \"auto\",\n    ) -&gt; pl.Trainer:\n        \"\"\"Getter for the lightning trainer.\n\n        Args:\n            callbacks: a list of lightning callbacks preconfigured to be used\n                for training\n            logger: the Wandb logger used for logging during training\n            devices: The number of gpus to be used. 0 means cpu\n            accelerator: either \"gpu\" or \"cpu\" specifies which device to use\n\n        Returns:\n            A lightning Trainer with specified params\n        \"\"\"\n        if \"trainer\" in self.cfg:\n            trainer_params = OmegaConf.to_container(self.cfg.trainer, resolve=True)\n\n        else:\n            trainer_params = {}\n\n        profiler = trainer_params.pop(\"profiler\", None)\n        if \"accelerator\" not in trainer_params:\n            trainer_params[\"accelerator\"] = accelerator\n        if \"devices\" not in trainer_params:\n            trainer_params[\"devices\"] = devices\n\n        if \"profiler\":\n            profiler = pl.profilers.AdvancedProfiler(filename=\"profile.txt\")\n        else:\n            profiler = None\n\n        return pl.Trainer(\n            callbacks=callbacks,\n            logger=logger,\n            profiler=profiler,\n            **trainer_params,\n        )\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.__init__","title":"<code>__init__(cfg, params_cfg=None)</code>","text":"<p>Initialize the class with config from hydra/omega conf.</p> <p>First uses <code>base_param</code> file then overwrites with specific <code>params_config</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>The <code>DictConfig</code> containing all the hyperparameters needed for training/evaluation.</p> required <code>params_cfg</code> <code>DictConfig | None</code> <p>The <code>DictConfig</code> containing subset of hyperparameters to override. training/evaluation</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>def __init__(self, cfg: DictConfig, params_cfg: DictConfig | None = None):\n    \"\"\"Initialize the class with config from hydra/omega conf.\n\n    First uses `base_param` file then overwrites with specific `params_config`.\n\n    Args:\n        cfg: The `DictConfig` containing all the hyperparameters needed for\n            training/evaluation.\n        params_cfg: The `DictConfig` containing subset of hyperparameters to override.\n            training/evaluation\n    \"\"\"\n    base_cfg = cfg\n    logger.info(f\"Base Config: {cfg}\")\n\n    if \"params_config\" in cfg:\n        params_cfg = OmegaConf.load(cfg.params_config)\n\n    if params_cfg:\n        logger.info(f\"Overwriting base config with {params_cfg}\")\n        with open_dict(base_cfg):\n            self.cfg = OmegaConf.merge(base_cfg, params_cfg)  # merge configs\n    else:\n        self.cfg = cfg\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.__repr__","title":"<code>__repr__()</code>","text":"<p>Object representation of config class.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def __repr__(self):\n    \"\"\"Object representation of config class.\"\"\"\n    return f\"Config({self.cfg})\"\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of config class.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def __str__(self):\n    \"\"\"Return a string representation of config class.\"\"\"\n    return f\"Config({self.cfg})\"\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.from_yaml","title":"<code>from_yaml(base_cfg_path, params_cfg_path=None)</code>  <code>classmethod</code>","text":"<p>Load config directly from yaml.</p> <p>Parameters:</p> Name Type Description Default <code>base_cfg_path</code> <code>str</code> <p>path to base config file.</p> required <code>params_cfg_path</code> <code>str | None</code> <p>path to override params.</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>@classmethod\ndef from_yaml(cls, base_cfg_path: str, params_cfg_path: str | None = None) -&gt; None:\n    \"\"\"Load config directly from yaml.\n\n    Args:\n        base_cfg_path: path to base config file.\n        params_cfg_path: path to override params.\n    \"\"\"\n    base_cfg = OmegaConf.load(base_cfg_path)\n    params_cfg = OmegaConf.load(params_cfg_path) if params_cfg else None\n    return cls(base_cfg, params_cfg)\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_checkpointing","title":"<code>get_checkpointing()</code>","text":"<p>Getter for lightning checkpointing callback.</p> <p>Returns:</p> Type Description <code>ModelCheckpoint</code> <p>A lightning checkpointing callback with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_checkpointing(self) -&gt; pl.callbacks.ModelCheckpoint:\n    \"\"\"Getter for lightning checkpointing callback.\n\n    Returns:\n        A lightning checkpointing callback with specified params\n    \"\"\"\n    # convert to dict to enable extracting/removing params\n    checkpoint_params = self.cfg.checkpointing\n    logging_params = self.cfg.logging\n    dirpath = checkpoint_params.pop(\"dirpath\", None)\n    if dirpath is None:\n        if \"group\" in logging_params:\n            dirpath = f\"./models/{logging_params.group}/{logging_params.name}\"\n        else:\n            dirpath = f\"./models/{logging_params.name}\"\n\n    dirpath = Path(dirpath).resolve()\n    if not Path(dirpath).exists():\n        try:\n            Path(dirpath).mkdir(parents=True, exist_ok=True)\n        except OSError as e:\n            logger.exception(\n                f\"Cannot create a new folder. Check the permissions to the given Checkpoint directory. \\n {e}\"\n            )\n    with open_dict(checkpoint_params):\n        _ = checkpoint_params.pop(\"dirpath\", None)\n        monitor = checkpoint_params.pop(\"monitor\", [\"val_loss\"])\n    checkpointers = []\n\n    for metric in monitor:\n        checkpointer = pl.callbacks.ModelCheckpoint(\n            monitor=metric,\n            dirpath=dirpath,\n            filename=f\"{{epoch}}-{{{metric}}}\",\n            **checkpoint_params,\n        )\n        checkpointer.CHECKPOINT_NAME_LAST = f\"{{epoch}}-best-{{{metric}}}\"\n        checkpointers.append(checkpointer)\n    return checkpointers\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_data_paths","title":"<code>get_data_paths(data_cfg)</code>","text":"<p>Get file paths from directory.</p> <p>Parameters:</p> Name Type Description Default <code>data_cfg</code> <code>dict</code> <p>Config for the dataset containing \"dir\" key.</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str]]</code> <p>lists of labels file paths and video file paths respectively</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_data_paths(self, data_cfg: dict) -&gt; tuple[list[str], list[str]]:\n    \"\"\"Get file paths from directory.\n\n    Args:\n        data_cfg: Config for the dataset containing \"dir\" key.\n\n    Returns:\n        lists of labels file paths and video file paths respectively\n    \"\"\"\n    with open_dict(data_cfg):\n        dir_cfg = data_cfg.pop(\"dir\", None)\n    label_files = vid_files = None\n    if dir_cfg:\n        labels_suff = dir_cfg.labels_suffix\n        vid_suff = dir_cfg.vid_suffix\n        labels_path = f\"{dir_cfg.path}/*{labels_suff}\"\n        vid_path = f\"{dir_cfg.path}/*{vid_suff}\"\n        logger.debug(f\"Searching for labels matching {labels_path}\")\n        label_files = glob.glob(labels_path)\n        logger.debug(f\"Searching for videos matching {vid_path}\")\n        vid_files = glob.glob(vid_path)\n        logger.debug(f\"Found {len(label_files)} labels and {len(vid_files)} videos\")\n\n    else:\n        if \"slp_files\" in data_cfg:\n            label_files = data_cfg.slp_files\n            vid_files = data_cfg.video_files\n        elif \"tracks\" in data_cfg or \"source\" in data_cfg:\n            label_files = data_cfg.tracks\n            vid_files = data_cfg.videos\n        elif \"raw_images\" in data_cfg:\n            label_files = data_cfg.gt_images\n            vid_files = data_cfg.raw_images\n\n    return label_files, vid_files\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_dataloader","title":"<code>get_dataloader(dataset, mode)</code>","text":"<p>Getter for dataloader.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>'SleapDataset' | 'MicroscopyDataset' | 'CellTrackingDataset'</code> <p>the Sleap or Microscopy Dataset used to initialize the dataloader</p> required <code>mode</code> <code>str</code> <p>either [\"train\", \"val\", or \"test\"] indicates which dataset config to use</p> required <p>Returns:</p> Type Description <code>DataLoader</code> <p>A torch dataloader for <code>dataset</code> with parameters configured as specified</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_dataloader(\n    self,\n    dataset: \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\",\n    mode: str,\n) -&gt; torch.utils.data.DataLoader:\n    \"\"\"Getter for dataloader.\n\n    Args:\n        dataset: the Sleap or Microscopy Dataset used to initialize the dataloader\n        mode: either [\"train\", \"val\", or \"test\"] indicates which dataset\n            config to use\n\n    Returns:\n        A torch dataloader for `dataset` with parameters configured as specified\n    \"\"\"\n    if mode.lower() == \"train\":\n        dataloader_params = self.cfg.dataloader.train_dataloader\n    elif mode.lower() == \"val\":\n        dataloader_params = self.cfg.dataloader.val_dataloader\n    elif mode.lower() == \"test\":\n        dataloader_params = self.cfg.dataloader.test_dataloader\n    else:\n        raise ValueError(\n            \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n        )\n    if dataloader_params.num_workers &gt; 0:\n        # prevent too many open files error\n        pin_memory = True\n        torch.multiprocessing.set_sharing_strategy(\"file_system\")\n    else:\n        pin_memory = False\n\n    return torch.utils.data.DataLoader(\n        dataset=dataset,\n        batch_size=1,\n        pin_memory=pin_memory,\n        collate_fn=dataset.no_batching_fn,\n        **dataloader_params,\n    )\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_dataset","title":"<code>get_dataset(mode, label_files=None, vid_files=None)</code>","text":"<p>Getter for datasets.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>[None, \"train\", \"test\", \"val\"]. Indicates whether to use train, val, or test params for dataset</p> required <code>label_files</code> <code>list[str] | None</code> <p>path to label_files for override</p> <code>None</code> <code>vid_files</code> <code>list[str | list[str]]</code> <p>path to vid_files for override</p> <code>None</code> <p>Returns:</p> Type Description <code>'SleapDataset' | 'MicroscopyDataset' | 'CellTrackingDataset'</code> <p>Either a <code>SleapDataset</code> or <code>MicroscopyDataset</code> with params indicated by cfg</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_dataset(\n    self,\n    mode: str,\n    label_files: list[str] | None = None,\n    vid_files: list[str | list[str]] = None,\n) -&gt; \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\":\n    \"\"\"Getter for datasets.\n\n    Args:\n        mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n            train, val, or test params for dataset\n        label_files: path to label_files for override\n        vid_files: path to vid_files for override\n\n    Returns:\n        Either a `SleapDataset` or `MicroscopyDataset` with params indicated by cfg\n    \"\"\"\n    from dreem.datasets import MicroscopyDataset, SleapDataset, CellTrackingDataset\n\n    if mode.lower() == \"train\":\n        dataset_params = self.cfg.dataset.train_dataset\n    elif mode.lower() == \"val\":\n        dataset_params = self.cfg.dataset.val_dataset\n    elif mode.lower() == \"test\":\n        dataset_params = self.cfg.dataset.test_dataset\n    else:\n        raise ValueError(\n            \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n        )\n    if label_files is None or vid_files is None:\n        with open_dict(dataset_params):\n            label_files, vid_files = self.get_data_paths(dataset_params)\n    # todo: handle this better\n    if \"slp_files\" in dataset_params:\n        if label_files is not None:\n            dataset_params.slp_files = label_files\n        if vid_files is not None:\n            dataset_params.video_files = vid_files\n        return SleapDataset(**dataset_params)\n\n    elif \"tracks\" in dataset_params or \"source\" in dataset_params:\n        if label_files is not None:\n            dataset_params.tracks = label_files\n        if vid_files is not None:\n            dataset_params.videos = vid_files\n        return MicroscopyDataset(**dataset_params)\n\n    elif \"raw_images\" in dataset_params:\n        if label_files is not None:\n            dataset_params.gt_images = label_files\n        if vid_files is not None:\n            dataset_params.raw_images = vid_files\n        return CellTrackingDataset(**dataset_params)\n\n    else:\n        raise ValueError(\n            \"Could not resolve dataset type from Config! Please include \\\n            either `slp_files` or `tracks`/`source`\"\n        )\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_early_stopping","title":"<code>get_early_stopping()</code>","text":"<p>Getter for lightning early stopping callback.</p> <p>Returns:</p> Type Description <code>EarlyStopping</code> <p>A lightning early stopping callback with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_early_stopping(self) -&gt; pl.callbacks.EarlyStopping:\n    \"\"\"Getter for lightning early stopping callback.\n\n    Returns:\n        A lightning early stopping callback with specified params\n    \"\"\"\n    early_stopping_params = self.cfg.early_stopping\n    return pl.callbacks.EarlyStopping(**early_stopping_params)\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_gtr_runner","title":"<code>get_gtr_runner(ckpt_path=None)</code>","text":"<p>Get lightning module for training, validation, and inference.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_path</code> <p>path to checkpoint for override</p> <code>None</code> <p>Returns:</p> Type Description <code>'GTRRunner'</code> <p>a gtr runner model</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_gtr_runner(self, ckpt_path=None) -&gt; \"GTRRunner\":\n    \"\"\"Get lightning module for training, validation, and inference.\n\n    Args:\n        ckpt_path: path to checkpoint for override\n\n    Returns:\n        a gtr runner model\n    \"\"\"\n    from dreem.models import GTRRunner\n\n    tracker_params = self.cfg.tracker\n    optimizer_params = self.cfg.optimizer\n    scheduler_params = self.cfg.scheduler\n    loss_params = self.cfg.loss\n    gtr_runner_params = self.cfg.runner\n    model_params = self.cfg.model\n\n    if ckpt_path is None:\n        with open_dict(model_params):\n            ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n    if ckpt_path is not None and ckpt_path != \"\":\n        model = GTRRunner.load_from_checkpoint(\n            ckpt_path,\n            tracker_cfg=tracker_params,\n            train_metrics=self.cfg.runner.metrics.train,\n            val_metrics=self.cfg.runner.metrics.val,\n            test_metrics=self.cfg.runner.metrics.test,\n            test_save_path=self.cfg.runner.save_path,\n        )\n\n    else:\n        model = GTRRunner(\n            model_params,\n            tracker_params,\n            loss_params,\n            optimizer_params,\n            scheduler_params,\n            **gtr_runner_params,\n        )\n\n    return model\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_logger","title":"<code>get_logger()</code>","text":"<p>Getter for logging callback.</p> <p>Returns:</p> Type Description <code>Logger</code> <p>A Logger with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_logger(self) -&gt; pl.loggers.Logger:\n    \"\"\"Getter for logging callback.\n\n    Returns:\n        A Logger with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_logger\n\n    logger_params = OmegaConf.to_container(self.cfg.logging, resolve=True)\n\n    return init_logger(\n        logger_params, OmegaConf.to_container(self.cfg, resolve=True)\n    )\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_loss","title":"<code>get_loss()</code>","text":"<p>Getter for loss functions.</p> <p>Returns:</p> Type Description <code>'dreem.training.losses.AssoLoss'</code> <p>An AssoLoss with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_loss(self) -&gt; \"dreem.training.losses.AssoLoss\":\n    \"\"\"Getter for loss functions.\n\n    Returns:\n        An AssoLoss with specified params\n    \"\"\"\n    from dreem.training.losses import AssoLoss\n\n    loss_params = self.cfg.loss\n\n    return AssoLoss(**loss_params)\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_model","title":"<code>get_model()</code>","text":"<p>Getter for gtr model.</p> <p>Returns:</p> Type Description <code>'GlobalTrackingTransformer'</code> <p>A global tracking transformer with parameters indicated by cfg</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_model(self) -&gt; \"GlobalTrackingTransformer\":\n    \"\"\"Getter for gtr model.\n\n    Returns:\n        A global tracking transformer with parameters indicated by cfg\n    \"\"\"\n    from dreem.models import GlobalTrackingTransformer\n\n    model_params = self.cfg.model\n    with open_dict(model_params):\n        ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n    if ckpt_path is not None and len(ckpt_path) &gt; 0:\n        return GTRRunner.load_from_checkpoint(ckpt_path).model\n\n    return GlobalTrackingTransformer(**model_params)\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_optimizer","title":"<code>get_optimizer(params)</code>","text":"<p>Getter for optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable</code> <p>iterable of model parameters to optimize or dicts defining parameter groups</p> required <p>Returns:</p> Type Description <code>Optimizer</code> <p>A torch Optimizer with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_optimizer(self, params: Iterable) -&gt; torch.optim.Optimizer:\n    \"\"\"Getter for optimizer.\n\n    Args:\n        params: iterable of model parameters to optimize or dicts defining\n            parameter groups\n\n    Returns:\n        A torch Optimizer with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_optimizer\n\n    optimizer_params = self.cfg.optimizer\n\n    return init_optimizer(params, optimizer_params)\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_scheduler","title":"<code>get_scheduler(optimizer)</code>","text":"<p>Getter for lr scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to wrap the scheduler around</p> required <p>Returns:</p> Type Description <code>LRScheduler</code> <p>A torch learning rate scheduler with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_scheduler(\n    self, optimizer: torch.optim.Optimizer\n) -&gt; torch.optim.lr_scheduler.LRScheduler:\n    \"\"\"Getter for lr scheduler.\n\n    Args:\n        optimizer: The optimizer to wrap the scheduler around\n\n    Returns:\n        A torch learning rate scheduler with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_scheduler\n\n    lr_scheduler_params = self.cfg.scheduler\n\n    return init_scheduler(optimizer, lr_scheduler_params)\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_tracker_cfg","title":"<code>get_tracker_cfg()</code>","text":"<p>Getter for tracker config params.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict containing the init params for <code>Tracker</code>.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_tracker_cfg(self) -&gt; dict:\n    \"\"\"Getter for tracker config params.\n\n    Returns:\n        A dict containing the init params for `Tracker`.\n    \"\"\"\n    tracker_params = self.cfg.tracker\n    tracker_cfg = {}\n    for key, val in tracker_params.items():\n        tracker_cfg[key] = val\n    return tracker_cfg\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_trainer","title":"<code>get_trainer(callbacks=None, logger=None, devices=1, accelerator='auto')</code>","text":"<p>Getter for the lightning trainer.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[Callback] | None</code> <p>a list of lightning callbacks preconfigured to be used for training</p> <code>None</code> <code>logger</code> <code>WandbLogger | None</code> <p>the Wandb logger used for logging during training</p> <code>None</code> <code>devices</code> <code>int</code> <p>The number of gpus to be used. 0 means cpu</p> <code>1</code> <code>accelerator</code> <code>str</code> <p>either \"gpu\" or \"cpu\" specifies which device to use</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>Trainer</code> <p>A lightning Trainer with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_trainer(\n    self,\n    callbacks: list[pl.callbacks.Callback] | None = None,\n    logger: pl.loggers.WandbLogger | None = None,\n    devices: int = 1,\n    accelerator: str = \"auto\",\n) -&gt; pl.Trainer:\n    \"\"\"Getter for the lightning trainer.\n\n    Args:\n        callbacks: a list of lightning callbacks preconfigured to be used\n            for training\n        logger: the Wandb logger used for logging during training\n        devices: The number of gpus to be used. 0 means cpu\n        accelerator: either \"gpu\" or \"cpu\" specifies which device to use\n\n    Returns:\n        A lightning Trainer with specified params\n    \"\"\"\n    if \"trainer\" in self.cfg:\n        trainer_params = OmegaConf.to_container(self.cfg.trainer, resolve=True)\n\n    else:\n        trainer_params = {}\n\n    profiler = trainer_params.pop(\"profiler\", None)\n    if \"accelerator\" not in trainer_params:\n        trainer_params[\"accelerator\"] = accelerator\n    if \"devices\" not in trainer_params:\n        trainer_params[\"devices\"] = devices\n\n    if \"profiler\":\n        profiler = pl.profilers.AdvancedProfiler(filename=\"profile.txt\")\n    else:\n        profiler = None\n\n    return pl.Trainer(\n        callbacks=callbacks,\n        logger=logger,\n        profiler=profiler,\n        **trainer_params,\n    )\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.set_hparams","title":"<code>set_hparams(hparams)</code>","text":"<p>Setter function for overwriting specific hparams.</p> <p>Useful for changing 1 or 2 hyperparameters such as dataset.</p> <p>Parameters:</p> Name Type Description Default <code>hparams</code> <code>dict</code> <p>A dict containing the hyperparameter to be overwritten and the value to be changed</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if config is successfully updated, <code>False</code> otherwise</p> Source code in <code>dreem/io/config.py</code> <pre><code>def set_hparams(self, hparams: dict) -&gt; bool:\n    \"\"\"Setter function for overwriting specific hparams.\n\n    Useful for changing 1 or 2 hyperparameters such as dataset.\n\n    Args:\n        hparams: A dict containing the hyperparameter to be overwritten and\n            the value to be changed\n\n    Returns:\n        `True` if config is successfully updated, `False` otherwise\n    \"\"\"\n    if hparams == {} or hparams is None:\n        logger.warning(\"Nothing to update!\")\n        return False\n    for hparam, val in hparams.items():\n        try:\n            OmegaConf.update(self.cfg, hparam, val)\n        except Exception as e:\n            logger.exception(f\"Failed to update {hparam} to {val} due to {e}\")\n            return False\n    return True\n</code></pre>"},{"location":"configs/eval/","title":"Description of inference params","text":"<p>Here we describe the parameters used for inference. See here for an example inference config.</p> <ul> <li><code>ckpt_path</code>: (<code>str</code>) the path to the saved model checkpoint. Can optionally provide a list of models and this will trigger batch inference where each pod gets a model to run inference with. e.g: <pre><code>...\nckpt_path: \"/path/to/model.ckpt\"\n...\n</code></pre></li> </ul>"},{"location":"configs/eval/#tracker","title":"<code>tracker</code>","text":"<p>This section configures the tracker.</p> <ul> <li><code>window_size</code>: (<code>int</code>) the size of the window used during sliding inference.</li> <li><code>use_vis_feats</code>: (<code>bool</code>) Whether or not to use visual feature extractor.</li> <li><code>overlap_thresh</code>: (<code>float</code>) the trajectory overlap threshold to be used for assignment.</li> <li><code>mult_thresh</code>: (<code>bool</code>) Whether or not to use weight threshold.</li> <li><code>decay_time</code>: (<code>float</code>) weight for <code>decay_time</code> postprocessing.</li> <li><code>iou</code>: (<code>str</code> | <code>None</code>) Either <code>{None, '', \"mult\" or \"max\"}</code>. Whether to use multiplicative or max iou reweighting.</li> <li><code>max_center_dist</code>: (<code>float</code>) distance threshold for filtering trajectory score matrix.</li> <li><code>persistent_tracking</code>: (<code>bool</code>) whether to keep a buffer across chunks or not.</li> <li><code>max_gap</code>: (<code>int</code>) the max number of frames a trajectory can be missing before termination.</li> <li><code>max_tracks</code>: (<code>int</code>) the maximum number of tracks that can be created while tracking.     We force the tracker to assign instances to a track instead of creating a new track if <code>max_tracks</code>has been reached.</li> </ul>"},{"location":"configs/eval/#examples","title":"Examples:","text":"<pre><code>...\ntracker:\n    window_size: 8\n    overlap_thresh: 0.01\n    mult_thresh: false\n    decay_time: 0.9\n    iou: \"mult\"\n    max_center_dist: 0.1\n...\n</code></pre>"},{"location":"configs/eval/#dataset","title":"<code>dataset</code>","text":"<p>This section contains the params for initializing the datasets for training. Requires a <code>test_dataset</code> keys. </p>"},{"location":"configs/eval/#basedataset-args","title":"<code>BaseDataset</code> args","text":"<ul> <li><code>padding</code>: An <code>int</code> representing the amount of padding to be added to each side of the bounding box size</li> <li><code>crop_size</code>: (<code>int</code>|<code>tuple</code>) the size of the bounding box around which a crop will form.</li> <li><code>chunk</code>: Whether or not to chunk videos into smaller clips to feed to model</li> <li><code>clip_length</code>: the number of frames in each chunk</li> <li><code>mode</code>: <code>train</code> or <code>val</code>. Determines whether this dataset is used for training or validation.</li> <li><code>n_chunks</code>: Number of chunks to subsample from. Can either a fraction of the dataset (ie <code>(0,1.0]</code>) or number of chunks</li> <li><code>seed</code>: set a seed for reproducibility</li> <li><code>gt_list</code>: An optional path to .txt file containing ground truth for cell tracking challenge datasets.</li> </ul>"},{"location":"configs/eval/#dir","title":"<code>dir</code>:","text":"<p>This section allows you to pass a directory rather than paths to labels/videos individually</p> <ul> <li><code>path</code>: The path to the dir where the data is stored (recommend absolute path)</li> <li><code>labels_suffix</code>: (<code>str</code>) containing the file extension to search for labels files. e.g. <code>.slp</code>, <code>.csv</code>, or <code>.xml</code>.</li> <li><code>vid_suffix</code>: (<code>str</code>) containing the file extension to search for video files e.g <code>.mp4</code>, <code>.avi</code> or <code>.tif</code>.</li> </ul>"},{"location":"configs/eval/#examples_1","title":"Examples:","text":"<pre><code>...\ndataset:\n    ...\n    {MODE}_dataset:\n        dir:\n            path: \"/path/to/data/dir/mode\"\n            labels_suffix: \".slp\"\n            vid_suffix: \".mp4\"\n        ...\n    ...\n...\n</code></pre>"},{"location":"configs/eval/#augmentations","title":"<code>augmentations</code>:","text":"<p>This subsection contains params for albumentations. See <code>albumentations</code> for available visual augmentations. Other available augmentations include <code>NodeDropout</code> and <code>InstanceDropout</code>. Keys must match augmentation class name exactly and contain subsections with parameters for the augmentation</p>"},{"location":"configs/eval/#example","title":"Example","text":"<pre><code>augmentations: \n    Rotate:\n        limit: 45\n        p: 0.3\n    ...\n    MotionBlur:\n        blur_limit: [3,7]\n        p: 0.3\n</code></pre>"},{"location":"configs/eval/#sleapdataset-args","title":"<code>SleapDataset</code> Args:","text":"<ul> <li><code>slp_files</code>: (<code>str</code>) a list of .slp files storing tracking annotations</li> <li><code>video_files</code>: (<code>str</code>) a list of paths to video files</li> <li><code>anchors</code>: (<code>str</code> | <code>list</code> | <code>int</code>) One of:<ul> <li>a string indicating a single node to center crops around</li> <li>a list of skeleton node names to be used as the center of crops</li> <li>an int indicating the number of anchors to randomly select If unavailable then crop around the midpoint between all visible anchors.</li> </ul> </li> <li><code>handle_missing</code>: how to handle missing single nodes. one of [<code>\"drop\"</code>, <code>\"ignore\"</code>, <code>\"centroid\"</code>].<ul> <li>if <code>drop</code> then we dont include instances which are missing the <code>anchor</code>.</li> <li>if <code>ignore</code> then we use a mask instead of a crop and nan centroids/bboxes.</li> <li>if <code>centroid</code> then we default to the pose centroid as the node to crop around.</li> </ul> </li> </ul>"},{"location":"configs/eval/#microscopydataset","title":"<code>MicroscopyDataset</code>","text":"<ul> <li><code>videos</code>: (<code>list[str | list[str]]</code>) paths to raw microscopy videos</li> <li><code>tracks</code>: (<code>list[str]</code>) paths to trackmate gt labels (either <code>.xml</code> or <code>.csv</code>)</li> <li><code>source</code>: file format of gt labels based on label generator. Either <code>\"trackmate\"</code> or <code>\"isbi\"</code>.</li> </ul>"},{"location":"configs/eval/#celltrackingdataset","title":"<code>CellTrackingDataset</code>","text":"<ul> <li><code>raw_images</code>: (<code>list[list[str] | list[list[str]]]</code>) paths to raw microscopy images</li> <li><code>gt_images</code>: (<code>list[list[str] | list[list[str]]]</code>) paths to gt label images</li> <li><code>gt_list</code>: (<code>list[str]</code>) An optional path to .txt file containing gt ids stored in cell                 tracking challenge format: <code>\"track_id\", \"start_frame\",                 \"end_frame\", \"parent_id\"</code></li> </ul>"},{"location":"configs/eval/#dataset-examples","title":"<code>dataset</code> Examples","text":""},{"location":"configs/eval/#sleapdataset","title":"<code>SleapDataset</code>","text":"<pre><code>...\ndataset:\n    test_dataset:\n        slp_files: [\"/path/to/test/labels1.slp\", \"/path/to/test/labels2.slp\", ..., \"/path/to/test/labelsN.slp\"]\n        video_files: [\"/path/to/test/video1.mp4\", \"/path/to/test/video2.mp4\", ..., \"/path/to/test/videoN.mp4\"]\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        anchors: [\"node1\", \"node2\", ...\"node_n\"]\n        handle_missing: \"drop\"\n        ... # we don't include augmentations bc usually you shouldnt use augmentations during val/test\n...\n</code></pre>"},{"location":"configs/eval/#microscopydataset_1","title":"<code>MicroscopyDataset</code>","text":"<pre><code>dataset:\n    test_dataset:\n        tracks: [\"/path/to/test/labels1.csv\", \"/path/to/test/labels2.csv\", ..., \"/path/to/test/labelsN.csv\"]\n        videos: [\"/path/to/test/video1.tiff\", \"/path/to/test/video2.tiff\", ..., \"/path/to/test/videoN.tiff\"]\n        source: \"trackmate\"\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        ... # we don't include augmentations bc usually you shouldnt use augmentations during val/test\n</code></pre>"},{"location":"configs/eval/#dataloader","title":"dataloader","text":"<p>This section outlines the params needed for the dataloader. Should have a <code>test_dataloader</code> </p> <p>Below we list the args we found useful/necessary for the dataloaders. For more advanced users see <code>torch.utils.data.Dataloader</code> for more ways to initialize the dataloaders</p> <ul> <li><code>shuffle</code>: (<code>bool</code>) Set to <code>True</code> to have the data reshuffled at every epoch (during training, this should always be <code>True</code> and during val/test usually <code>False</code>) </li> <li><code>num_workers</code>: (<code>int</code>) How many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.</li> </ul>"},{"location":"configs/eval/#example_1","title":"Example","text":"<pre><code>...\ndataloader:\n    test_dataloader: # we leave out the `shuffle` field as default=`False` which is what we want\n        num_workers: 4\n...\n</code></pre>"},{"location":"configs/eval/#runner","title":"<code>runner</code>","text":"<p>This section outlines arguments to be overridden for the GTR Runner * <code>save_path</code>: Path to <code>*.hdf5</code> file where eval results will be saved * <code>metrics</code>: Contains a subkey called <code>test</code> with a list of pymotmetrics to be computed or <code>\"all\"</code> to compute all metrics</p>"},{"location":"configs/eval/#example-computing-all-metrics","title":"Example (Computing all metrics):","text":"<pre><code>runner:\n    save_path: \"./test_eval.hdf5\"\n    metrics:\n        test: \"all\"\n</code></pre>"},{"location":"configs/eval/#example-only-computing-num_switches","title":"Example (Only computing <code>num_switches</code>)","text":"<pre><code>runner:\n    save_path: \"./test_eval.hdf5\"\n    metrics:\n        test: [\"num_switches\"]\n</code></pre>"},{"location":"configs/eval/#example-config","title":"Example Config","text":"<pre><code>ckpt_path: \"../training/models/example/example_train/epoch=0-best-val_sw_cnt=31.06133270263672.ckpt\"\ntracker:\n  overlap_thresh: 0.01\n  decay_time: 0.9\n  iou: \"mult\"\n  max_center_dist: 1.0\n  persistent_tracking: True\n\ndataset:\n  test_dataset:\n    slp_files: [\"../training/190612_110405_wt_18159111_rig2.2@11730.slp\", \"../training/190612_110405_wt_18159111_rig2.2@11730.slp\"]\n    video_files: [\"../training/190612_110405_wt_18159111_rig2.2@11730.mp4\", \"../training/190612_110405_wt_18159111_rig2.2@11730.mp4\"]\n    chunk: True\n    clip_length: 32\n    anchor: \"centroid\"\n\ndataloader:\n  test_dataloader:\n    shuffle: False\n    num_workers: 0\n</code></pre>"},{"location":"configs/inference/","title":"Description of inference params","text":"<p>Here we describe the parameters used for inference. See here for an example inference config.</p> <ul> <li><code>ckpt_path</code>: (<code>str</code>) the path to the saved model checkpoint. Can optionally provide a list of models and this will trigger batch inference where each pod gets a model to run inference with. e.g: <pre><code>...\nckpt_path: \"/path/to/model.ckpt\"\n...\n</code></pre></li> <li><code>out_dir</code>: (<code>str</code>) a directory path where to store outputs. e.g: <pre><code>...\nout_dir: \"/path/to/results/dir\"\n...\n</code></pre></li> </ul>"},{"location":"configs/inference/#tracker","title":"<code>tracker</code>","text":"<p>This section configures the tracker.</p> <ul> <li><code>window_size</code>: (<code>int</code>) the size of the window used during sliding inference.</li> <li><code>use_vis_feats</code>: (<code>bool</code>) Whether or not to use visual feature extractor.</li> <li><code>overlap_thresh</code>: (<code>float</code>) the trajectory overlap threshold to be used for assignment.</li> <li><code>mult_thresh</code>: (<code>bool</code>) Whether or not to use weight threshold.</li> <li><code>decay_time</code>: (<code>float</code>) weight for <code>decay_time</code> postprocessing.</li> <li><code>iou</code>: (<code>str</code> | <code>None</code>) Either <code>{None, '', \"mult\" or \"max\"}</code>. Whether to use multiplicative or max iou reweighting.</li> <li><code>max_center_dist</code>: (<code>float</code>) distance threshold for filtering trajectory score matrix.</li> <li><code>persistent_tracking</code>: (<code>bool</code>) whether to keep a buffer across chunks or not.</li> <li><code>max_gap</code>: (<code>int</code>) the max number of frames a trajectory can be missing before termination.</li> <li><code>max_tracks</code>: (<code>int</code>) the maximum number of tracks that can be created while tracking.     We force the tracker to assign instances to a track instead of creating a new track if <code>max_tracks</code>has been reached.</li> </ul>"},{"location":"configs/inference/#examples","title":"Examples:","text":"<pre><code>...\ntracker:\n    window_size: 8\n    overlap_thresh: 0.01\n    mult_thresh: false\n    decay_time: 0.9\n    iou: \"mult\"\n    max_center_dist: 0.1\n...\n</code></pre>"},{"location":"configs/inference/#dataset","title":"<code>dataset</code>","text":"<p>This section contains the params for initializing the datasets for training. Requires a <code>test_dataset</code> keys. </p>"},{"location":"configs/inference/#basedataset-args","title":"<code>BaseDataset</code> args","text":"<ul> <li><code>padding</code>: An <code>int</code> representing the amount of padding to be added to each side of the bounding box size</li> <li><code>crop_size</code>: (<code>int</code>|<code>tuple</code>) the size of the bounding box around which a crop will form.</li> <li><code>chunk</code>: Whether or not to chunk videos into smaller clips to feed to model</li> <li><code>clip_length</code>: the number of frames in each chunk</li> <li><code>mode</code>: <code>train</code> or <code>val</code>. Determines whether this dataset is used for training or validation.</li> <li><code>n_chunks</code>: Number of chunks to subsample from. Can either a fraction of the dataset (ie <code>(0,1.0]</code>) or number of chunks</li> <li><code>seed</code>: set a seed for reproducibility</li> <li><code>gt_list</code>: An optional path to .txt file containing ground truth for cell tracking challenge datasets.</li> </ul>"},{"location":"configs/inference/#dir","title":"<code>dir</code>:","text":"<p>This section allows you to pass a directory rather than paths to labels/videos individually</p> <ul> <li><code>path</code>: The path to the dir where the data is stored (recommend absolute path)</li> <li><code>labels_suffix</code>: (<code>str</code>) containing the file extension to search for labels files. e.g. <code>.slp</code>, <code>.csv</code>, or <code>.xml</code>.</li> <li><code>vid_suffix</code>: (<code>str</code>) containing the file extension to search for video files e.g <code>.mp4</code>, <code>.avi</code> or <code>.tif</code>.</li> </ul>"},{"location":"configs/inference/#examples_1","title":"Examples:","text":"<pre><code>...\ndataset:\n    ...\n    {MODE}_dataset:\n        dir:\n            path: \"/path/to/data/dir/mode\"\n            labels_suffix: \".slp\"\n            vid_suffix: \".mp4\"\n        ...\n    ...\n...\n</code></pre>"},{"location":"configs/inference/#augmentations","title":"<code>augmentations</code>:","text":"<p>This subsection contains params for albumentations. See <code>albumentations</code> for available visual augmentations. Other available augmentations include <code>NodeDropout</code> and <code>InstanceDropout</code>. Keys must match augmentation class name exactly and contain subsections with parameters for the augmentation</p>"},{"location":"configs/inference/#example","title":"Example","text":"<pre><code>augmentations: \n    Rotate:\n        limit: 45\n        p: 0.3\n    ...\n    MotionBlur:\n        blur_limit: [3,7]\n        p: 0.3\n</code></pre>"},{"location":"configs/inference/#sleapdataset-args","title":"<code>SleapDataset</code> Args:","text":"<ul> <li><code>slp_files</code>: (<code>str</code>) a list of .slp files storing tracking annotations</li> <li><code>video_files</code>: (<code>str</code>) a list of paths to video files</li> <li><code>anchors</code>: (<code>str</code> | <code>list</code> | <code>int</code>) One of:<ul> <li>a string indicating a single node to center crops around</li> <li>a list of skeleton node names to be used as the center of crops</li> <li>an int indicating the number of anchors to randomly select If unavailable then crop around the midpoint between all visible anchors.</li> </ul> </li> <li><code>handle_missing</code>: how to handle missing single nodes. one of [<code>\"drop\"</code>, <code>\"ignore\"</code>, <code>\"centroid\"</code>].<ul> <li>if <code>drop</code> then we dont include instances which are missing the <code>anchor</code>.</li> <li>if <code>ignore</code> then we use a mask instead of a crop and nan centroids/bboxes.</li> <li>if <code>centroid</code> then we default to the pose centroid as the node to crop around.</li> </ul> </li> </ul>"},{"location":"configs/inference/#microscopydataset","title":"<code>MicroscopyDataset</code>","text":"<ul> <li><code>videos</code>: (<code>list[str | list[str]]</code>) paths to raw microscopy videos</li> <li><code>tracks</code>: (<code>list[str]</code>) paths to trackmate gt labels (either <code>.xml</code> or <code>.csv</code>)</li> <li><code>source</code>: file format of gt labels based on label generator. Either <code>\"trackmate\"</code> or <code>\"isbi\"</code>.</li> </ul>"},{"location":"configs/inference/#celltrackingdataset","title":"<code>CellTrackingDataset</code>","text":"<ul> <li><code>raw_images</code>: (<code>list[list[str] | list[list[str]]]</code>) paths to raw microscopy images</li> <li><code>gt_images</code>: (<code>list[list[str] | list[list[str]]]</code>) paths to gt label images</li> <li><code>gt_list</code>: (<code>list[str]</code>) An optional path to .txt file containing gt ids stored in cell                 tracking challenge format: <code>\"track_id\", \"start_frame\",                 \"end_frame\", \"parent_id\"</code></li> </ul>"},{"location":"configs/inference/#dataset-examples","title":"<code>dataset</code> Examples","text":""},{"location":"configs/inference/#sleapdataset","title":"<code>SleapDataset</code>","text":"<pre><code>...\ndataset:\n    test_dataset:\n        slp_files: [\"/path/to/test/labels1.slp\", \"/path/to/test/labels2.slp\", ..., \"/path/to/test/labelsN.slp\"]\n        video_files: [\"/path/to/test/video1.mp4\", \"/path/to/test/video2.mp4\", ..., \"/path/to/test/videoN.mp4\"]\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        anchors: [\"node1\", \"node2\", ...\"node_n\"]\n        handle_missing: \"drop\"\n        ... # we don't include augmentations bc usually you shouldnt use augmentations during val/test\n...\n</code></pre>"},{"location":"configs/inference/#microscopydataset_1","title":"<code>MicroscopyDataset</code>","text":"<pre><code>dataset:\n    test_dataset:\n        tracks: [\"/path/to/test/labels1.csv\", \"/path/to/test/labels2.csv\", ..., \"/path/to/test/labelsN.csv\"]\n        videos: [\"/path/to/test/video1.tiff\", \"/path/to/test/video2.tiff\", ..., \"/path/to/test/videoN.tiff\"]\n        source: \"trackmate\"\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        ... # we don't include augmentations bc usually you shouldnt use augmentations during val/test\n</code></pre>"},{"location":"configs/inference/#dataloader","title":"dataloader","text":"<p>This section outlines the params needed for the dataloader. Should have a <code>train_dataloader</code> and optionally <code>val_dataloader</code>/<code>test_dataloader</code> keys. </p> <p>Below we list the args we found useful/necessary for the dataloaders. For more advanced users see <code>torch.utils.data.Dataloader</code> for more ways to initialize the dataloaders</p> <ul> <li><code>shuffle</code>: (<code>bool</code>) Set to <code>True</code> to have the data reshuffled at every epoch (during training, this should always be <code>True</code> and during val/test usually <code>False</code>) </li> <li><code>num_workers</code>: (<code>int</code>) How many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.</li> </ul>"},{"location":"configs/inference/#example_1","title":"Example","text":"<pre><code>...\ndataloader:\n    test_dataloader: # we leave out the `shuffle` field as default=`False` which is what we want\n        num_workers: 4\n...\n</code></pre>"},{"location":"configs/inference/#example-config","title":"Example Config","text":"<pre><code>ckpt_path: \"../training/models/example/example_train/epoch=0-best-val_sw_cnt=31.06133270263672.ckpt\"\ntracker:\n  overlap_thresh: 0.01\n  decay_time: 0.9\n  iou: \"mult\"\n  max_center_dist: 1.0\n  persistent_tracking: True\n\ndataset:\n  test_dataset:\n    slp_files: [\"../training/190612_110405_wt_18159111_rig2.2@11730.slp\", \"../training/190612_110405_wt_18159111_rig2.2@11730.slp\"]\n    video_files: [\"../training/190612_110405_wt_18159111_rig2.2@11730.mp4\", \"../training/190612_110405_wt_18159111_rig2.2@11730.mp4\"]\n    chunk: True\n    clip_length: 32\n    anchor: \"centroid\"\n\ndataloader:\n  test_dataloader:\n    shuffle: False\n    num_workers: 0\n</code></pre>"},{"location":"configs/training/","title":"Description of training parameters","text":"<p>Here, we describe the hyperparameters used for setting up training. Please see here for an example training config.</p> <p>Note: for using defaults, simply leave the field blank or don't include the key. Using <code>null</code> will initialize the value to <code>None</code> e.g <pre><code>model:\n  d_model: #defaults to 1024 \n  nhead: 8\n  ...\n</code></pre> vs. <pre><code>model:\n  d_model: null # sets model.d_model=None \n  nhead: 8\n  ...\n</code></pre></p>"},{"location":"configs/training/#model","title":"<code>model</code>","text":"<p>This section contains all the parameters for initializing a <code>GTRRunner</code> object</p> <ul> <li><code>ckpt_path</code>: (<code>str</code>) the path to model <code>.ckpt</code> file. Used for resuming training.</li> <li><code>d_model</code>: (<code>int</code>) the size of the embedding dimensions used for input into the transformer</li> <li><code>nhead</code>: (<code>int</code>) the number of attention heads used in the transformer's encoder/decoder layers.</li> <li><code>num_encoder_layers</code>: (<code>int</code>) the number of layers in the transformer encoder block</li> <li><code>num_decoder_layers</code>: (<code>int</code>) the number of layers in the transformer decoder block</li> <li><code>dropout</code>: a <code>float</code> the dropout probability used in each transformer layer</li> <li><code>activation</code>: One of {<code>\"relu\"</code>, <code>\"gelu\"</code> <code>\"glu\"</code>}. Which activation function to use in the transformer.</li> <li><code>return_intermediate_dec</code>: (<code>bool</code>) whether or not to return the output from the intermediate decoder layers.</li> <li><code>norm</code>: (<code>bool</code>) whether or not to normalize output of encoder and decoder.</li> <li><code>num_layers_attn_head</code>: An <code>int</code> The number of layers in the <code>AttentionHead</code> block.</li> <li><code>dropout_attn_head</code>: (<code>float</code>)  the dropout probability for the <code>AttentionHead</code> block.</li> <li><code>return_embedding</code>: (<code>bool</code>) whether to return the spatiotemporal embeddings</li> <li><code>decoder_self_attn</code>: (<code>bool</code>) whether to use self attention in the decoder.</li> </ul>"},{"location":"configs/training/#embedding_meta","title":"<code>embedding_meta</code>:","text":"<p>This section contains parameters for initializing the <code>Embedding</code> Layer.</p>"},{"location":"configs/training/#pos","title":"<code>pos</code>","text":"<p>This subsection contains the parameters for initializing a Spatial <code>Embedding</code>.</p> <ul> <li><code>mode</code>: (<code>str</code>) One of {<code>\"fixed\"</code>, <code>\"learned\"</code>, <code>\"None\"</code>}. Indicates whether to use a fixed sinusoidal, learned, or no embedding.</li> <li><code>n_points</code>: (<code>int</code>) the number of points that will be embedded.</li> </ul>"},{"location":"configs/training/#fixed-sinusoidal-params","title":"Fixed Sinusoidal Params","text":"<ul> <li><code>temperature</code>: (<code>float</code>) the temperature constant to be used when computing the sinusoidal position embedding</li> <li><code>normalize</code>: (<code>bool</code>) whether or not to normalize the positions (Only used in fixed embeddings).</li> <li><code>scale</code>: (<code>float</code>) factor by which to scale the positions after normalizing (Only used in fixed embeddings).</li> </ul>"},{"location":"configs/training/#learned-params","title":"Learned Params:","text":"<ul> <li><code>emb_num</code>: (<code>int</code>) the number of embeddings in the <code>self.lookup</code> table (Only used in learned embeddings).</li> <li><code>over_boxes</code>: (<code>bool</code>) Whether to compute the position embedding for each bbox coordinate (<code>y1x1y2x2</code>) or the centroid + bbox size (<code>yxwh</code>).</li> </ul>"},{"location":"configs/training/#mlp_cfg","title":"<code>mlp_cfg</code>","text":"<p>This subsection contains <code>MLP</code> hyperparameters for projecting embedding to correct space. Required when <code>n_points &gt; 1</code>, optional otherwise.</p> <ul> <li><code>hidden_dims</code>: (<code>int</code>) The dimensionality of the MLP hidden layers.</li> <li><code>num_layers</code>: (<code>int</code>) Number of hidden layers.</li> <li><code>dropout</code>: (<code>float</code>) The dropout probability for each hidden layer.</li> </ul> <p>Example:  <pre><code>model:\n    ...\n    embedding_meta:\n        pos:\n            ...\n            n_points: 3 #could also be 1\n            ...\n            mlp_cfg: #cannot be null\n                hidden_dims: 256\n                num_layers: 3\n                dropout: 0.3\n</code></pre></p>"},{"location":"configs/training/#examples","title":"Examples:","text":""},{"location":"configs/training/#with-mlp","title":"With MLP:","text":"<pre><code>...\nmodel:\n    ...\n    embedding_meta:\n        pos:\n            mode: \"fixed\"\n            normalize: true\n            temperature: 10000\n            scale: null\n            n_points: 3 #could also be 1\n            mlp_cfg: \n                hidden_dims: 256\n                num_layers: 3\n                dropout: 0.3\n            ...\n        ...\n    ...\n...\n</code></pre>"},{"location":"configs/training/#with-no-mlp","title":"With no MLP","text":"<pre><code>model:\n    ...\n    embedding_meta:\n        pos:\n            mode: \"fixed\"\n            normalize: true\n            temperature: 10000\n            scale: null\n            n_points: 1 #must be 1\n            mlp_cfg: null\n        ...\n    ...\n...\n</code></pre>"},{"location":"configs/training/#temp","title":"<code>temp</code>","text":"<p>This subsection contains the parameters for initializing a Temporal <code>Embedding</code></p> <ul> <li><code>mode</code>: (<code>str</code>) One of {<code>\"fixed\"</code>, <code>\"learned\"</code>, <code>\"None\"</code>}. Indicates whether to use a fixed sinusoidal, learned, or no embedding.</li> </ul>"},{"location":"configs/training/#fixed-sinusoidal-params_1","title":"Fixed Sinusoidal Params","text":"<ul> <li><code>temperature</code>: (<code>float</code>) the temperature constant to be used when computing the sinusoidal position embedding</li> </ul>"},{"location":"configs/training/#learned-params_1","title":"Learned Params:","text":"<ul> <li><code>emb_num</code>: (<code>int</code>) the number of embeddings in the lookup table. Note: See <code>dreem.models.Embedding</code> for additional <code>kwargs</code> that can be passed</li> </ul>"},{"location":"configs/training/#examples_1","title":"Examples:","text":""},{"location":"configs/training/#fixed","title":"Fixed:","text":"<pre><code>model:\n    ...\n    embedding_meta:\n        temp:\n            mode: \"fixed\"\n            temperature: 10000\n        ...\n    ...\n...\n</code></pre>"},{"location":"configs/training/#turned-off","title":"Turned Off:","text":"<p><pre><code>model:\n    ...\n    embedding_meta:\n        temp: null\n        ...\n    ...\n...\n</code></pre> or <pre><code>model:\n    ...\n    embedding_meta:\n        temp: \n            mode: \"off\" #null also accepted\n            ...\n        ...\n    ...\n...\n</code></pre></p>"},{"location":"configs/training/#embedding_meta-example","title":"<code>embedding_meta</code> Example:","text":"<p>Putting it all together, your <code>embedding_meta</code> section should look something like this</p> <pre><code>...\nmodel:\n    ...\n    embedding_meta:\n        pos:\n            mode: \"fixed\"\n            normalize: true\n            temperature: 10000\n            scale: null\n            n_points: 3 #could also be 1\n            mlp_cfg: \n                hidden_dims: 256\n                num_layers: 3\n                dropout: 0.3\n        temp:\n            mode: \"fixed\"\n            temperature: 10000\n    ...\n...\n</code></pre>"},{"location":"configs/training/#encoder_cfg","title":"<code>encoder_cfg</code>","text":"<p>This section contains all the parameters for initializing a <code>VisualEncoder</code> model.</p> <ul> <li><code>model_name</code>: (<code>str</code>) Thhe name of the visual encoder backbone to be used. When using <code>timm</code> as a backend, all models in <code>timm.list_model</code> are available. However, when using <code>torchvision</code> as a backend, only <code>resnet</code>s are available for now.</li> <li><code>backend</code>: (<code>str</code>) Either <code>\"timm\"</code> or <code>\"torchvision\"</code>. Indicates which deep learning library to use for initializing the visual encoder</li> <li><code>in_chans</code>: (<code>int</code>)  the number of input channels input images contain. Mostly used for multi-anchor crops</li> <li><code>pretrained</code>: (<code>bool</code>) Whether or not to use a pretrained backbone or initialize from random</li> </ul> <p>Note: For more advanced users, see <code>timm.create_model</code> or <code>torchvision.models.resnet</code> for additional <code>kwargs</code> that can be passed to the visual encoder.</p>"},{"location":"configs/training/#example","title":"Example:","text":""},{"location":"configs/training/#timm","title":"<code>timm</code>:","text":"<pre><code>...\nmodel:\n    ...\n    encoder_cfg:\n        model_name: \"resnet18\"\n        backend: \"timm\"\n        in_chans: 3\n        pretrained: false\n        ...\n    ...\n...\n</code></pre>"},{"location":"configs/training/#torchvision","title":"<code>torchvision</code>:","text":"<pre><code>...\nmodel:\n    ...\n    encoder_cfg:\n        model_name: \"resnet32\"\n        backend: \"torchvision\"\n        in_chans: 3\n        pretrained: false\n        ...\n    ...\n...\n</code></pre>"},{"location":"configs/training/#model-example","title":"<code>model</code> Example:","text":"<p>Putting it all together your <code>model</code> config section will look something like this <pre><code>...\nmodel:\n  ckpt_path: null\n  encoder_cfg: \n    model_name: \"resnet18\"\n    backend: \"timm\"\n    in_chans: 3\n  d_model: 1024\n  nhead: 8\n  num_encoder_layers: 1\n  num_decoder_layers: 1\n  dropout: 0.1\n  activation: \"relu\"\n  return_intermediate_dec: False\n  norm: False\n  num_layers_attn_head: 2\n  dropout_attn_head: 0.1\n  embedding_meta: \n    pos:\n        mode: \"fixed\"\n        normalize: true\n    temp:\n        mode: \"fixed\"\n  return_embedding: False\n  decoder_self_attn: False\n...\n</code></pre></p>"},{"location":"configs/training/#loss","title":"<code>loss</code>","text":"<p>This section contains parameters for the Association Loss function</p> <ul> <li><code>neg_unmatched</code> a bool whether to set unmatched objects to the background</li> <li><code>epsilon</code>: A small <code>float</code> used for numeric precision to prevent dividing by zero</li> <li><code>asso_weight</code>: (<code>float</code>) how much to weight the association loss by</li> </ul>"},{"location":"configs/training/#examples_2","title":"Examples:","text":"<pre><code>...\nloss:\n    neg_unmatched: false\n    epsilon: 1e-8\n    asso_weight: 1.0\n...\n</code></pre>"},{"location":"configs/training/#optimizer","title":"<code>optimizer</code>","text":"<p>This section contains the parameters for initializing the training optimizer</p> <ul> <li><code>name</code>: (<code>str</code>) representation of the optimizer.      &gt; See <code>torch.optim</code> for available optimizers.(<code>name</code> must match the optimizer name exactly (case-sensitive)).</li> </ul> <p>Below, we list the arguments we use for <code>Adam</code> which is the optimizer we use and is our default. For more advanced users please see the respective pytorch documentation page for the arguments expected in your requested optimizer.</p> <ul> <li><code>lr</code>: (<code>float</code>) learning rate</li> <li><code>betas</code>: (<code>tuple[float, float]</code>) coefficients used for computing running averages of gradient and its square</li> <li><code>eps</code>: (<code>float</code>): term added to the denominator to improve numerical stability</li> <li><code>weight_decay</code>: (<code>float</code>) weight decay (\\(L_2\\) penalty)</li> </ul>"},{"location":"configs/training/#examples_3","title":"Examples:","text":"<p>Here we provide a couple examples for different optimizers:</p>"},{"location":"configs/training/#adam","title":"<code>Adam</code>","text":"<pre><code>...\noptimizer:\n    name: \"Adam\"\n    lr: 0.001\n    betas: [0.9, 0.999]\n    eps: 1e-8\n    weight_decay: 0.01\n    ...\n...\n</code></pre>"},{"location":"configs/training/#stochastic-gradient-descent","title":"<code>Stochastic Gradient Descent</code>","text":"<pre><code>...\noptimizer:\n    name: \"SGD\" #must match `torch.optim` class name\n    lr: 0.001\n    momentum: 0.9\n    weight_decay: 0.01\n    dampening: 1e-8\n    nesterov: true\n    ...\n...\n</code></pre>"},{"location":"configs/training/#scheduler","title":"<code>scheduler</code>","text":"<p>This section contains parameters for initializing the learning rate scheduler.</p> <ul> <li><code>name</code>: (<code>str</code>) Representation of the scheduler.      &gt; See <code>torch.optim.lr_scheduler</code> for available schedulers. <code>name</code> must match the scheduler name exactly (case-sensitive).</li> </ul> <p>Below, we list the arguments we use for <code>ReduceLROnPlateau</code> which is the scheduler we use and is our default. For more advanced users please see the respective pytorch documentation page for the arguments expected in your requested scheduler.</p> <ul> <li><code>mode</code>: (<code>str</code>) One of {<code>\"min\"</code>, <code>\"max\"</code>}. In <code>min</code> mode, lr will be reduced when the quantity monitored has stopped decreasing; in <code>max</code> mode it will be reduced when the quantity monitored has stopped increasing.</li> <li><code>factor</code>: (<code>float</code>) Factor by which the learning rate will be reduced. <code>new_lr = lr * factor</code></li> <li><code>patience</code>: (<code>int</code>) The number of allowed epochs with no improvement after which the learning rate will be reduced.</li> <li><code>threshold</code>: (<code>float</code>) Threshold for measuring the new optimum, to only focus on significant changes. </li> <li><code>threshold_mode</code>: (<code>str</code>)  One of {<code>\"rel\"</code>, \"<code>abs</code>\"}. In <code>rel</code> mode, <code>dynamic_threshold = best * ( 1 + threshold )</code> in <code>max</code> mode or <code>best * ( 1 - threshold )</code> in <code>min</code> mode. In <code>abs</code> mode, <code>dynamic_threshold = best + threshold</code> in <code>max</code> mode or <code>best - threshold</code> in <code>min</code> mode.</li> </ul>"},{"location":"configs/training/#examples_4","title":"Examples:","text":"<p>Here we give a couple examples of configs for different schedulers:</p>"},{"location":"configs/training/#reduce-learning-rate-on-plateau","title":"<code>Reduce Learning Rate on Plateau</code>","text":"<pre><code>...\nscheduler:\n  name: \"ReduceLROnPlateau\" #must match torch.optim class name\n  mode: \"min\"\n  factor: 0.5\n  patience: 10\n  threshold: 1e-4\n  threshold_mode: \"rel\"\n  ...\n...\n</code></pre>"},{"location":"configs/training/#cosine-annealing-with-warm-restarts","title":"<code>Cosine Annealing with Warm Restarts</code>","text":"<pre><code>...\nscheduler:\n    name: \"CosineAnnealingWarmRestarts\"\n    T_0: 10\n    T_mult: 1\n    eta_min: 0\n    last_epoch: 50\n    verbose: True\n...\n</code></pre>"},{"location":"configs/training/#tracker","title":"<code>tracker</code>:","text":"<p>This section contains parameters for initializing the <code>Tracker</code></p> <ul> <li><code>window_size</code>: the size of the window used during sliding inference.</li> <li><code>use_vis_feats</code>: Whether or not to use visual feature extractor.</li> <li><code>overlap_thresh</code>: the trajectory overlap threshold to be used for assignment.</li> <li><code>mult_thresh</code>: Whether or not to use weight threshold.</li> <li><code>decay_time</code>: weight for <code>decay_time</code> postprocessing.</li> <li><code>iou</code>: Either <code>{None, '', \"mult\" or \"max\"}</code>. Whether to use multiplicative or max iou reweighting.</li> <li><code>max_center_dist</code>: distance threshold for filtering trajectory score matrix.</li> <li><code>persistent_tracking</code>: whether to keep a buffer across chunks or not.</li> <li><code>max_gap</code>: the max number of frames a trajectory can be missing before termination.</li> <li><code>max_tracks</code>: the maximum number of tracks that can be created while tracking.     We force the tracker to assign instances to a track instead of creating a new track if <code>max_tracks</code> has been reached.</li> </ul>"},{"location":"configs/training/#examples_5","title":"Examples:","text":"<pre><code>...\ntracker:\n    window_size: 8\n    overlap_thresh: 0.01\n    mult_thresh: false\n    decay_time: 0.9\n    iou: \"mult\"\n    max_center_dist: 0.1\n    ...\n...\n</code></pre>"},{"location":"configs/training/#runner","title":"<code>runner</code>","text":"<p>This section contains parameters for how to handle training/validation/testing</p>"},{"location":"configs/training/#metrics","title":"<code>metrics</code>","text":"<p>This section contains config for which metrics to compute during training/validation/testing. See <code>pymotmetrics.list_metrics</code> for available metrics.</p> <p>Should have a <code>train</code>, <code>val</code> and <code>test</code> key with corresponding list of metrics to compute during training.</p>"},{"location":"configs/training/#examples_6","title":"Examples:","text":""},{"location":"configs/training/#only-computing-the-loss","title":"Only computing the loss:","text":"<pre><code>...\nrunner:\n    ...\n    metrics:\n        train: []\n        val: []\n        test: []\n    ...\n...\n</code></pre>"},{"location":"configs/training/#computing-num_switches-during-validation","title":"Computing <code>num_switches</code> during validation:","text":"<pre><code>...\nrunner:\n    ...\n    metrics:\n        train: []\n        val: [\"num_switches\"]\n        test: []\n    ...\n...\n</code></pre>"},{"location":"configs/training/#computing-num_switches-and-mota-during-testing","title":"Computing <code>num_switches</code> and  <code>mota</code> during testing:","text":"<pre><code>...\nrunner:\n    ...\n    metrics:\n        train: []\n        val: [\"num_switches\"]\n        test: [\"num_switches\", \"mota\"]\n    ...\n...\n</code></pre>"},{"location":"configs/training/#persistent_tracking","title":"<code>persistent_tracking</code>","text":"<p>This section indicates whether or not to track across chunks during training/validation/testing</p> <p>Should have a <code>train</code>, <code>val</code> and <code>test</code> key with a corresponding <code>bool</code> whether to use persistent tracking. <code>persistent_tracking</code> should almost always be <code>False</code> during training. During validation and testing it may depend on whether you are testing on full videos or subsampled clips</p>"},{"location":"configs/training/#examples_7","title":"Examples:","text":"<pre><code>...\nrunner\n    ...\n    persistent_tracking:\n        train: false\n        val: false # assuming we validate on a subsample of clips\n        test: true # assuming we test on a contiguous video.\n</code></pre>"},{"location":"configs/training/#dataset","title":"<code>dataset</code>","text":"<p>This section contains the params for initializing the datasets for training. Requires a <code>train_dataset</code> and optionally <code>val_dataset</code>, <code>test_dataset</code> keys. </p>"},{"location":"configs/training/#basedataset-args","title":"<code>BaseDataset</code> args","text":"<ul> <li><code>padding</code>: An <code>int</code> representing the amount of padding to be added to each side of the bounding box size</li> <li><code>crop_size</code>: (<code>int</code>|<code>tuple</code>) the size of the bounding box around which a crop will form.</li> <li><code>chunk</code>: Whether or not to chunk videos into smaller clips to feed to model</li> <li><code>clip_length</code>: the number of frames in each chunk</li> <li><code>mode</code>: <code>train</code> or <code>val</code>. Determines whether this dataset is used for training or validation.</li> <li><code>n_chunks</code>: Number of chunks to subsample from. Can either a fraction of the dataset (ie <code>(0,1.0]</code>) or number of chunks</li> <li><code>seed</code>: set a seed for reproducibility</li> <li><code>gt_list</code>: An optional path to .txt file containing ground truth for cell tracking challenge datasets.</li> </ul>"},{"location":"configs/training/#dir","title":"<code>dir</code>:","text":"<p>This section allows you to pass a directory rather than paths to labels/videos individually</p> <ul> <li><code>path</code>: The path to the dir where the data is stored (recommend absolute path)</li> <li><code>labels_suffix</code>: (<code>str</code>) containing the file extension to search for labels files. e.g. <code>.slp</code>, <code>.csv</code>, or <code>.xml</code>.</li> <li><code>vid_suffix</code>: (<code>str</code>) containing the file extension to search for video files e.g <code>.mp4</code>, <code>.avi</code> or <code>.tif</code>.</li> </ul>"},{"location":"configs/training/#examples_8","title":"Examples:","text":"<pre><code>...\ndataset:\n    ...\n    {MODE}_dataset:\n        dir:\n            path: \"/path/to/data/dir/mode\"\n            labels_suffix: \".slp\"\n            vid_suffix: \".mp4\"\n        ...\n    ...\n...\n</code></pre>"},{"location":"configs/training/#augmentations","title":"<code>augmentations</code>:","text":"<p>This subsection contains params for albumentations. See <code>albumentations</code> for available visual augmentations. Other available augmentations include <code>NodeDropout</code> and <code>InstanceDropout</code>. Keys must match augmentation class name exactly and contain subsections with parameters for the augmentation</p>"},{"location":"configs/training/#example_1","title":"Example","text":"<pre><code>augmentations: \n    Rotate:\n        limit: 45\n        p: 0.3\n    ...\n    MotionBlur:\n        blur_limit: [3,7]\n        p: 0.3\n</code></pre>"},{"location":"configs/training/#sleapdataset-args","title":"<code>SleapDataset</code> Args:","text":"<ul> <li><code>slp_files</code>: (<code>str</code>) a list of .slp files storing tracking annotations</li> <li><code>video_files</code>: (<code>str</code>) a list of paths to video files</li> <li><code>anchors</code>: (<code>str</code> | <code>list</code> | <code>int</code>) One of:<ul> <li>a string indicating a single node to center crops around</li> <li>a list of skeleton node names to be used as the center of crops</li> <li>an int indicating the number of anchors to randomly select If unavailable then crop around the midpoint between all visible anchors.</li> </ul> </li> <li><code>handle_missing</code>: how to handle missing single nodes. one of [<code>\"drop\"</code>, <code>\"ignore\"</code>, <code>\"centroid\"</code>].<ul> <li>if <code>drop</code> then we dont include instances which are missing the <code>anchor</code>.</li> <li>if <code>ignore</code> then we use a mask instead of a crop and nan centroids/bboxes.</li> <li>if <code>centroid</code> then we default to the pose centroid as the node to crop around.</li> </ul> </li> </ul>"},{"location":"configs/training/#microscopydataset","title":"<code>MicroscopyDataset</code>","text":"<ul> <li><code>videos</code>: (<code>list[str | list[str]]</code>) paths to raw microscopy videos</li> <li><code>tracks</code>: (<code>list[str]</code>) paths to trackmate gt labels (either <code>.xml</code> or <code>.csv</code>)</li> <li><code>source</code>: file format of gt labels based on label generator. Either <code>\"trackmate\"</code> or <code>\"isbi\"</code>.</li> </ul>"},{"location":"configs/training/#celltrackingdataset","title":"<code>CellTrackingDataset</code>","text":"<ul> <li><code>raw_images</code>: (<code>list[list[str] | list[list[str]]]</code>) paths to raw microscopy images</li> <li><code>gt_images</code>: (<code>list[list[str] | list[list[str]]]</code>) paths to gt label images</li> <li><code>gt_list</code>: (<code>list[str]</code>) An optional path to .txt file containing gt ids stored in cell                 tracking challenge format: <code>\"track_id\", \"start_frame\",                 \"end_frame\", \"parent_id\"</code></li> </ul>"},{"location":"configs/training/#dataset-examples","title":"<code>dataset</code> Examples","text":""},{"location":"configs/training/#sleapdataset","title":"<code>SleapDataset</code>","text":"<pre><code>...\ndataset:\n    train_dataset:\n        slp_files: [\"/path/to/train/labels1.slp\", \"/path/to/train/labels2.slp\", ..., \"/path/to/train/labelsN.slp\"]\n        video_files: [\"/path/to/train/video1.mp4\", \"/path/to/train/video2.mp4\", ..., \"/path/to/train/videoN.mp4\"]\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        anchors: [\"node1\", \"node2\", ...\"node_n\"]\n        handle_missing: \"drop\"\n        augmentations: \n            Rotate:\n                limit: 45\n                p: 0.3\n            ...\n            MotionBlur:\n                blur_limit: [3,7]\n                p: 0.3\n        ...\n    val_dataset:\n        slp_files: [\"/path/to/val/labels1.slp\", \"/path/to/val/labels2.slp\", ..., \"/path/to/val/labelsN.slp\"]\n        video_files: [\"/path/to/val/video1.mp4\", \"/path/to/val/video2.mp4\", ..., \"/path/to/val/videoN.mp4\"]\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        anchors: [\"node1\", \"node2\", ...\"node_n\"]\n        handle_missing: \"drop\"\n        ... # we don't include augmentations bc usually you shouldnt use augmentations during val/test\n    test_dataset:\n        slp_files: [\"/path/to/test/labels1.slp\", \"/path/to/test/labels2.slp\", ..., \"/path/to/test/labelsN.slp\"]\n        video_files: [\"/path/to/test/video1.mp4\", \"/path/to/test/video2.mp4\", ..., \"/path/to/test/videoN.mp4\"]\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        anchors: [\"node1\", \"node2\", ...\"node_n\"]\n        handle_missing: \"drop\"\n        ... # we don't include augmentations bc usually you shouldnt use augmentations during val/test\n...\n</code></pre>"},{"location":"configs/training/#microscopydataset_1","title":"<code>MicroscopyDataset</code>","text":"<pre><code>dataset:\n    train_dataset:\n        tracks: [\"/path/to/train/labels1.csv\", \"/path/to/train/labels2.csv\", ..., \"/path/to/train/labelsN.csv\"]\n        videos: [\"/path/to/train/video1.tiff\", \"/path/to/train/video2.tiff\", ..., \"/path/to/train/videoN.tiff\"]\n        source: \"trackmate\"\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        augmentations: \n            Rotate:\n                limit: 45\n                p: 0.3\n            ...\n            MotionBlur:\n                blur_limit: [3,7]\n                p: 0.3\n        ...\n    val_dataset:\n        tracks: [\"/path/to/val/labels1.csv\", \"/path/to/val/labels2.csv\", ..., \"/path/to/val/labelsN.csv\"]\n        video: [\"/path/to/val/video1.tiff\", \"/path/to/val/video2.tiff\", ..., \"/path/to/val/videoN.tiff\"]\n        source: \"trackmate\"\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        ... # we don't include augmentations bc usually you shouldnt use augmentations during val/test\n    test_dataset:\n        tracks: [\"/path/to/test/labels1.csv\", \"/path/to/test/labels2.csv\", ..., \"/path/to/test/labelsN.csv\"]\n        videos: [\"/path/to/test/video1.tiff\", \"/path/to/test/video2.tiff\", ..., \"/path/to/test/videoN.tiff\"]\n        source: \"trackmate\"\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        ... # we don't include augmentations bc usually you shouldnt use augmentations during val/test\n</code></pre>"},{"location":"configs/training/#dataloader","title":"<code>dataloader</code>","text":"<p>This section outlines the params needed for the dataloader. Should have a <code>train_dataloader</code> and optionally <code>val_dataloader</code>/<code>test_dataloader</code> keys. </p> <p>Below we list the args we found useful/necessary for the dataloaders. For more advanced users see <code>torch.utils.data.Dataloader</code> for more ways to initialize the dataloaders</p> <ul> <li><code>shuffle</code>: (<code>bool</code>) Set to <code>True</code> to have the data reshuffled at every epoch (during training, this should always be <code>True</code> and during val/test usually <code>False</code>) </li> <li><code>num_workers</code>: (<code>int</code>) How many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.</li> </ul>"},{"location":"configs/training/#example_2","title":"Example","text":"<pre><code>...\ndataloader:\n    train_dataloader:\n        shuffle: true\n        num_workers: 4\n    val_dataloader: # we leave out the `shuffle` field as default=`False` which is what we want\n        num_workers: 4\n    test_dataloader: # we leave out the `shuffle` field as default=`False` which is what we want\n        num_workers: 4\n</code></pre>"},{"location":"configs/training/#logging","title":"<code>logging</code>:","text":"<p>This section sets up logging for the training job. </p> <ul> <li><code>logger_type</code>: (<code>str</code>) Which logger to use. Available loggers are {<code>\"CSVLogger\"</code>, <code>\"TensorBoardLogger\"</code>,<code>\"WandbLogger\"</code>}</li> </ul> <p>Below we list the arguments we found useful for the <code>WandbLogger</code> as this is the logger we use and recommend. Please see the documentation for the corresponding logger at <code>lightning.loggers</code> for respective available parameters.</p> <ul> <li><code>name</code>: (<code>str</code>) A short display name for this run, which is how you'll identify this run in the UI.</li> <li><code>save_dir</code>: (<code>str</code>) An absolute path to a directory where metadata will be stored. </li> <li><code>version</code>: (<code>str</code>) A unique ID for this run, used for resuming. It must be unique in the project, and if you delete a run you can't reuse the ID.</li> <li><code>project</code>: (<code>str</code>)  The name of the project where you're sending the new run.</li> <li><code>log_model</code>: (<code>str</code>) Log checkpoints created by <code>ModelCheckpoint</code> as W&amp;B artifacts</li> <li><code>group</code>: (<code>str</code>) Specify a group to organize individual runs into a larger experiment</li> <li><code>entity</code>: (<code>str</code>) An entity is a username or team name where you're sending runs</li> <li><code>notes</code>: (<code>str</code>) A longer description of the run, like a <code>-m</code>commit message in git.</li> </ul> <p>See <code>wandb.init()</code> and <code>WandbLogger</code> for more fine-grained config args.</p>"},{"location":"configs/training/#examples_9","title":"Examples:","text":"<p>Here we provide a couple examples for different available loggers</p>"},{"location":"configs/training/#wandb","title":"<code>wandb</code>","text":"<pre><code>...\nlogging:\n  logger_type: \"WandbLogger\"\n  name: \"example_train\"\n  entity: \"example_user\"\n  job_type: \"train\"\n  notes: \"Example train job\"\n  dir: \"./logs\"\n  group: \"example\"\n  save_dir: './logs'\n  project: \"GTR\"\n  log_model: \"all\"\n  ...\n...\n</code></pre>"},{"location":"configs/training/#csv-logger","title":"<code>csv logger</code>:","text":"<pre><code>...\nlogging:\n    save_dir: \"./logs\"\n    name: \"example_train.csv\"\n    version: 1\n    flush_logs_every_n_steps: 1\n    ...\n...\n</code></pre>"},{"location":"configs/training/#early_stopping","title":"<code>early_stopping</code>","text":"<p>This section configures early stopping for training runs. </p> <p>Below we provide descriptions of the arguments we found useful for EarlyStopping. For advanced users, see `lightning.callbacks.EarlyStopping for available arguments for more fine grained control</p> <ul> <li><code>monitor</code> (<code>str</code>): quantity to be monitored.</li> <li><code>min_delta</code> (<code>float</code>): minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than or equal to min_delta, will count as no improvement.</li> <li><code>patience</code> (<code>int</code>): number of checks with no improvement after which training will be stopped. </li> <li><code>mode</code> (<code>str</code>): one of 'min', 'max'. In 'min' mode, training will stop when the quantity monitored has stopped decreasing and in 'max' mode it will stop when the quantity monitored has stopped increasing.</li> <li><code>check_finite</code> (<code>bool</code>): When set True, stops training when the monitor becomes NaN or infinite.</li> <li><code>stopping_threshold</code> (<code>float</code>): Stop training immediately once the monitored quantity reaches this threshold.</li> <li><code>divergence_threshold</code> (<code>float</code>): Stop training as soon as the monitored quantity becomes worse than this threshold.</li> </ul>"},{"location":"configs/training/#example_3","title":"Example:","text":"<pre><code>...\nearly_stopping:\n  monitor: \"val_loss\"\n  min_delta: 0.1\n  patience: 10\n  mode: \"min\"\n  check_finite: true\n  stopping_threshold: 1e-8\n  divergence_threshold: 30\n  ...\n...\n</code></pre>"},{"location":"configs/training/#checkpointing","title":"<code>checkpointing</code>","text":"<p>This section enables model checkpointing during training</p> <ul> <li><code>monitor</code>: A list of metrics to save best models for. Usually should be <code>\"val_{METRIC}\"</code> notation.     &gt; Note: We initialize a separate <code>ModelCheckpoint</code> for each metric to monitor.     &gt; This means that you'll save at least \\(|monitor|\\) checkpoints at the end of training.</li> </ul> <p>Below we describe the arguments we found useful for checkpointing. For more fine grained control see <code>lightning.callbacks.ModelCheckpoint</code> for available checkpointing params and generally more info on how <code>lightning</code> sets up checkpoints</p> <ul> <li><code>dirpath</code>: (<code>str</code>) Directory to save the models. If left empty then we first try to save to <code>./models/[GROUP]/[NAME]</code> or <code>./models/[NAME]</code> if logger is <code>wandb</code> otherwise we just save to <code>./models</code> </li> <li><code>save_last</code>: (<code>bool</code>): When <code>True</code>, saves a last.ckpt copy whenever a checkpoint file gets saved. Can be set to 'link' on a local filesystem to create a symbolic link. This allows accessing the latest checkpoint in a deterministic manner.</li> <li><code>save_top_k</code>: (<code>int</code>): if <code>save_top_k == k</code>, the best k models according to the quantity monitored will be saved. if <code>save_top_k == 0</code>, no models are saved. if <code>save_top_k == -1</code>, all models are saved. (Recommend -1)</li> <li><code>every_n_epochs</code>: (<code>int</code>) Number of epochs between checkpoints. This value must be <code>None</code> or non-negative. To disable saving top-k checkpoints, set <code>every_n_epochs = 0</code>. This argument does not impact the saving of <code>save_last=True</code> checkpoints.</li> </ul>"},{"location":"configs/training/#example_4","title":"Example:","text":"<pre><code>...\ncheckpointing:\n    monitor: [\"val_loss\", \"val_num_switches\"] #saves a model for best validation loss and a model for best validation switch count separately\n    dirpath: \"./models/example_run\"\n    save_last: true # will always save the best run\n    save_top_k: -1\n    every_n_epochs: 10 # saves the every 10th model regardless of if its the best.\n    ...\n...\n</code></pre>"},{"location":"configs/training/#trainer","title":"<code>trainer</code>","text":"<p>This section configures the <code>lightning.Trainer</code> object for training. </p> <p>Below we describe the arguments we found useful for the <code>Trainer</code>. If you're an advanced user, Please see <code>lightning.Trainer</code>(https://lightning.ai/docs/pytorch/stable/common/trainer.html) for more fine grained control and how the <code>trainer</code> works in general</p> <ul> <li><code>accelerator</code>: (<code>str</code>) Supports passing different accelerator types <code>(\u201ccpu\u201d, \u201cgpu\u201d, \u201ctpu\u201d, \u201cipu\u201d, \u201chpu\u201d, \u201cmps\u201d, \u201cauto\u201d)</code> as well as custom accelerator instances.</li> <li><code>strategy</code>: (<code>str</code>) Supports different training strategies with aliases as well custom strategies</li> <li><code>devices</code>: (<code>list[int]</code> | <code>str</code>| <code>int</code>)`The devices to use. Can be set to:<ul> <li>a positive number (<code>int</code> | <code>str</code>) </li> <li>a sequence of device indices (<code>list</code> | <code>str</code>), </li> <li>the value <code>-1</code> to indicate all available devices should be used</li> <li>\"auto\" for automatic selection based on the chosen accelerator </li> </ul> </li> <li><code>fast_dev_run</code>: (<code>int</code> | <code>bool</code>) Runs <code>n</code> (if set to <code>n</code> (<code>int</code>)) else <code>1</code> (if set to <code>True</code>) batch(es) of train, val and test to find any bugs (ie: a sort of unit test).</li> <li><code>check_val_every_n_epoch</code>: (<code>int</code>) Perform a validation loop every after every <code>N</code> training epochs</li> <li><code>enable_checkpointing</code>: (<code>bool</code>) If <code>True</code>, enable checkpointing. It will configure a default <code>ModelCheckpoint</code> callback if there is no user-defined <code>ModelCheckpoint</code> in callbacks.</li> <li><code>gradient_clip_val</code>:  (<code>float</code>) The value at which to clip gradients</li> <li><code>limit_train_batches</code>: (<code>int</code> | <code>float</code>) How much of training dataset to check (<code>float</code> = fraction, <code>int</code> = num_batches) (mostly for debugging)</li> <li><code>limit_test_batches</code>: (<code>int</code> | <code>float</code>) How much of test dataset to check (<code>float</code> = fraction, <code>int</code> = num_batches). (mostly for debugging)</li> <li><code>limit_val_batches</code>: (<code>int</code> | <code>float</code>) How much of validation dataset to check (<code>float</code> = fraction, <code>int</code> = num_batches) (mostly for debugging)</li> <li><code>limit_predict_batches</code>: (<code>int</code> | <code>float</code>) How much of prediction dataset to check (<code>float</code> = fraction, <code>int</code> = num_batches)</li> <li><code>log_every_n_steps</code>:  (<code>int</code>) How often to log within steps</li> <li><code>max_epochs</code>: (<code>int</code>) Stop training once this number of epochs is reached. To enable infinite training, set <code>max_epochs</code> = -1.</li> <li><code>min_epochs</code>: (<code>int</code>) Force training for at least these many epochs</li> </ul>"},{"location":"configs/training/#examples_10","title":"Examples:","text":"<pre><code>trainer:\n  check_val_every_n_epoch: 1\n  enable_checkpointing: true\n  gradient_clip_val: null\n  limit_train_batches: 1.0\n  limit_test_batches: 1.0\n  limit_val_batches: 1.0\n  log_every_n_steps: 1\n  max_epochs: 100\n  min_epochs: 10\n</code></pre>"},{"location":"configs/training/#view_batch","title":"<code>view_batch</code>","text":"<p>This section allows you to visualize the data before training</p> <ul> <li><code>enable</code>: (<code>bool</code>) whether or not to view a batch</li> <li><code>num_frames</code>: (<code>int</code>) The number of frames in the batch to visualize</li> <li><code>no_train</code>: (<code>bool</code>)  whether or not to train after visualization is complete</li> </ul>"},{"location":"configs/training/#examples_11","title":"Examples:","text":""},{"location":"configs/training/#off","title":"Off","text":"<pre><code>view_batch:\n  enable: False\n  num_frames: 0 #this arg can be anything\n  no_train: False #This can be false\n</code></pre>"},{"location":"configs/training/#on-no-training","title":"On, no training:","text":"<pre><code>view_batch:\n  enable: False\n  num_frames: 32 #this arg can be anything\n  no_train: True #training will not occur\n</code></pre>"},{"location":"configs/training/#on-with-training","title":"On, with training:","text":"<pre><code>view_batch:\n  enable: False\n  num_frames: 32 #this arg can be anything\n  no_train: True #training will not occur\n</code></pre>"},{"location":"configs/training/#example-config","title":"Example Config","text":""},{"location":"configs/training/#base-config","title":"Base Config","text":"<pre><code>model:\n  ckpt_path: null\n  encoder_cfg: \n    model_name: \"resnet18\"\n    backend: \"timm\"\n    in_chans: 3\n  d_model: 1024\n  nhead: 8\n  num_encoder_layers: 1\n  num_decoder_layers: 1\n  dropout: 0.1\n  activation: \"relu\"\n  return_intermediate_dec: False\n  norm: False\n  num_layers_attn_head: 2\n  dropout_attn_head: 0.1\n  embedding_meta: \n    pos:\n        mode: \"fixed\"\n        normalize: true\n    temp:\n        mode: \"fixed\"\n  return_embedding: False\n  decoder_self_attn: False\n\nloss:\n  neg_unmatched: false\n  epsilon: 1e-4\n  asso_weight: 1.0\n\n#currently assumes adam. TODO adapt logic for other optimizers like sgd\noptimizer:\n  name: \"Adam\"\n  lr: 0.001\n  betas: [0.9, 0.999]\n  eps: 1e-8\n  weight_decay: 0.01\n\n#currently assumes reduce lr on plateau\nscheduler:\n  name: \"ReduceLROnPlateau\"\n  mode: \"min\"\n  factor: 0.5\n  patience: 10\n  threshold: 1e-4\n  threshold_mode: \"rel\"\n\ntracker:\n  window_size: 8\n  use_vis_feats: true\n  overlap_thresh: 0.01\n  mult_thresh: true\n  decay_time: null\n  iou: null\n  max_center_dist: null\n\nrunner:\n  metrics:\n      train: ['num_switches']\n      val: ['num_switches']\n      test: ['num_switches']\n  persistent_tracking:\n      train: false\n      val: true\n      test: true\n\ndataset:\n  train_dataset:\n    slp_files: [\"../../tests/data/sleap/two_flies.slp\"]\n    video_files: [\"../../tests/data/sleap/two_flies.mp4\"]\n    padding: 5\n    crop_size: 128\n    chunk: true\n    clip_length: 32\n\n  val_dataset:\n    slp_files: [\"../../tests/data/sleap/two_flies.slp\"]\n    video_files: [\"../../tests/data/sleap/two_flies.mp4\"]\n    padding: 5\n    crop_size: 128 \n    chunk: True\n    clip_length: 32\n\n  test_dataset:\n    slp_files: [\"../../tests/data/sleap/two_flies.slp\"]\n    video_files: [\"../../tests/data/sleap/two_flies.mp4\"]\n    padding: 5\n    crop_size: 128 \n    chunk: True\n    clip_length: 32\n\ndataloader:\n  train_dataloader:\n    shuffle: true\n    num_workers: 0\n  val_dataloader:\n    shuffle: false\n    num_workers: 0\n  test_dataloader: \n    shuffle: false\n    num_workers: 0\n\nlogging:\n  logger_type: null\n  name: \"example_train\"\n  entity: null\n  job_type: \"train\"\n  notes: \"Example train job\"\n  dir: \"./logs\"\n  group: \"example\"\n  save_dir: './logs'\n  project: \"GTR\"\n  log_model: \"all\"\n\nearly_stopping:\n  monitor: \"val_loss\"\n  min_delta: 0.1\n  patience: 10\n  mode: \"min\"\n  check_finite: true\n  stopping_threshold: 1e-8\n  divergence_threshold: 30\n\ncheckpointing:\n  monitor: [\"val_loss\",\"val_num_switches\"]\n  verbose: true\n  save_last: true\n  dirpath: null\n  auto_insert_metric_name: true\n  every_n_epochs: 10\n\ntrainer:\n  check_val_every_n_epoch: 1\n  enable_checkpointing: true\n  gradient_clip_val: null\n  limit_train_batches: 1.0\n  limit_test_batches: 1.0\n  limit_val_batches: 1.0\n  log_every_n_steps: 1\n  max_epochs: 100\n  min_epochs: 10\n\nview_batch:\n  enable: False\n  num_frames: 0\n  no_train: False\n</code></pre>"},{"location":"configs/training/#override-config","title":"Override Config","text":"<p>```YAML model:   num_encoder_layers: 2   num_decoder_layers: 2   embedding_meta:     pos:       mode: learned       emb_num: 16       over_boxes: True     temp:        mode: \"off\" dataset:   train_dataset:     slp_files: ['190612_110405_wt_18159111_rig2.2@11730.slp']     video_files: ['190612_110405_wt_18159111_rig2.2@11730.mp4']     clip_length: 16</p>"},{"location":"io/","title":"DREEM <code>io</code> module.","text":"<p>The <code>io</code> module contains classes for storing, manipulating and generating model inputs and outputs.</p> <p>There are 6 main submodules:</p> <ol> <li><code>Instance</code> which represent detections. They are the main input into the model and contain <ul> <li>GT Track ID</li> <li>Predicted Track ID</li> <li>the crop of the image</li> <li>the location as a centroid <code>(x,y)</code>, bounding box <code>(y_1, x_1, y_2, x_2)</code> and (optionally) the pose <code>(1, n, 2)</code></li> <li>the features extracted by the visual encoder + spatiotemporal embedding</li> <li>association score</li> </ul> </li> <li><code>Frame</code> which stores the metadata for a single frame of a video. It contains:<ul> <li>The video index</li> <li>The video name</li> <li>The frame index</li> <li>The list of <code>Instance</code>s that appear in that frame</li> <li>The <code>AssociationMatrix</code> between the instances in that frame and all instances in the window</li> </ul> </li> <li><code>Config</code> which stores and parses the configs for training and/or</li> <li><code>AssociationMatrix</code> which stores the <code>GlobalTrackingTransformer</code> output. <code>AssociationMatrix[i,j]</code> represents the association score between the <code>i</code>th query instance and the <code>j</code>th instance in the window.</li> <li><code>Track</code> which stores references to all instances belonging to the same trajectory</li> <li><code>visualize</code> which contains util functions for generating videos annotated by the predicted tracks and scores.</li> </ol>"},{"location":"io/asso_matrix/","title":"<code>AssociationMatrix</code>","text":""},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix","title":"<code>dreem.io.AssociationMatrix</code>","text":"<p>Class representing the associations between detections.</p> <p>Attributes:</p> Name Type Description <code>matrix</code> <code>ndarray | Tensor</code> <p>the <code>n_query x n_ref</code> association matrix`</p> <code>ref_instances</code> <code>list[Instance]</code> <p>all instances used to associate against.</p> <code>query_instances</code> <code>list[Instance]</code> <p>query instances that were associated against ref instances.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>@attrs.define\nclass AssociationMatrix:\n    \"\"\"Class representing the associations between detections.\n\n    Attributes:\n        matrix: the `n_query x n_ref` association matrix`\n        ref_instances: all instances used to associate against.\n        query_instances: query instances that were associated against ref instances.\n    \"\"\"\n\n    matrix: np.ndarray | torch.Tensor\n    ref_instances: list[Instance] = attrs.field()\n    query_instances: list[Instance] = attrs.field()\n\n    @ref_instances.validator\n    def _check_ref_instances(self, attribute, value):\n        \"\"\"Check to ensure that the number of association matrix columns and reference instances match.\n\n        Args:\n            attribute: The ref instances.\n            value: the list of ref instances.\n\n        Raises:\n            ValueError if the number of columns and reference instances don't match.\n        \"\"\"\n        if len(value) != self.matrix.shape[-1]:\n            raise ValueError(\n                (\n                    \"Ref instances must equal number of columns in Association matrix\"\n                    f\"Found {len(value)} ref instances but {self.matrix.shape[-1]} columns.\"\n                )\n            )\n\n    @query_instances.validator\n    def _check_query_instances(self, attribute, value):\n        \"\"\"Check to ensure that the number of association matrix rows and query instances match.\n\n        Args:\n            attribute: The query instances.\n            value: the list of query instances.\n\n        Raises:\n            ValueError if the number of rows and query instances don't match.\n        \"\"\"\n        if len(value) != self.matrix.shape[0]:\n            raise ValueError(\n                (\n                    \"Query instances must equal number of rows in Association matrix\"\n                    f\"Found {len(value)} query instances but {self.matrix.shape[0]} rows.\"\n                )\n            )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get the string representation of the Association Matrix.\n\n        Returns:\n            the string representation of the association matrix.\n        \"\"\"\n        return (\n            f\"AssociationMatrix({self.matrix},\"\n            f\"query_instances={len(self.query_instances)},\"\n            f\"ref_instances={len(self.ref_instances)})\"\n        )\n\n    def numpy(self) -&gt; np.ndarray:\n        \"\"\"Convert association matrix to a numpy array.\n\n        Returns:\n            The association matrix as a numpy array.\n        \"\"\"\n        if isinstance(self.matrix, torch.Tensor):\n            return self.matrix.detach().cpu().numpy()\n        return self.matrix\n\n    def to_dataframe(\n        self, row_labels: str = \"gt\", col_labels: str = \"gt\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Convert the association matrix to a pandas DataFrame.\n\n        Args:\n            row_labels: How to label the rows(queries).\n                If list, then must match # of rows/queries\n                If `\"gt\"` then label by gt track id.\n                If `\"pred\"` then label by pred track id.\n                Otherwise label by the query_instance indices\n            col_labels: How to label the columns(references).\n                If list, then must match # of columns/refs\n                If `\"gt\"` then label by gt track id.\n                If `\"pred\"` then label by pred track id.\n                Otherwise label by the ref_instance indices\n\n        Returns:\n            The association matrix as a pandas dataframe.\n        \"\"\"\n        matrix = self.numpy()\n\n        if not isinstance(row_labels, str):\n            if len(row_labels) == len(self.query_instances):\n                row_inds = row_labels\n\n            else:\n                raise ValueError(\n                    (\n                        f\"Mismatched # of rows and labels!\",\n                        f\"Found {len(row_labels)} with {len(self.query_instances)} rows\",\n                    )\n                )\n\n        else:\n            if row_labels == \"gt\":\n                row_inds = [\n                    instance.gt_track_id.item() for instance in self.query_instances\n                ]\n\n            elif row_labels == \"pred\":\n                row_inds = [\n                    instance.pred_track_id.item() for instance in self.query_instances\n                ]\n\n            else:\n                row_inds = np.arange(len(self.query_instances))\n\n        if not isinstance(col_labels, str):\n            if len(col_labels) == len(self.ref_instances):\n                col_inds = col_labels\n\n            else:\n                raise ValueError(\n                    (\n                        f\"Mismatched # of columns and labels!\",\n                        f\"Found {len(col_labels)} with {len(self.ref_instances)} columns\",\n                    )\n                )\n\n        else:\n            if col_labels == \"gt\":\n                col_inds = [\n                    instance.gt_track_id.item() for instance in self.ref_instances\n                ]\n\n            elif col_labels == \"pred\":\n                col_inds = [\n                    instance.pred_track_id.item() for instance in self.ref_instances\n                ]\n\n            else:\n                col_inds = np.arange(len(self.ref_instances))\n\n        asso_df = pd.DataFrame(matrix, index=row_inds, columns=col_inds)\n\n        return asso_df\n\n    def reduce(\n        self,\n        row_dims: str = \"instance\",\n        col_dims: str = \"track\",\n        row_grouping: str | None = None,\n        col_grouping: str = \"pred\",\n        reduce_method: callable = np.sum,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Aggregate the association matrix by specified dimensions and grouping.\n\n        Args:\n           row_dims: A str indicating how to what dimensions to reduce rows to.\n                Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).\n           col_dims: A str indicating how to dimensions to reduce rows to.\n                Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)\n           row_grouping: A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".\n           col_grouping: A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".\n           reduce_method: A callable function that operates on numpy matrices and can take an `axis` arg for reducing.\n\n        Returns:\n            The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.\n        \"\"\"\n        n_rows = len(self.query_instances)\n        n_cols = len(self.ref_instances)\n\n        col_tracks = {-1: self.ref_instances}\n        row_tracks = {-1: self.query_instances}\n\n        col_inds = [i for i in range(len(self.ref_instances))]\n        row_inds = [i for i in range(len(self.query_instances))]\n\n        if col_dims == \"track\":\n            col_tracks = self.get_tracks(self.ref_instances, col_grouping)\n            col_inds = list(col_tracks.keys())\n            n_cols = len(col_inds)\n\n        if row_dims == \"track\":\n            row_tracks = self.get_tracks(self.query_instances, row_grouping)\n            row_inds = list(row_tracks.keys())\n            n_rows = len(row_inds)\n\n        reduced_matrix = []\n        for row_track, row_instances in row_tracks.items():\n            for col_track, col_instances in col_tracks.items():\n\n                asso_matrix = self[row_instances, col_instances]\n\n                if col_dims == \"track\":\n                    asso_matrix = reduce_method(asso_matrix, axis=1)\n\n                if row_dims == \"track\":\n                    asso_matrix = reduce_method(asso_matrix, axis=0)\n\n                reduced_matrix.append(asso_matrix)\n\n        reduced_matrix = np.array(reduced_matrix).reshape(n_cols, n_rows).T\n\n        return pd.DataFrame(reduced_matrix, index=row_inds, columns=col_inds)\n\n    def __getitem__(\n        self, inds: tuple[int | Instance | list[int | Instance]]\n    ) -&gt; np.ndarray:\n        \"\"\"Get elements of the association matrix.\n\n        Args:\n            inds: A tuple of query indices and reference indices.\n                Indices can be either:\n                    A single instance or integer.\n                    A list of instances or integers.\n\n        Returns:\n            An np.ndarray containing the elements requested.\n        \"\"\"\n        query_inst, ref_inst = inds\n\n        query_ind = self.__getindices__(query_inst, self.query_instances)\n        ref_ind = self.__getindices__(ref_inst, self.ref_instances)\n\n        try:\n            return self.numpy()[query_ind[:, None], ref_ind].squeeze()\n        except IndexError as e:\n            logger.exception(f\"Query_insts: {type(query_inst)}\")\n            logger.exception(f\"Query_inds: {query_ind}\")\n            logger.exception(f\"Ref_insts: {type(ref_inst)}\")\n            logger.exception(f\"Ref_ind: {ref_ind}\")\n            logger.exception(e)\n            raise (e)\n\n    def __getindices__(\n        self,\n        instance: Instance | int | np.typing.ArrayLike,\n        instance_lookup: list[Instance],\n    ) -&gt; np.ndarray:\n        \"\"\"Get the indices of the instance for lookup.\n\n        Args:\n            instance: The instance(s) to be retrieved\n                Can either be a single int/instance or a list of int/instances\n            instance_lookup: A list of Instances to be used to retrieve indices\n\n        Returns:\n            A np array of indices.\n        \"\"\"\n        if isinstance(instance, Instance):\n            ind = np.array([instance_lookup.index(instance)])\n\n        elif instance is None:\n            ind = np.arange(len(instance_lookup))\n\n        elif np.isscalar(instance):\n            ind = np.array([instance])\n\n        else:\n            instances = instance\n            if not [isinstance(inst, (Instance, int)) for inst in instance]:\n                raise ValueError(\n                    f\"List of indices must be `int` or `Instance`. Found {set([type(inst) for inst in instance])}\"\n                )\n            ind = np.array(\n                [\n                    (\n                        instance_lookup.index(instance)\n                        if isinstance(instance, Instance)\n                        else instance\n                    )\n                    for instance in instances\n                ]\n            )\n\n        return ind\n\n    def get_tracks(\n        self, instances: list[\"Instance\"], label: str = \"pred\"\n    ) -&gt; dict[int, list[\"Instance\"]]:\n        \"\"\"Group instances by track.\n\n        Args:\n            instances: The list of instances to group\n            label: the track id type to group by. Either `pred` or `gt`.\n\n        Returns:\n            A dictionary of track_id:instances\n        \"\"\"\n        if label == \"pred\":\n            traj_ids = set([instance.pred_track_id.item() for instance in instances])\n            traj = {\n                track_id: [\n                    instance\n                    for instance in instances\n                    if instance.pred_track_id.item() == track_id\n                ]\n                for track_id in traj_ids\n            }\n\n        elif label == \"gt\":\n            traj_ids = set(\n                [instance.gt_track_id.item() for instance in self.ref_instances]\n            )\n            traj = {\n                track_id: [\n                    instance\n                    for instance in self.ref_instances\n                    if instance.gt_track_id.item() == track_id\n                ]\n                for track_id in traj_ids\n            }\n\n        else:\n            raise ValueError(f\"Unsupported label '{label}'. Expected 'pred' or 'gt'.\")\n\n        return traj\n</code></pre>"},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix.__getindices__","title":"<code>__getindices__(instance, instance_lookup)</code>","text":"<p>Get the indices of the instance for lookup.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>Instance | int | ArrayLike</code> <p>The instance(s) to be retrieved Can either be a single int/instance or a list of int/instances</p> required <code>instance_lookup</code> <code>list[Instance]</code> <p>A list of Instances to be used to retrieve indices</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A np array of indices.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __getindices__(\n    self,\n    instance: Instance | int | np.typing.ArrayLike,\n    instance_lookup: list[Instance],\n) -&gt; np.ndarray:\n    \"\"\"Get the indices of the instance for lookup.\n\n    Args:\n        instance: The instance(s) to be retrieved\n            Can either be a single int/instance or a list of int/instances\n        instance_lookup: A list of Instances to be used to retrieve indices\n\n    Returns:\n        A np array of indices.\n    \"\"\"\n    if isinstance(instance, Instance):\n        ind = np.array([instance_lookup.index(instance)])\n\n    elif instance is None:\n        ind = np.arange(len(instance_lookup))\n\n    elif np.isscalar(instance):\n        ind = np.array([instance])\n\n    else:\n        instances = instance\n        if not [isinstance(inst, (Instance, int)) for inst in instance]:\n            raise ValueError(\n                f\"List of indices must be `int` or `Instance`. Found {set([type(inst) for inst in instance])}\"\n            )\n        ind = np.array(\n            [\n                (\n                    instance_lookup.index(instance)\n                    if isinstance(instance, Instance)\n                    else instance\n                )\n                for instance in instances\n            ]\n        )\n\n    return ind\n</code></pre>"},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix.__getitem__","title":"<code>__getitem__(inds)</code>","text":"<p>Get elements of the association matrix.</p> <p>Parameters:</p> Name Type Description Default <code>inds</code> <code>tuple[int | Instance | list[int | Instance]]</code> <p>A tuple of query indices and reference indices. Indices can be either:     A single instance or integer.     A list of instances or integers.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>An np.ndarray containing the elements requested.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __getitem__(\n    self, inds: tuple[int | Instance | list[int | Instance]]\n) -&gt; np.ndarray:\n    \"\"\"Get elements of the association matrix.\n\n    Args:\n        inds: A tuple of query indices and reference indices.\n            Indices can be either:\n                A single instance or integer.\n                A list of instances or integers.\n\n    Returns:\n        An np.ndarray containing the elements requested.\n    \"\"\"\n    query_inst, ref_inst = inds\n\n    query_ind = self.__getindices__(query_inst, self.query_instances)\n    ref_ind = self.__getindices__(ref_inst, self.ref_instances)\n\n    try:\n        return self.numpy()[query_ind[:, None], ref_ind].squeeze()\n    except IndexError as e:\n        logger.exception(f\"Query_insts: {type(query_inst)}\")\n        logger.exception(f\"Query_inds: {query_ind}\")\n        logger.exception(f\"Ref_insts: {type(ref_inst)}\")\n        logger.exception(f\"Ref_ind: {ref_ind}\")\n        logger.exception(e)\n        raise (e)\n</code></pre>"},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix.__repr__","title":"<code>__repr__()</code>","text":"<p>Get the string representation of the Association Matrix.</p> <p>Returns:</p> Type Description <code>str</code> <p>the string representation of the association matrix.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get the string representation of the Association Matrix.\n\n    Returns:\n        the string representation of the association matrix.\n    \"\"\"\n    return (\n        f\"AssociationMatrix({self.matrix},\"\n        f\"query_instances={len(self.query_instances)},\"\n        f\"ref_instances={len(self.ref_instances)})\"\n    )\n</code></pre>"},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix.get_tracks","title":"<code>get_tracks(instances, label='pred')</code>","text":"<p>Group instances by track.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[Instance]</code> <p>The list of instances to group</p> required <code>label</code> <code>str</code> <p>the track id type to group by. Either <code>pred</code> or <code>gt</code>.</p> <code>'pred'</code> <p>Returns:</p> Type Description <code>dict[int, list[Instance]]</code> <p>A dictionary of track_id:instances</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def get_tracks(\n    self, instances: list[\"Instance\"], label: str = \"pred\"\n) -&gt; dict[int, list[\"Instance\"]]:\n    \"\"\"Group instances by track.\n\n    Args:\n        instances: The list of instances to group\n        label: the track id type to group by. Either `pred` or `gt`.\n\n    Returns:\n        A dictionary of track_id:instances\n    \"\"\"\n    if label == \"pred\":\n        traj_ids = set([instance.pred_track_id.item() for instance in instances])\n        traj = {\n            track_id: [\n                instance\n                for instance in instances\n                if instance.pred_track_id.item() == track_id\n            ]\n            for track_id in traj_ids\n        }\n\n    elif label == \"gt\":\n        traj_ids = set(\n            [instance.gt_track_id.item() for instance in self.ref_instances]\n        )\n        traj = {\n            track_id: [\n                instance\n                for instance in self.ref_instances\n                if instance.gt_track_id.item() == track_id\n            ]\n            for track_id in traj_ids\n        }\n\n    else:\n        raise ValueError(f\"Unsupported label '{label}'. Expected 'pred' or 'gt'.\")\n\n    return traj\n</code></pre>"},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix.numpy","title":"<code>numpy()</code>","text":"<p>Convert association matrix to a numpy array.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The association matrix as a numpy array.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def numpy(self) -&gt; np.ndarray:\n    \"\"\"Convert association matrix to a numpy array.\n\n    Returns:\n        The association matrix as a numpy array.\n    \"\"\"\n    if isinstance(self.matrix, torch.Tensor):\n        return self.matrix.detach().cpu().numpy()\n    return self.matrix\n</code></pre>"},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix.reduce","title":"<code>reduce(row_dims='instance', col_dims='track', row_grouping=None, col_grouping='pred', reduce_method=np.sum)</code>","text":"<p>Aggregate the association matrix by specified dimensions and grouping.</p> <p>Parameters:</p> Name Type Description Default <code>row_dims</code> <code>str</code> <p>A str indicating how to what dimensions to reduce rows to.   Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).</p> <code>'instance'</code> <code>col_dims</code> <code>str</code> <p>A str indicating how to dimensions to reduce rows to.   Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)</p> <code>'track'</code> <code>row_grouping</code> <code>str | None</code> <p>A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".</p> <code>None</code> <code>col_grouping</code> <code>str</code> <p>A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".</p> <code>'pred'</code> <code>reduce_method</code> <code>callable</code> <p>A callable function that operates on numpy matrices and can take an <code>axis</code> arg for reducing.</p> <code>sum</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def reduce(\n    self,\n    row_dims: str = \"instance\",\n    col_dims: str = \"track\",\n    row_grouping: str | None = None,\n    col_grouping: str = \"pred\",\n    reduce_method: callable = np.sum,\n) -&gt; pd.DataFrame:\n    \"\"\"Aggregate the association matrix by specified dimensions and grouping.\n\n    Args:\n       row_dims: A str indicating how to what dimensions to reduce rows to.\n            Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).\n       col_dims: A str indicating how to dimensions to reduce rows to.\n            Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)\n       row_grouping: A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".\n       col_grouping: A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".\n       reduce_method: A callable function that operates on numpy matrices and can take an `axis` arg for reducing.\n\n    Returns:\n        The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.\n    \"\"\"\n    n_rows = len(self.query_instances)\n    n_cols = len(self.ref_instances)\n\n    col_tracks = {-1: self.ref_instances}\n    row_tracks = {-1: self.query_instances}\n\n    col_inds = [i for i in range(len(self.ref_instances))]\n    row_inds = [i for i in range(len(self.query_instances))]\n\n    if col_dims == \"track\":\n        col_tracks = self.get_tracks(self.ref_instances, col_grouping)\n        col_inds = list(col_tracks.keys())\n        n_cols = len(col_inds)\n\n    if row_dims == \"track\":\n        row_tracks = self.get_tracks(self.query_instances, row_grouping)\n        row_inds = list(row_tracks.keys())\n        n_rows = len(row_inds)\n\n    reduced_matrix = []\n    for row_track, row_instances in row_tracks.items():\n        for col_track, col_instances in col_tracks.items():\n\n            asso_matrix = self[row_instances, col_instances]\n\n            if col_dims == \"track\":\n                asso_matrix = reduce_method(asso_matrix, axis=1)\n\n            if row_dims == \"track\":\n                asso_matrix = reduce_method(asso_matrix, axis=0)\n\n            reduced_matrix.append(asso_matrix)\n\n    reduced_matrix = np.array(reduced_matrix).reshape(n_cols, n_rows).T\n\n    return pd.DataFrame(reduced_matrix, index=row_inds, columns=col_inds)\n</code></pre>"},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix.to_dataframe","title":"<code>to_dataframe(row_labels='gt', col_labels='gt')</code>","text":"<p>Convert the association matrix to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>row_labels</code> <code>str</code> <p>How to label the rows(queries). If list, then must match # of rows/queries If <code>\"gt\"</code> then label by gt track id. If <code>\"pred\"</code> then label by pred track id. Otherwise label by the query_instance indices</p> <code>'gt'</code> <code>col_labels</code> <code>str</code> <p>How to label the columns(references). If list, then must match # of columns/refs If <code>\"gt\"</code> then label by gt track id. If <code>\"pred\"</code> then label by pred track id. Otherwise label by the ref_instance indices</p> <code>'gt'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The association matrix as a pandas dataframe.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def to_dataframe(\n    self, row_labels: str = \"gt\", col_labels: str = \"gt\"\n) -&gt; pd.DataFrame:\n    \"\"\"Convert the association matrix to a pandas DataFrame.\n\n    Args:\n        row_labels: How to label the rows(queries).\n            If list, then must match # of rows/queries\n            If `\"gt\"` then label by gt track id.\n            If `\"pred\"` then label by pred track id.\n            Otherwise label by the query_instance indices\n        col_labels: How to label the columns(references).\n            If list, then must match # of columns/refs\n            If `\"gt\"` then label by gt track id.\n            If `\"pred\"` then label by pred track id.\n            Otherwise label by the ref_instance indices\n\n    Returns:\n        The association matrix as a pandas dataframe.\n    \"\"\"\n    matrix = self.numpy()\n\n    if not isinstance(row_labels, str):\n        if len(row_labels) == len(self.query_instances):\n            row_inds = row_labels\n\n        else:\n            raise ValueError(\n                (\n                    f\"Mismatched # of rows and labels!\",\n                    f\"Found {len(row_labels)} with {len(self.query_instances)} rows\",\n                )\n            )\n\n    else:\n        if row_labels == \"gt\":\n            row_inds = [\n                instance.gt_track_id.item() for instance in self.query_instances\n            ]\n\n        elif row_labels == \"pred\":\n            row_inds = [\n                instance.pred_track_id.item() for instance in self.query_instances\n            ]\n\n        else:\n            row_inds = np.arange(len(self.query_instances))\n\n    if not isinstance(col_labels, str):\n        if len(col_labels) == len(self.ref_instances):\n            col_inds = col_labels\n\n        else:\n            raise ValueError(\n                (\n                    f\"Mismatched # of columns and labels!\",\n                    f\"Found {len(col_labels)} with {len(self.ref_instances)} columns\",\n                )\n            )\n\n    else:\n        if col_labels == \"gt\":\n            col_inds = [\n                instance.gt_track_id.item() for instance in self.ref_instances\n            ]\n\n        elif col_labels == \"pred\":\n            col_inds = [\n                instance.pred_track_id.item() for instance in self.ref_instances\n            ]\n\n        else:\n            col_inds = np.arange(len(self.ref_instances))\n\n    asso_df = pd.DataFrame(matrix, index=row_inds, columns=col_inds)\n\n    return asso_df\n</code></pre>"},{"location":"io/config/","title":"<code>Config</code>","text":""},{"location":"io/config/#dreem.io.Config","title":"<code>dreem.io.Config</code>","text":"<p>Class handling loading components based on config params.</p> Source code in <code>dreem/io/config.py</code> <pre><code>class Config:\n    \"\"\"Class handling loading components based on config params.\"\"\"\n\n    def __init__(self, cfg: DictConfig, params_cfg: DictConfig | None = None):\n        \"\"\"Initialize the class with config from hydra/omega conf.\n\n        First uses `base_param` file then overwrites with specific `params_config`.\n\n        Args:\n            cfg: The `DictConfig` containing all the hyperparameters needed for\n                training/evaluation.\n            params_cfg: The `DictConfig` containing subset of hyperparameters to override.\n                training/evaluation\n        \"\"\"\n        base_cfg = cfg\n        logger.info(f\"Base Config: {cfg}\")\n\n        if \"params_config\" in cfg:\n            params_cfg = OmegaConf.load(cfg.params_config)\n\n        if params_cfg:\n            logger.info(f\"Overwriting base config with {params_cfg}\")\n            with open_dict(base_cfg):\n                self.cfg = OmegaConf.merge(base_cfg, params_cfg)  # merge configs\n        else:\n            self.cfg = cfg\n\n    def __repr__(self):\n        \"\"\"Object representation of config class.\"\"\"\n        return f\"Config({self.cfg})\"\n\n    def __str__(self):\n        \"\"\"Return a string representation of config class.\"\"\"\n        return f\"Config({self.cfg})\"\n\n    @classmethod\n    def from_yaml(cls, base_cfg_path: str, params_cfg_path: str | None = None) -&gt; None:\n        \"\"\"Load config directly from yaml.\n\n        Args:\n            base_cfg_path: path to base config file.\n            params_cfg_path: path to override params.\n        \"\"\"\n        base_cfg = OmegaConf.load(base_cfg_path)\n        params_cfg = OmegaConf.load(params_cfg_path) if params_cfg else None\n        return cls(base_cfg, params_cfg)\n\n    def set_hparams(self, hparams: dict) -&gt; bool:\n        \"\"\"Setter function for overwriting specific hparams.\n\n        Useful for changing 1 or 2 hyperparameters such as dataset.\n\n        Args:\n            hparams: A dict containing the hyperparameter to be overwritten and\n                the value to be changed\n\n        Returns:\n            `True` if config is successfully updated, `False` otherwise\n        \"\"\"\n        if hparams == {} or hparams is None:\n            logger.warning(\"Nothing to update!\")\n            return False\n        for hparam, val in hparams.items():\n            try:\n                OmegaConf.update(self.cfg, hparam, val)\n            except Exception as e:\n                logger.exception(f\"Failed to update {hparam} to {val} due to {e}\")\n                return False\n        return True\n\n    def get_model(self) -&gt; \"GlobalTrackingTransformer\":\n        \"\"\"Getter for gtr model.\n\n        Returns:\n            A global tracking transformer with parameters indicated by cfg\n        \"\"\"\n        from dreem.models import GlobalTrackingTransformer\n\n        model_params = self.cfg.model\n        with open_dict(model_params):\n            ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n        if ckpt_path is not None and len(ckpt_path) &gt; 0:\n            return GTRRunner.load_from_checkpoint(ckpt_path).model\n\n        return GlobalTrackingTransformer(**model_params)\n\n    def get_tracker_cfg(self) -&gt; dict:\n        \"\"\"Getter for tracker config params.\n\n        Returns:\n            A dict containing the init params for `Tracker`.\n        \"\"\"\n        tracker_params = self.cfg.tracker\n        tracker_cfg = {}\n        for key, val in tracker_params.items():\n            tracker_cfg[key] = val\n        return tracker_cfg\n\n    def get_gtr_runner(self, ckpt_path=None) -&gt; \"GTRRunner\":\n        \"\"\"Get lightning module for training, validation, and inference.\n\n        Args:\n            ckpt_path: path to checkpoint for override\n\n        Returns:\n            a gtr runner model\n        \"\"\"\n        from dreem.models import GTRRunner\n\n        tracker_params = self.cfg.tracker\n        optimizer_params = self.cfg.optimizer\n        scheduler_params = self.cfg.scheduler\n        loss_params = self.cfg.loss\n        gtr_runner_params = self.cfg.runner\n        model_params = self.cfg.model\n\n        if ckpt_path is None:\n            with open_dict(model_params):\n                ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n        if ckpt_path is not None and ckpt_path != \"\":\n            model = GTRRunner.load_from_checkpoint(\n                ckpt_path,\n                tracker_cfg=tracker_params,\n                train_metrics=self.cfg.runner.metrics.train,\n                val_metrics=self.cfg.runner.metrics.val,\n                test_metrics=self.cfg.runner.metrics.test,\n                test_save_path=self.cfg.runner.save_path,\n            )\n\n        else:\n            model = GTRRunner(\n                model_params,\n                tracker_params,\n                loss_params,\n                optimizer_params,\n                scheduler_params,\n                **gtr_runner_params,\n            )\n\n        return model\n\n    def get_data_paths(self, data_cfg: dict) -&gt; tuple[list[str], list[str]]:\n        \"\"\"Get file paths from directory.\n\n        Args:\n            data_cfg: Config for the dataset containing \"dir\" key.\n\n        Returns:\n            lists of labels file paths and video file paths respectively\n        \"\"\"\n        with open_dict(data_cfg):\n            dir_cfg = data_cfg.pop(\"dir\", None)\n        label_files = vid_files = None\n        if dir_cfg:\n            labels_suff = dir_cfg.labels_suffix\n            vid_suff = dir_cfg.vid_suffix\n            labels_path = f\"{dir_cfg.path}/*{labels_suff}\"\n            vid_path = f\"{dir_cfg.path}/*{vid_suff}\"\n            logger.debug(f\"Searching for labels matching {labels_path}\")\n            label_files = glob.glob(labels_path)\n            logger.debug(f\"Searching for videos matching {vid_path}\")\n            vid_files = glob.glob(vid_path)\n            logger.debug(f\"Found {len(label_files)} labels and {len(vid_files)} videos\")\n\n        else:\n            if \"slp_files\" in data_cfg:\n                label_files = data_cfg.slp_files\n                vid_files = data_cfg.video_files\n            elif \"tracks\" in data_cfg or \"source\" in data_cfg:\n                label_files = data_cfg.tracks\n                vid_files = data_cfg.videos\n            elif \"raw_images\" in data_cfg:\n                label_files = data_cfg.gt_images\n                vid_files = data_cfg.raw_images\n\n        return label_files, vid_files\n\n    def get_dataset(\n        self,\n        mode: str,\n        label_files: list[str] | None = None,\n        vid_files: list[str | list[str]] = None,\n    ) -&gt; \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\":\n        \"\"\"Getter for datasets.\n\n        Args:\n            mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n                train, val, or test params for dataset\n            label_files: path to label_files for override\n            vid_files: path to vid_files for override\n\n        Returns:\n            Either a `SleapDataset` or `MicroscopyDataset` with params indicated by cfg\n        \"\"\"\n        from dreem.datasets import MicroscopyDataset, SleapDataset, CellTrackingDataset\n\n        if mode.lower() == \"train\":\n            dataset_params = self.cfg.dataset.train_dataset\n        elif mode.lower() == \"val\":\n            dataset_params = self.cfg.dataset.val_dataset\n        elif mode.lower() == \"test\":\n            dataset_params = self.cfg.dataset.test_dataset\n        else:\n            raise ValueError(\n                \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n            )\n        if label_files is None or vid_files is None:\n            with open_dict(dataset_params):\n                label_files, vid_files = self.get_data_paths(dataset_params)\n        # todo: handle this better\n        if \"slp_files\" in dataset_params:\n            if label_files is not None:\n                dataset_params.slp_files = label_files\n            if vid_files is not None:\n                dataset_params.video_files = vid_files\n            return SleapDataset(**dataset_params)\n\n        elif \"tracks\" in dataset_params or \"source\" in dataset_params:\n            if label_files is not None:\n                dataset_params.tracks = label_files\n            if vid_files is not None:\n                dataset_params.videos = vid_files\n            return MicroscopyDataset(**dataset_params)\n\n        elif \"raw_images\" in dataset_params:\n            if label_files is not None:\n                dataset_params.gt_images = label_files\n            if vid_files is not None:\n                dataset_params.raw_images = vid_files\n            return CellTrackingDataset(**dataset_params)\n\n        else:\n            raise ValueError(\n                \"Could not resolve dataset type from Config! Please include \\\n                either `slp_files` or `tracks`/`source`\"\n            )\n\n    def get_dataloader(\n        self,\n        dataset: \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\",\n        mode: str,\n    ) -&gt; torch.utils.data.DataLoader:\n        \"\"\"Getter for dataloader.\n\n        Args:\n            dataset: the Sleap or Microscopy Dataset used to initialize the dataloader\n            mode: either [\"train\", \"val\", or \"test\"] indicates which dataset\n                config to use\n\n        Returns:\n            A torch dataloader for `dataset` with parameters configured as specified\n        \"\"\"\n        if mode.lower() == \"train\":\n            dataloader_params = self.cfg.dataloader.train_dataloader\n        elif mode.lower() == \"val\":\n            dataloader_params = self.cfg.dataloader.val_dataloader\n        elif mode.lower() == \"test\":\n            dataloader_params = self.cfg.dataloader.test_dataloader\n        else:\n            raise ValueError(\n                \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n            )\n        if dataloader_params.num_workers &gt; 0:\n            # prevent too many open files error\n            pin_memory = True\n            torch.multiprocessing.set_sharing_strategy(\"file_system\")\n        else:\n            pin_memory = False\n\n        return torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size=1,\n            pin_memory=pin_memory,\n            collate_fn=dataset.no_batching_fn,\n            **dataloader_params,\n        )\n\n    def get_optimizer(self, params: Iterable) -&gt; torch.optim.Optimizer:\n        \"\"\"Getter for optimizer.\n\n        Args:\n            params: iterable of model parameters to optimize or dicts defining\n                parameter groups\n\n        Returns:\n            A torch Optimizer with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_optimizer\n\n        optimizer_params = self.cfg.optimizer\n\n        return init_optimizer(params, optimizer_params)\n\n    def get_scheduler(\n        self, optimizer: torch.optim.Optimizer\n    ) -&gt; torch.optim.lr_scheduler.LRScheduler:\n        \"\"\"Getter for lr scheduler.\n\n        Args:\n            optimizer: The optimizer to wrap the scheduler around\n\n        Returns:\n            A torch learning rate scheduler with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_scheduler\n\n        lr_scheduler_params = self.cfg.scheduler\n\n        return init_scheduler(optimizer, lr_scheduler_params)\n\n    def get_loss(self) -&gt; \"dreem.training.losses.AssoLoss\":\n        \"\"\"Getter for loss functions.\n\n        Returns:\n            An AssoLoss with specified params\n        \"\"\"\n        from dreem.training.losses import AssoLoss\n\n        loss_params = self.cfg.loss\n\n        return AssoLoss(**loss_params)\n\n    def get_logger(self) -&gt; pl.loggers.Logger:\n        \"\"\"Getter for logging callback.\n\n        Returns:\n            A Logger with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_logger\n\n        logger_params = OmegaConf.to_container(self.cfg.logging, resolve=True)\n\n        return init_logger(\n            logger_params, OmegaConf.to_container(self.cfg, resolve=True)\n        )\n\n    def get_early_stopping(self) -&gt; pl.callbacks.EarlyStopping:\n        \"\"\"Getter for lightning early stopping callback.\n\n        Returns:\n            A lightning early stopping callback with specified params\n        \"\"\"\n        early_stopping_params = self.cfg.early_stopping\n        return pl.callbacks.EarlyStopping(**early_stopping_params)\n\n    def get_checkpointing(self) -&gt; pl.callbacks.ModelCheckpoint:\n        \"\"\"Getter for lightning checkpointing callback.\n\n        Returns:\n            A lightning checkpointing callback with specified params\n        \"\"\"\n        # convert to dict to enable extracting/removing params\n        checkpoint_params = self.cfg.checkpointing\n        logging_params = self.cfg.logging\n        dirpath = checkpoint_params.pop(\"dirpath\", None)\n        if dirpath is None:\n            if \"group\" in logging_params:\n                dirpath = f\"./models/{logging_params.group}/{logging_params.name}\"\n            else:\n                dirpath = f\"./models/{logging_params.name}\"\n\n        dirpath = Path(dirpath).resolve()\n        if not Path(dirpath).exists():\n            try:\n                Path(dirpath).mkdir(parents=True, exist_ok=True)\n            except OSError as e:\n                logger.exception(\n                    f\"Cannot create a new folder. Check the permissions to the given Checkpoint directory. \\n {e}\"\n                )\n        with open_dict(checkpoint_params):\n            _ = checkpoint_params.pop(\"dirpath\", None)\n            monitor = checkpoint_params.pop(\"monitor\", [\"val_loss\"])\n        checkpointers = []\n\n        for metric in monitor:\n            checkpointer = pl.callbacks.ModelCheckpoint(\n                monitor=metric,\n                dirpath=dirpath,\n                filename=f\"{{epoch}}-{{{metric}}}\",\n                **checkpoint_params,\n            )\n            checkpointer.CHECKPOINT_NAME_LAST = f\"{{epoch}}-best-{{{metric}}}\"\n            checkpointers.append(checkpointer)\n        return checkpointers\n\n    def get_trainer(\n        self,\n        callbacks: list[pl.callbacks.Callback] | None = None,\n        logger: pl.loggers.WandbLogger | None = None,\n        devices: int = 1,\n        accelerator: str = \"auto\",\n    ) -&gt; pl.Trainer:\n        \"\"\"Getter for the lightning trainer.\n\n        Args:\n            callbacks: a list of lightning callbacks preconfigured to be used\n                for training\n            logger: the Wandb logger used for logging during training\n            devices: The number of gpus to be used. 0 means cpu\n            accelerator: either \"gpu\" or \"cpu\" specifies which device to use\n\n        Returns:\n            A lightning Trainer with specified params\n        \"\"\"\n        if \"trainer\" in self.cfg:\n            trainer_params = OmegaConf.to_container(self.cfg.trainer, resolve=True)\n\n        else:\n            trainer_params = {}\n\n        profiler = trainer_params.pop(\"profiler\", None)\n        if \"accelerator\" not in trainer_params:\n            trainer_params[\"accelerator\"] = accelerator\n        if \"devices\" not in trainer_params:\n            trainer_params[\"devices\"] = devices\n\n        if \"profiler\":\n            profiler = pl.profilers.AdvancedProfiler(filename=\"profile.txt\")\n        else:\n            profiler = None\n\n        return pl.Trainer(\n            callbacks=callbacks,\n            logger=logger,\n            profiler=profiler,\n            **trainer_params,\n        )\n</code></pre>"},{"location":"io/config/#dreem.io.Config.__init__","title":"<code>__init__(cfg, params_cfg=None)</code>","text":"<p>Initialize the class with config from hydra/omega conf.</p> <p>First uses <code>base_param</code> file then overwrites with specific <code>params_config</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>The <code>DictConfig</code> containing all the hyperparameters needed for training/evaluation.</p> required <code>params_cfg</code> <code>DictConfig | None</code> <p>The <code>DictConfig</code> containing subset of hyperparameters to override. training/evaluation</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>def __init__(self, cfg: DictConfig, params_cfg: DictConfig | None = None):\n    \"\"\"Initialize the class with config from hydra/omega conf.\n\n    First uses `base_param` file then overwrites with specific `params_config`.\n\n    Args:\n        cfg: The `DictConfig` containing all the hyperparameters needed for\n            training/evaluation.\n        params_cfg: The `DictConfig` containing subset of hyperparameters to override.\n            training/evaluation\n    \"\"\"\n    base_cfg = cfg\n    logger.info(f\"Base Config: {cfg}\")\n\n    if \"params_config\" in cfg:\n        params_cfg = OmegaConf.load(cfg.params_config)\n\n    if params_cfg:\n        logger.info(f\"Overwriting base config with {params_cfg}\")\n        with open_dict(base_cfg):\n            self.cfg = OmegaConf.merge(base_cfg, params_cfg)  # merge configs\n    else:\n        self.cfg = cfg\n</code></pre>"},{"location":"io/config/#dreem.io.Config.__repr__","title":"<code>__repr__()</code>","text":"<p>Object representation of config class.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def __repr__(self):\n    \"\"\"Object representation of config class.\"\"\"\n    return f\"Config({self.cfg})\"\n</code></pre>"},{"location":"io/config/#dreem.io.Config.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of config class.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def __str__(self):\n    \"\"\"Return a string representation of config class.\"\"\"\n    return f\"Config({self.cfg})\"\n</code></pre>"},{"location":"io/config/#dreem.io.Config.from_yaml","title":"<code>from_yaml(base_cfg_path, params_cfg_path=None)</code>  <code>classmethod</code>","text":"<p>Load config directly from yaml.</p> <p>Parameters:</p> Name Type Description Default <code>base_cfg_path</code> <code>str</code> <p>path to base config file.</p> required <code>params_cfg_path</code> <code>str | None</code> <p>path to override params.</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>@classmethod\ndef from_yaml(cls, base_cfg_path: str, params_cfg_path: str | None = None) -&gt; None:\n    \"\"\"Load config directly from yaml.\n\n    Args:\n        base_cfg_path: path to base config file.\n        params_cfg_path: path to override params.\n    \"\"\"\n    base_cfg = OmegaConf.load(base_cfg_path)\n    params_cfg = OmegaConf.load(params_cfg_path) if params_cfg else None\n    return cls(base_cfg, params_cfg)\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_checkpointing","title":"<code>get_checkpointing()</code>","text":"<p>Getter for lightning checkpointing callback.</p> <p>Returns:</p> Type Description <code>ModelCheckpoint</code> <p>A lightning checkpointing callback with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_checkpointing(self) -&gt; pl.callbacks.ModelCheckpoint:\n    \"\"\"Getter for lightning checkpointing callback.\n\n    Returns:\n        A lightning checkpointing callback with specified params\n    \"\"\"\n    # convert to dict to enable extracting/removing params\n    checkpoint_params = self.cfg.checkpointing\n    logging_params = self.cfg.logging\n    dirpath = checkpoint_params.pop(\"dirpath\", None)\n    if dirpath is None:\n        if \"group\" in logging_params:\n            dirpath = f\"./models/{logging_params.group}/{logging_params.name}\"\n        else:\n            dirpath = f\"./models/{logging_params.name}\"\n\n    dirpath = Path(dirpath).resolve()\n    if not Path(dirpath).exists():\n        try:\n            Path(dirpath).mkdir(parents=True, exist_ok=True)\n        except OSError as e:\n            logger.exception(\n                f\"Cannot create a new folder. Check the permissions to the given Checkpoint directory. \\n {e}\"\n            )\n    with open_dict(checkpoint_params):\n        _ = checkpoint_params.pop(\"dirpath\", None)\n        monitor = checkpoint_params.pop(\"monitor\", [\"val_loss\"])\n    checkpointers = []\n\n    for metric in monitor:\n        checkpointer = pl.callbacks.ModelCheckpoint(\n            monitor=metric,\n            dirpath=dirpath,\n            filename=f\"{{epoch}}-{{{metric}}}\",\n            **checkpoint_params,\n        )\n        checkpointer.CHECKPOINT_NAME_LAST = f\"{{epoch}}-best-{{{metric}}}\"\n        checkpointers.append(checkpointer)\n    return checkpointers\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_data_paths","title":"<code>get_data_paths(data_cfg)</code>","text":"<p>Get file paths from directory.</p> <p>Parameters:</p> Name Type Description Default <code>data_cfg</code> <code>dict</code> <p>Config for the dataset containing \"dir\" key.</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str]]</code> <p>lists of labels file paths and video file paths respectively</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_data_paths(self, data_cfg: dict) -&gt; tuple[list[str], list[str]]:\n    \"\"\"Get file paths from directory.\n\n    Args:\n        data_cfg: Config for the dataset containing \"dir\" key.\n\n    Returns:\n        lists of labels file paths and video file paths respectively\n    \"\"\"\n    with open_dict(data_cfg):\n        dir_cfg = data_cfg.pop(\"dir\", None)\n    label_files = vid_files = None\n    if dir_cfg:\n        labels_suff = dir_cfg.labels_suffix\n        vid_suff = dir_cfg.vid_suffix\n        labels_path = f\"{dir_cfg.path}/*{labels_suff}\"\n        vid_path = f\"{dir_cfg.path}/*{vid_suff}\"\n        logger.debug(f\"Searching for labels matching {labels_path}\")\n        label_files = glob.glob(labels_path)\n        logger.debug(f\"Searching for videos matching {vid_path}\")\n        vid_files = glob.glob(vid_path)\n        logger.debug(f\"Found {len(label_files)} labels and {len(vid_files)} videos\")\n\n    else:\n        if \"slp_files\" in data_cfg:\n            label_files = data_cfg.slp_files\n            vid_files = data_cfg.video_files\n        elif \"tracks\" in data_cfg or \"source\" in data_cfg:\n            label_files = data_cfg.tracks\n            vid_files = data_cfg.videos\n        elif \"raw_images\" in data_cfg:\n            label_files = data_cfg.gt_images\n            vid_files = data_cfg.raw_images\n\n    return label_files, vid_files\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_dataloader","title":"<code>get_dataloader(dataset, mode)</code>","text":"<p>Getter for dataloader.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>'SleapDataset' | 'MicroscopyDataset' | 'CellTrackingDataset'</code> <p>the Sleap or Microscopy Dataset used to initialize the dataloader</p> required <code>mode</code> <code>str</code> <p>either [\"train\", \"val\", or \"test\"] indicates which dataset config to use</p> required <p>Returns:</p> Type Description <code>DataLoader</code> <p>A torch dataloader for <code>dataset</code> with parameters configured as specified</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_dataloader(\n    self,\n    dataset: \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\",\n    mode: str,\n) -&gt; torch.utils.data.DataLoader:\n    \"\"\"Getter for dataloader.\n\n    Args:\n        dataset: the Sleap or Microscopy Dataset used to initialize the dataloader\n        mode: either [\"train\", \"val\", or \"test\"] indicates which dataset\n            config to use\n\n    Returns:\n        A torch dataloader for `dataset` with parameters configured as specified\n    \"\"\"\n    if mode.lower() == \"train\":\n        dataloader_params = self.cfg.dataloader.train_dataloader\n    elif mode.lower() == \"val\":\n        dataloader_params = self.cfg.dataloader.val_dataloader\n    elif mode.lower() == \"test\":\n        dataloader_params = self.cfg.dataloader.test_dataloader\n    else:\n        raise ValueError(\n            \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n        )\n    if dataloader_params.num_workers &gt; 0:\n        # prevent too many open files error\n        pin_memory = True\n        torch.multiprocessing.set_sharing_strategy(\"file_system\")\n    else:\n        pin_memory = False\n\n    return torch.utils.data.DataLoader(\n        dataset=dataset,\n        batch_size=1,\n        pin_memory=pin_memory,\n        collate_fn=dataset.no_batching_fn,\n        **dataloader_params,\n    )\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_dataset","title":"<code>get_dataset(mode, label_files=None, vid_files=None)</code>","text":"<p>Getter for datasets.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>[None, \"train\", \"test\", \"val\"]. Indicates whether to use train, val, or test params for dataset</p> required <code>label_files</code> <code>list[str] | None</code> <p>path to label_files for override</p> <code>None</code> <code>vid_files</code> <code>list[str | list[str]]</code> <p>path to vid_files for override</p> <code>None</code> <p>Returns:</p> Type Description <code>'SleapDataset' | 'MicroscopyDataset' | 'CellTrackingDataset'</code> <p>Either a <code>SleapDataset</code> or <code>MicroscopyDataset</code> with params indicated by cfg</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_dataset(\n    self,\n    mode: str,\n    label_files: list[str] | None = None,\n    vid_files: list[str | list[str]] = None,\n) -&gt; \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\":\n    \"\"\"Getter for datasets.\n\n    Args:\n        mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n            train, val, or test params for dataset\n        label_files: path to label_files for override\n        vid_files: path to vid_files for override\n\n    Returns:\n        Either a `SleapDataset` or `MicroscopyDataset` with params indicated by cfg\n    \"\"\"\n    from dreem.datasets import MicroscopyDataset, SleapDataset, CellTrackingDataset\n\n    if mode.lower() == \"train\":\n        dataset_params = self.cfg.dataset.train_dataset\n    elif mode.lower() == \"val\":\n        dataset_params = self.cfg.dataset.val_dataset\n    elif mode.lower() == \"test\":\n        dataset_params = self.cfg.dataset.test_dataset\n    else:\n        raise ValueError(\n            \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n        )\n    if label_files is None or vid_files is None:\n        with open_dict(dataset_params):\n            label_files, vid_files = self.get_data_paths(dataset_params)\n    # todo: handle this better\n    if \"slp_files\" in dataset_params:\n        if label_files is not None:\n            dataset_params.slp_files = label_files\n        if vid_files is not None:\n            dataset_params.video_files = vid_files\n        return SleapDataset(**dataset_params)\n\n    elif \"tracks\" in dataset_params or \"source\" in dataset_params:\n        if label_files is not None:\n            dataset_params.tracks = label_files\n        if vid_files is not None:\n            dataset_params.videos = vid_files\n        return MicroscopyDataset(**dataset_params)\n\n    elif \"raw_images\" in dataset_params:\n        if label_files is not None:\n            dataset_params.gt_images = label_files\n        if vid_files is not None:\n            dataset_params.raw_images = vid_files\n        return CellTrackingDataset(**dataset_params)\n\n    else:\n        raise ValueError(\n            \"Could not resolve dataset type from Config! Please include \\\n            either `slp_files` or `tracks`/`source`\"\n        )\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_early_stopping","title":"<code>get_early_stopping()</code>","text":"<p>Getter for lightning early stopping callback.</p> <p>Returns:</p> Type Description <code>EarlyStopping</code> <p>A lightning early stopping callback with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_early_stopping(self) -&gt; pl.callbacks.EarlyStopping:\n    \"\"\"Getter for lightning early stopping callback.\n\n    Returns:\n        A lightning early stopping callback with specified params\n    \"\"\"\n    early_stopping_params = self.cfg.early_stopping\n    return pl.callbacks.EarlyStopping(**early_stopping_params)\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_gtr_runner","title":"<code>get_gtr_runner(ckpt_path=None)</code>","text":"<p>Get lightning module for training, validation, and inference.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_path</code> <p>path to checkpoint for override</p> <code>None</code> <p>Returns:</p> Type Description <code>'GTRRunner'</code> <p>a gtr runner model</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_gtr_runner(self, ckpt_path=None) -&gt; \"GTRRunner\":\n    \"\"\"Get lightning module for training, validation, and inference.\n\n    Args:\n        ckpt_path: path to checkpoint for override\n\n    Returns:\n        a gtr runner model\n    \"\"\"\n    from dreem.models import GTRRunner\n\n    tracker_params = self.cfg.tracker\n    optimizer_params = self.cfg.optimizer\n    scheduler_params = self.cfg.scheduler\n    loss_params = self.cfg.loss\n    gtr_runner_params = self.cfg.runner\n    model_params = self.cfg.model\n\n    if ckpt_path is None:\n        with open_dict(model_params):\n            ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n    if ckpt_path is not None and ckpt_path != \"\":\n        model = GTRRunner.load_from_checkpoint(\n            ckpt_path,\n            tracker_cfg=tracker_params,\n            train_metrics=self.cfg.runner.metrics.train,\n            val_metrics=self.cfg.runner.metrics.val,\n            test_metrics=self.cfg.runner.metrics.test,\n            test_save_path=self.cfg.runner.save_path,\n        )\n\n    else:\n        model = GTRRunner(\n            model_params,\n            tracker_params,\n            loss_params,\n            optimizer_params,\n            scheduler_params,\n            **gtr_runner_params,\n        )\n\n    return model\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_logger","title":"<code>get_logger()</code>","text":"<p>Getter for logging callback.</p> <p>Returns:</p> Type Description <code>Logger</code> <p>A Logger with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_logger(self) -&gt; pl.loggers.Logger:\n    \"\"\"Getter for logging callback.\n\n    Returns:\n        A Logger with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_logger\n\n    logger_params = OmegaConf.to_container(self.cfg.logging, resolve=True)\n\n    return init_logger(\n        logger_params, OmegaConf.to_container(self.cfg, resolve=True)\n    )\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_loss","title":"<code>get_loss()</code>","text":"<p>Getter for loss functions.</p> <p>Returns:</p> Type Description <code>'dreem.training.losses.AssoLoss'</code> <p>An AssoLoss with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_loss(self) -&gt; \"dreem.training.losses.AssoLoss\":\n    \"\"\"Getter for loss functions.\n\n    Returns:\n        An AssoLoss with specified params\n    \"\"\"\n    from dreem.training.losses import AssoLoss\n\n    loss_params = self.cfg.loss\n\n    return AssoLoss(**loss_params)\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_model","title":"<code>get_model()</code>","text":"<p>Getter for gtr model.</p> <p>Returns:</p> Type Description <code>'GlobalTrackingTransformer'</code> <p>A global tracking transformer with parameters indicated by cfg</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_model(self) -&gt; \"GlobalTrackingTransformer\":\n    \"\"\"Getter for gtr model.\n\n    Returns:\n        A global tracking transformer with parameters indicated by cfg\n    \"\"\"\n    from dreem.models import GlobalTrackingTransformer\n\n    model_params = self.cfg.model\n    with open_dict(model_params):\n        ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n    if ckpt_path is not None and len(ckpt_path) &gt; 0:\n        return GTRRunner.load_from_checkpoint(ckpt_path).model\n\n    return GlobalTrackingTransformer(**model_params)\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_optimizer","title":"<code>get_optimizer(params)</code>","text":"<p>Getter for optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable</code> <p>iterable of model parameters to optimize or dicts defining parameter groups</p> required <p>Returns:</p> Type Description <code>Optimizer</code> <p>A torch Optimizer with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_optimizer(self, params: Iterable) -&gt; torch.optim.Optimizer:\n    \"\"\"Getter for optimizer.\n\n    Args:\n        params: iterable of model parameters to optimize or dicts defining\n            parameter groups\n\n    Returns:\n        A torch Optimizer with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_optimizer\n\n    optimizer_params = self.cfg.optimizer\n\n    return init_optimizer(params, optimizer_params)\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_scheduler","title":"<code>get_scheduler(optimizer)</code>","text":"<p>Getter for lr scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to wrap the scheduler around</p> required <p>Returns:</p> Type Description <code>LRScheduler</code> <p>A torch learning rate scheduler with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_scheduler(\n    self, optimizer: torch.optim.Optimizer\n) -&gt; torch.optim.lr_scheduler.LRScheduler:\n    \"\"\"Getter for lr scheduler.\n\n    Args:\n        optimizer: The optimizer to wrap the scheduler around\n\n    Returns:\n        A torch learning rate scheduler with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_scheduler\n\n    lr_scheduler_params = self.cfg.scheduler\n\n    return init_scheduler(optimizer, lr_scheduler_params)\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_tracker_cfg","title":"<code>get_tracker_cfg()</code>","text":"<p>Getter for tracker config params.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict containing the init params for <code>Tracker</code>.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_tracker_cfg(self) -&gt; dict:\n    \"\"\"Getter for tracker config params.\n\n    Returns:\n        A dict containing the init params for `Tracker`.\n    \"\"\"\n    tracker_params = self.cfg.tracker\n    tracker_cfg = {}\n    for key, val in tracker_params.items():\n        tracker_cfg[key] = val\n    return tracker_cfg\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_trainer","title":"<code>get_trainer(callbacks=None, logger=None, devices=1, accelerator='auto')</code>","text":"<p>Getter for the lightning trainer.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[Callback] | None</code> <p>a list of lightning callbacks preconfigured to be used for training</p> <code>None</code> <code>logger</code> <code>WandbLogger | None</code> <p>the Wandb logger used for logging during training</p> <code>None</code> <code>devices</code> <code>int</code> <p>The number of gpus to be used. 0 means cpu</p> <code>1</code> <code>accelerator</code> <code>str</code> <p>either \"gpu\" or \"cpu\" specifies which device to use</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>Trainer</code> <p>A lightning Trainer with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_trainer(\n    self,\n    callbacks: list[pl.callbacks.Callback] | None = None,\n    logger: pl.loggers.WandbLogger | None = None,\n    devices: int = 1,\n    accelerator: str = \"auto\",\n) -&gt; pl.Trainer:\n    \"\"\"Getter for the lightning trainer.\n\n    Args:\n        callbacks: a list of lightning callbacks preconfigured to be used\n            for training\n        logger: the Wandb logger used for logging during training\n        devices: The number of gpus to be used. 0 means cpu\n        accelerator: either \"gpu\" or \"cpu\" specifies which device to use\n\n    Returns:\n        A lightning Trainer with specified params\n    \"\"\"\n    if \"trainer\" in self.cfg:\n        trainer_params = OmegaConf.to_container(self.cfg.trainer, resolve=True)\n\n    else:\n        trainer_params = {}\n\n    profiler = trainer_params.pop(\"profiler\", None)\n    if \"accelerator\" not in trainer_params:\n        trainer_params[\"accelerator\"] = accelerator\n    if \"devices\" not in trainer_params:\n        trainer_params[\"devices\"] = devices\n\n    if \"profiler\":\n        profiler = pl.profilers.AdvancedProfiler(filename=\"profile.txt\")\n    else:\n        profiler = None\n\n    return pl.Trainer(\n        callbacks=callbacks,\n        logger=logger,\n        profiler=profiler,\n        **trainer_params,\n    )\n</code></pre>"},{"location":"io/config/#dreem.io.Config.set_hparams","title":"<code>set_hparams(hparams)</code>","text":"<p>Setter function for overwriting specific hparams.</p> <p>Useful for changing 1 or 2 hyperparameters such as dataset.</p> <p>Parameters:</p> Name Type Description Default <code>hparams</code> <code>dict</code> <p>A dict containing the hyperparameter to be overwritten and the value to be changed</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if config is successfully updated, <code>False</code> otherwise</p> Source code in <code>dreem/io/config.py</code> <pre><code>def set_hparams(self, hparams: dict) -&gt; bool:\n    \"\"\"Setter function for overwriting specific hparams.\n\n    Useful for changing 1 or 2 hyperparameters such as dataset.\n\n    Args:\n        hparams: A dict containing the hyperparameter to be overwritten and\n            the value to be changed\n\n    Returns:\n        `True` if config is successfully updated, `False` otherwise\n    \"\"\"\n    if hparams == {} or hparams is None:\n        logger.warning(\"Nothing to update!\")\n        return False\n    for hparam, val in hparams.items():\n        try:\n            OmegaConf.update(self.cfg, hparam, val)\n        except Exception as e:\n            logger.exception(f\"Failed to update {hparam} to {val} due to {e}\")\n            return False\n    return True\n</code></pre>"},{"location":"io/frame/","title":"<code>Frame</code>","text":""},{"location":"io/frame/#dreem.io.Frame","title":"<code>dreem.io.Frame</code>","text":"<p>Data structure containing metadata for a single frame of a video.</p> <p>Attributes:</p> Name Type Description <code>video_id</code> <code>Tensor</code> <p>The video index in the dataset.</p> <code>frame_id</code> <code>Tensor</code> <p>The index of the frame in a video.</p> <code>vid_file</code> <code>Tensor</code> <p>The path to the video the frame is from.</p> <code>img_shape</code> <code>Tensor</code> <p>The shape of the original frame (not the crop).</p> <code>instances</code> <code>list['Instance']</code> <p>A list of Instance objects that appear in the frame.</p> <code>asso_output</code> <code>'AssociationMatrix'</code> <p>The association matrix between instances output directly from the transformer.</p> <code>matches</code> <code>tuple</code> <p>matches from LSA algorithm between the instances and available trajectories during tracking.</p> <code>traj_score</code> <code>tuple</code> <p>Either a dict containing the association matrix between instances and trajectories along postprocessing pipeline or a single association matrix.</p> <code>device</code> <code>str</code> <p>The device the frame should be moved to.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>@attrs.define(eq=False)\nclass Frame:\n    \"\"\"Data structure containing metadata for a single frame of a video.\n\n    Attributes:\n        video_id: The video index in the dataset.\n        frame_id: The index of the frame in a video.\n        vid_file: The path to the video the frame is from.\n        img_shape: The shape of the original frame (not the crop).\n        instances: A list of Instance objects that appear in the frame.\n        asso_output: The association matrix between instances\n            output directly from the transformer.\n        matches: matches from LSA algorithm between the instances and\n            available trajectories during tracking.\n        traj_score: Either a dict containing the association matrix\n            between instances and trajectories along postprocessing pipeline\n            or a single association matrix.\n        device: The device the frame should be moved to.\n    \"\"\"\n\n    _video_id: int = attrs.field(alias=\"video_id\", converter=_to_tensor)\n    _frame_id: int = attrs.field(alias=\"frame_id\", converter=_to_tensor)\n    _video: str = attrs.field(alias=\"vid_file\", default=\"\")\n    _img_shape: ArrayLike = attrs.field(\n        alias=\"img_shape\", converter=_to_tensor, factory=list\n    )\n\n    _instances: list[\"Instance\"] = attrs.field(alias=\"instances\", factory=list)\n    _asso_output: \"AssociationMatrix\" | None = attrs.field(\n        alias=\"asso_output\", default=None\n    )\n    _matches: tuple = attrs.field(alias=\"matches\", factory=tuple)\n    _traj_score: dict = attrs.field(alias=\"traj_score\", factory=dict)\n    _device: str | torch.device | None = attrs.field(alias=\"device\", default=None)\n\n    def __attrs_post_init__(self) -&gt; None:\n        \"\"\"Handle more intricate default initializations and moving to device.\"\"\"\n        if len(self.img_shape) == 0:\n            self.img_shape = torch.tensor([0, 0, 0])\n\n        for instance in self.instances:\n            instance.frame = self\n\n        self.to(self.device)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return String representation of the Frame.\n\n        Returns:\n            The string representation of the frame.\n        \"\"\"\n        return (\n            \"Frame(\"\n            f\"video={self._video.filename if isinstance(self._video, sio.Video) else self._video}, \"\n            f\"video_id={self._video_id.item()}, \"\n            f\"frame_id={self._frame_id.item()}, \"\n            f\"img_shape={self._img_shape}, \"\n            f\"num_detected={self.num_detected}, \"\n            f\"asso_output={self._asso_output}, \"\n            f\"traj_score={self._traj_score}, \"\n            f\"matches={self._matches}, \"\n            f\"instances={self._instances}, \"\n            f\"device={self._device}\"\n            \")\"\n        )\n\n    def to(self, map_location: str | torch.device) -&gt; Self:\n        \"\"\"Move frame to different device or dtype (See `torch.to` for more info).\n\n        Args:\n            map_location: A string representing the device to move to.\n\n        Returns:\n            The frame moved to a different device/dtype.\n        \"\"\"\n        self._video_id = self._video_id.to(map_location)\n        self._frame_id = self._frame_id.to(map_location)\n        self._img_shape = self._img_shape.to(map_location)\n\n        if isinstance(self._asso_output, torch.Tensor):\n            self._asso_output = self._asso_output.to(map_location)\n\n        if isinstance(self._matches, torch.Tensor):\n            self._matches = self._matches.to(map_location)\n\n        for key, val in self._traj_score.items():\n            if isinstance(val, torch.Tensor):\n                self._traj_score[key] = val.to(map_location)\n        for instance in self.instances:\n            instance = instance.to(map_location)\n\n        if isinstance(map_location, (str, torch.device)):\n            self._device = map_location\n\n        return self\n\n    @classmethod\n    def from_slp(\n        cls,\n        lf: sio.LabeledFrame,\n        video_id: int = 0,\n        device: str | None = None,\n        **kwargs,\n    ) -&gt; Self:\n        \"\"\"Convert `sio.LabeledFrame` to `dreem.io.Frame`.\n\n        Args:\n            lf: A sio.LabeledFrame object\n\n        Returns:\n            A dreem.io.Frame object\n        \"\"\"\n        from dreem.io import Instance\n\n        img_shape = lf.image.shape\n        if len(img_shape) == 2:\n            img_shape = (1, *img_shape)\n        elif len(img_shape) &gt; 2 and img_shape[-1] &lt;= 3:\n            img_shape = (lf.image.shape[-1], lf.image.shape[0], lf.image.shape[1])\n        return cls(\n            video_id=video_id,\n            frame_id=(\n                lf.frame_idx.astype(np.int32)\n                if isinstance(lf.frame_idx, np.number)\n                else lf.frame_idx\n            ),\n            vid_file=lf.video.filename,\n            img_shape=img_shape,\n            instances=[Instance.from_slp(instance, **kwargs) for instance in lf],\n            device=device,\n        )\n\n    def to_slp(\n        self,\n        track_lookup: dict[int, sio.Track] | None = None,\n        video: sio.Video | None = None,\n    ) -&gt; tuple[sio.LabeledFrame, dict[int, sio.Track]]:\n        \"\"\"Convert Frame to sleap_io.LabeledFrame object.\n\n        Args:\n            track_lookup: A lookup dictionary containing the track_id and sio.Track for persistence\n            video: An sio.Video object used for overriding.\n\n        Returns: A tuple containing a LabeledFrame object with necessary metadata and\n        a lookup dictionary containing the track_id and sio.Track for persistence\n        \"\"\"\n        if track_lookup is None:\n            track_lookup = {}\n\n        slp_instances = []\n        for instance in self.instances:\n            slp_instance, track_lookup = instance.to_slp(track_lookup=track_lookup)\n            slp_instances.append(slp_instance)\n\n        if video is None:\n            video = (\n                self.video\n                if isinstance(self.video, sio.Video)\n                else sio.load_video(self.video)\n            )\n\n        return (\n            sio.LabeledFrame(\n                video=video,\n                frame_idx=self.frame_id.item(),\n                instances=slp_instances,\n            ),\n            track_lookup,\n        )\n\n    def to_h5(\n        self,\n        clip_group: h5py.Group,\n        instance_labels: list | None = None,\n        save: dict[str, bool] | None = None,\n    ) -&gt; h5py.Group:\n        \"\"\"Convert frame to h5py group.\n\n        Args:\n            clip_group: the h5py group representing the clip (e.g batch/video) the frame belongs to\n            instance_labels: the labels used to create instance group names\n            save: whether to save crops, features and embeddings for the instance\n        Returns:\n            An h5py group containing the frame\n        \"\"\"\n        if save is None:\n            save = {\"crop\": False, \"features\": False, \"embeddings\": False}\n        frame_group = clip_group.require_group(f\"frame_{self.frame_id.item()}\")\n        frame_group.attrs.create(\"frame_id\", self.frame_id.item())\n        frame_group.attrs.create(\"vid_id\", self.video_id.item())\n        frame_group.attrs.create(\"vid_name\", self.vid_name)\n\n        frame_group.create_dataset(\n            \"asso_matrix\",\n            data=self.asso_output.numpy() if self.asso_output is not None else [],\n        )\n        asso_group = frame_group.require_group(\"traj_scores\")\n        for key, value in self.get_traj_score().items():\n            asso_group.create_dataset(\n                key, data=value.to_numpy() if value is not None else []\n            )\n\n        if instance_labels is None:\n            instance_labels = self.get_gt_track_ids.cpu().numpy()\n        for instance_label, instance in zip(instance_labels, self.instances):\n            kwargs = {}\n            if save.get(\"crop\", False):\n                kwargs[\"crop\"] = instance.crop.cpu().numpy()\n            if save.get(\"features\", False):\n                kwargs[\"features\"] = instance.features.cpu().numpy()\n            if save.get(\"embeddings\", False):\n                for key, val in instance.get_embedding().items():\n                    kwargs[f\"{key}_emb\"] = val.cpu().numpy()\n            _ = instance.to_h5(frame_group, f\"instance_{instance_label}\", **kwargs)\n\n        return frame_group\n\n    @property\n    def device(self) -&gt; str:\n        \"\"\"The device the frame is on.\n\n        Returns:\n            The string representation of the device the frame is on.\n        \"\"\"\n        return self._device\n\n    @device.setter\n    def device(self, device: str) -&gt; None:\n        \"\"\"Set the device.\n\n        Note: Do not set `frame.device = device` normally. Use `frame.to(device)` instead.\n\n        Args:\n            device: the device the function should be on.\n        \"\"\"\n        self._device = device\n\n    @property\n    def video_id(self) -&gt; torch.Tensor:\n        \"\"\"The index of the video the frame comes from.\n\n        Returns:\n            A tensor containing the video index.\n        \"\"\"\n        return self._video_id\n\n    @video_id.setter\n    def video_id(self, video_id: int) -&gt; None:\n        \"\"\"Set the video index.\n\n        Note: Generally the video_id should be immutable after initialization.\n\n        Args:\n            video_id: an int representing the index of the video that the frame came from.\n        \"\"\"\n        self._video_id = torch.tensor([video_id])\n\n    @property\n    def frame_id(self) -&gt; torch.Tensor:\n        \"\"\"The index of the frame in a full video.\n\n        Returns:\n            A torch tensor containing the index of the frame in the video.\n        \"\"\"\n        return self._frame_id\n\n    @frame_id.setter\n    def frame_id(self, frame_id: int) -&gt; None:\n        \"\"\"Set the frame index of the frame.\n\n        Note: The frame_id should generally be immutable after initialization.\n\n        Args:\n            frame_id: The int index of the frame in the full video.\n        \"\"\"\n        self._frame_id = torch.tensor([frame_id])\n\n    @property\n    def video(self) -&gt; sio.Video | str:\n        \"\"\"Get the video associated with the frame.\n\n        Returns: An sio.Video object representing the video or a placeholder string\n        if it is not possible to create the sio.Video\n        \"\"\"\n        return self._video\n\n    @video.setter\n    def video(self, video: sio.Video | str) -&gt; None:\n        \"\"\"Set the video associated with the frame.\n\n        Note: we try to store the video in an sio.Video object.\n        However, if this is not possible (e.g. incompatible format or missing filepath)\n        then we simply store the string.\n\n        Args:\n            video: sio.Video containing the vid reader or string path to video_file\n        \"\"\"\n        if isinstance(video, sio.Video):\n            self._video = video\n        else:\n            try:\n                self._video = sio.load_video(video)\n            except ValueError:\n                self._video = video\n\n    @property\n    def vid_name(self) -&gt; str:\n        \"\"\"Get the path to the video corresponding to this frame.\n\n        Returns: A str file path corresponding to the frame.\n        \"\"\"\n        if isinstance(self.video, str):\n            return self.video\n        else:\n            return self.video.name\n\n    @property\n    def img_shape(self) -&gt; torch.Tensor:\n        \"\"\"The shape of the pre-cropped frame.\n\n        Returns:\n            A torch tensor containing the shape of the frame. Should generally be (c, h, w)\n        \"\"\"\n        return self._img_shape\n\n    @img_shape.setter\n    def img_shape(self, img_shape: ArrayLike) -&gt; None:\n        \"\"\"Set the shape of the frame image.\n\n        Note: the img_shape should generally be immutable after initialization.\n\n        Args:\n            img_shape: an ArrayLike object containing the shape of the frame image.\n        \"\"\"\n        self._img_shape = _to_tensor(img_shape)\n\n    @property\n    def instances(self) -&gt; list[\"Instance\"]:\n        \"\"\"A list of instances in the frame.\n\n        Returns:\n            The list of instances that appear in the frame.\n        \"\"\"\n        return self._instances\n\n    @instances.setter\n    def instances(self, instances: list[\"Instance\"]) -&gt; None:\n        \"\"\"Set the frame's instance.\n\n        Args:\n            instances: A list of Instances that appear in the frame.\n        \"\"\"\n        for instance in instances:\n            instance.frame = self\n        self._instances = instances\n\n    def has_instances(self) -&gt; bool:\n        \"\"\"Determine whether there are instances in the frame.\n\n        Returns:\n            True if there are instances in the frame, otherwise False.\n        \"\"\"\n        if self.num_detected == 0:\n            return False\n        return True\n\n    @property\n    def num_detected(self) -&gt; int:\n        \"\"\"The number of instances in the frame.\n\n        Returns:\n            the number of instances in the frame.\n        \"\"\"\n        return len(self.instances)\n\n    @property\n    def asso_output(self) -&gt; \"AssociationMatrix\":\n        \"\"\"The association matrix between instances outputed directly by transformer.\n\n        Returns:\n            An arraylike (n_query, n_nonquery) association matrix between instances.\n        \"\"\"\n        return self._asso_output\n\n    def has_asso_output(self) -&gt; bool:\n        \"\"\"Determine whether the frame has an association matrix computed.\n\n        Returns:\n            True if the frame has an association matrix otherwise, False.\n        \"\"\"\n        if self._asso_output is None or len(self._asso_output.matrix) == 0:\n            return False\n        return True\n\n    @asso_output.setter\n    def asso_output(self, asso_output: \"AssociationMatrix\") -&gt; None:\n        \"\"\"Set the association matrix of a frame.\n\n        Args:\n            asso_output: An arraylike (n_query, n_nonquery) association matrix between instances.\n        \"\"\"\n        self._asso_output = asso_output\n\n    @property\n    def matches(self) -&gt; tuple:\n        \"\"\"Matches between frame instances and availabel trajectories.\n\n        Returns:\n            A tuple containing the instance idx and trajectory idx for the matched instance.\n        \"\"\"\n        return self._matches\n\n    @matches.setter\n    def matches(self, matches: tuple) -&gt; None:\n        \"\"\"Set the frame matches.\n\n        Args:\n            matches: A tuple containing the instance idx and trajectory idx for the matched instance.\n        \"\"\"\n        self._matches = matches\n\n    def has_matches(self) -&gt; bool:\n        \"\"\"Check whether or not matches have been computed for frame.\n\n        Returns:\n            True if frame contains matches otherwise False.\n        \"\"\"\n        if self._matches is not None and len(self._matches) &gt; 0:\n            return True\n        return False\n\n    def get_traj_score(self, key: str | None = None) -&gt; dict | ArrayLike | None:\n        \"\"\"Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.\n\n        Args:\n            key: The key of the trajectory score to be accessed.\n                Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}\n\n        Returns:\n            - dictionary containing all trajectory scores if key is None\n            - trajectory score associated with key\n            - None if the key is not found\n        \"\"\"\n        if key is None:\n            return self._traj_score\n        else:\n            try:\n                return self._traj_score[key]\n            except KeyError as e:\n                logger.exception(f\"Could not access {key} traj_score due to {e}\")\n                return None\n\n    def add_traj_score(self, key: str, traj_score: ArrayLike) -&gt; None:\n        \"\"\"Add trajectory score to dictionary.\n\n        Args:\n            key: key associated with traj score to be used in dictionary\n            traj_score: association matrix between instances and trajectories\n        \"\"\"\n        self._traj_score[key] = traj_score\n\n    def has_traj_score(self) -&gt; bool:\n        \"\"\"Check if any trajectory association matrix has been saved.\n\n        Returns:\n            True there is at least one association matrix otherwise, false.\n        \"\"\"\n        if len(self._traj_score) == 0:\n            return False\n        return True\n\n    def has_gt_track_ids(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a gt track id.\n\n        Returns:\n            True if at least 1 instance has a gt track id otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_gt_track_id() for instance in self.instances])\n        return False\n\n    def get_gt_track_ids(self) -&gt; torch.Tensor:\n        \"\"\"Get the gt track ids of all instances in the frame.\n\n        Returns:\n            an (N,) shaped tensor with the gt track ids of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.gt_track_id for instance in self.instances])\n\n    def has_pred_track_ids(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a pred track id.\n\n        Returns:\n            True if at least 1 instance has a pred track id otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_pred_track_id() for instance in self.instances])\n        return False\n\n    def get_pred_track_ids(self) -&gt; torch.Tensor:\n        \"\"\"Get the pred track ids of all instances in the frame.\n\n        Returns:\n            an (N,) shaped tensor with the pred track ids of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.pred_track_id for instance in self.instances])\n\n    def has_bboxes(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a bounding box.\n\n        Returns:\n            True if at least 1 instance has a bounding box otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_bboxes() for instance in self.instances])\n        return False\n\n    def get_bboxes(self) -&gt; torch.Tensor:\n        \"\"\"Get the bounding boxes of all instances in the frame.\n\n        Returns:\n            an (N,4) shaped tensor with bounding boxes of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.empty(0, 4)\n        return torch.cat([instance.bbox for instance in self.instances], dim=0)\n\n    def has_crops(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a crop.\n\n        Returns:\n            True if at least 1 instance has a crop otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_crop() for instance in self.instances])\n        return False\n\n    def get_crops(self) -&gt; torch.Tensor:\n        \"\"\"Get the crops of all instances in the frame.\n\n        Returns:\n            an (N, C, H, W) shaped tensor with crops of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n\n        return torch.cat([instance.crop for instance in self.instances], dim=0)\n\n    def has_features(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has reid features already computed.\n\n        Returns:\n            True if at least 1 instance have reid features otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_features() for instance in self.instances])\n        return False\n\n    def get_features(self) -&gt; torch.Tensor:\n        \"\"\"Get the reid feature vectors of all instances in the frame.\n\n        Returns:\n            an (N, D) shaped tensor with reid feature vectors of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.features for instance in self.instances], dim=0)\n\n    def get_anchors(self) -&gt; list[str]:\n        \"\"\"Get the anchor names of instances in the frame.\n\n        Returns:\n            A list of anchor names used by the instances to get the crop.\n        \"\"\"\n        return [instance.anchor for instance in self.instances]\n\n    def get_centroids(self) -&gt; tuple[list[str], ArrayLike]:\n        \"\"\"Get the centroids around which each instance's crop was formed.\n\n        Returns:\n            anchors: the node names for the corresponding point\n            points: an n_instances x 2 array containing the centroids\n        \"\"\"\n        anchors = [\n            anchor for instance in self.instances for anchor in instance.centroid.keys()\n        ]\n\n        points = np.array(\n            [\n                point\n                for instance in self.instances\n                for point in instance.centroid.values()\n            ]\n        )\n\n        return (anchors, points)\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.asso_output","title":"<code>asso_output: 'AssociationMatrix'</code>  <code>property</code> <code>writable</code>","text":"<p>The association matrix between instances outputed directly by transformer.</p> <p>Returns:</p> Type Description <code>'AssociationMatrix'</code> <p>An arraylike (n_query, n_nonquery) association matrix between instances.</p>"},{"location":"io/frame/#dreem.io.Frame.device","title":"<code>device: str</code>  <code>property</code> <code>writable</code>","text":"<p>The device the frame is on.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the device the frame is on.</p>"},{"location":"io/frame/#dreem.io.Frame.frame_id","title":"<code>frame_id: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>The index of the frame in a full video.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor containing the index of the frame in the video.</p>"},{"location":"io/frame/#dreem.io.Frame.img_shape","title":"<code>img_shape: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>The shape of the pre-cropped frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor containing the shape of the frame. Should generally be (c, h, w)</p>"},{"location":"io/frame/#dreem.io.Frame.instances","title":"<code>instances: list['Instance']</code>  <code>property</code> <code>writable</code>","text":"<p>A list of instances in the frame.</p> <p>Returns:</p> Type Description <code>list['Instance']</code> <p>The list of instances that appear in the frame.</p>"},{"location":"io/frame/#dreem.io.Frame.matches","title":"<code>matches: tuple</code>  <code>property</code> <code>writable</code>","text":"<p>Matches between frame instances and availabel trajectories.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the instance idx and trajectory idx for the matched instance.</p>"},{"location":"io/frame/#dreem.io.Frame.num_detected","title":"<code>num_detected: int</code>  <code>property</code>","text":"<p>The number of instances in the frame.</p> <p>Returns:</p> Type Description <code>int</code> <p>the number of instances in the frame.</p>"},{"location":"io/frame/#dreem.io.Frame.vid_name","title":"<code>vid_name: str</code>  <code>property</code>","text":"<p>Get the path to the video corresponding to this frame.</p> <p>Returns: A str file path corresponding to the frame.</p>"},{"location":"io/frame/#dreem.io.Frame.video","title":"<code>video: sio.Video | str</code>  <code>property</code> <code>writable</code>","text":"<p>Get the video associated with the frame.</p> <p>Returns: An sio.Video object representing the video or a placeholder string if it is not possible to create the sio.Video</p>"},{"location":"io/frame/#dreem.io.Frame.video_id","title":"<code>video_id: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>The index of the video the frame comes from.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the video index.</p>"},{"location":"io/frame/#dreem.io.Frame.__attrs_post_init__","title":"<code>__attrs_post_init__()</code>","text":"<p>Handle more intricate default initializations and moving to device.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def __attrs_post_init__(self) -&gt; None:\n    \"\"\"Handle more intricate default initializations and moving to device.\"\"\"\n    if len(self.img_shape) == 0:\n        self.img_shape = torch.tensor([0, 0, 0])\n\n    for instance in self.instances:\n        instance.frame = self\n\n    self.to(self.device)\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.__repr__","title":"<code>__repr__()</code>","text":"<p>Return String representation of the Frame.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return String representation of the Frame.\n\n    Returns:\n        The string representation of the frame.\n    \"\"\"\n    return (\n        \"Frame(\"\n        f\"video={self._video.filename if isinstance(self._video, sio.Video) else self._video}, \"\n        f\"video_id={self._video_id.item()}, \"\n        f\"frame_id={self._frame_id.item()}, \"\n        f\"img_shape={self._img_shape}, \"\n        f\"num_detected={self.num_detected}, \"\n        f\"asso_output={self._asso_output}, \"\n        f\"traj_score={self._traj_score}, \"\n        f\"matches={self._matches}, \"\n        f\"instances={self._instances}, \"\n        f\"device={self._device}\"\n        \")\"\n    )\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.add_traj_score","title":"<code>add_traj_score(key, traj_score)</code>","text":"<p>Add trajectory score to dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key associated with traj score to be used in dictionary</p> required <code>traj_score</code> <code>ArrayLike</code> <p>association matrix between instances and trajectories</p> required Source code in <code>dreem/io/frame.py</code> <pre><code>def add_traj_score(self, key: str, traj_score: ArrayLike) -&gt; None:\n    \"\"\"Add trajectory score to dictionary.\n\n    Args:\n        key: key associated with traj score to be used in dictionary\n        traj_score: association matrix between instances and trajectories\n    \"\"\"\n    self._traj_score[key] = traj_score\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.from_slp","title":"<code>from_slp(lf, video_id=0, device=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Convert <code>sio.LabeledFrame</code> to <code>dreem.io.Frame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>lf</code> <code>LabeledFrame</code> <p>A sio.LabeledFrame object</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A dreem.io.Frame object</p> Source code in <code>dreem/io/frame.py</code> <pre><code>@classmethod\ndef from_slp(\n    cls,\n    lf: sio.LabeledFrame,\n    video_id: int = 0,\n    device: str | None = None,\n    **kwargs,\n) -&gt; Self:\n    \"\"\"Convert `sio.LabeledFrame` to `dreem.io.Frame`.\n\n    Args:\n        lf: A sio.LabeledFrame object\n\n    Returns:\n        A dreem.io.Frame object\n    \"\"\"\n    from dreem.io import Instance\n\n    img_shape = lf.image.shape\n    if len(img_shape) == 2:\n        img_shape = (1, *img_shape)\n    elif len(img_shape) &gt; 2 and img_shape[-1] &lt;= 3:\n        img_shape = (lf.image.shape[-1], lf.image.shape[0], lf.image.shape[1])\n    return cls(\n        video_id=video_id,\n        frame_id=(\n            lf.frame_idx.astype(np.int32)\n            if isinstance(lf.frame_idx, np.number)\n            else lf.frame_idx\n        ),\n        vid_file=lf.video.filename,\n        img_shape=img_shape,\n        instances=[Instance.from_slp(instance, **kwargs) for instance in lf],\n        device=device,\n    )\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.get_anchors","title":"<code>get_anchors()</code>","text":"<p>Get the anchor names of instances in the frame.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of anchor names used by the instances to get the crop.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_anchors(self) -&gt; list[str]:\n    \"\"\"Get the anchor names of instances in the frame.\n\n    Returns:\n        A list of anchor names used by the instances to get the crop.\n    \"\"\"\n    return [instance.anchor for instance in self.instances]\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.get_bboxes","title":"<code>get_bboxes()</code>","text":"<p>Get the bounding boxes of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,4) shaped tensor with bounding boxes of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_bboxes(self) -&gt; torch.Tensor:\n    \"\"\"Get the bounding boxes of all instances in the frame.\n\n    Returns:\n        an (N,4) shaped tensor with bounding boxes of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.empty(0, 4)\n    return torch.cat([instance.bbox for instance in self.instances], dim=0)\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.get_centroids","title":"<code>get_centroids()</code>","text":"<p>Get the centroids around which each instance's crop was formed.</p> <p>Returns:</p> Name Type Description <code>anchors</code> <code>tuple[list[str], ArrayLike]</code> <p>the node names for the corresponding point points: an n_instances x 2 array containing the centroids</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_centroids(self) -&gt; tuple[list[str], ArrayLike]:\n    \"\"\"Get the centroids around which each instance's crop was formed.\n\n    Returns:\n        anchors: the node names for the corresponding point\n        points: an n_instances x 2 array containing the centroids\n    \"\"\"\n    anchors = [\n        anchor for instance in self.instances for anchor in instance.centroid.keys()\n    ]\n\n    points = np.array(\n        [\n            point\n            for instance in self.instances\n            for point in instance.centroid.values()\n        ]\n    )\n\n    return (anchors, points)\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.get_crops","title":"<code>get_crops()</code>","text":"<p>Get the crops of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N, C, H, W) shaped tensor with crops of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_crops(self) -&gt; torch.Tensor:\n    \"\"\"Get the crops of all instances in the frame.\n\n    Returns:\n        an (N, C, H, W) shaped tensor with crops of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n\n    return torch.cat([instance.crop for instance in self.instances], dim=0)\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.get_features","title":"<code>get_features()</code>","text":"<p>Get the reid feature vectors of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N, D) shaped tensor with reid feature vectors of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_features(self) -&gt; torch.Tensor:\n    \"\"\"Get the reid feature vectors of all instances in the frame.\n\n    Returns:\n        an (N, D) shaped tensor with reid feature vectors of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.features for instance in self.instances], dim=0)\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.get_gt_track_ids","title":"<code>get_gt_track_ids()</code>","text":"<p>Get the gt track ids of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,) shaped tensor with the gt track ids of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_gt_track_ids(self) -&gt; torch.Tensor:\n    \"\"\"Get the gt track ids of all instances in the frame.\n\n    Returns:\n        an (N,) shaped tensor with the gt track ids of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.gt_track_id for instance in self.instances])\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.get_pred_track_ids","title":"<code>get_pred_track_ids()</code>","text":"<p>Get the pred track ids of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,) shaped tensor with the pred track ids of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_pred_track_ids(self) -&gt; torch.Tensor:\n    \"\"\"Get the pred track ids of all instances in the frame.\n\n    Returns:\n        an (N,) shaped tensor with the pred track ids of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.pred_track_id for instance in self.instances])\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.get_traj_score","title":"<code>get_traj_score(key=None)</code>","text":"<p>Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str | None</code> <p>The key of the trajectory score to be accessed. Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}</p> <code>None</code> <p>Returns:</p> Type Description <code>dict | ArrayLike | None</code> <ul> <li>dictionary containing all trajectory scores if key is None</li> <li>trajectory score associated with key</li> <li>None if the key is not found</li> </ul> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_traj_score(self, key: str | None = None) -&gt; dict | ArrayLike | None:\n    \"\"\"Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.\n\n    Args:\n        key: The key of the trajectory score to be accessed.\n            Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}\n\n    Returns:\n        - dictionary containing all trajectory scores if key is None\n        - trajectory score associated with key\n        - None if the key is not found\n    \"\"\"\n    if key is None:\n        return self._traj_score\n    else:\n        try:\n            return self._traj_score[key]\n        except KeyError as e:\n            logger.exception(f\"Could not access {key} traj_score due to {e}\")\n            return None\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_asso_output","title":"<code>has_asso_output()</code>","text":"<p>Determine whether the frame has an association matrix computed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the frame has an association matrix otherwise, False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_asso_output(self) -&gt; bool:\n    \"\"\"Determine whether the frame has an association matrix computed.\n\n    Returns:\n        True if the frame has an association matrix otherwise, False.\n    \"\"\"\n    if self._asso_output is None or len(self._asso_output.matrix) == 0:\n        return False\n    return True\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_bboxes","title":"<code>has_bboxes()</code>","text":"<p>Check if any of frames instances has a bounding box.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a bounding box otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_bboxes(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a bounding box.\n\n    Returns:\n        True if at least 1 instance has a bounding box otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_bboxes() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_crops","title":"<code>has_crops()</code>","text":"<p>Check if any of frames instances has a crop.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a crop otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_crops(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a crop.\n\n    Returns:\n        True if at least 1 instance has a crop otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_crop() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_features","title":"<code>has_features()</code>","text":"<p>Check if any of frames instances has reid features already computed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance have reid features otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_features(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has reid features already computed.\n\n    Returns:\n        True if at least 1 instance have reid features otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_features() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_gt_track_ids","title":"<code>has_gt_track_ids()</code>","text":"<p>Check if any of frames instances has a gt track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a gt track id otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_gt_track_ids(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a gt track id.\n\n    Returns:\n        True if at least 1 instance has a gt track id otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_gt_track_id() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_instances","title":"<code>has_instances()</code>","text":"<p>Determine whether there are instances in the frame.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if there are instances in the frame, otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_instances(self) -&gt; bool:\n    \"\"\"Determine whether there are instances in the frame.\n\n    Returns:\n        True if there are instances in the frame, otherwise False.\n    \"\"\"\n    if self.num_detected == 0:\n        return False\n    return True\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_matches","title":"<code>has_matches()</code>","text":"<p>Check whether or not matches have been computed for frame.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if frame contains matches otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_matches(self) -&gt; bool:\n    \"\"\"Check whether or not matches have been computed for frame.\n\n    Returns:\n        True if frame contains matches otherwise False.\n    \"\"\"\n    if self._matches is not None and len(self._matches) &gt; 0:\n        return True\n    return False\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_pred_track_ids","title":"<code>has_pred_track_ids()</code>","text":"<p>Check if any of frames instances has a pred track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a pred track id otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_pred_track_ids(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a pred track id.\n\n    Returns:\n        True if at least 1 instance has a pred track id otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_pred_track_id() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_traj_score","title":"<code>has_traj_score()</code>","text":"<p>Check if any trajectory association matrix has been saved.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True there is at least one association matrix otherwise, false.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_traj_score(self) -&gt; bool:\n    \"\"\"Check if any trajectory association matrix has been saved.\n\n    Returns:\n        True there is at least one association matrix otherwise, false.\n    \"\"\"\n    if len(self._traj_score) == 0:\n        return False\n    return True\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.to","title":"<code>to(map_location)</code>","text":"<p>Move frame to different device or dtype (See <code>torch.to</code> for more info).</p> <p>Parameters:</p> Name Type Description Default <code>map_location</code> <code>str | device</code> <p>A string representing the device to move to.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The frame moved to a different device/dtype.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to(self, map_location: str | torch.device) -&gt; Self:\n    \"\"\"Move frame to different device or dtype (See `torch.to` for more info).\n\n    Args:\n        map_location: A string representing the device to move to.\n\n    Returns:\n        The frame moved to a different device/dtype.\n    \"\"\"\n    self._video_id = self._video_id.to(map_location)\n    self._frame_id = self._frame_id.to(map_location)\n    self._img_shape = self._img_shape.to(map_location)\n\n    if isinstance(self._asso_output, torch.Tensor):\n        self._asso_output = self._asso_output.to(map_location)\n\n    if isinstance(self._matches, torch.Tensor):\n        self._matches = self._matches.to(map_location)\n\n    for key, val in self._traj_score.items():\n        if isinstance(val, torch.Tensor):\n            self._traj_score[key] = val.to(map_location)\n    for instance in self.instances:\n        instance = instance.to(map_location)\n\n    if isinstance(map_location, (str, torch.device)):\n        self._device = map_location\n\n    return self\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.to_h5","title":"<code>to_h5(clip_group, instance_labels=None, save=None)</code>","text":"<p>Convert frame to h5py group.</p> <p>Parameters:</p> Name Type Description Default <code>clip_group</code> <code>Group</code> <p>the h5py group representing the clip (e.g batch/video) the frame belongs to</p> required <code>instance_labels</code> <code>list | None</code> <p>the labels used to create instance group names</p> <code>None</code> <code>save</code> <code>dict[str, bool] | None</code> <p>whether to save crops, features and embeddings for the instance</p> <code>None</code> <p>Returns:     An h5py group containing the frame</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to_h5(\n    self,\n    clip_group: h5py.Group,\n    instance_labels: list | None = None,\n    save: dict[str, bool] | None = None,\n) -&gt; h5py.Group:\n    \"\"\"Convert frame to h5py group.\n\n    Args:\n        clip_group: the h5py group representing the clip (e.g batch/video) the frame belongs to\n        instance_labels: the labels used to create instance group names\n        save: whether to save crops, features and embeddings for the instance\n    Returns:\n        An h5py group containing the frame\n    \"\"\"\n    if save is None:\n        save = {\"crop\": False, \"features\": False, \"embeddings\": False}\n    frame_group = clip_group.require_group(f\"frame_{self.frame_id.item()}\")\n    frame_group.attrs.create(\"frame_id\", self.frame_id.item())\n    frame_group.attrs.create(\"vid_id\", self.video_id.item())\n    frame_group.attrs.create(\"vid_name\", self.vid_name)\n\n    frame_group.create_dataset(\n        \"asso_matrix\",\n        data=self.asso_output.numpy() if self.asso_output is not None else [],\n    )\n    asso_group = frame_group.require_group(\"traj_scores\")\n    for key, value in self.get_traj_score().items():\n        asso_group.create_dataset(\n            key, data=value.to_numpy() if value is not None else []\n        )\n\n    if instance_labels is None:\n        instance_labels = self.get_gt_track_ids.cpu().numpy()\n    for instance_label, instance in zip(instance_labels, self.instances):\n        kwargs = {}\n        if save.get(\"crop\", False):\n            kwargs[\"crop\"] = instance.crop.cpu().numpy()\n        if save.get(\"features\", False):\n            kwargs[\"features\"] = instance.features.cpu().numpy()\n        if save.get(\"embeddings\", False):\n            for key, val in instance.get_embedding().items():\n                kwargs[f\"{key}_emb\"] = val.cpu().numpy()\n        _ = instance.to_h5(frame_group, f\"instance_{instance_label}\", **kwargs)\n\n    return frame_group\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.to_slp","title":"<code>to_slp(track_lookup=None, video=None)</code>","text":"<p>Convert Frame to sleap_io.LabeledFrame object.</p> <p>Parameters:</p> Name Type Description Default <code>track_lookup</code> <code>dict[int, Track] | None</code> <p>A lookup dictionary containing the track_id and sio.Track for persistence</p> <code>None</code> <code>video</code> <code>Video | None</code> <p>An sio.Video object used for overriding.</p> <code>None</code> <p>Returns: A tuple containing a LabeledFrame object with necessary metadata and a lookup dictionary containing the track_id and sio.Track for persistence</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to_slp(\n    self,\n    track_lookup: dict[int, sio.Track] | None = None,\n    video: sio.Video | None = None,\n) -&gt; tuple[sio.LabeledFrame, dict[int, sio.Track]]:\n    \"\"\"Convert Frame to sleap_io.LabeledFrame object.\n\n    Args:\n        track_lookup: A lookup dictionary containing the track_id and sio.Track for persistence\n        video: An sio.Video object used for overriding.\n\n    Returns: A tuple containing a LabeledFrame object with necessary metadata and\n    a lookup dictionary containing the track_id and sio.Track for persistence\n    \"\"\"\n    if track_lookup is None:\n        track_lookup = {}\n\n    slp_instances = []\n    for instance in self.instances:\n        slp_instance, track_lookup = instance.to_slp(track_lookup=track_lookup)\n        slp_instances.append(slp_instance)\n\n    if video is None:\n        video = (\n            self.video\n            if isinstance(self.video, sio.Video)\n            else sio.load_video(self.video)\n        )\n\n    return (\n        sio.LabeledFrame(\n            video=video,\n            frame_idx=self.frame_id.item(),\n            instances=slp_instances,\n        ),\n        track_lookup,\n    )\n</code></pre>"},{"location":"io/instance/","title":"<code>Instance</code>","text":""},{"location":"io/instance/#dreem.io.Instance","title":"<code>dreem.io.Instance</code>","text":"<p>Class representing a single instance to be tracked.</p> <p>Attributes:</p> Name Type Description <code>gt_track_id</code> <code>Tensor</code> <p>Ground truth track id - only used for train/eval.</p> <code>pred_track_id</code> <code>Tensor</code> <p>Predicted track id. Untracked instance is represented by -1.</p> <code>bbox</code> <code>Tensor</code> <p>The bounding box coordinate of the instance. Defaults to an empty tensor.</p> <code>crop</code> <code>Tensor</code> <p>The crop of the instance.</p> <code>centroid</code> <code>dict[str, ArrayLike]</code> <p>the centroid around which the bbox was cropped.</p> <code>features</code> <code>Tensor</code> <p>The reid features extracted from the CNN backbone used in the transformer.</p> <code>track_score</code> <code>float</code> <p>The track score output from the association matrix.</p> <code>point_scores</code> <code>ArrayLike</code> <p>The point scores from sleap.</p> <code>instance_score</code> <code>float</code> <p>The instance scores from sleap.</p> <code>skeleton</code> <code>Skeleton</code> <p>The sleap skeleton used for the instance.</p> <code>pose</code> <code>dict[str, ArrayLike]</code> <p>A dictionary containing the node name and corresponding point.</p> <code>device</code> <code>str</code> <p>String representation of the device the instance should be on.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>@attrs.define(eq=False)\nclass Instance:\n    \"\"\"Class representing a single instance to be tracked.\n\n    Attributes:\n        gt_track_id: Ground truth track id - only used for train/eval.\n        pred_track_id: Predicted track id. Untracked instance is represented by -1.\n        bbox: The bounding box coordinate of the instance. Defaults to an empty tensor.\n        crop: The crop of the instance.\n        centroid: the centroid around which the bbox was cropped.\n        features: The reid features extracted from the CNN backbone used in the transformer.\n        track_score: The track score output from the association matrix.\n        point_scores: The point scores from sleap.\n        instance_score: The instance scores from sleap.\n        skeleton: The sleap skeleton used for the instance.\n        pose: A dictionary containing the node name and corresponding point.\n        device: String representation of the device the instance should be on.\n    \"\"\"\n\n    _gt_track_id: int = attrs.field(\n        alias=\"gt_track_id\", default=-1, converter=_to_tensor\n    )\n    _pred_track_id: int = attrs.field(\n        alias=\"pred_track_id\", default=-1, converter=_to_tensor\n    )\n    _bbox: ArrayLike = attrs.field(alias=\"bbox\", factory=list, converter=_to_tensor)\n    _crop: ArrayLike = attrs.field(alias=\"crop\", factory=list, converter=_to_tensor)\n    _centroid: dict[str, ArrayLike] = attrs.field(alias=\"centroid\", factory=dict)\n    _features: ArrayLike = attrs.field(\n        alias=\"features\", factory=list, converter=_to_tensor\n    )\n    _embeddings: dict = attrs.field(alias=\"embeddings\", factory=dict)\n    _track_score: float = attrs.field(alias=\"track_score\", default=-1.0)\n    _instance_score: float = attrs.field(alias=\"instance_score\", default=-1.0)\n    _point_scores: ArrayLike | None = attrs.field(alias=\"point_scores\", default=None)\n    _skeleton: sio.Skeleton | None = attrs.field(alias=\"skeleton\", default=None)\n    _pose: dict[str, ArrayLike] = attrs.field(alias=\"pose\", factory=dict)\n    _device: str | torch.device | None = attrs.field(alias=\"device\", default=None)\n    _frame: \"Frame\" = None\n\n    def __attrs_post_init__(self) -&gt; None:\n        \"\"\"Handle dimensionality and more intricate default initializations post-init.\"\"\"\n        self.bbox = _expand_to_rank(self.bbox, 3)\n        self.crop = _expand_to_rank(self.crop, 4)\n        self.features = _expand_to_rank(self.features, 2)\n\n        if self.skeleton is None:\n            self.skeleton = sio.Skeleton([\"centroid\"])\n\n        if self.bbox.shape[-1] == 0:\n            self.bbox = torch.empty([1, 0, 4])\n\n        if self.crop.shape[-1] == 0 and self.bbox.shape[1] != 0:\n            y1, x1, y2, x2 = self.bbox.squeeze(dim=0).nanmean(dim=0)\n            self.centroid = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        if len(self.pose) == 0 and self.bbox.shape[1]:\n            y1, x1, y2, x2 = self.bbox.squeeze(dim=0).mean(dim=0)\n            self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        if self.point_scores is None and len(self.pose) != 0:\n            self._point_scores = np.zeros((len(self.pose), 2))\n\n        self.to(self.device)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation of the Instance.\"\"\"\n        return (\n            \"Instance(\"\n            f\"gt_track_id={self._gt_track_id.item()}, \"\n            f\"pred_track_id={self._pred_track_id.item()}, \"\n            f\"bbox={self._bbox}, \"\n            f\"centroid={self._centroid}, \"\n            f\"crop={self._crop.shape}, \"\n            f\"features={self._features.shape}, \"\n            f\"device={self._device}\"\n            \")\"\n        )\n\n    def to(self, map_location: str | torch.device) -&gt; Self:\n        \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n        Args:\n            map_location: Either the device or dtype for the instance to be moved.\n\n        Returns:\n            self: reference to the instance moved to correct device/dtype.\n        \"\"\"\n        if map_location is not None and map_location != \"\":\n            self._gt_track_id = self._gt_track_id.to(map_location)\n            self._pred_track_id = self._pred_track_id.to(map_location)\n            self._bbox = self._bbox.to(map_location)\n            self._crop = self._crop.to(map_location)\n            self._features = self._features.to(map_location)\n            if isinstance(map_location, (str, torch.device)):\n                self.device = map_location\n\n        return self\n\n    @classmethod\n    def from_slp(\n        cls,\n        slp_instance: sio.PredictedInstance | sio.Instance,\n        bbox_size: int | tuple[int, int] = 64,\n        crop: ArrayLike | None = None,\n        device: str | None = None,\n    ) -&gt; Self:\n        \"\"\"Convert a slp instance to a dreem instance.\n\n        Args:\n            slp_instance: A `sleap_io.Instance` object representing a detection\n            bbox_size: size of the pose-centered bbox to form.\n            crop: The corresponding crop of the bbox\n            device: which device to keep the instance on\n        Returns:\n            A dreem.Instance object with a pose-centered bbox and no crop.\n        \"\"\"\n        try:\n            track_id = int(slp_instance.track.name)\n        except ValueError:\n            track_id = int(\n                \"\".join([str(ord(c)) for c in slp_instance.track.name])\n            )  # better way to handle this?\n        if isinstance(bbox_size, int):\n            bbox_size = (bbox_size, bbox_size)\n\n        track_score = -1.0\n        point_scores = np.full(len(slp_instance.points), -1)\n        instance_score = -1\n        if isinstance(slp_instance, sio.PredictedInstance):\n            track_score = slp_instance.tracking_score\n            point_scores = slp_instance.numpy()[:, -1]\n            instance_score = slp_instance.score\n\n        centroid = np.nanmean(slp_instance.numpy(), axis=1)\n        bbox = [\n            centroid[1] - bbox_size[1],\n            centroid[0] - bbox_size[0],\n            centroid[1] + bbox_size[1],\n            centroid[0] + bbox_size[0],\n        ]\n        return cls(\n            gt_track_id=track_id,\n            bbox=bbox,\n            crop=crop,\n            centroid={\"centroid\": centroid},\n            track_score=track_score,\n            point_scores=point_scores,\n            instance_score=instance_score,\n            skeleton=slp_instance.skeleton,\n            pose={\n                node.name: point.numpy() for node, point in slp_instance.points.items()\n            },\n            device=device,\n        )\n\n    def to_slp(\n        self, track_lookup: dict[int, sio.Track] = {}\n    ) -&gt; tuple[sio.PredictedInstance, dict[int, sio.Track]]:\n        \"\"\"Convert instance to sleap_io.PredictedInstance object.\n\n        Args:\n            track_lookup: A track look up dictionary containing track_id:sio.Track.\n        Returns: A sleap_io.PredictedInstance with necessary metadata\n            and a track_lookup dictionary to persist tracks.\n        \"\"\"\n        try:\n            track_id = self.pred_track_id.item()\n            if track_id not in track_lookup:\n                track_lookup[track_id] = sio.Track(name=self.pred_track_id.item())\n\n            track = track_lookup[track_id]\n\n            return (\n                sio.PredictedInstance.from_numpy(\n                    points=np.array(list(self.pose.values())),\n                    skeleton=self.skeleton,\n                    point_scores=self.point_scores,\n                    instance_score=self.instance_score,\n                    tracking_score=self.track_score,\n                    track=track,\n                ),\n                track_lookup,\n            )\n        except Exception as e:\n            logger.exception(\n                f\"Pose: {np.array(list(self.pose.values())).shape}, Pose score shape {self.point_scores.shape}\"\n            )\n            raise RuntimeError(f\"Failed to convert to sio.PredictedInstance: {e}\")\n\n    def to_h5(self, frame_group=h5py.Group, label=None, **kwargs: dict) -&gt; h5py.Group:\n        \"\"\"Convert instance to an h5 group\".\n\n        By default we always save:\n            - the gt/pred track id\n            - bbox\n            - centroid\n            - pose\n            - instance/traj/points score\n        Larger arrays (crops/features/embeddings) can be saved by passing as kwargs\n\n        Args:\n            frame_group: the h5py group representing the frame the instance appears on\n            label: the name of the instance group that will be created\n            **kwargs: additional key:value pairs to be saved as datasets.\n\n        Returns:\n            The h5 group representing this instance.\n        \"\"\"\n        if label is None:\n            if pred_track_id != -1:\n                label = f\"instance_{self.pred_track_id.item()}\"\n            else:\n                label = f\"instance_{self.gt_track_id.item()}\"\n        instance_group = frame_group.create_group(label)\n        instance_group.attrs.create(\"gt_track_id\", self.gt_track_id.item())\n        instance_group.attrs.create(\"pred_track_id\", self.pred_track_id.item())\n        instance_group.attrs.create(\"track_score\", self.track_score)\n        instance_group.attrs.create(\"instance_score\", self.instance_score)\n\n        instance_group.create_dataset(\"bbox\", data=self.bbox.cpu().numpy())\n\n        pose_group = instance_group.create_group(\"pose\")\n        pose_group.create_dataset(\"points\", data=np.array(list(self.pose.values())))\n        pose_group.attrs.create(\"nodes\", list(self.pose.keys()))\n        pose_group.create_dataset(\"scores\", data=self.point_scores)\n\n        for key, value in kwargs.items():\n            if \"emb\" in key:\n                emb_group = instance_group.require_group(\"emb\")\n                emb_group.create_dataset(key, data=value)\n            else:\n                instance_group.create_dataset(key, data=value)\n\n        return instance_group\n\n    @property\n    def device(self) -&gt; str:\n        \"\"\"The device the instance is on.\n\n        Returns:\n            The str representation of the device the gpu is on.\n        \"\"\"\n        return self._device\n\n    @device.setter\n    def device(self, device) -&gt; None:\n        \"\"\"Set for the device property.\n\n        Args:\n            device: The str representation of the device.\n        \"\"\"\n        self._device = device\n\n    @property\n    def gt_track_id(self) -&gt; torch.Tensor:\n        \"\"\"The ground truth track id of the instance.\n\n        Returns:\n            A tensor containing the ground truth track id\n        \"\"\"\n        return self._gt_track_id\n\n    @gt_track_id.setter\n    def gt_track_id(self, track: int):\n        \"\"\"Set the instance ground-truth track id.\n\n        Args:\n           track: An int representing the ground-truth track id.\n        \"\"\"\n        if track is not None:\n            self._gt_track_id = torch.tensor([track])\n        else:\n            self._gt_track_id = torch.tensor([])\n\n    def has_gt_track_id(self) -&gt; bool:\n        \"\"\"Determine if instance has a gt track assignment.\n\n        Returns:\n            True if the gt track id is set, otherwise False.\n        \"\"\"\n        if self._gt_track_id.shape[0] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def pred_track_id(self) -&gt; torch.Tensor:\n        \"\"\"The track id predicted by the tracker using asso_output from model.\n\n        Returns:\n            A tensor containing the predicted track id.\n        \"\"\"\n        return self._pred_track_id\n\n    @pred_track_id.setter\n    def pred_track_id(self, track: int) -&gt; None:\n        \"\"\"Set predicted track id.\n\n        Args:\n            track: an int representing the predicted track id.\n        \"\"\"\n        if track is not None:\n            self._pred_track_id = torch.tensor([track])\n        else:\n            self._pred_track_id = torch.tensor([])\n\n    def has_pred_track_id(self) -&gt; bool:\n        \"\"\"Determine whether instance has predicted track id.\n\n        Returns:\n            True if instance has a pred track id, False otherwise.\n        \"\"\"\n        if self._pred_track_id.item() == -1 or self._pred_track_id.shape[0] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def bbox(self) -&gt; torch.Tensor:\n        \"\"\"The bounding box coordinates of the instance in the original frame.\n\n        Returns:\n            A (1,4) tensor containing the bounding box coordinates.\n        \"\"\"\n        return self._bbox\n\n    @bbox.setter\n    def bbox(self, bbox: ArrayLike) -&gt; None:\n        \"\"\"Set the instance bounding box.\n\n        Args:\n            bbox: an arraylike object containing the bounding box coordinates.\n        \"\"\"\n        if bbox is None or len(bbox) == 0:\n            self._bbox = torch.empty((0, 4))\n        else:\n            if not isinstance(bbox, torch.Tensor):\n                self._bbox = torch.tensor(bbox)\n            else:\n                self._bbox = bbox\n\n        if self._bbox.shape[0] and len(self._bbox.shape) == 1:\n            self._bbox = self._bbox.unsqueeze(0)\n        if self._bbox.shape[1] and len(self._bbox.shape) == 2:\n            self._bbox = self._bbox.unsqueeze(0)\n\n    def has_bbox(self) -&gt; bool:\n        \"\"\"Determine if the instance has a bbox.\n\n        Returns:\n            True if the instance has a bounding box, false otherwise.\n        \"\"\"\n        if self._bbox.shape[1] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def centroid(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"The centroid around which the crop was formed.\n\n        Returns:\n            A dict containing the anchor name and the x, y bbox midpoint.\n        \"\"\"\n        return self._centroid\n\n    @centroid.setter\n    def centroid(self, centroid: dict[str, ArrayLike]) -&gt; None:\n        \"\"\"Set the centroid of the instance.\n\n        Args:\n            centroid: A dict containing the anchor name and points.\n        \"\"\"\n        self._centroid = centroid\n\n    @property\n    def anchor(self) -&gt; list[str]:\n        \"\"\"The anchor node name around which the crop was formed.\n\n        Returns:\n            the list of anchors around which each crop was formed\n            the list of anchors around which each crop was formed\n        \"\"\"\n        if self.centroid:\n            return list(self.centroid.keys())\n        return \"\"\n\n    @property\n    def crop(self) -&gt; torch.Tensor:\n        \"\"\"The crop of the instance.\n\n        Returns:\n            A (1, c, h , w) tensor containing the cropped image centered around the instance.\n        \"\"\"\n        return self._crop\n\n    @crop.setter\n    def crop(self, crop: ArrayLike) -&gt; None:\n        \"\"\"Set the crop of the instance.\n\n        Args:\n            crop: an arraylike object containing the cropped image of the centered instance.\n        \"\"\"\n        if crop is None or len(crop) == 0:\n            self._crop = torch.tensor([])\n        else:\n            if not isinstance(crop, torch.Tensor):\n                self._crop = torch.tensor(crop)\n            else:\n                self._crop = crop\n\n        if len(self._crop.shape) == 2:\n            self._crop = self._crop.unsqueeze(0)\n        if len(self._crop.shape) == 3:\n            self._crop = self._crop.unsqueeze(0)\n\n    def has_crop(self) -&gt; bool:\n        \"\"\"Determine if the instance has a crop.\n\n        Returns:\n            True if the instance has an image otherwise False.\n        \"\"\"\n        if self._crop.shape[-1] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def features(self) -&gt; torch.Tensor:\n        \"\"\"Re-ID feature vector from backbone model to be used as input to transformer.\n\n        Returns:\n            a (1, d) tensor containing the reid feature vector.\n        \"\"\"\n        return self._features\n\n    @features.setter\n    def features(self, features: ArrayLike) -&gt; None:\n        \"\"\"Set the reid feature vector of the instance.\n\n        Args:\n            features: a (1,d) array like object containing the reid features for the instance.\n        \"\"\"\n        if features is None or len(features) == 0:\n            self._features = torch.tensor([])\n\n        elif not isinstance(features, torch.Tensor):\n            self._features = torch.tensor(features)\n        else:\n            self._features = features\n\n        if self._features.shape[0] and len(self._features.shape) == 1:\n            self._features = self._features.unsqueeze(0)\n\n    def has_features(self) -&gt; bool:\n        \"\"\"Determine if the instance has computed reid features.\n\n        Returns:\n            True if the instance has reid features, False otherwise.\n        \"\"\"\n        if self._features.shape[-1] == 0:\n            return False\n        else:\n            return True\n\n    def has_embedding(self, emb_type: str | None = None) -&gt; bool:\n        \"\"\"Determine if the instance has embedding type requested.\n\n        Args:\n            emb_type: The key to check in the embedding dictionary.\n\n        Returns:\n            True if `emb_type` in embedding_dict else false\n        \"\"\"\n        return emb_type in self._embeddings\n\n    def get_embedding(\n        self, emb_type: str = \"all\"\n    ) -&gt; dict[str, torch.Tensor] | torch.Tensor | None:\n        \"\"\"Retrieve instance's spatial/temporal embedding.\n\n        Args:\n            emb_type: The string key of the embedding to retrieve. Should be \"pos\", \"temp\"\n\n        Returns:\n            * A torch tensor representing the spatial/temporal location of the instance.\n            * None if the embedding is not stored\n        \"\"\"\n        if emb_type.lower() == \"all\":\n            return self._embeddings\n        else:\n            try:\n                return self._embeddings[emb_type]\n            except KeyError:\n                logger.exception(\n                    f\"{emb_type} not saved! Only {list(self._embeddings.keys())} are available\"\n                )\n        return None\n\n    def add_embedding(self, emb_type: str, embedding: torch.Tensor) -&gt; None:\n        \"\"\"Save embedding to instance embedding dictionary.\n\n        Args:\n            emb_type: Key/embedding type to be saved to dictionary\n            embedding: The actual torch tensor embedding.\n        \"\"\"\n        embedding = _expand_to_rank(embedding, 2)\n        self._embeddings[emb_type] = embedding\n\n    @property\n    def frame(self) -&gt; \"Frame\":\n        \"\"\"Get the frame the instance belongs to.\n\n        Returns:\n            The back reference to the `Frame` that this `Instance` belongs to.\n        \"\"\"\n        return self._frame\n\n    @frame.setter\n    def frame(self, frame: \"Frame\") -&gt; None:\n        \"\"\"Set the back reference to the `Frame` that this `Instance` belongs to.\n\n        This field is set when instances are added to `Frame` object.\n\n        Args:\n            frame: A `Frame` object containing the metadata for the frame that the instance belongs to\n        \"\"\"\n        self._frame = frame\n\n    @property\n    def pose(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"Get the pose of the instance.\n\n        Returns:\n            A dictionary containing the node and corresponding x,y points\n        \"\"\"\n        return self._pose\n\n    @pose.setter\n    def pose(self, pose: dict[str, ArrayLike]) -&gt; None:\n        \"\"\"Set the pose of the instance.\n\n        Args:\n            pose: A nodes x 2 array containing the pose coordinates.\n        \"\"\"\n        if pose is not None:\n            self._pose = pose\n\n        elif self.bbox.shape[0]:\n            y1, x1, y2, x2 = self.bbox.squeeze()\n            self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        else:\n            self._pose = {}\n\n    def has_pose(self) -&gt; bool:\n        \"\"\"Check if the instance has a pose.\n\n        Returns True if the instance has a pose.\n        \"\"\"\n        if len(self.pose):\n            return True\n        return False\n\n    @property\n    def shown_pose(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"Get the pose with shown nodes only.\n\n        Returns: A dictionary filtered by nodes that are shown (points are not nan).\n        \"\"\"\n        pose = self.pose\n        return {node: point for node, point in pose.items() if not np.isna(point).any()}\n\n    @property\n    def skeleton(self) -&gt; sio.Skeleton:\n        \"\"\"Get the skeleton associated with the instance.\n\n        Returns: The sio.Skeleton associated with the instance.\n        \"\"\"\n        return self._skeleton\n\n    @skeleton.setter\n    def skeleton(self, skeleton: sio.Skeleton) -&gt; None:\n        \"\"\"Set the skeleton associated with the instance.\n\n        Args:\n            skeleton: The sio.Skeleton associated with the instance.\n        \"\"\"\n        self._skeleton = skeleton\n\n    @property\n    def point_scores(self) -&gt; ArrayLike:\n        \"\"\"Get the point scores associated with the pose prediction.\n\n        Returns: a vector of shape n containing the point scores outputed from sleap associated with pose predictions.\n        \"\"\"\n        return self._point_scores\n\n    @point_scores.setter\n    def point_scores(self, point_scores: ArrayLike) -&gt; None:\n        \"\"\"Set the point scores associated with the pose prediction.\n\n        Args:\n            point_scores: a vector of shape n containing the point scores\n            outputted from sleap associated with pose predictions.\n        \"\"\"\n        self._point_scores = point_scores\n\n    @property\n    def instance_score(self) -&gt; float:\n        \"\"\"Get the pose prediction score associated with the instance.\n\n        Returns: a float from 0-1 representing an instance_score.\n        \"\"\"\n        return self._instance_score\n\n    @instance_score.setter\n    def instance_score(self, instance_score: float) -&gt; None:\n        \"\"\"Set the pose prediction score associated with the instance.\n\n        Args:\n            instance_score: a float from 0-1 representing an instance_score.\n        \"\"\"\n        self._instance_score = instance_score\n\n    @property\n    def track_score(self) -&gt; float:\n        \"\"\"Get the track_score of the instance.\n\n        Returns: A float from 0-1 representing the output used in the tracker for assignment.\n        \"\"\"\n        return self._track_score\n\n    @track_score.setter\n    def track_score(self, track_score: float) -&gt; None:\n        \"\"\"Set the track_score of the instance.\n\n        Args:\n            track_score: A float from 0-1 representing the output used in the tracker for assignment.\n        \"\"\"\n        self._track_score = track_score\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.anchor","title":"<code>anchor: list[str]</code>  <code>property</code>","text":"<p>The anchor node name around which the crop was formed.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>the list of anchors around which each crop was formed the list of anchors around which each crop was formed</p>"},{"location":"io/instance/#dreem.io.Instance.bbox","title":"<code>bbox: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>The bounding box coordinates of the instance in the original frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A (1,4) tensor containing the bounding box coordinates.</p>"},{"location":"io/instance/#dreem.io.Instance.centroid","title":"<code>centroid: dict[str, ArrayLike]</code>  <code>property</code> <code>writable</code>","text":"<p>The centroid around which the crop was formed.</p> <p>Returns:</p> Type Description <code>dict[str, ArrayLike]</code> <p>A dict containing the anchor name and the x, y bbox midpoint.</p>"},{"location":"io/instance/#dreem.io.Instance.crop","title":"<code>crop: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>The crop of the instance.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A (1, c, h , w) tensor containing the cropped image centered around the instance.</p>"},{"location":"io/instance/#dreem.io.Instance.device","title":"<code>device: str</code>  <code>property</code> <code>writable</code>","text":"<p>The device the instance is on.</p> <p>Returns:</p> Type Description <code>str</code> <p>The str representation of the device the gpu is on.</p>"},{"location":"io/instance/#dreem.io.Instance.features","title":"<code>features: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>Re-ID feature vector from backbone model to be used as input to transformer.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>a (1, d) tensor containing the reid feature vector.</p>"},{"location":"io/instance/#dreem.io.Instance.frame","title":"<code>frame: Frame</code>  <code>property</code> <code>writable</code>","text":"<p>Get the frame the instance belongs to.</p> <p>Returns:</p> Type Description <code>Frame</code> <p>The back reference to the <code>Frame</code> that this <code>Instance</code> belongs to.</p>"},{"location":"io/instance/#dreem.io.Instance.gt_track_id","title":"<code>gt_track_id: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>The ground truth track id of the instance.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the ground truth track id</p>"},{"location":"io/instance/#dreem.io.Instance.instance_score","title":"<code>instance_score: float</code>  <code>property</code> <code>writable</code>","text":"<p>Get the pose prediction score associated with the instance.</p> <p>Returns: a float from 0-1 representing an instance_score.</p>"},{"location":"io/instance/#dreem.io.Instance.point_scores","title":"<code>point_scores: ArrayLike</code>  <code>property</code> <code>writable</code>","text":"<p>Get the point scores associated with the pose prediction.</p> <p>Returns: a vector of shape n containing the point scores outputed from sleap associated with pose predictions.</p>"},{"location":"io/instance/#dreem.io.Instance.pose","title":"<code>pose: dict[str, ArrayLike]</code>  <code>property</code> <code>writable</code>","text":"<p>Get the pose of the instance.</p> <p>Returns:</p> Type Description <code>dict[str, ArrayLike]</code> <p>A dictionary containing the node and corresponding x,y points</p>"},{"location":"io/instance/#dreem.io.Instance.pred_track_id","title":"<code>pred_track_id: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>The track id predicted by the tracker using asso_output from model.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the predicted track id.</p>"},{"location":"io/instance/#dreem.io.Instance.shown_pose","title":"<code>shown_pose: dict[str, ArrayLike]</code>  <code>property</code>","text":"<p>Get the pose with shown nodes only.</p> <p>Returns: A dictionary filtered by nodes that are shown (points are not nan).</p>"},{"location":"io/instance/#dreem.io.Instance.skeleton","title":"<code>skeleton: sio.Skeleton</code>  <code>property</code> <code>writable</code>","text":"<p>Get the skeleton associated with the instance.</p> <p>Returns: The sio.Skeleton associated with the instance.</p>"},{"location":"io/instance/#dreem.io.Instance.track_score","title":"<code>track_score: float</code>  <code>property</code> <code>writable</code>","text":"<p>Get the track_score of the instance.</p> <p>Returns: A float from 0-1 representing the output used in the tracker for assignment.</p>"},{"location":"io/instance/#dreem.io.Instance.__attrs_post_init__","title":"<code>__attrs_post_init__()</code>","text":"<p>Handle dimensionality and more intricate default initializations post-init.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def __attrs_post_init__(self) -&gt; None:\n    \"\"\"Handle dimensionality and more intricate default initializations post-init.\"\"\"\n    self.bbox = _expand_to_rank(self.bbox, 3)\n    self.crop = _expand_to_rank(self.crop, 4)\n    self.features = _expand_to_rank(self.features, 2)\n\n    if self.skeleton is None:\n        self.skeleton = sio.Skeleton([\"centroid\"])\n\n    if self.bbox.shape[-1] == 0:\n        self.bbox = torch.empty([1, 0, 4])\n\n    if self.crop.shape[-1] == 0 and self.bbox.shape[1] != 0:\n        y1, x1, y2, x2 = self.bbox.squeeze(dim=0).nanmean(dim=0)\n        self.centroid = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n    if len(self.pose) == 0 and self.bbox.shape[1]:\n        y1, x1, y2, x2 = self.bbox.squeeze(dim=0).mean(dim=0)\n        self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n    if self.point_scores is None and len(self.pose) != 0:\n        self._point_scores = np.zeros((len(self.pose), 2))\n\n    self.to(self.device)\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of the Instance.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation of the Instance.\"\"\"\n    return (\n        \"Instance(\"\n        f\"gt_track_id={self._gt_track_id.item()}, \"\n        f\"pred_track_id={self._pred_track_id.item()}, \"\n        f\"bbox={self._bbox}, \"\n        f\"centroid={self._centroid}, \"\n        f\"crop={self._crop.shape}, \"\n        f\"features={self._features.shape}, \"\n        f\"device={self._device}\"\n        \")\"\n    )\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.add_embedding","title":"<code>add_embedding(emb_type, embedding)</code>","text":"<p>Save embedding to instance embedding dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>Key/embedding type to be saved to dictionary</p> required <code>embedding</code> <code>Tensor</code> <p>The actual torch tensor embedding.</p> required Source code in <code>dreem/io/instance.py</code> <pre><code>def add_embedding(self, emb_type: str, embedding: torch.Tensor) -&gt; None:\n    \"\"\"Save embedding to instance embedding dictionary.\n\n    Args:\n        emb_type: Key/embedding type to be saved to dictionary\n        embedding: The actual torch tensor embedding.\n    \"\"\"\n    embedding = _expand_to_rank(embedding, 2)\n    self._embeddings[emb_type] = embedding\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.from_slp","title":"<code>from_slp(slp_instance, bbox_size=64, crop=None, device=None)</code>  <code>classmethod</code>","text":"<p>Convert a slp instance to a dreem instance.</p> <p>Parameters:</p> Name Type Description Default <code>slp_instance</code> <code>PredictedInstance | Instance</code> <p>A <code>sleap_io.Instance</code> object representing a detection</p> required <code>bbox_size</code> <code>int | tuple[int, int]</code> <p>size of the pose-centered bbox to form.</p> <code>64</code> <code>crop</code> <code>ArrayLike | None</code> <p>The corresponding crop of the bbox</p> <code>None</code> <code>device</code> <code>str | None</code> <p>which device to keep the instance on</p> <code>None</code> <p>Returns:     A dreem.Instance object with a pose-centered bbox and no crop.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>@classmethod\ndef from_slp(\n    cls,\n    slp_instance: sio.PredictedInstance | sio.Instance,\n    bbox_size: int | tuple[int, int] = 64,\n    crop: ArrayLike | None = None,\n    device: str | None = None,\n) -&gt; Self:\n    \"\"\"Convert a slp instance to a dreem instance.\n\n    Args:\n        slp_instance: A `sleap_io.Instance` object representing a detection\n        bbox_size: size of the pose-centered bbox to form.\n        crop: The corresponding crop of the bbox\n        device: which device to keep the instance on\n    Returns:\n        A dreem.Instance object with a pose-centered bbox and no crop.\n    \"\"\"\n    try:\n        track_id = int(slp_instance.track.name)\n    except ValueError:\n        track_id = int(\n            \"\".join([str(ord(c)) for c in slp_instance.track.name])\n        )  # better way to handle this?\n    if isinstance(bbox_size, int):\n        bbox_size = (bbox_size, bbox_size)\n\n    track_score = -1.0\n    point_scores = np.full(len(slp_instance.points), -1)\n    instance_score = -1\n    if isinstance(slp_instance, sio.PredictedInstance):\n        track_score = slp_instance.tracking_score\n        point_scores = slp_instance.numpy()[:, -1]\n        instance_score = slp_instance.score\n\n    centroid = np.nanmean(slp_instance.numpy(), axis=1)\n    bbox = [\n        centroid[1] - bbox_size[1],\n        centroid[0] - bbox_size[0],\n        centroid[1] + bbox_size[1],\n        centroid[0] + bbox_size[0],\n    ]\n    return cls(\n        gt_track_id=track_id,\n        bbox=bbox,\n        crop=crop,\n        centroid={\"centroid\": centroid},\n        track_score=track_score,\n        point_scores=point_scores,\n        instance_score=instance_score,\n        skeleton=slp_instance.skeleton,\n        pose={\n            node.name: point.numpy() for node, point in slp_instance.points.items()\n        },\n        device=device,\n    )\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.get_embedding","title":"<code>get_embedding(emb_type='all')</code>","text":"<p>Retrieve instance's spatial/temporal embedding.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>The string key of the embedding to retrieve. Should be \"pos\", \"temp\"</p> <code>'all'</code> <p>Returns:</p> Type Description <code>dict[str, Tensor] | Tensor | None</code> <ul> <li>A torch tensor representing the spatial/temporal location of the instance.</li> <li>None if the embedding is not stored</li> </ul> Source code in <code>dreem/io/instance.py</code> <pre><code>def get_embedding(\n    self, emb_type: str = \"all\"\n) -&gt; dict[str, torch.Tensor] | torch.Tensor | None:\n    \"\"\"Retrieve instance's spatial/temporal embedding.\n\n    Args:\n        emb_type: The string key of the embedding to retrieve. Should be \"pos\", \"temp\"\n\n    Returns:\n        * A torch tensor representing the spatial/temporal location of the instance.\n        * None if the embedding is not stored\n    \"\"\"\n    if emb_type.lower() == \"all\":\n        return self._embeddings\n    else:\n        try:\n            return self._embeddings[emb_type]\n        except KeyError:\n            logger.exception(\n                f\"{emb_type} not saved! Only {list(self._embeddings.keys())} are available\"\n            )\n    return None\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.has_bbox","title":"<code>has_bbox()</code>","text":"<p>Determine if the instance has a bbox.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has a bounding box, false otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_bbox(self) -&gt; bool:\n    \"\"\"Determine if the instance has a bbox.\n\n    Returns:\n        True if the instance has a bounding box, false otherwise.\n    \"\"\"\n    if self._bbox.shape[1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.has_crop","title":"<code>has_crop()</code>","text":"<p>Determine if the instance has a crop.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has an image otherwise False.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_crop(self) -&gt; bool:\n    \"\"\"Determine if the instance has a crop.\n\n    Returns:\n        True if the instance has an image otherwise False.\n    \"\"\"\n    if self._crop.shape[-1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.has_embedding","title":"<code>has_embedding(emb_type=None)</code>","text":"<p>Determine if the instance has embedding type requested.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str | None</code> <p>The key to check in the embedding dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if <code>emb_type</code> in embedding_dict else false</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_embedding(self, emb_type: str | None = None) -&gt; bool:\n    \"\"\"Determine if the instance has embedding type requested.\n\n    Args:\n        emb_type: The key to check in the embedding dictionary.\n\n    Returns:\n        True if `emb_type` in embedding_dict else false\n    \"\"\"\n    return emb_type in self._embeddings\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.has_features","title":"<code>has_features()</code>","text":"<p>Determine if the instance has computed reid features.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has reid features, False otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_features(self) -&gt; bool:\n    \"\"\"Determine if the instance has computed reid features.\n\n    Returns:\n        True if the instance has reid features, False otherwise.\n    \"\"\"\n    if self._features.shape[-1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.has_gt_track_id","title":"<code>has_gt_track_id()</code>","text":"<p>Determine if instance has a gt track assignment.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the gt track id is set, otherwise False.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_gt_track_id(self) -&gt; bool:\n    \"\"\"Determine if instance has a gt track assignment.\n\n    Returns:\n        True if the gt track id is set, otherwise False.\n    \"\"\"\n    if self._gt_track_id.shape[0] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.has_pose","title":"<code>has_pose()</code>","text":"<p>Check if the instance has a pose.</p> <p>Returns True if the instance has a pose.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_pose(self) -&gt; bool:\n    \"\"\"Check if the instance has a pose.\n\n    Returns True if the instance has a pose.\n    \"\"\"\n    if len(self.pose):\n        return True\n    return False\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.has_pred_track_id","title":"<code>has_pred_track_id()</code>","text":"<p>Determine whether instance has predicted track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if instance has a pred track id, False otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_pred_track_id(self) -&gt; bool:\n    \"\"\"Determine whether instance has predicted track id.\n\n    Returns:\n        True if instance has a pred track id, False otherwise.\n    \"\"\"\n    if self._pred_track_id.item() == -1 or self._pred_track_id.shape[0] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.to","title":"<code>to(map_location)</code>","text":"<p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <p>Parameters:</p> Name Type Description Default <code>map_location</code> <code>str | device</code> <p>Either the device or dtype for the instance to be moved.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>Self</code> <p>reference to the instance moved to correct device/dtype.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to(self, map_location: str | torch.device) -&gt; Self:\n    \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n    Args:\n        map_location: Either the device or dtype for the instance to be moved.\n\n    Returns:\n        self: reference to the instance moved to correct device/dtype.\n    \"\"\"\n    if map_location is not None and map_location != \"\":\n        self._gt_track_id = self._gt_track_id.to(map_location)\n        self._pred_track_id = self._pred_track_id.to(map_location)\n        self._bbox = self._bbox.to(map_location)\n        self._crop = self._crop.to(map_location)\n        self._features = self._features.to(map_location)\n        if isinstance(map_location, (str, torch.device)):\n            self.device = map_location\n\n    return self\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.to_h5","title":"<code>to_h5(frame_group=h5py.Group, label=None, **kwargs)</code>","text":"<p>Convert instance to an h5 group\".</p> By default we always save <ul> <li>the gt/pred track id</li> <li>bbox</li> <li>centroid</li> <li>pose</li> <li>instance/traj/points score</li> </ul> <p>Larger arrays (crops/features/embeddings) can be saved by passing as kwargs</p> <p>Parameters:</p> Name Type Description Default <code>frame_group</code> <p>the h5py group representing the frame the instance appears on</p> <code>Group</code> <code>label</code> <p>the name of the instance group that will be created</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>additional key:value pairs to be saved as datasets.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Group</code> <p>The h5 group representing this instance.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to_h5(self, frame_group=h5py.Group, label=None, **kwargs: dict) -&gt; h5py.Group:\n    \"\"\"Convert instance to an h5 group\".\n\n    By default we always save:\n        - the gt/pred track id\n        - bbox\n        - centroid\n        - pose\n        - instance/traj/points score\n    Larger arrays (crops/features/embeddings) can be saved by passing as kwargs\n\n    Args:\n        frame_group: the h5py group representing the frame the instance appears on\n        label: the name of the instance group that will be created\n        **kwargs: additional key:value pairs to be saved as datasets.\n\n    Returns:\n        The h5 group representing this instance.\n    \"\"\"\n    if label is None:\n        if pred_track_id != -1:\n            label = f\"instance_{self.pred_track_id.item()}\"\n        else:\n            label = f\"instance_{self.gt_track_id.item()}\"\n    instance_group = frame_group.create_group(label)\n    instance_group.attrs.create(\"gt_track_id\", self.gt_track_id.item())\n    instance_group.attrs.create(\"pred_track_id\", self.pred_track_id.item())\n    instance_group.attrs.create(\"track_score\", self.track_score)\n    instance_group.attrs.create(\"instance_score\", self.instance_score)\n\n    instance_group.create_dataset(\"bbox\", data=self.bbox.cpu().numpy())\n\n    pose_group = instance_group.create_group(\"pose\")\n    pose_group.create_dataset(\"points\", data=np.array(list(self.pose.values())))\n    pose_group.attrs.create(\"nodes\", list(self.pose.keys()))\n    pose_group.create_dataset(\"scores\", data=self.point_scores)\n\n    for key, value in kwargs.items():\n        if \"emb\" in key:\n            emb_group = instance_group.require_group(\"emb\")\n            emb_group.create_dataset(key, data=value)\n        else:\n            instance_group.create_dataset(key, data=value)\n\n    return instance_group\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.to_slp","title":"<code>to_slp(track_lookup={})</code>","text":"<p>Convert instance to sleap_io.PredictedInstance object.</p> <p>Parameters:</p> Name Type Description Default <code>track_lookup</code> <code>dict[int, Track]</code> <p>A track look up dictionary containing track_id:sio.Track.</p> <code>{}</code> <p>Returns: A sleap_io.PredictedInstance with necessary metadata     and a track_lookup dictionary to persist tracks.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to_slp(\n    self, track_lookup: dict[int, sio.Track] = {}\n) -&gt; tuple[sio.PredictedInstance, dict[int, sio.Track]]:\n    \"\"\"Convert instance to sleap_io.PredictedInstance object.\n\n    Args:\n        track_lookup: A track look up dictionary containing track_id:sio.Track.\n    Returns: A sleap_io.PredictedInstance with necessary metadata\n        and a track_lookup dictionary to persist tracks.\n    \"\"\"\n    try:\n        track_id = self.pred_track_id.item()\n        if track_id not in track_lookup:\n            track_lookup[track_id] = sio.Track(name=self.pred_track_id.item())\n\n        track = track_lookup[track_id]\n\n        return (\n            sio.PredictedInstance.from_numpy(\n                points=np.array(list(self.pose.values())),\n                skeleton=self.skeleton,\n                point_scores=self.point_scores,\n                instance_score=self.instance_score,\n                tracking_score=self.track_score,\n                track=track,\n            ),\n            track_lookup,\n        )\n    except Exception as e:\n        logger.exception(\n            f\"Pose: {np.array(list(self.pose.values())).shape}, Pose score shape {self.point_scores.shape}\"\n        )\n        raise RuntimeError(f\"Failed to convert to sio.PredictedInstance: {e}\")\n</code></pre>"},{"location":"io/track/","title":"<code>Track</code>","text":""},{"location":"io/track/#dreem.io.Track","title":"<code>dreem.io.Track</code>","text":"<p>Object for storing instances of the same track.</p> <p>Attributes:</p> Name Type Description <code>id</code> <p>the track label.</p> <code>instances</code> <code>list['Instances']</code> <p>A list of instances belonging to the track.</p> Source code in <code>dreem/io/track.py</code> <pre><code>@attrs.define(eq=False)\nclass Track:\n    \"\"\"Object for storing instances of the same track.\n\n    Attributes:\n        id: the track label.\n        instances: A list of instances belonging to the track.\n    \"\"\"\n\n    _id: int = attrs.field(alias=\"id\")\n    _instances: list[\"Instance\"] = attrs.field(alias=\"instances\", factory=list)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get the string representation of the track.\n\n        Returns:\n            the string representation of the Track.\n        \"\"\"\n        return f\"Track(id={self.id}, len={len(self)})\"\n\n    @property\n    def track_id(self) -&gt; int:\n        \"\"\"Get the id of the track.\n\n        Returns:\n            The integer id of the track.\n        \"\"\"\n        return self._id\n\n    @track_id.setter\n    def track_id(self, track_id: int) -&gt; None:\n        \"\"\"Set the id of the track.\n\n        Args:\n            track_id: the int id of the track.\n        \"\"\"\n        self._id = track_id\n\n    @property\n    def instances(self) -&gt; list[\"Instances\"]:\n        \"\"\"Get the instances belonging to this track.\n\n        Returns:\n            A list of instances with this track id.\n        \"\"\"\n        return self._instances\n\n    @instances.setter\n    def instances(self, instances) -&gt; None:\n        \"\"\"Set the instances belonging to this track.\n\n        Args:\n            instances: A list of instances that belong to the same track.\n        \"\"\"\n        self._instances = instances\n\n    @property\n    def frames(self) -&gt; set[\"Frame\"]:\n        \"\"\"Get the frames where this track appears.\n\n        Returns:\n            A set of `Frame` objects where this track appears.\n        \"\"\"\n        return set([instance.frame for instance in self.instances])\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the length of the track.\n\n        Returns:\n            The number of instances/frames in the track.\n        \"\"\"\n        return len(self.instances)\n\n    def __getitem__(self, ind: int | list[int]) -&gt; \"Instance\" | list[\"Instance\"]:\n        \"\"\"Get an instance from the track.\n\n        Args:\n            ind: Either a single int or list of int indices.\n\n        Returns:\n            the instance at that index of the track.instances.\n        \"\"\"\n        if isinstance(ind, int):\n            return self.instances[ind]\n        elif isinstance(ind, list):\n            return [self.instances[i] for i in ind]\n        else:\n            raise ValueError(f\"Ind must be an int or list of ints, found {type(ind)}\")\n</code></pre>"},{"location":"io/track/#dreem.io.Track.frames","title":"<code>frames: set['Frame']</code>  <code>property</code>","text":"<p>Get the frames where this track appears.</p> <p>Returns:</p> Type Description <code>set['Frame']</code> <p>A set of <code>Frame</code> objects where this track appears.</p>"},{"location":"io/track/#dreem.io.Track.instances","title":"<code>instances: list['Instances']</code>  <code>property</code> <code>writable</code>","text":"<p>Get the instances belonging to this track.</p> <p>Returns:</p> Type Description <code>list['Instances']</code> <p>A list of instances with this track id.</p>"},{"location":"io/track/#dreem.io.Track.track_id","title":"<code>track_id: int</code>  <code>property</code> <code>writable</code>","text":"<p>Get the id of the track.</p> <p>Returns:</p> Type Description <code>int</code> <p>The integer id of the track.</p>"},{"location":"io/track/#dreem.io.Track.__getitem__","title":"<code>__getitem__(ind)</code>","text":"<p>Get an instance from the track.</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> <code>int | list[int]</code> <p>Either a single int or list of int indices.</p> required <p>Returns:</p> Type Description <code>'Instance' | list['Instance']</code> <p>the instance at that index of the track.instances.</p> Source code in <code>dreem/io/track.py</code> <pre><code>def __getitem__(self, ind: int | list[int]) -&gt; \"Instance\" | list[\"Instance\"]:\n    \"\"\"Get an instance from the track.\n\n    Args:\n        ind: Either a single int or list of int indices.\n\n    Returns:\n        the instance at that index of the track.instances.\n    \"\"\"\n    if isinstance(ind, int):\n        return self.instances[ind]\n    elif isinstance(ind, list):\n        return [self.instances[i] for i in ind]\n    else:\n        raise ValueError(f\"Ind must be an int or list of ints, found {type(ind)}\")\n</code></pre>"},{"location":"io/track/#dreem.io.Track.__len__","title":"<code>__len__()</code>","text":"<p>Get the length of the track.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of instances/frames in the track.</p> Source code in <code>dreem/io/track.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the length of the track.\n\n    Returns:\n        The number of instances/frames in the track.\n    \"\"\"\n    return len(self.instances)\n</code></pre>"},{"location":"io/track/#dreem.io.Track.__repr__","title":"<code>__repr__()</code>","text":"<p>Get the string representation of the track.</p> <p>Returns:</p> Type Description <code>str</code> <p>the string representation of the Track.</p> Source code in <code>dreem/io/track.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get the string representation of the track.\n\n    Returns:\n        the string representation of the Track.\n    \"\"\"\n    return f\"Track(id={self.id}, len={len(self)})\"\n</code></pre>"},{"location":"io/visualize/","title":"<code>visualize</code>","text":""},{"location":"io/visualize/#dreem.io.visualize","title":"<code>dreem.io.visualize</code>","text":"<p>Helper functions for visualizing tracking.</p>"},{"location":"io/visualize/#dreem.io.visualize.annotate_video","title":"<code>annotate_video(video, labels, key, color_palette=palette, trails=2, boxes=(64, 64), names=True, track_scores=0.5, centroids=4, poses=False, save_path='debug_animal.mp4', fps=30, alpha=0.2)</code>","text":"<p>Annotate video frames with labels.</p> <p>Labels video with bboxes, centroids, trajectory trails, and/or poses.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>Reader</code> <p>The video to be annotated in an ndarray</p> required <code>labels</code> <code>DataFrame</code> <p>The pandas dataframe containing the centroid and/or pose locations of the instances</p> required <code>key</code> <code>str</code> <p>The key where labels are stored in the dataframe - mostly used for choosing whether to annotate based on pred or gt labels</p> required <code>color_palette</code> <code>list | str</code> <p>The matplotlib colorpalette to use for annotating the video. Defaults to <code>tab10</code></p> <code>palette</code> <code>trails</code> <code>int</code> <p>The size of the trajectory trail. If trails size &lt;= 0 or None then it is not added</p> <code>2</code> <code>boxes</code> <code>int</code> <p>The size of the bbox. If bbox size &lt;= 0 or None then it is not added</p> <code>(64, 64)</code> <code>names</code> <code>bool</code> <p>Whether or not to annotate with name</p> <code>True</code> <code>centroids</code> <code>int</code> <p>The size of the centroid. If centroid size &lt;= 0 or None then it is not added</p> <code>4</code> <code>poses</code> <code>bool</code> <p>Whether or not to annotate with poses</p> <code>False</code> <code>fps</code> <code>int</code> <p>The frame rate of the generated video</p> <code>30</code> <code>alpha</code> <code>float</code> <p>The opacity of the annotations.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of annotated video frames</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def annotate_video(\n    video: \"imageio.core.format.Reader\",\n    labels: pd.DataFrame,\n    key: str,\n    color_palette: list | str = palette,\n    trails: int = 2,\n    boxes: int = (64, 64),\n    names: bool = True,\n    track_scores=0.5,\n    centroids: int = 4,\n    poses: bool = False,\n    save_path: str = \"debug_animal.mp4\",\n    fps: int = 30,\n    alpha: float = 0.2,\n) -&gt; list:\n    \"\"\"Annotate video frames with labels.\n\n    Labels video with bboxes, centroids, trajectory trails, and/or poses.\n\n    Args:\n        video: The video to be annotated in an ndarray\n        labels: The pandas dataframe containing the centroid and/or pose locations of the instances\n        key: The key where labels are stored in the dataframe - mostly used for choosing whether to annotate based on pred or gt labels\n        color_palette: The matplotlib colorpalette to use for annotating the video. Defaults to `tab10`\n        trails: The size of the trajectory trail. If trails size &lt;= 0 or None then it is not added\n        boxes: The size of the bbox. If bbox size &lt;= 0 or None then it is not added\n        names: Whether or not to annotate with name\n        centroids: The size of the centroid. If centroid size &lt;= 0 or None then it is not added\n        poses: Whether or not to annotate with poses\n        fps: The frame rate of the generated video\n        alpha: The opacity of the annotations.\n\n    Returns:\n        A list of annotated video frames\n    \"\"\"\n    writer = imageio.get_writer(save_path, fps=fps)\n    color_palette = (\n        sns.color_palette(color_palette)\n        if isinstance(color_palette, str)\n        else deepcopy(color_palette)\n    )\n\n    if trails:\n        track_trails = {}\n    try:\n        for i in tqdm(sorted(labels[\"Frame\"].unique()), desc=\"Frame\", unit=\"Frame\"):\n            frame = video.get_data(i)\n            if frame.shape[0] == 1 or frame.shape[-1] == 1:\n                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n            # else:\n            #     frame = frame.copy()\n\n            lf = labels[labels[\"Frame\"] == i]\n            for idx, instance in lf.iterrows():\n                if not trails:\n                    track_trails = {}\n\n                if poses:\n                    # TODO figure out best way to store poses (maybe pass a slp labels file too?)\n                    trails = False\n                    centroids = False\n                    for idx, (pose, edge) in enumerate(\n                        zip(instance[\"poses\"], instance[\"edges\"])\n                    ):\n                        pose = fill_missing(pose.numpy())\n\n                        pred_track_id = instance[key][idx].numpy().tolist()\n\n                        # Add midpt to track trail.\n                        if pred_track_id not in list(track_trails.keys()):\n                            track_trails[pred_track_id] = []\n\n                        # Select a color based on track_id.\n                        track_color_idx = pred_track_id % len(color_palette)\n                        track_color = (\n                            (np.array(color_palette[track_color_idx]) * 255)\n                            .astype(np.uint8)\n                            .tolist()[::-1]\n                        )\n\n                        for p in pose:\n                            # try:\n                            #    p = tuple([int(i) for i in p.numpy()][::-1])\n                            # except:\n                            #    continue\n\n                            p = tuple(int(i) for i in p)[::-1]\n\n                            track_trails[pred_track_id].append(p)\n\n                            frame = cv2.circle(\n                                frame, p, radius=2, color=track_color, thickness=-1\n                            )\n\n                        for e in edge:\n                            source = tuple(int(i) for i in pose[int(e[0])])[::-1]\n                            target = tuple(int(i) for i in pose[int(e[1])])[::-1]\n\n                            frame = cv2.line(frame, source, target, track_color, 1)\n\n                if (boxes) or centroids:\n                    # Get coordinates for detected objects in the current frame.\n                    if isinstance(boxes, int):\n                        boxes = (boxes, boxes)\n\n                    box_w, box_h = boxes\n                    x = instance[\"X\"]\n                    y = instance[\"Y\"]\n                    min_x, min_y, max_x, max_y = (\n                        int(x - box_w / 2),\n                        int(y - box_h / 2),\n                        int(x + box_w / 2),\n                        int(y + box_h / 2),\n                    )\n                    midpt = (int(x), int(y))\n\n                    pred_track_id = instance[key]\n\n                    if \"Track_score\" in instance.index:\n                        track_score = instance[\"Track_score\"]\n                    else:\n                        track_scores = 0\n\n                    # Add midpt to track trail.\n                    if pred_track_id not in list(track_trails.keys()):\n                        track_trails[pred_track_id] = []\n                    track_trails[pred_track_id].append(midpt)\n\n                    # Select a color based on track_id.\n                    track_color_idx = int(pred_track_id) % len(color_palette)\n                    track_color = (\n                        (np.array(color_palette[track_color_idx]) * 255)\n                        .astype(np.uint8)\n                        .tolist()[::-1]\n                    )\n\n                    # Bbox.\n                    if boxes is not None:\n                        frame = cv2.rectangle(\n                            frame,\n                            (min_x, min_y),\n                            (max_x, max_y),\n                            color=track_color,\n                            thickness=2,\n                        )\n\n                    # Track trail.\n                    if centroids:\n                        frame = cv2.circle(\n                            frame,\n                            midpt,\n                            radius=centroids,\n                            color=track_color,\n                            thickness=-1,\n                        )\n                        for i in range(0, len(track_trails[pred_track_id]) - 1):\n                            frame = cv2.addWeighted(\n                                cv2.circle(\n                                    frame,  # .copy(),\n                                    track_trails[pred_track_id][i],\n                                    radius=4,\n                                    color=track_color,\n                                    thickness=-1,\n                                ),\n                                alpha,\n                                frame,\n                                1 - alpha,\n                                0,\n                            )\n                            if trails:\n                                frame = cv2.line(\n                                    frame,\n                                    track_trails[pred_track_id][i],\n                                    track_trails[pred_track_id][i + 1],\n                                    color=track_color,\n                                    thickness=trails,\n                                )\n\n                # Track name.\n                name_str = \"\"\n\n                if names:\n                    name_str += f\"track_{pred_track_id}\"\n                if names and track_scores:\n                    name_str += \" | \"\n                if track_scores:\n                    name_str += f\"score: {track_score:0.3f}\"\n\n                if len(name_str) &gt; 0:\n                    frame = cv2.putText(\n                        frame,\n                        # f\"idx:{idx} | track_{pred_track_id}\",\n                        name_str,\n                        org=(int(min_x), max(0, int(min_y) - 10)),\n                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n                        fontScale=0.9,\n                        color=track_color,\n                        thickness=2,\n                    )\n            writer.append_data(frame)\n            # if i % fps == 0:\n            #     gc.collect()\n\n    except Exception as e:\n        writer.close()\n        logger.exception(e)\n        return False\n\n    writer.close()\n    return True\n</code></pre>"},{"location":"io/visualize/#dreem.io.visualize.bold","title":"<code>bold(val, thresh=0.01)</code>","text":"<p>Bold value if it is over a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>float</code> <p>The value to bold or not</p> required <code>thresh</code> <code>float</code> <p>The threshold the value has to exceed to be bolded</p> <code>0.01</code> <p>Returns:</p> Type Description <code>str</code> <p>A string indicating how to bold the item.</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def bold(val: float, thresh: float = 0.01) -&gt; str:\n    \"\"\"Bold value if it is over a threshold.\n\n    Args:\n        val: The value to bold or not\n        thresh: The threshold the value has to exceed to be bolded\n\n    Returns:\n        A string indicating how to bold the item.\n    \"\"\"\n    bold = \"bold\" if float(val) &gt; thresh else \"\"\n    return f\"font-weight: {bold}\"\n</code></pre>"},{"location":"io/visualize/#dreem.io.visualize.color","title":"<code>color(val, thresh=0.01)</code>","text":"<p>Highlight value in dataframe if it is over a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>float</code> <p>The value to color</p> required <code>thresh</code> <code>float</code> <p>The threshold for which to color</p> <code>0.01</code> <p>Returns:</p> Type Description <code>str</code> <p>A string containing how to highlight the value</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def color(val: float, thresh: float = 0.01) -&gt; str:\n    \"\"\"Highlight value in dataframe if it is over a threshold.\n\n    Args:\n        val: The value to color\n        thresh: The threshold for which to color\n\n    Returns:\n        A string containing how to highlight the value\n    \"\"\"\n    color = \"lightblue\" if float(val) &gt; thresh else \"\"\n    return f\"background-color: {color}\"\n</code></pre>"},{"location":"io/visualize/#dreem.io.visualize.fill_missing","title":"<code>fill_missing(data, kind='linear')</code>","text":"<p>Fill missing values independently along each dimension after the first.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>the array for which to fill missing value</p> required <code>kind</code> <code>str</code> <p>How to interpolate missing values using <code>scipy.interpoloate.interp1d</code></p> <code>'linear'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The array with missing values filled in</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def fill_missing(data: np.ndarray, kind: str = \"linear\") -&gt; np.ndarray:\n    \"\"\"Fill missing values independently along each dimension after the first.\n\n    Args:\n        data: the array for which to fill missing value\n        kind: How to interpolate missing values using `scipy.interpoloate.interp1d`\n\n    Returns:\n        The array with missing values filled in\n    \"\"\"\n    # Store initial shape.\n    initial_shape = data.shape\n\n    # Flatten after first dim.\n    data = data.reshape((initial_shape[0], -1))\n\n    # Interpolate along each slice.\n    for i in range(data.shape[-1]):\n        y = data[:, i]\n\n        # Build interpolant.\n        x = np.flatnonzero(~np.isnan(y))\n        f = interp1d(x, y[x], kind=kind, fill_value=np.nan, bounds_error=False)\n\n        # Fill missing\n        xq = np.flatnonzero(np.isnan(y))\n        y[xq] = f(xq)\n\n        # Fill leading or trailing NaNs with the nearest non-NaN values\n        mask = np.isnan(y)\n        y[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), y[~mask])\n\n        # Save slice\n        data[:, i] = y\n\n    # Restore to initial shape.\n    data = data.reshape(initial_shape)\n\n    return data\n</code></pre>"},{"location":"io/visualize/#dreem.io.visualize.main","title":"<code>main(cfg)</code>","text":"<p>Take in a path to a video + labels file, annotates a video and saves it to the specified path.</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>@hydra.main(config_path=None, config_name=None, version_base=None)\ndef main(cfg: DictConfig):\n    \"\"\"Take in a path to a video + labels file, annotates a video and saves it to the specified path.\"\"\"\n    labels = pd.read_csv(cfg.labels_path)\n    video = imageio.get_reader(cfg.vid_path, \"ffmpeg\")\n    frames_annotated = annotate_video(\n        video, labels, save_path=cfg.save_path, **cfg.annotate\n    )\n\n    if frames_annotated:\n        logger.info(\"Video saved to {cfg.save_path}!\")\n    else:\n        logger.error(\"Failed to annotate video!\")\n</code></pre>"},{"location":"io/visualize/#dreem.io.visualize.save_vid","title":"<code>save_vid(annotated_frames, save_path='debug_animal', fps=30)</code>","text":"<p>Save video to file.</p> <p>Parameters:</p> Name Type Description Default <code>annotated_frames</code> <code>list</code> <p>a list of frames annotated by <code>annotate_frames</code></p> required <code>save_path</code> <code>str</code> <p>The path of the annotated file.</p> <code>'debug_animal'</code> <code>fps</code> <code>int</code> <p>The frame rate in frames per second of the annotated video</p> <code>30</code> Source code in <code>dreem/io/visualize.py</code> <pre><code>def save_vid(\n    annotated_frames: list,\n    save_path: str = \"debug_animal\",\n    fps: int = 30,\n):\n    \"\"\"Save video to file.\n\n    Args:\n        annotated_frames: a list of frames annotated by `annotate_frames`\n        save_path: The path of the annotated file.\n        fps: The frame rate in frames per second of the annotated video\n    \"\"\"\n    for idx, (ds_name, data) in enumerate([(save_path, annotated_frames)]):\n        imageio.mimwrite(f\"{ds_name}.mp4\", data, fps=fps, macro_block_size=1)\n</code></pre>"},{"location":"models/","title":"DREEM Models","text":""},{"location":"models/#user-facing-models","title":"User-facing models","text":"<p>There are two main model APIs users should interact with.</p> <ol> <li><code>GlobalTrackingTransformer</code> is the underlying model architecture we use for tracking. It is made up of a <code>VisualEncoder</code> and a <code>Transformer</code> <code>Encoder-Decoder</code>. Only more advanced users who have familiarity with python and pytorch should interact with this model. For others see below</li> <li><code>GTRRunner</code> is a <code>pytorch_lightning</code> around the <code>GlobalTrackingTransformer</code>. It implements the basic routines you need for training, validation and testing. Most users will interact with this model.</li> </ol>"},{"location":"models/#model-parts","title":"Model Parts","text":"<p>For advanced users who are interested in extending our model, we have modularized each component so that its easy to compose into your own custom model. The model parts are</p> <ol> <li><code>VisualEncoder</code>: A CNN backbone used for feature extraction.</li> <li><code>Transformer</code> which is composed of a:<ul> <li>SpatioTemporal <code>Embedding</code> which computes the spatial and temporal embedding of each detection.</li> <li><code>TransformerEncoder</code>: A stack of <code>TransformerEncoderLayer</code>s</li> <li><code>TransformerDecoder</code>: A stack of <code>TransformerDecoderLayer</code>s</li> </ul> </li> <li>An <code>AttentionHead</code> which computes the association matrix from the transformer output.</li> </ol>"},{"location":"models/global_tracking_transformer/","title":"<code>GlobalTrackingTransformer</code>","text":""},{"location":"models/global_tracking_transformer/#dreem.models.GlobalTrackingTransformer","title":"<code>dreem.models.GlobalTrackingTransformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Modular GTR model composed of visual encoder + transformer used for tracking.</p> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>class GlobalTrackingTransformer(torch.nn.Module):\n    \"\"\"Modular GTR model composed of visual encoder + transformer used for tracking.\"\"\"\n\n    def __init__(\n        self,\n        encoder_cfg: dict | None = None,\n        d_model: int = 1024,\n        nhead: int = 8,\n        num_encoder_layers: int = 6,\n        num_decoder_layers: int = 6,\n        dropout: int = 0.1,\n        activation: str = \"relu\",\n        return_intermediate_dec: bool = False,\n        norm: bool = False,\n        num_layers_attn_head: int = 2,\n        dropout_attn_head: int = 0.1,\n        embedding_meta: dict | None = None,\n        return_embedding: bool = False,\n        decoder_self_attn: bool = False,\n    ):\n        \"\"\"Initialize GTR.\n\n        Args:\n            encoder_cfg: Dictionary of arguments to pass to the CNN constructor,\n                e.g: `cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}`\n            d_model: The number of features in the encoder/decoder inputs.\n            nhead: The number of heads in the transfomer encoder/decoder.\n            num_encoder_layers: The number of encoder-layers in the encoder.\n            num_decoder_layers: The number of decoder-layers in the decoder.\n            dropout: Dropout value applied to the output of transformer layers.\n            activation: Activation function to use.\n            return_intermediate_dec: Return intermediate layers from decoder.\n            norm: If True, normalize output of encoder and decoder.\n            num_layers_attn_head: The number of layers in the attention head.\n            dropout_attn_head: Dropout value for the attention_head.\n            embedding_meta: Metadata for positional embeddings. See below.\n            return_embedding: Whether to return the positional embeddings\n            decoder_self_attn: If True, use decoder self attention.\n\n                More details on `embedding_meta`:\n                    By default this will be an empty dict and indicate\n                    that no positional embeddings should be used. To use the positional embeddings\n                    pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                    `{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},\n                    \"temp\": {'mode': 'learned', 'emb_num': 16}}`. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                    and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n        \"\"\"\n        super().__init__()\n\n        if encoder_cfg is not None:\n            self.visual_encoder = VisualEncoder(d_model=d_model, **encoder_cfg)\n        else:\n            self.visual_encoder = VisualEncoder(d_model=d_model)\n\n        self.transformer = Transformer(\n            d_model=d_model,\n            nhead=nhead,\n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers,\n            dropout=dropout,\n            activation=activation,\n            return_intermediate_dec=return_intermediate_dec,\n            norm=norm,\n            num_layers_attn_head=num_layers_attn_head,\n            dropout_attn_head=dropout_attn_head,\n            embedding_meta=embedding_meta,\n            return_embedding=return_embedding,\n            decoder_self_attn=decoder_self_attn,\n        )\n\n    def forward(\n        self, ref_instances: list[\"Instance\"], query_instances: list[\"Instance\"] = None\n    ) -&gt; list[\"AssociationMatrix\"]:\n        \"\"\"Execute forward pass of GTR Model to get asso matrix.\n\n        Args:\n            ref_instances: List of instances from chunk containing crops of objects + gt label info\n            query_instances: list of instances used as query in decoder.\n\n        Returns:\n            An N_T x N association matrix\n        \"\"\"\n        # Extract feature representations with pre-trained encoder.\n        self.extract_features(ref_instances)\n\n        if query_instances:\n            self.extract_features(query_instances)\n\n        asso_preds = self.transformer(ref_instances, query_instances)\n\n        return asso_preds\n\n    def extract_features(\n        self, instances: list[\"Instance\"], force_recompute: bool = False\n    ) -&gt; None:\n        \"\"\"Extract features from instances using visual encoder backbone.\n\n        Args:\n            instances: A list of instances to compute features for\n            force_recompute: indicate whether to compute features for all instances regardless of if they have instances\n        \"\"\"\n        if not force_recompute:\n            instances_to_compute = [\n                instance\n                for instance in instances\n                if instance.has_crop() and not instance.has_features()\n            ]\n        else:\n            instances_to_compute = instances\n\n        if len(instances_to_compute) == 0:\n            return\n        elif len(instances_to_compute) == 1:  # handle batch norm error when B=1\n            instances_to_compute = instances\n\n        crops = torch.concatenate([instance.crop for instance in instances_to_compute])\n\n        features = self.visual_encoder(crops)\n\n        for i, z_i in enumerate(features):\n            instances_to_compute[i].features = z_i\n</code></pre>"},{"location":"models/global_tracking_transformer/#dreem.models.GlobalTrackingTransformer.__init__","title":"<code>__init__(encoder_cfg=None, d_model=1024, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dropout=0.1, activation='relu', return_intermediate_dec=False, norm=False, num_layers_attn_head=2, dropout_attn_head=0.1, embedding_meta=None, return_embedding=False, decoder_self_attn=False)</code>","text":"<p>Initialize GTR.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_cfg</code> <code>dict | None</code> <p>Dictionary of arguments to pass to the CNN constructor, e.g: <code>cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}</code></p> <code>None</code> <code>d_model</code> <code>int</code> <p>The number of features in the encoder/decoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads in the transfomer encoder/decoder.</p> <code>8</code> <code>num_encoder_layers</code> <code>int</code> <p>The number of encoder-layers in the encoder.</p> <code>6</code> <code>num_decoder_layers</code> <code>int</code> <p>The number of decoder-layers in the decoder.</p> <code>6</code> <code>dropout</code> <code>int</code> <p>Dropout value applied to the output of transformer layers.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>return_intermediate_dec</code> <code>bool</code> <p>Return intermediate layers from decoder.</p> <code>False</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of encoder and decoder.</p> <code>False</code> <code>num_layers_attn_head</code> <code>int</code> <p>The number of layers in the attention head.</p> <code>2</code> <code>dropout_attn_head</code> <code>int</code> <p>Dropout value for the attention_head.</p> <code>0.1</code> <code>embedding_meta</code> <code>dict | None</code> <p>Metadata for positional embeddings. See below.</p> <code>None</code> <code>return_embedding</code> <code>bool</code> <p>Whether to return the positional embeddings</p> <code>False</code> <code>decoder_self_attn</code> <code>bool</code> <p>If True, use decoder self attention.</p> <p>More details on <code>embedding_meta</code>:     By default this will be an empty dict and indicate     that no positional embeddings should be used. To use the positional embeddings     pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:     <code>{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},     \"temp\": {'mode': 'learned', 'emb_num': 16}}</code>. (see <code>dreem.models.embeddings.Embedding.EMB_TYPES</code>     and <code>dreem.models.embeddings.Embedding.EMB_MODES</code> for embedding parameters).</p> <code>False</code> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def __init__(\n    self,\n    encoder_cfg: dict | None = None,\n    d_model: int = 1024,\n    nhead: int = 8,\n    num_encoder_layers: int = 6,\n    num_decoder_layers: int = 6,\n    dropout: int = 0.1,\n    activation: str = \"relu\",\n    return_intermediate_dec: bool = False,\n    norm: bool = False,\n    num_layers_attn_head: int = 2,\n    dropout_attn_head: int = 0.1,\n    embedding_meta: dict | None = None,\n    return_embedding: bool = False,\n    decoder_self_attn: bool = False,\n):\n    \"\"\"Initialize GTR.\n\n    Args:\n        encoder_cfg: Dictionary of arguments to pass to the CNN constructor,\n            e.g: `cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}`\n        d_model: The number of features in the encoder/decoder inputs.\n        nhead: The number of heads in the transfomer encoder/decoder.\n        num_encoder_layers: The number of encoder-layers in the encoder.\n        num_decoder_layers: The number of decoder-layers in the decoder.\n        dropout: Dropout value applied to the output of transformer layers.\n        activation: Activation function to use.\n        return_intermediate_dec: Return intermediate layers from decoder.\n        norm: If True, normalize output of encoder and decoder.\n        num_layers_attn_head: The number of layers in the attention head.\n        dropout_attn_head: Dropout value for the attention_head.\n        embedding_meta: Metadata for positional embeddings. See below.\n        return_embedding: Whether to return the positional embeddings\n        decoder_self_attn: If True, use decoder self attention.\n\n            More details on `embedding_meta`:\n                By default this will be an empty dict and indicate\n                that no positional embeddings should be used. To use the positional embeddings\n                pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                `{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},\n                \"temp\": {'mode': 'learned', 'emb_num': 16}}`. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n    \"\"\"\n    super().__init__()\n\n    if encoder_cfg is not None:\n        self.visual_encoder = VisualEncoder(d_model=d_model, **encoder_cfg)\n    else:\n        self.visual_encoder = VisualEncoder(d_model=d_model)\n\n    self.transformer = Transformer(\n        d_model=d_model,\n        nhead=nhead,\n        num_encoder_layers=num_encoder_layers,\n        num_decoder_layers=num_decoder_layers,\n        dropout=dropout,\n        activation=activation,\n        return_intermediate_dec=return_intermediate_dec,\n        norm=norm,\n        num_layers_attn_head=num_layers_attn_head,\n        dropout_attn_head=dropout_attn_head,\n        embedding_meta=embedding_meta,\n        return_embedding=return_embedding,\n        decoder_self_attn=decoder_self_attn,\n    )\n</code></pre>"},{"location":"models/global_tracking_transformer/#dreem.models.GlobalTrackingTransformer.extract_features","title":"<code>extract_features(instances, force_recompute=False)</code>","text":"<p>Extract features from instances using visual encoder backbone.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[Instance]</code> <p>A list of instances to compute features for</p> required <code>force_recompute</code> <code>bool</code> <p>indicate whether to compute features for all instances regardless of if they have instances</p> <code>False</code> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def extract_features(\n    self, instances: list[\"Instance\"], force_recompute: bool = False\n) -&gt; None:\n    \"\"\"Extract features from instances using visual encoder backbone.\n\n    Args:\n        instances: A list of instances to compute features for\n        force_recompute: indicate whether to compute features for all instances regardless of if they have instances\n    \"\"\"\n    if not force_recompute:\n        instances_to_compute = [\n            instance\n            for instance in instances\n            if instance.has_crop() and not instance.has_features()\n        ]\n    else:\n        instances_to_compute = instances\n\n    if len(instances_to_compute) == 0:\n        return\n    elif len(instances_to_compute) == 1:  # handle batch norm error when B=1\n        instances_to_compute = instances\n\n    crops = torch.concatenate([instance.crop for instance in instances_to_compute])\n\n    features = self.visual_encoder(crops)\n\n    for i, z_i in enumerate(features):\n        instances_to_compute[i].features = z_i\n</code></pre>"},{"location":"models/global_tracking_transformer/#dreem.models.GlobalTrackingTransformer.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute forward pass of GTR Model to get asso matrix.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>List of instances from chunk containing crops of objects + gt label info</p> required <code>query_instances</code> <code>list[Instance]</code> <p>list of instances used as query in decoder.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[AssociationMatrix]</code> <p>An N_T x N association matrix</p> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def forward(\n    self, ref_instances: list[\"Instance\"], query_instances: list[\"Instance\"] = None\n) -&gt; list[\"AssociationMatrix\"]:\n    \"\"\"Execute forward pass of GTR Model to get asso matrix.\n\n    Args:\n        ref_instances: List of instances from chunk containing crops of objects + gt label info\n        query_instances: list of instances used as query in decoder.\n\n    Returns:\n        An N_T x N association matrix\n    \"\"\"\n    # Extract feature representations with pre-trained encoder.\n    self.extract_features(ref_instances)\n\n    if query_instances:\n        self.extract_features(query_instances)\n\n    asso_preds = self.transformer(ref_instances, query_instances)\n\n    return asso_preds\n</code></pre>"},{"location":"models/gtr_runner/","title":"<code>GTRRunner</code>","text":""},{"location":"models/gtr_runner/#dreem.models.GTRRunner","title":"<code>dreem.models.GTRRunner</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A lightning wrapper around GTR model.</p> <p>Used for training, validation and inference.</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>class GTRRunner(LightningModule):\n    \"\"\"A lightning wrapper around GTR model.\n\n    Used for training, validation and inference.\n    \"\"\"\n\n    DEFAULT_METRICS = {\n        \"train\": [],\n        \"val\": [\"num_switches\"],\n        \"test\": [\"num_switches\"],\n    }\n    DEFAULT_TRACKING = {\n        \"train\": False,\n        \"val\": True,\n        \"test\": True,\n    }\n    DEFAULT_SAVE = {\"train\": False, \"val\": False, \"test\": False}\n\n    def __init__(\n        self,\n        model_cfg: dict | None = None,\n        tracker_cfg: dict | None = None,\n        loss_cfg: dict | None = None,\n        optimizer_cfg: dict | None = None,\n        scheduler_cfg: dict | None = None,\n        metrics: dict[str, list[str]] | None = None,\n        persistent_tracking: dict[str, bool] | None = None,\n        test_save_path=\"./test_results.h5\",\n    ):\n        \"\"\"Initialize a lightning module for GTR.\n\n        Args:\n            model_cfg: hyperparameters for GlobalTrackingTransformer\n            tracker_cfg: The parameters used for the tracker post-processing\n            loss_cfg: hyperparameters for AssoLoss\n            optimizer_cfg: hyper parameters used for optimizer.\n                       Only used to overwrite `configure_optimizer`\n            scheduler_cfg: hyperparameters for lr_scheduler used to overwrite `configure_optimizer\n            metrics: a dict containing the metrics to be computed during train, val, and test.\n            persistent_tracking: a dict containing whether to use persistent tracking during train, val and test inference.\n            test_save_path: path to an .h5 file to save the test results to\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.model_cfg = model_cfg if model_cfg else {}\n        self.loss_cfg = loss_cfg if loss_cfg else {}\n        self.tracker_cfg = tracker_cfg if tracker_cfg else {}\n\n        _ = self.model_cfg.pop(\"ckpt_path\", None)\n        self.model = GlobalTrackingTransformer(**self.model_cfg)\n        self.loss = AssoLoss(**self.loss_cfg)\n        self.tracker = Tracker(**self.tracker_cfg)\n\n        self.optimizer_cfg = optimizer_cfg\n        self.scheduler_cfg = scheduler_cfg\n\n        self.metrics = metrics if metrics is not None else self.DEFAULT_METRICS\n        self.persistent_tracking = (\n            persistent_tracking\n            if persistent_tracking is not None\n            else self.DEFAULT_TRACKING\n        )\n        self.test_results = {\"metrics\": [], \"preds\": [], \"save_path\": test_save_path}\n\n    def forward(\n        self,\n        ref_instances: list[\"dreem.io.Instance\"],\n        query_instances: list[\"dreem.io.Instance\"] | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute forward pass of the lightning module.\n\n        Args:\n            ref_instances: a list of `Instance` objects containing crops and other data needed for transformer model\n            query_instances: a list of `Instance` objects used as queries in the decoder. Mostly used for inference.\n\n        Returns:\n            An association matrix between objects\n        \"\"\"\n        asso_preds = self.model(ref_instances, query_instances)\n        return asso_preds\n\n    def training_step(\n        self, train_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single training step for model.\n\n        Args:\n            train_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the train loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(train_batch[0], mode=\"train\")\n        self.log_metrics(result, len(train_batch[0]), \"train\")\n\n        return result\n\n    def validation_step(\n        self, val_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single val step for model.\n\n        Args:\n            val_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the val loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(val_batch[0], mode=\"val\")\n        self.log_metrics(result, len(val_batch[0]), \"val\")\n\n        return result\n\n    def test_step(\n        self, test_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single test step for model.\n\n        Args:\n            test_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the val loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(test_batch[0], mode=\"test\")\n        self.log_metrics(result, len(test_batch[0]), \"test\")\n\n        return result\n\n    def predict_step(\n        self, batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; list[\"dreem.io.Frame\"]:\n        \"\"\"Run inference for model.\n\n        Computes association + assignment.\n\n        Args:\n            batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A list of dicts where each dict is a frame containing the predicted track ids\n        \"\"\"\n        frames_pred = self.tracker(self.model, batch[0])\n        return frames_pred\n\n    def _shared_eval_step(\n        self, frames: list[\"dreem.io.Frame\"], mode: str\n    ) -&gt; dict[str, float]:\n        \"\"\"Run evaluation used by train, test, and val steps.\n\n        Args:\n            frames: A list of dicts where each dict is a frame containing gt data\n            mode: which metrics to compute and whether to use persistent tracking or not\n\n        Returns:\n            a dict containing the loss and any other metrics specified by `eval_metrics`\n        \"\"\"\n        try:\n            instances = [instance for frame in frames for instance in frame.instances]\n\n            if len(instances) == 0:\n                return None\n\n            eval_metrics = self.metrics[mode]\n            persistent_tracking = self.persistent_tracking[mode]\n\n            logits = self(instances)\n            logits = [asso.matrix for asso in logits]\n            loss = self.loss(logits, frames)\n\n            return_metrics = {\"loss\": loss}\n            if eval_metrics is not None and len(eval_metrics) &gt; 0:\n                self.tracker.persistent_tracking = persistent_tracking\n\n                frames_pred = self.tracker(self.model, frames)\n\n                frames_mm = metrics.to_track_eval(frames_pred)\n                clearmot = metrics.get_pymotmetrics(frames_mm, eval_metrics)\n\n                return_metrics.update(clearmot.to_dict())\n\n                if mode == \"test\":\n                    self.test_results[\"preds\"].append(\n                        [frame.to(\"cpu\") for frame in frames_pred]\n                    )\n                    self.test_results[\"metrics\"].append(return_metrics)\n            return_metrics[\"batch_size\"] = len(frames)\n        except Exception as e:\n            logger.exception(\n                f\"Failed on frame {frames[0].frame_id} of video {frames[0].video_id}\"\n            )\n            logger.exception(e)\n            raise (e)\n\n        return return_metrics\n\n    def configure_optimizers(self) -&gt; dict:\n        \"\"\"Get optimizers and schedulers for training.\n\n        Is overridden by config but defaults to Adam + ReduceLROnPlateau.\n\n        Returns:\n            an optimizer config dict containing the optimizer, scheduler, and scheduler params\n        \"\"\"\n        # todo: init from config\n        if self.optimizer_cfg is None:\n            optimizer = torch.optim.Adam(self.parameters(), lr=1e-4, betas=(0.9, 0.999))\n        else:\n            optimizer = init_optimizer(self.parameters(), self.optimizer_cfg)\n\n        if self.scheduler_cfg is None:\n            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer, \"min\", 0.5, 10\n            )\n        else:\n            scheduler = init_scheduler(optimizer, self.scheduler_cfg)\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n                \"interval\": \"epoch\",\n                \"frequency\": 10,\n            },\n        }\n\n    def log_metrics(self, result: dict, batch_size: int, mode: str) -&gt; None:\n        \"\"\"Log metrics computed during evaluation.\n\n        Args:\n            result: A dict containing metrics to be logged.\n            batch_size: the size of the batch used to compute the metrics\n            mode: One of {'train', 'test' or 'val'}. Used as prefix while logging.\n        \"\"\"\n        if result:\n            batch_size = result.pop(\"batch_size\")\n            for metric, val in result.items():\n                if isinstance(val, torch.Tensor):\n                    val = val.item()\n                self.log(f\"{mode}_{metric}\", val, batch_size=batch_size)\n\n    def on_validation_epoch_end(self):\n        \"\"\"Execute hook for validation end.\n\n        Currently, we simply clear the gpu cache and do garbage collection.\n        \"\"\"\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def on_test_epoch_end(self):\n        \"\"\"Execute hook for test end.\n\n        Currently, we save results to an h5py file. and clear the predictions\n        \"\"\"\n        fname = self.test_results[\"save_path\"]\n        test_results = {\n            key: val for key, val in self.test_results.items() if key != \"save_path\"\n        }\n        metrics_dict = [\n            {\n                key: (\n                    val.detach().cpu().numpy() if isinstance(val, torch.Tensor) else val\n                )\n                for key, val in metrics.items()\n            }\n            for metrics in test_results[\"metrics\"]\n        ]\n        results_df = pd.DataFrame(metrics_dict)\n        preds = test_results[\"preds\"]\n\n        with h5py.File(fname, \"a\") as results_file:\n            for key in results_df.columns:\n                avg_result = results_df[key].mean()\n                results_file.attrs.create(key, avg_result)\n            for i, (metrics, frames) in enumerate(zip(metrics_dict, preds)):\n                vid_name = frames[0].vid_name.split(\"/\")[-1].split(\".\")[0]\n                vid_group = results_file.require_group(vid_name)\n                clip_group = vid_group.require_group(f\"clip_{i}\")\n                for key, val in metrics.items():\n                    clip_group.attrs.create(key, val)\n                for frame in frames:\n                    if metrics.get(\"num_switches\", 0) &gt; 0:\n                        _ = frame.to_h5(\n                            clip_group,\n                            frame.get_gt_track_ids().cpu().numpy(),\n                            save={\"crop\": True, \"features\": True, \"embeddings\": True},\n                        )\n                    else:\n                        _ = frame.to_h5(\n                            clip_group, frame.get_gt_track_ids().cpu().numpy()\n                        )\n        self.test_results = {\"metrics\": [], \"preds\": [], \"save_path\": fname}\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.__init__","title":"<code>__init__(model_cfg=None, tracker_cfg=None, loss_cfg=None, optimizer_cfg=None, scheduler_cfg=None, metrics=None, persistent_tracking=None, test_save_path='./test_results.h5')</code>","text":"<p>Initialize a lightning module for GTR.</p> <p>Parameters:</p> Name Type Description Default <code>model_cfg</code> <code>dict | None</code> <p>hyperparameters for GlobalTrackingTransformer</p> <code>None</code> <code>tracker_cfg</code> <code>dict | None</code> <p>The parameters used for the tracker post-processing</p> <code>None</code> <code>loss_cfg</code> <code>dict | None</code> <p>hyperparameters for AssoLoss</p> <code>None</code> <code>optimizer_cfg</code> <code>dict | None</code> <p>hyper parameters used for optimizer.        Only used to overwrite <code>configure_optimizer</code></p> <code>None</code> <code>scheduler_cfg</code> <code>dict | None</code> <p>hyperparameters for lr_scheduler used to overwrite `configure_optimizer</p> <code>None</code> <code>metrics</code> <code>dict[str, list[str]] | None</code> <p>a dict containing the metrics to be computed during train, val, and test.</p> <code>None</code> <code>persistent_tracking</code> <code>dict[str, bool] | None</code> <p>a dict containing whether to use persistent tracking during train, val and test inference.</p> <code>None</code> <code>test_save_path</code> <p>path to an .h5 file to save the test results to</p> <code>'./test_results.h5'</code> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def __init__(\n    self,\n    model_cfg: dict | None = None,\n    tracker_cfg: dict | None = None,\n    loss_cfg: dict | None = None,\n    optimizer_cfg: dict | None = None,\n    scheduler_cfg: dict | None = None,\n    metrics: dict[str, list[str]] | None = None,\n    persistent_tracking: dict[str, bool] | None = None,\n    test_save_path=\"./test_results.h5\",\n):\n    \"\"\"Initialize a lightning module for GTR.\n\n    Args:\n        model_cfg: hyperparameters for GlobalTrackingTransformer\n        tracker_cfg: The parameters used for the tracker post-processing\n        loss_cfg: hyperparameters for AssoLoss\n        optimizer_cfg: hyper parameters used for optimizer.\n                   Only used to overwrite `configure_optimizer`\n        scheduler_cfg: hyperparameters for lr_scheduler used to overwrite `configure_optimizer\n        metrics: a dict containing the metrics to be computed during train, val, and test.\n        persistent_tracking: a dict containing whether to use persistent tracking during train, val and test inference.\n        test_save_path: path to an .h5 file to save the test results to\n    \"\"\"\n    super().__init__()\n    self.save_hyperparameters()\n\n    self.model_cfg = model_cfg if model_cfg else {}\n    self.loss_cfg = loss_cfg if loss_cfg else {}\n    self.tracker_cfg = tracker_cfg if tracker_cfg else {}\n\n    _ = self.model_cfg.pop(\"ckpt_path\", None)\n    self.model = GlobalTrackingTransformer(**self.model_cfg)\n    self.loss = AssoLoss(**self.loss_cfg)\n    self.tracker = Tracker(**self.tracker_cfg)\n\n    self.optimizer_cfg = optimizer_cfg\n    self.scheduler_cfg = scheduler_cfg\n\n    self.metrics = metrics if metrics is not None else self.DEFAULT_METRICS\n    self.persistent_tracking = (\n        persistent_tracking\n        if persistent_tracking is not None\n        else self.DEFAULT_TRACKING\n    )\n    self.test_results = {\"metrics\": [], \"preds\": [], \"save_path\": test_save_path}\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Get optimizers and schedulers for training.</p> <p>Is overridden by config but defaults to Adam + ReduceLROnPlateau.</p> <p>Returns:</p> Type Description <code>dict</code> <p>an optimizer config dict containing the optimizer, scheduler, and scheduler params</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def configure_optimizers(self) -&gt; dict:\n    \"\"\"Get optimizers and schedulers for training.\n\n    Is overridden by config but defaults to Adam + ReduceLROnPlateau.\n\n    Returns:\n        an optimizer config dict containing the optimizer, scheduler, and scheduler params\n    \"\"\"\n    # todo: init from config\n    if self.optimizer_cfg is None:\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4, betas=(0.9, 0.999))\n    else:\n        optimizer = init_optimizer(self.parameters(), self.optimizer_cfg)\n\n    if self.scheduler_cfg is None:\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, \"min\", 0.5, 10\n        )\n    else:\n        scheduler = init_scheduler(optimizer, self.scheduler_cfg)\n\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"monitor\": \"val_loss\",\n            \"interval\": \"epoch\",\n            \"frequency\": 10,\n        },\n    }\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute forward pass of the lightning module.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>a list of <code>Instance</code> objects containing crops and other data needed for transformer model</p> required <code>query_instances</code> <code>list[Instance] | None</code> <p>a list of <code>Instance</code> objects used as queries in the decoder. Mostly used for inference.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>An association matrix between objects</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def forward(\n    self,\n    ref_instances: list[\"dreem.io.Instance\"],\n    query_instances: list[\"dreem.io.Instance\"] | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Execute forward pass of the lightning module.\n\n    Args:\n        ref_instances: a list of `Instance` objects containing crops and other data needed for transformer model\n        query_instances: a list of `Instance` objects used as queries in the decoder. Mostly used for inference.\n\n    Returns:\n        An association matrix between objects\n    \"\"\"\n    asso_preds = self.model(ref_instances, query_instances)\n    return asso_preds\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.log_metrics","title":"<code>log_metrics(result, batch_size, mode)</code>","text":"<p>Log metrics computed during evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>dict</code> <p>A dict containing metrics to be logged.</p> required <code>batch_size</code> <code>int</code> <p>the size of the batch used to compute the metrics</p> required <code>mode</code> <code>str</code> <p>One of {'train', 'test' or 'val'}. Used as prefix while logging.</p> required Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def log_metrics(self, result: dict, batch_size: int, mode: str) -&gt; None:\n    \"\"\"Log metrics computed during evaluation.\n\n    Args:\n        result: A dict containing metrics to be logged.\n        batch_size: the size of the batch used to compute the metrics\n        mode: One of {'train', 'test' or 'val'}. Used as prefix while logging.\n    \"\"\"\n    if result:\n        batch_size = result.pop(\"batch_size\")\n        for metric, val in result.items():\n            if isinstance(val, torch.Tensor):\n                val = val.item()\n            self.log(f\"{mode}_{metric}\", val, batch_size=batch_size)\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.on_test_epoch_end","title":"<code>on_test_epoch_end()</code>","text":"<p>Execute hook for test end.</p> <p>Currently, we save results to an h5py file. and clear the predictions</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def on_test_epoch_end(self):\n    \"\"\"Execute hook for test end.\n\n    Currently, we save results to an h5py file. and clear the predictions\n    \"\"\"\n    fname = self.test_results[\"save_path\"]\n    test_results = {\n        key: val for key, val in self.test_results.items() if key != \"save_path\"\n    }\n    metrics_dict = [\n        {\n            key: (\n                val.detach().cpu().numpy() if isinstance(val, torch.Tensor) else val\n            )\n            for key, val in metrics.items()\n        }\n        for metrics in test_results[\"metrics\"]\n    ]\n    results_df = pd.DataFrame(metrics_dict)\n    preds = test_results[\"preds\"]\n\n    with h5py.File(fname, \"a\") as results_file:\n        for key in results_df.columns:\n            avg_result = results_df[key].mean()\n            results_file.attrs.create(key, avg_result)\n        for i, (metrics, frames) in enumerate(zip(metrics_dict, preds)):\n            vid_name = frames[0].vid_name.split(\"/\")[-1].split(\".\")[0]\n            vid_group = results_file.require_group(vid_name)\n            clip_group = vid_group.require_group(f\"clip_{i}\")\n            for key, val in metrics.items():\n                clip_group.attrs.create(key, val)\n            for frame in frames:\n                if metrics.get(\"num_switches\", 0) &gt; 0:\n                    _ = frame.to_h5(\n                        clip_group,\n                        frame.get_gt_track_ids().cpu().numpy(),\n                        save={\"crop\": True, \"features\": True, \"embeddings\": True},\n                    )\n                else:\n                    _ = frame.to_h5(\n                        clip_group, frame.get_gt_track_ids().cpu().numpy()\n                    )\n    self.test_results = {\"metrics\": [], \"preds\": [], \"save_path\": fname}\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.on_validation_epoch_end","title":"<code>on_validation_epoch_end()</code>","text":"<p>Execute hook for validation end.</p> <p>Currently, we simply clear the gpu cache and do garbage collection.</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Execute hook for validation end.\n\n    Currently, we simply clear the gpu cache and do garbage collection.\n    \"\"\"\n    gc.collect()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.predict_step","title":"<code>predict_step(batch, batch_idx)</code>","text":"<p>Run inference for model.</p> <p>Computes association + assignment.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects     with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A list of dicts where each dict is a frame containing the predicted track ids</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def predict_step(\n    self, batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; list[\"dreem.io.Frame\"]:\n    \"\"\"Run inference for model.\n\n    Computes association + assignment.\n\n    Args:\n        batch: A single batch from the dataset which is a list of `Frame` objects\n                with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A list of dicts where each dict is a frame containing the predicted track ids\n    \"\"\"\n    frames_pred = self.tracker(self.model, batch[0])\n    return frames_pred\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.test_step","title":"<code>test_step(test_batch, batch_idx)</code>","text":"<p>Execute single test step for model.</p> <p>Parameters:</p> Name Type Description Default <code>test_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the val loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def test_step(\n    self, test_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single test step for model.\n\n    Args:\n        test_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the val loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(test_batch[0], mode=\"test\")\n    self.log_metrics(result, len(test_batch[0]), \"test\")\n\n    return result\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.training_step","title":"<code>training_step(train_batch, batch_idx)</code>","text":"<p>Execute single training step for model.</p> <p>Parameters:</p> Name Type Description Default <code>train_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the train loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def training_step(\n    self, train_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single training step for model.\n\n    Args:\n        train_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the train loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(train_batch[0], mode=\"train\")\n    self.log_metrics(result, len(train_batch[0]), \"train\")\n\n    return result\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.validation_step","title":"<code>validation_step(val_batch, batch_idx)</code>","text":"<p>Execute single val step for model.</p> <p>Parameters:</p> Name Type Description Default <code>val_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the val loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def validation_step(\n    self, val_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single val step for model.\n\n    Args:\n        val_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the val loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(val_batch[0], mode=\"val\")\n    self.log_metrics(result, len(val_batch[0]), \"val\")\n\n    return result\n</code></pre>"},{"location":"models/model_parts/","title":"Model Parts","text":""},{"location":"models/model_parts/#dreem.models.VisualEncoder","title":"<code>dreem.models.VisualEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Class wrapping around a visual feature extractor backbone.</p> <p>Currently CNN only.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>class VisualEncoder(torch.nn.Module):\n    \"\"\"Class wrapping around a visual feature extractor backbone.\n\n    Currently CNN only.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"resnet18\",\n        d_model: int = 512,\n        in_chans: int = 3,\n        backend: int = \"timm\",\n        **kwargs: Any | None,\n    ):\n        \"\"\"Initialize Visual Encoder.\n\n        Args:\n            model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n            d_model (int): Output embedding dimension.\n            in_chans: the number of input channels of the image.\n            backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n            kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n        \"\"\"\n        super().__init__()\n\n        self.model_name = model_name.lower()\n        self.d_model = d_model\n        self.backend = backend\n        if in_chans == 1:\n            self.in_chans = 3\n        else:\n            self.in_chans = in_chans\n\n        self.feature_extractor = self.select_feature_extractor(\n            model_name=self.model_name,\n            in_chans=self.in_chans,\n            backend=self.backend,\n            **kwargs,\n        )\n\n        self.out_layer = torch.nn.Linear(\n            self.encoder_dim(self.feature_extractor), self.d_model\n        )\n\n    def select_feature_extractor(\n        self, model_name: str, in_chans: int, backend: str, **kwargs: Any\n    ) -&gt; torch.nn.Module:\n        \"\"\"Select the appropriate feature extractor based on config.\n\n        Args:\n            model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n            in_chans: the number of input channels of the image.\n            backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n            kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n\n        Returns:\n            a CNN encoder based on the config and backend selected.\n        \"\"\"\n        if \"timm\" in backend.lower():\n            feature_extractor = timm.create_model(\n                model_name=self.model_name,\n                in_chans=self.in_chans,\n                num_classes=0,\n                **kwargs,\n            )\n        elif \"torch\" in backend.lower():\n            if model_name.lower() == \"resnet18\":\n                feature_extractor = torchvision.models.resnet18(**kwargs)\n\n            elif model_name.lower() == \"resnet50\":\n                feature_extractor = torchvision.models.resnet50(**kwargs)\n\n            else:\n                raise ValueError(\n                    f\"Only `[resnet18, resnet50]` are available when backend is {backend}. Found {model_name}\"\n                )\n            feature_extractor = torch.nn.Sequential(\n                *list(feature_extractor.children())[:-1]\n            )\n            input_layer = feature_extractor[0]\n            if in_chans != 3:\n                feature_extractor[0] = torch.nn.Conv2d(\n                    in_channels=in_chans,\n                    out_channels=input_layer.out_channels,\n                    kernel_size=input_layer.kernel_size,\n                    stride=input_layer.stride,\n                    padding=input_layer.padding,\n                    dilation=input_layer.dilation,\n                    groups=input_layer.groups,\n                    bias=input_layer.bias,\n                    padding_mode=input_layer.padding_mode,\n                )\n\n        else:\n            raise ValueError(\n                f\"Only ['timm', 'torch'] backends are available! Found {backend}.\"\n            )\n        return feature_extractor\n\n    def encoder_dim(self, model: torch.nn.Module) -&gt; int:\n        \"\"\"Compute dummy forward pass of encoder model and get embedding dimension.\n\n        Args:\n            model: a vision encoder model.\n\n        Returns:\n            The embedding dimension size.\n        \"\"\"\n        _ = model.eval()\n        dummy_output = model(torch.randn(1, self.in_chans, 224, 224)).squeeze()\n        _ = model.train()  # to be safe\n        return dummy_output.shape[-1]\n\n    def forward(self, img: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of feature extractor to get feature vector.\n\n        Args:\n            img: Input image tensor of shape (B, C, H, W).\n\n        Returns:\n            feats: Normalized output tensor of shape (B, d_model).\n        \"\"\"\n        # If grayscale, tile the image to 3 channels.\n        if img.shape[1] == 1:\n            img = img.repeat([1, 3, 1, 1])  # (B, nc=3, H, W)\n        # Extract image features\n        feats = self.feature_extractor(\n            img\n        )  # (B, out_dim, 1, 1) if using resnet18 backbone.\n\n        # Reshape feature vectors\n        feats = feats.reshape([img.shape[0], -1])  # (B, out_dim)\n        # Map feature vectors to output dimension using linear layer.\n        feats = self.out_layer(feats)  # (B, d_model)\n        # Normalize output feature vectors.\n        feats = F.normalize(feats)  # (B, d_model)\n        return feats\n</code></pre>"},{"location":"models/model_parts/#dreem.models.VisualEncoder.__init__","title":"<code>__init__(model_name='resnet18', d_model=512, in_chans=3, backend='timm', **kwargs)</code>","text":"<p>Initialize Visual Encoder.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").</p> <code>'resnet18'</code> <code>d_model</code> <code>int</code> <p>Output embedding dimension.</p> <code>512</code> <code>in_chans</code> <code>int</code> <p>the number of input channels of the image.</p> <code>3</code> <code>backend</code> <code>int</code> <p>Which model backend to use. One of {\"timm\", \"torchvision\"}</p> <code>'timm'</code> <code>kwargs</code> <code>Any | None</code> <p>see <code>timm.create_model</code> and <code>torchvision.models.resnetX</code> for kwargs.</p> <code>{}</code> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"resnet18\",\n    d_model: int = 512,\n    in_chans: int = 3,\n    backend: int = \"timm\",\n    **kwargs: Any | None,\n):\n    \"\"\"Initialize Visual Encoder.\n\n    Args:\n        model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n        d_model (int): Output embedding dimension.\n        in_chans: the number of input channels of the image.\n        backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n        kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n    \"\"\"\n    super().__init__()\n\n    self.model_name = model_name.lower()\n    self.d_model = d_model\n    self.backend = backend\n    if in_chans == 1:\n        self.in_chans = 3\n    else:\n        self.in_chans = in_chans\n\n    self.feature_extractor = self.select_feature_extractor(\n        model_name=self.model_name,\n        in_chans=self.in_chans,\n        backend=self.backend,\n        **kwargs,\n    )\n\n    self.out_layer = torch.nn.Linear(\n        self.encoder_dim(self.feature_extractor), self.d_model\n    )\n</code></pre>"},{"location":"models/model_parts/#dreem.models.VisualEncoder.encoder_dim","title":"<code>encoder_dim(model)</code>","text":"<p>Compute dummy forward pass of encoder model and get embedding dimension.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>a vision encoder model.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The embedding dimension size.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def encoder_dim(self, model: torch.nn.Module) -&gt; int:\n    \"\"\"Compute dummy forward pass of encoder model and get embedding dimension.\n\n    Args:\n        model: a vision encoder model.\n\n    Returns:\n        The embedding dimension size.\n    \"\"\"\n    _ = model.eval()\n    dummy_output = model(torch.randn(1, self.in_chans, 224, 224)).squeeze()\n    _ = model.train()  # to be safe\n    return dummy_output.shape[-1]\n</code></pre>"},{"location":"models/model_parts/#dreem.models.VisualEncoder.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of feature extractor to get feature vector.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor</code> <p>Input image tensor of shape (B, C, H, W).</p> required <p>Returns:</p> Name Type Description <code>feats</code> <code>Tensor</code> <p>Normalized output tensor of shape (B, d_model).</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def forward(self, img: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of feature extractor to get feature vector.\n\n    Args:\n        img: Input image tensor of shape (B, C, H, W).\n\n    Returns:\n        feats: Normalized output tensor of shape (B, d_model).\n    \"\"\"\n    # If grayscale, tile the image to 3 channels.\n    if img.shape[1] == 1:\n        img = img.repeat([1, 3, 1, 1])  # (B, nc=3, H, W)\n    # Extract image features\n    feats = self.feature_extractor(\n        img\n    )  # (B, out_dim, 1, 1) if using resnet18 backbone.\n\n    # Reshape feature vectors\n    feats = feats.reshape([img.shape[0], -1])  # (B, out_dim)\n    # Map feature vectors to output dimension using linear layer.\n    feats = self.out_layer(feats)  # (B, d_model)\n    # Normalize output feature vectors.\n    feats = F.normalize(feats)  # (B, d_model)\n    return feats\n</code></pre>"},{"location":"models/model_parts/#dreem.models.VisualEncoder.select_feature_extractor","title":"<code>select_feature_extractor(model_name, in_chans, backend, **kwargs)</code>","text":"<p>Select the appropriate feature extractor based on config.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").</p> required <code>in_chans</code> <code>int</code> <p>the number of input channels of the image.</p> required <code>backend</code> <code>str</code> <p>Which model backend to use. One of {\"timm\", \"torchvision\"}</p> required <code>kwargs</code> <code>Any</code> <p>see <code>timm.create_model</code> and <code>torchvision.models.resnetX</code> for kwargs.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Module</code> <p>a CNN encoder based on the config and backend selected.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def select_feature_extractor(\n    self, model_name: str, in_chans: int, backend: str, **kwargs: Any\n) -&gt; torch.nn.Module:\n    \"\"\"Select the appropriate feature extractor based on config.\n\n    Args:\n        model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n        in_chans: the number of input channels of the image.\n        backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n        kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n\n    Returns:\n        a CNN encoder based on the config and backend selected.\n    \"\"\"\n    if \"timm\" in backend.lower():\n        feature_extractor = timm.create_model(\n            model_name=self.model_name,\n            in_chans=self.in_chans,\n            num_classes=0,\n            **kwargs,\n        )\n    elif \"torch\" in backend.lower():\n        if model_name.lower() == \"resnet18\":\n            feature_extractor = torchvision.models.resnet18(**kwargs)\n\n        elif model_name.lower() == \"resnet50\":\n            feature_extractor = torchvision.models.resnet50(**kwargs)\n\n        else:\n            raise ValueError(\n                f\"Only `[resnet18, resnet50]` are available when backend is {backend}. Found {model_name}\"\n            )\n        feature_extractor = torch.nn.Sequential(\n            *list(feature_extractor.children())[:-1]\n        )\n        input_layer = feature_extractor[0]\n        if in_chans != 3:\n            feature_extractor[0] = torch.nn.Conv2d(\n                in_channels=in_chans,\n                out_channels=input_layer.out_channels,\n                kernel_size=input_layer.kernel_size,\n                stride=input_layer.stride,\n                padding=input_layer.padding,\n                dilation=input_layer.dilation,\n                groups=input_layer.groups,\n                bias=input_layer.bias,\n                padding_mode=input_layer.padding_mode,\n            )\n\n    else:\n        raise ValueError(\n            f\"Only ['timm', 'torch'] backends are available! Found {backend}.\"\n        )\n    return feature_extractor\n</code></pre>"},{"location":"models/model_parts/#dreem.models.Transformer","title":"<code>dreem.models.Transformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer class.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class Transformer(torch.nn.Module):\n    \"\"\"Transformer class.\"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 1024,\n        nhead: int = 8,\n        num_encoder_layers: int = 6,\n        num_decoder_layers: int = 6,\n        dropout: float = 0.1,\n        activation: str = \"relu\",\n        return_intermediate_dec: bool = False,\n        norm: bool = False,\n        num_layers_attn_head: int = 2,\n        dropout_attn_head: float = 0.1,\n        embedding_meta: dict | None = None,\n        return_embedding: bool = False,\n        decoder_self_attn: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize Transformer.\n\n        Args:\n            d_model: The number of features in the encoder/decoder inputs.\n            nhead: The number of heads in the transfomer encoder/decoder.\n            num_encoder_layers: The number of encoder-layers in the encoder.\n            num_decoder_layers: The number of decoder-layers in the decoder.\n            dropout: Dropout value applied to the output of transformer layers.\n            activation: Activation function to use.\n            return_intermediate_dec: Return intermediate layers from decoder.\n            norm: If True, normalize output of encoder and decoder.\n            num_layers_attn_head: The number of layers in the attention head.\n            dropout_attn_head: Dropout value for the attention_head.\n            embedding_meta: Metadata for positional embeddings. See below.\n            return_embedding: Whether to return the positional embeddings\n            decoder_self_attn: If True, use decoder self attention.\n\n                More details on `embedding_meta`:\n                    By default this will be an empty dict and indicate\n                    that no positional embeddings should be used. To use the positional embeddings\n                    pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                    {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},\n                    \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                    and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n        \"\"\"\n        super().__init__()\n\n        self.d_model = dim_feedforward = feature_dim_attn_head = d_model\n\n        self.embedding_meta = embedding_meta\n        self.return_embedding = return_embedding\n\n        self.pos_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n        self.temp_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n\n        if self.embedding_meta:\n            if \"pos\" in self.embedding_meta:\n                pos_emb_cfg = self.embedding_meta[\"pos\"]\n                if pos_emb_cfg:\n                    self.pos_emb = Embedding(\n                        emb_type=\"pos\", features=self.d_model, **pos_emb_cfg\n                    )\n            if \"temp\" in self.embedding_meta:\n                temp_emb_cfg = self.embedding_meta[\"temp\"]\n                if temp_emb_cfg:\n                    self.temp_emb = Embedding(\n                        emb_type=\"temp\", features=self.d_model, **temp_emb_cfg\n                    )\n\n        # Transformer Encoder\n        encoder_layer = TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, activation, norm\n        )\n\n        encoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n        self.encoder = TransformerEncoder(\n            encoder_layer, num_encoder_layers, encoder_norm\n        )\n\n        # Transformer Decoder\n        decoder_layer = TransformerDecoderLayer(\n            d_model,\n            nhead,\n            dim_feedforward,\n            dropout,\n            activation,\n            norm,\n            decoder_self_attn,\n        )\n\n        decoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n        self.decoder = TransformerDecoder(\n            decoder_layer, num_decoder_layers, return_intermediate_dec, decoder_norm\n        )\n\n        # Transformer attention head\n        self.attn_head = ATTWeightHead(\n            feature_dim=feature_dim_attn_head,\n            num_layers=num_layers_attn_head,\n            dropout=dropout_attn_head,\n        )\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        \"\"\"Initialize model weights from xavier distribution.\"\"\"\n        for p in self.parameters():\n            if not torch.nn.parameter.is_lazy(p) and p.dim() &gt; 1:\n                try:\n                    nn.init.xavier_uniform_(p)\n                except ValueError as e:\n                    print(f\"Failed Trying to initialize {p}\")\n                    raise (e)\n\n    def forward(\n        self,\n        ref_instances: list[\"dreem.io.Instance\"],\n        query_instances: list[\"dreem.io.Instance\"] | None = None,\n    ) -&gt; list[AssociationMatrix]:\n        \"\"\"Execute a forward pass through the transformer and attention head.\n\n        Args:\n            ref_instances: A list of instance objects (See `dreem.io.Instance` for more info.)\n            query_instances: An set of instances to be used as decoder queries.\n\n        Returns:\n            asso_output: A list of torch.Tensors of shape (L, n_query, total_instances) where:\n                L: number of decoder blocks\n                n_query: number of instances in current query/frame\n                total_instances: number of instances in window\n        \"\"\"\n        ref_features = torch.cat(\n            [instance.features for instance in ref_instances], dim=0\n        ).unsqueeze(0)\n\n        # window_length = len(frames)\n        # instances_per_frame = [frame.num_detected for frame in frames]\n        total_instances = len(ref_instances)\n        embed_dim = ref_features.shape[-1]\n        # print(f'T: {window_length}; N: {total_instances}; N_t: {instances_per_frame} n_reid: {reid_features.shape}')\n        ref_boxes = get_boxes(ref_instances)  # total_instances, 4\n        ref_boxes = torch.nan_to_num(ref_boxes, -1.0)\n        ref_times, query_times = get_times(ref_instances, query_instances)\n\n        window_length = len(ref_times.unique())\n\n        ref_temp_emb = self.temp_emb(ref_times / window_length)\n\n        ref_pos_emb = self.pos_emb(ref_boxes)\n\n        if self.return_embedding:\n            for i, instance in enumerate(ref_instances):\n                instance.add_embedding(\"pos\", ref_pos_emb[i])\n                instance.add_embedding(\"temp\", ref_temp_emb[i])\n\n        ref_emb = (ref_pos_emb + ref_temp_emb) / 2.0\n\n        ref_emb = ref_emb.view(1, total_instances, embed_dim)\n\n        ref_emb = ref_emb.permute(1, 0, 2)  # (total_instances, batch_size, embed_dim)\n\n        batch_size, total_instances, embed_dim = ref_features.shape\n\n        ref_features = ref_features.permute(\n            1, 0, 2\n        )  # (total_instances, batch_size, embed_dim)\n\n        encoder_queries = ref_features\n\n        encoder_features = self.encoder(\n            encoder_queries, pos_emb=ref_emb\n        )  # (total_instances, batch_size, embed_dim)\n\n        n_query = total_instances\n\n        query_features = ref_features\n        query_pos_emb = ref_pos_emb\n        query_temp_emb = ref_temp_emb\n        query_emb = ref_emb\n\n        if query_instances is not None:\n            n_query = len(query_instances)\n\n            query_features = torch.cat(\n                [instance.features for instance in query_instances], dim=0\n            ).unsqueeze(0)\n\n            query_features = query_features.permute(\n                1, 0, 2\n            )  # (n_query, batch_size, embed_dim)\n\n            query_boxes = get_boxes(query_instances)\n            query_boxes = torch.nan_to_num(query_boxes, -1.0)\n            query_temp_emb = self.temp_emb(query_times / window_length)\n\n            query_pos_emb = self.pos_emb(query_boxes)\n\n            query_emb = (query_pos_emb + query_temp_emb) / 2.0\n            query_emb = query_emb.view(1, n_query, embed_dim)\n            query_emb = query_emb.permute(1, 0, 2)  # (n_query, batch_size, embed_dim)\n\n        else:\n            query_instances = ref_instances\n\n        if self.return_embedding:\n            for i, instance in enumerate(query_instances):\n                instance.add_embedding(\"pos\", query_pos_emb[i])\n                instance.add_embedding(\"temp\", query_temp_emb[i])\n\n        decoder_features = self.decoder(\n            query_features,\n            encoder_features,\n            ref_pos_emb=ref_emb,\n            query_pos_emb=query_emb,\n        )  # (L, n_query, batch_size, embed_dim)\n\n        decoder_features = decoder_features.transpose(\n            1, 2\n        )  # # (L, batch_size, n_query, embed_dim)\n        encoder_features = encoder_features.permute(1, 0, 2).view(\n            batch_size, total_instances, embed_dim\n        )  # (batch_size, total_instances, embed_dim)\n\n        asso_output = []\n        for frame_features in decoder_features:\n            asso_matrix = self.attn_head(frame_features, encoder_features).view(\n                n_query, total_instances\n            )\n            asso_matrix = AssociationMatrix(asso_matrix, ref_instances, query_instances)\n\n            asso_output.append(asso_matrix)\n\n        # (L=1, n_query, total_instances)\n        return asso_output\n</code></pre>"},{"location":"models/model_parts/#dreem.models.Transformer.__init__","title":"<code>__init__(d_model=1024, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dropout=0.1, activation='relu', return_intermediate_dec=False, norm=False, num_layers_attn_head=2, dropout_attn_head=0.1, embedding_meta=None, return_embedding=False, decoder_self_attn=False)</code>","text":"<p>Initialize Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of features in the encoder/decoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads in the transfomer encoder/decoder.</p> <code>8</code> <code>num_encoder_layers</code> <code>int</code> <p>The number of encoder-layers in the encoder.</p> <code>6</code> <code>num_decoder_layers</code> <code>int</code> <p>The number of decoder-layers in the decoder.</p> <code>6</code> <code>dropout</code> <code>float</code> <p>Dropout value applied to the output of transformer layers.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>return_intermediate_dec</code> <code>bool</code> <p>Return intermediate layers from decoder.</p> <code>False</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of encoder and decoder.</p> <code>False</code> <code>num_layers_attn_head</code> <code>int</code> <p>The number of layers in the attention head.</p> <code>2</code> <code>dropout_attn_head</code> <code>float</code> <p>Dropout value for the attention_head.</p> <code>0.1</code> <code>embedding_meta</code> <code>dict | None</code> <p>Metadata for positional embeddings. See below.</p> <code>None</code> <code>return_embedding</code> <code>bool</code> <p>Whether to return the positional embeddings</p> <code>False</code> <code>decoder_self_attn</code> <code>bool</code> <p>If True, use decoder self attention.</p> <p>More details on <code>embedding_meta</code>:     By default this will be an empty dict and indicate     that no positional embeddings should be used. To use the positional embeddings     pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:     {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},     \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see <code>dreem.models.embeddings.Embedding.EMB_TYPES</code>     and <code>dreem.models.embeddings.Embedding.EMB_MODES</code> for embedding parameters).</p> <code>False</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    d_model: int = 1024,\n    nhead: int = 8,\n    num_encoder_layers: int = 6,\n    num_decoder_layers: int = 6,\n    dropout: float = 0.1,\n    activation: str = \"relu\",\n    return_intermediate_dec: bool = False,\n    norm: bool = False,\n    num_layers_attn_head: int = 2,\n    dropout_attn_head: float = 0.1,\n    embedding_meta: dict | None = None,\n    return_embedding: bool = False,\n    decoder_self_attn: bool = False,\n) -&gt; None:\n    \"\"\"Initialize Transformer.\n\n    Args:\n        d_model: The number of features in the encoder/decoder inputs.\n        nhead: The number of heads in the transfomer encoder/decoder.\n        num_encoder_layers: The number of encoder-layers in the encoder.\n        num_decoder_layers: The number of decoder-layers in the decoder.\n        dropout: Dropout value applied to the output of transformer layers.\n        activation: Activation function to use.\n        return_intermediate_dec: Return intermediate layers from decoder.\n        norm: If True, normalize output of encoder and decoder.\n        num_layers_attn_head: The number of layers in the attention head.\n        dropout_attn_head: Dropout value for the attention_head.\n        embedding_meta: Metadata for positional embeddings. See below.\n        return_embedding: Whether to return the positional embeddings\n        decoder_self_attn: If True, use decoder self attention.\n\n            More details on `embedding_meta`:\n                By default this will be an empty dict and indicate\n                that no positional embeddings should be used. To use the positional embeddings\n                pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},\n                \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n    \"\"\"\n    super().__init__()\n\n    self.d_model = dim_feedforward = feature_dim_attn_head = d_model\n\n    self.embedding_meta = embedding_meta\n    self.return_embedding = return_embedding\n\n    self.pos_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n    self.temp_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n\n    if self.embedding_meta:\n        if \"pos\" in self.embedding_meta:\n            pos_emb_cfg = self.embedding_meta[\"pos\"]\n            if pos_emb_cfg:\n                self.pos_emb = Embedding(\n                    emb_type=\"pos\", features=self.d_model, **pos_emb_cfg\n                )\n        if \"temp\" in self.embedding_meta:\n            temp_emb_cfg = self.embedding_meta[\"temp\"]\n            if temp_emb_cfg:\n                self.temp_emb = Embedding(\n                    emb_type=\"temp\", features=self.d_model, **temp_emb_cfg\n                )\n\n    # Transformer Encoder\n    encoder_layer = TransformerEncoderLayer(\n        d_model, nhead, dim_feedforward, dropout, activation, norm\n    )\n\n    encoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n    self.encoder = TransformerEncoder(\n        encoder_layer, num_encoder_layers, encoder_norm\n    )\n\n    # Transformer Decoder\n    decoder_layer = TransformerDecoderLayer(\n        d_model,\n        nhead,\n        dim_feedforward,\n        dropout,\n        activation,\n        norm,\n        decoder_self_attn,\n    )\n\n    decoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n    self.decoder = TransformerDecoder(\n        decoder_layer, num_decoder_layers, return_intermediate_dec, decoder_norm\n    )\n\n    # Transformer attention head\n    self.attn_head = ATTWeightHead(\n        feature_dim=feature_dim_attn_head,\n        num_layers=num_layers_attn_head,\n        dropout=dropout_attn_head,\n    )\n\n    self._reset_parameters()\n</code></pre>"},{"location":"models/model_parts/#dreem.models.Transformer.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute a forward pass through the transformer and attention head.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>A list of instance objects (See <code>dreem.io.Instance</code> for more info.)</p> required <code>query_instances</code> <code>list[Instance] | None</code> <p>An set of instances to be used as decoder queries.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>asso_output</code> <code>list[AssociationMatrix]</code> <p>A list of torch.Tensors of shape (L, n_query, total_instances) where:     L: number of decoder blocks     n_query: number of instances in current query/frame     total_instances: number of instances in window</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self,\n    ref_instances: list[\"dreem.io.Instance\"],\n    query_instances: list[\"dreem.io.Instance\"] | None = None,\n) -&gt; list[AssociationMatrix]:\n    \"\"\"Execute a forward pass through the transformer and attention head.\n\n    Args:\n        ref_instances: A list of instance objects (See `dreem.io.Instance` for more info.)\n        query_instances: An set of instances to be used as decoder queries.\n\n    Returns:\n        asso_output: A list of torch.Tensors of shape (L, n_query, total_instances) where:\n            L: number of decoder blocks\n            n_query: number of instances in current query/frame\n            total_instances: number of instances in window\n    \"\"\"\n    ref_features = torch.cat(\n        [instance.features for instance in ref_instances], dim=0\n    ).unsqueeze(0)\n\n    # window_length = len(frames)\n    # instances_per_frame = [frame.num_detected for frame in frames]\n    total_instances = len(ref_instances)\n    embed_dim = ref_features.shape[-1]\n    # print(f'T: {window_length}; N: {total_instances}; N_t: {instances_per_frame} n_reid: {reid_features.shape}')\n    ref_boxes = get_boxes(ref_instances)  # total_instances, 4\n    ref_boxes = torch.nan_to_num(ref_boxes, -1.0)\n    ref_times, query_times = get_times(ref_instances, query_instances)\n\n    window_length = len(ref_times.unique())\n\n    ref_temp_emb = self.temp_emb(ref_times / window_length)\n\n    ref_pos_emb = self.pos_emb(ref_boxes)\n\n    if self.return_embedding:\n        for i, instance in enumerate(ref_instances):\n            instance.add_embedding(\"pos\", ref_pos_emb[i])\n            instance.add_embedding(\"temp\", ref_temp_emb[i])\n\n    ref_emb = (ref_pos_emb + ref_temp_emb) / 2.0\n\n    ref_emb = ref_emb.view(1, total_instances, embed_dim)\n\n    ref_emb = ref_emb.permute(1, 0, 2)  # (total_instances, batch_size, embed_dim)\n\n    batch_size, total_instances, embed_dim = ref_features.shape\n\n    ref_features = ref_features.permute(\n        1, 0, 2\n    )  # (total_instances, batch_size, embed_dim)\n\n    encoder_queries = ref_features\n\n    encoder_features = self.encoder(\n        encoder_queries, pos_emb=ref_emb\n    )  # (total_instances, batch_size, embed_dim)\n\n    n_query = total_instances\n\n    query_features = ref_features\n    query_pos_emb = ref_pos_emb\n    query_temp_emb = ref_temp_emb\n    query_emb = ref_emb\n\n    if query_instances is not None:\n        n_query = len(query_instances)\n\n        query_features = torch.cat(\n            [instance.features for instance in query_instances], dim=0\n        ).unsqueeze(0)\n\n        query_features = query_features.permute(\n            1, 0, 2\n        )  # (n_query, batch_size, embed_dim)\n\n        query_boxes = get_boxes(query_instances)\n        query_boxes = torch.nan_to_num(query_boxes, -1.0)\n        query_temp_emb = self.temp_emb(query_times / window_length)\n\n        query_pos_emb = self.pos_emb(query_boxes)\n\n        query_emb = (query_pos_emb + query_temp_emb) / 2.0\n        query_emb = query_emb.view(1, n_query, embed_dim)\n        query_emb = query_emb.permute(1, 0, 2)  # (n_query, batch_size, embed_dim)\n\n    else:\n        query_instances = ref_instances\n\n    if self.return_embedding:\n        for i, instance in enumerate(query_instances):\n            instance.add_embedding(\"pos\", query_pos_emb[i])\n            instance.add_embedding(\"temp\", query_temp_emb[i])\n\n    decoder_features = self.decoder(\n        query_features,\n        encoder_features,\n        ref_pos_emb=ref_emb,\n        query_pos_emb=query_emb,\n    )  # (L, n_query, batch_size, embed_dim)\n\n    decoder_features = decoder_features.transpose(\n        1, 2\n    )  # # (L, batch_size, n_query, embed_dim)\n    encoder_features = encoder_features.permute(1, 0, 2).view(\n        batch_size, total_instances, embed_dim\n    )  # (batch_size, total_instances, embed_dim)\n\n    asso_output = []\n    for frame_features in decoder_features:\n        asso_matrix = self.attn_head(frame_features, encoder_features).view(\n            n_query, total_instances\n        )\n        asso_matrix = AssociationMatrix(asso_matrix, ref_instances, query_instances)\n\n        asso_output.append(asso_matrix)\n\n    # (L=1, n_query, total_instances)\n    return asso_output\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerEncoder","title":"<code>dreem.models.transformer.TransformerEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>A transformer encoder block composed of encoder layers.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class TransformerEncoder(nn.Module):\n    \"\"\"A transformer encoder block composed of encoder layers.\"\"\"\n\n    def __init__(\n        self,\n        encoder_layer: TransformerEncoderLayer,\n        num_layers: int,\n        norm: nn.Module | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize transformer encoder.\n\n        Args:\n            encoder_layer: An instance of the TransformerEncoderLayer.\n            num_layers: The number of encoder layers to be stacked.\n            norm: The normalization layer to be applied.\n        \"\"\"\n        super().__init__()\n\n        self.layers = _get_clones(encoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm if norm is not None else nn.Identity()\n\n    def forward(\n        self, queries: torch.Tensor, pos_emb: torch.Tensor = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute a forward pass of encoder layer.\n\n        Args:\n            queries: The input tensor of shape (n_query, batch_size, embed_dim).\n            pos_emb: The positional embedding tensor of shape (n_query, embed_dim).\n\n        Returns:\n            The output tensor of shape (n_query, batch_size, embed_dim).\n        \"\"\"\n        for layer in self.layers:\n            queries = layer(queries, pos_emb=pos_emb)\n\n        encoder_features = self.norm(queries)\n        return encoder_features\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerEncoder.__init__","title":"<code>__init__(encoder_layer, num_layers, norm=None)</code>","text":"<p>Initialize transformer encoder.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_layer</code> <code>TransformerEncoderLayer</code> <p>An instance of the TransformerEncoderLayer.</p> required <code>num_layers</code> <code>int</code> <p>The number of encoder layers to be stacked.</p> required <code>norm</code> <code>Module | None</code> <p>The normalization layer to be applied.</p> <code>None</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    encoder_layer: TransformerEncoderLayer,\n    num_layers: int,\n    norm: nn.Module | None = None,\n) -&gt; None:\n    \"\"\"Initialize transformer encoder.\n\n    Args:\n        encoder_layer: An instance of the TransformerEncoderLayer.\n        num_layers: The number of encoder layers to be stacked.\n        norm: The normalization layer to be applied.\n    \"\"\"\n    super().__init__()\n\n    self.layers = _get_clones(encoder_layer, num_layers)\n    self.num_layers = num_layers\n    self.norm = norm if norm is not None else nn.Identity()\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerEncoder.forward","title":"<code>forward(queries, pos_emb=None)</code>","text":"<p>Execute a forward pass of encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Tensor</code> <p>The input tensor of shape (n_query, batch_size, embed_dim).</p> required <code>pos_emb</code> <code>Tensor</code> <p>The positional embedding tensor of shape (n_query, embed_dim).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self, queries: torch.Tensor, pos_emb: torch.Tensor = None\n) -&gt; torch.Tensor:\n    \"\"\"Execute a forward pass of encoder layer.\n\n    Args:\n        queries: The input tensor of shape (n_query, batch_size, embed_dim).\n        pos_emb: The positional embedding tensor of shape (n_query, embed_dim).\n\n    Returns:\n        The output tensor of shape (n_query, batch_size, embed_dim).\n    \"\"\"\n    for layer in self.layers:\n        queries = layer(queries, pos_emb=pos_emb)\n\n    encoder_features = self.norm(queries)\n    return encoder_features\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerEncoderLayer","title":"<code>dreem.models.transformer.TransformerEncoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A single transformer encoder layer.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class TransformerEncoderLayer(nn.Module):\n    \"\"\"A single transformer encoder layer.\"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 1024,\n        nhead: int = 6,\n        dim_feedforward: int = 1024,\n        dropout: float = 0.1,\n        activation: str = \"relu\",\n        norm: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize a transformer encoder layer.\n\n        Args:\n            d_model: The number of features in the encoder inputs.\n            nhead: The number of heads for the encoder.\n            dim_feedforward: Dimension of the feedforward layers of encoder.\n            dropout: Dropout value applied to the output of encoder.\n            activation: Activation function to use.\n            norm: If True, normalize output of encoder.\n        \"\"\"\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model) if norm else nn.Identity()\n        self.norm2 = nn.LayerNorm(d_model) if norm else nn.Identity()\n\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n\n    def forward(\n        self, queries: torch.Tensor, pos_emb: torch.Tensor = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute a forward pass of the encoder layer.\n\n        Args:\n            queries: Input sequence for encoder (n_query, batch_size, embed_dim).\n            pos_emb: Position embedding, if provided is added to src\n\n        Returns:\n            The output tensor of shape (n_query, batch_size, embed_dim).\n        \"\"\"\n        if pos_emb is None:\n            pos_emb = torch.zeros_like(queries)\n\n        queries = queries + pos_emb\n\n        # q = k = src\n\n        attn_features = self.self_attn(\n            query=queries,\n            key=queries,\n            value=queries,\n        )[0]\n\n        queries = queries + self.dropout1(attn_features)\n        queries = self.norm1(queries)\n        projection = self.linear2(self.dropout(self.activation(self.linear1(queries))))\n        queries = queries + self.dropout2(projection)\n        encoder_features = self.norm2(queries)\n\n        return encoder_features\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerEncoderLayer.__init__","title":"<code>__init__(d_model=1024, nhead=6, dim_feedforward=1024, dropout=0.1, activation='relu', norm=False)</code>","text":"<p>Initialize a transformer encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of features in the encoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads for the encoder.</p> <code>6</code> <code>dim_feedforward</code> <code>int</code> <p>Dimension of the feedforward layers of encoder.</p> <code>1024</code> <code>dropout</code> <code>float</code> <p>Dropout value applied to the output of encoder.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of encoder.</p> <code>False</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    d_model: int = 1024,\n    nhead: int = 6,\n    dim_feedforward: int = 1024,\n    dropout: float = 0.1,\n    activation: str = \"relu\",\n    norm: bool = False,\n) -&gt; None:\n    \"\"\"Initialize a transformer encoder layer.\n\n    Args:\n        d_model: The number of features in the encoder inputs.\n        nhead: The number of heads for the encoder.\n        dim_feedforward: Dimension of the feedforward layers of encoder.\n        dropout: Dropout value applied to the output of encoder.\n        activation: Activation function to use.\n        norm: If True, normalize output of encoder.\n    \"\"\"\n    super().__init__()\n    self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n    self.linear1 = nn.Linear(d_model, dim_feedforward)\n    self.dropout = nn.Dropout(dropout)\n    self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n    self.norm1 = nn.LayerNorm(d_model) if norm else nn.Identity()\n    self.norm2 = nn.LayerNorm(d_model) if norm else nn.Identity()\n\n    self.dropout1 = nn.Dropout(dropout)\n    self.dropout2 = nn.Dropout(dropout)\n\n    self.activation = _get_activation_fn(activation)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerEncoderLayer.forward","title":"<code>forward(queries, pos_emb=None)</code>","text":"<p>Execute a forward pass of the encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Tensor</code> <p>Input sequence for encoder (n_query, batch_size, embed_dim).</p> required <code>pos_emb</code> <code>Tensor</code> <p>Position embedding, if provided is added to src</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self, queries: torch.Tensor, pos_emb: torch.Tensor = None\n) -&gt; torch.Tensor:\n    \"\"\"Execute a forward pass of the encoder layer.\n\n    Args:\n        queries: Input sequence for encoder (n_query, batch_size, embed_dim).\n        pos_emb: Position embedding, if provided is added to src\n\n    Returns:\n        The output tensor of shape (n_query, batch_size, embed_dim).\n    \"\"\"\n    if pos_emb is None:\n        pos_emb = torch.zeros_like(queries)\n\n    queries = queries + pos_emb\n\n    # q = k = src\n\n    attn_features = self.self_attn(\n        query=queries,\n        key=queries,\n        value=queries,\n    )[0]\n\n    queries = queries + self.dropout1(attn_features)\n    queries = self.norm1(queries)\n    projection = self.linear2(self.dropout(self.activation(self.linear1(queries))))\n    queries = queries + self.dropout2(projection)\n    encoder_features = self.norm2(queries)\n\n    return encoder_features\n</code></pre>"},{"location":"models/model_parts/#dreem.models.Embedding","title":"<code>dreem.models.Embedding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Class that wraps around different embedding types.</p> <p>Used for both learned and fixed embeddings.</p> Source code in <code>dreem/models/embedding.py</code> <pre><code>class Embedding(torch.nn.Module):\n    \"\"\"Class that wraps around different embedding types.\n\n    Used for both learned and fixed embeddings.\n    \"\"\"\n\n    EMB_TYPES = {\n        \"temp\": {},\n        \"pos\": {\"over_boxes\"},\n        \"off\": {},\n        None: {},\n    }  # dict of valid args:keyword params\n    EMB_MODES = {\n        \"fixed\": {\"temperature\", \"scale\", \"normalize\"},\n        \"learned\": {\"emb_num\"},\n        \"off\": {},\n    }  # dict of valid args:keyword params\n\n    def __init__(\n        self,\n        emb_type: str,\n        mode: str,\n        features: int,\n        n_points: int = 1,\n        emb_num: int = 16,\n        over_boxes: bool = True,\n        temperature: int = 10000,\n        normalize: bool = False,\n        scale: float | None = None,\n        mlp_cfg: dict | None = None,\n    ):\n        \"\"\"Initialize embeddings.\n\n        Args:\n            emb_type: The type of embedding to compute. Must be one of `{\"temp\", \"pos\", \"off\"}`\n            mode: The mode or function used to map positions to vector embeddings.\n                  Must be one of `{\"fixed\", \"learned\", \"off\"}`\n            features: The embedding dimensions. Must match the dimension of the\n                      input vectors for the transformer model.\n            n_points: the number of points that will be embedded.\n            emb_num: the number of embeddings in the `self.lookup` table (Only used in learned embeddings).\n            over_boxes: Whether to compute the position embedding for each bbox coordinate (y1x1y2x2) or the centroid + bbox size (yxwh).\n            temperature: the temperature constant to be used when computing the sinusoidal position embedding\n            normalize: whether or not to normalize the positions (Only used in fixed embeddings).\n            scale: factor by which to scale the positions after normalizing (Only used in fixed embeddings).\n            mlp_cfg: A dictionary of mlp hyperparameters for projecting embedding to correct space.\n                    Example: {\"hidden_dims\": 256, \"num_layers\":3, \"dropout\": 0.3}\n        \"\"\"\n        self._check_init_args(emb_type, mode)\n\n        super().__init__()\n\n        self.emb_type = emb_type\n        self.mode = mode\n        self.features = features\n        self.emb_num = emb_num\n        self.over_boxes = over_boxes\n        self.temperature = temperature\n        self.normalize = normalize\n        self.scale = scale\n        self.n_points = n_points\n\n        if self.normalize and self.scale is None:\n            self.scale = 2 * math.pi\n\n        if self.emb_type == \"pos\" and mlp_cfg is not None and mlp_cfg[\"num_layers\"] &gt; 0:\n            if self.mode == \"fixed\":\n                self.mlp = MLP(\n                    input_dim=n_points * self.features,\n                    output_dim=self.features,\n                    **mlp_cfg,\n                )\n            else:\n                in_dim = (self.features // (4 * n_points)) * (4 * n_points)\n                self.mlp = MLP(\n                    input_dim=in_dim,\n                    output_dim=self.features,\n                    **mlp_cfg,\n                )\n        else:\n            self.mlp = torch.nn.Identity()\n\n        self._emb_func = lambda tensor: torch.zeros(\n            (tensor.shape[0], self.features), dtype=tensor.dtype, device=tensor.device\n        )  # turn off embedding by returning zeros\n\n        self.lookup = None\n\n        if self.mode == \"learned\":\n            if self.emb_type == \"pos\":\n                self.lookup = torch.nn.Embedding(\n                    self.emb_num * 4 * self.n_points, self.features // (4 * n_points)\n                )\n                self._emb_func = self._learned_pos_embedding\n            elif self.emb_type == \"temp\":\n                self.lookup = torch.nn.Embedding(self.emb_num, self.features)\n                self._emb_func = self._learned_temp_embedding\n\n        elif self.mode == \"fixed\":\n            if self.emb_type == \"pos\":\n                self._emb_func = self._sine_box_embedding\n            elif self.emb_type == \"temp\":\n                self._emb_func = self._sine_temp_embedding\n\n    def _check_init_args(self, emb_type: str, mode: str):\n        \"\"\"Check whether the correct arguments were passed to initialization.\n\n        Args:\n            emb_type: The type of embedding to compute. Must be one of `{\"temp\", \"pos\", \"\"}`\n            mode: The mode or function used to map positions to vector embeddings.\n                Must be one of `{\"fixed\", \"learned\"}`\n\n        Raises:\n            ValueError:\n              * if the incorrect `emb_type` or `mode` string are passed\n            NotImplementedError: if `emb_type` is `temp` and `mode` is `fixed`.\n        \"\"\"\n        if emb_type.lower() not in self.EMB_TYPES:\n            raise ValueError(\n                f\"Embedding `emb_type` must be one of {self.EMB_TYPES} not {emb_type}\"\n            )\n\n        if mode.lower() not in self.EMB_MODES:\n            raise ValueError(\n                f\"Embedding `mode` must be one of {self.EMB_MODES} not {mode}\"\n            )\n\n    def forward(self, seq_positions: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Get the sequence positional embeddings.\n\n        Args:\n            seq_positions:\n                * An (`N`, 1) tensor where seq_positions[i] represents the temporal position of instance_i in the sequence.\n                * An (`N`, n_anchors x 4) tensor where seq_positions[i, j, :] represents the [y1, x1, y2, x2] spatial locations of jth point of instance_i in the sequence.\n\n        Returns:\n            An `N` x `self.features` tensor representing the corresponding spatial or temporal embedding.\n        \"\"\"\n        emb = self._emb_func(seq_positions)\n\n        if emb.shape[-1] != self.features:\n            raise RuntimeError(\n                (\n                    f\"Output embedding dimension is {emb.shape[-1]} but requested {self.features} dimensions! \\n\"\n                    f\"hint: Try turning the MLP on by passing `mlp_cfg` to the constructor to project to the correct embedding dimensions.\"\n                )\n            )\n        return emb\n\n    def _torch_int_div(\n        self, tensor1: torch.Tensor, tensor2: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Perform integer division of two tensors.\n\n        Args:\n            tensor1: dividend tensor.\n            tensor2: divisor tensor.\n\n        Returns:\n            torch.Tensor, resulting tensor.\n        \"\"\"\n        return torch.div(tensor1, tensor2, rounding_mode=\"floor\")\n\n    def _sine_box_embedding(self, boxes: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute sine positional embeddings for boxes using given parameters.\n\n         Args:\n             boxes: the input boxes of shape N, n_anchors, 4 or B, N, n_anchors, 4\n                    where the last dimension is the bbox coords in [y1, x1, y2, x2].\n                    (Note currently `B=batch_size=1`).\n\n        Returns:\n             torch.Tensor, the sine positional embeddings\n             (embedding[:, 4i] = sin(x)\n              embedding[:, 4i+1] = cos(x)\n              embedding[:, 4i+2] = sin(y)\n              embedding[:, 4i+3] = cos(y)\n              )\n        \"\"\"\n        if self.scale is not None and self.normalize is False:\n            raise ValueError(\"normalize should be True if scale is passed\")\n\n        if len(boxes.size()) == 3:\n            boxes = boxes.unsqueeze(0)\n\n        if self.normalize:\n            boxes = boxes / (boxes[:, :, -1:] + 1e-6) * self.scale\n\n        dim_t = torch.arange(self.features // 4, dtype=torch.float32)\n\n        dim_t = self.temperature ** (\n            2 * self._torch_int_div(dim_t, 2) / (self.features // 4)\n        )\n\n        # (b, n_t, n_anchors, 4, D//4)\n        pos_emb = boxes[:, :, :, :, None] / dim_t.to(boxes.device)\n\n        pos_emb = torch.stack(\n            (pos_emb[:, :, :, :, 0::2].sin(), pos_emb[:, :, :, :, 1::2].cos()), dim=4\n        )\n        pos_emb = pos_emb.flatten(2).squeeze(0)  # (N_t, n_anchors * D)\n\n        pos_emb = self.mlp(pos_emb)\n\n        pos_emb = pos_emb.view(boxes.shape[1], self.features)\n\n        return pos_emb\n\n    def _sine_temp_embedding(self, times: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute fixed sine temporal embeddings.\n\n        Args:\n            times: the input times of shape (N,) or (N,1) where N = (sum(instances_per_frame))\n            which is the frame index of the instance relative\n            to the batch size\n            (e.g. `torch.tensor([0, 0, ..., 0, 1, 1, ..., 1, 2, 2, ..., 2,..., B, B, ...B])`).\n\n        Returns:\n            an n_instances x D embedding representing the temporal embedding.\n        \"\"\"\n        T = times.int().max().item() + 1\n        d = self.features\n        n = self.temperature\n\n        positions = torch.arange(0, T).unsqueeze(1)\n        temp_lookup = torch.zeros(T, d, device=times.device)\n\n        denominators = torch.pow(\n            n, 2 * torch.arange(0, d // 2) / d\n        )  # 10000^(2i/d_model), i is the index of embedding\n        temp_lookup[:, 0::2] = torch.sin(\n            positions / denominators\n        )  # sin(pos/10000^(2i/d_model))\n        temp_lookup[:, 1::2] = torch.cos(\n            positions / denominators\n        )  # cos(pos/10000^(2i/d_model))\n\n        temp_emb = temp_lookup[times.int()]\n        return temp_emb  # .view(len(times), self.features)\n\n    def _learned_pos_embedding(self, boxes: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute learned positional embeddings for boxes using given parameters.\n\n        Args:\n            boxes: the input boxes of shape N x 4 or B x N x 4\n                   where the last dimension is the bbox coords in [y1, x1, y2, x2].\n                   (Note currently `B=batch_size=1`).\n\n        Returns:\n            torch.Tensor, the learned positional embeddings.\n        \"\"\"\n        pos_lookup = self.lookup\n\n        N, n_anchors, _ = boxes.shape\n        boxes = boxes.view(N, n_anchors, 4)\n\n        if self.over_boxes:\n            xywh = boxes\n        else:\n            xywh = torch.cat(\n                [\n                    (boxes[:, :, 2:] + boxes[:, :, :2]) / 2,\n                    (boxes[:, :, 2:] - boxes[:, :, :2]),\n                ],\n                dim=1,\n            )\n\n        left_ind, right_ind, left_weight, right_weight = self._compute_weights(xywh)\n        f = pos_lookup.weight.shape[1]  # self.features // 4\n\n        try:\n            pos_emb_table = pos_lookup.weight.view(\n                self.emb_num, n_anchors, 4, f\n            )  # T x 4 x (D * 4)\n        except RuntimeError as e:\n            logger.exception(\n                f\"Hint: `n_points` ({self.n_points}) may be set incorrectly!\"\n            )\n            logger.exception(e)\n            raise (e)\n\n        left_emb = pos_emb_table.gather(\n            0,\n            left_ind[:, :, :, None].to(pos_emb_table.device).expand(N, n_anchors, 4, f),\n        )  # N x 4 x d\n        right_emb = pos_emb_table.gather(\n            0,\n            right_ind[:, :, :, None]\n            .to(pos_emb_table.device)\n            .expand(N, n_anchors, 4, f),\n        )  # N x 4 x d\n        pos_emb = left_weight[:, :, :, None] * right_emb.to(\n            left_weight.device\n        ) + right_weight[:, :, :, None] * left_emb.to(right_weight.device)\n\n        pos_emb = pos_emb.flatten(1)\n        pos_emb = self.mlp(pos_emb)\n\n        return pos_emb.view(N, self.features)\n\n    def _learned_temp_embedding(self, times: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute learned temporal embeddings for times using given parameters.\n\n        Args:\n            times: the input times of shape (N,) or (N,1) where N = (sum(instances_per_frame))\n            which is the frame index of the instance relative\n            to the batch size\n            (e.g. `torch.tensor([0, 0, ..., 0, 1, 1, ..., 1, 2, 2, ..., 2,..., B, B, ...B])`).\n\n        Returns:\n            torch.Tensor, the learned temporal embeddings.\n        \"\"\"\n        temp_lookup = self.lookup\n        N = times.shape[0]\n\n        left_ind, right_ind, left_weight, right_weight = self._compute_weights(times)\n\n        left_emb = temp_lookup.weight[\n            left_ind.to(temp_lookup.weight.device)\n        ]  # T x D --&gt; N x D\n        right_emb = temp_lookup.weight[right_ind.to(temp_lookup.weight.device)]\n\n        temp_emb = left_weight[:, None] * right_emb.to(\n            left_weight.device\n        ) + right_weight[:, None] * left_emb.to(right_weight.device)\n\n        return temp_emb.view(N, self.features)\n\n    def _compute_weights(self, data: torch.Tensor) -&gt; tuple[torch.Tensor, ...]:\n        \"\"\"Compute left and right learned embedding weights.\n\n        Args:\n            data: the input data (e.g boxes or times).\n\n        Returns:\n            A torch.Tensor for each of the left/right indices and weights, respectively\n        \"\"\"\n        data = data * self.emb_num\n\n        left_ind = data.clamp(min=0, max=self.emb_num - 1).long()  # N x 4\n        right_ind = (left_ind + 1).clamp(min=0, max=self.emb_num - 1).long()  # N x 4\n\n        left_weight = data - left_ind.float()  # N x 4\n\n        right_weight = 1.0 - left_weight\n\n        return left_ind, right_ind, left_weight, right_weight\n</code></pre>"},{"location":"models/model_parts/#dreem.models.Embedding.__init__","title":"<code>__init__(emb_type, mode, features, n_points=1, emb_num=16, over_boxes=True, temperature=10000, normalize=False, scale=None, mlp_cfg=None)</code>","text":"<p>Initialize embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>The type of embedding to compute. Must be one of <code>{\"temp\", \"pos\", \"off\"}</code></p> required <code>mode</code> <code>str</code> <p>The mode or function used to map positions to vector embeddings.   Must be one of <code>{\"fixed\", \"learned\", \"off\"}</code></p> required <code>features</code> <code>int</code> <p>The embedding dimensions. Must match the dimension of the       input vectors for the transformer model.</p> required <code>n_points</code> <code>int</code> <p>the number of points that will be embedded.</p> <code>1</code> <code>emb_num</code> <code>int</code> <p>the number of embeddings in the <code>self.lookup</code> table (Only used in learned embeddings).</p> <code>16</code> <code>over_boxes</code> <code>bool</code> <p>Whether to compute the position embedding for each bbox coordinate (y1x1y2x2) or the centroid + bbox size (yxwh).</p> <code>True</code> <code>temperature</code> <code>int</code> <p>the temperature constant to be used when computing the sinusoidal position embedding</p> <code>10000</code> <code>normalize</code> <code>bool</code> <p>whether or not to normalize the positions (Only used in fixed embeddings).</p> <code>False</code> <code>scale</code> <code>float | None</code> <p>factor by which to scale the positions after normalizing (Only used in fixed embeddings).</p> <code>None</code> <code>mlp_cfg</code> <code>dict | None</code> <p>A dictionary of mlp hyperparameters for projecting embedding to correct space.     Example: {\"hidden_dims\": 256, \"num_layers\":3, \"dropout\": 0.3}</p> <code>None</code> Source code in <code>dreem/models/embedding.py</code> <pre><code>def __init__(\n    self,\n    emb_type: str,\n    mode: str,\n    features: int,\n    n_points: int = 1,\n    emb_num: int = 16,\n    over_boxes: bool = True,\n    temperature: int = 10000,\n    normalize: bool = False,\n    scale: float | None = None,\n    mlp_cfg: dict | None = None,\n):\n    \"\"\"Initialize embeddings.\n\n    Args:\n        emb_type: The type of embedding to compute. Must be one of `{\"temp\", \"pos\", \"off\"}`\n        mode: The mode or function used to map positions to vector embeddings.\n              Must be one of `{\"fixed\", \"learned\", \"off\"}`\n        features: The embedding dimensions. Must match the dimension of the\n                  input vectors for the transformer model.\n        n_points: the number of points that will be embedded.\n        emb_num: the number of embeddings in the `self.lookup` table (Only used in learned embeddings).\n        over_boxes: Whether to compute the position embedding for each bbox coordinate (y1x1y2x2) or the centroid + bbox size (yxwh).\n        temperature: the temperature constant to be used when computing the sinusoidal position embedding\n        normalize: whether or not to normalize the positions (Only used in fixed embeddings).\n        scale: factor by which to scale the positions after normalizing (Only used in fixed embeddings).\n        mlp_cfg: A dictionary of mlp hyperparameters for projecting embedding to correct space.\n                Example: {\"hidden_dims\": 256, \"num_layers\":3, \"dropout\": 0.3}\n    \"\"\"\n    self._check_init_args(emb_type, mode)\n\n    super().__init__()\n\n    self.emb_type = emb_type\n    self.mode = mode\n    self.features = features\n    self.emb_num = emb_num\n    self.over_boxes = over_boxes\n    self.temperature = temperature\n    self.normalize = normalize\n    self.scale = scale\n    self.n_points = n_points\n\n    if self.normalize and self.scale is None:\n        self.scale = 2 * math.pi\n\n    if self.emb_type == \"pos\" and mlp_cfg is not None and mlp_cfg[\"num_layers\"] &gt; 0:\n        if self.mode == \"fixed\":\n            self.mlp = MLP(\n                input_dim=n_points * self.features,\n                output_dim=self.features,\n                **mlp_cfg,\n            )\n        else:\n            in_dim = (self.features // (4 * n_points)) * (4 * n_points)\n            self.mlp = MLP(\n                input_dim=in_dim,\n                output_dim=self.features,\n                **mlp_cfg,\n            )\n    else:\n        self.mlp = torch.nn.Identity()\n\n    self._emb_func = lambda tensor: torch.zeros(\n        (tensor.shape[0], self.features), dtype=tensor.dtype, device=tensor.device\n    )  # turn off embedding by returning zeros\n\n    self.lookup = None\n\n    if self.mode == \"learned\":\n        if self.emb_type == \"pos\":\n            self.lookup = torch.nn.Embedding(\n                self.emb_num * 4 * self.n_points, self.features // (4 * n_points)\n            )\n            self._emb_func = self._learned_pos_embedding\n        elif self.emb_type == \"temp\":\n            self.lookup = torch.nn.Embedding(self.emb_num, self.features)\n            self._emb_func = self._learned_temp_embedding\n\n    elif self.mode == \"fixed\":\n        if self.emb_type == \"pos\":\n            self._emb_func = self._sine_box_embedding\n        elif self.emb_type == \"temp\":\n            self._emb_func = self._sine_temp_embedding\n</code></pre>"},{"location":"models/model_parts/#dreem.models.Embedding.forward","title":"<code>forward(seq_positions)</code>","text":"<p>Get the sequence positional embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>seq_positions</code> <code>Tensor</code> <ul> <li>An (<code>N</code>, 1) tensor where seq_positions[i] represents the temporal position of instance_i in the sequence.</li> <li>An (<code>N</code>, n_anchors x 4) tensor where seq_positions[i, j, :] represents the [y1, x1, y2, x2] spatial locations of jth point of instance_i in the sequence.</li> </ul> required <p>Returns:</p> Type Description <code>Tensor</code> <p>An <code>N</code> x <code>self.features</code> tensor representing the corresponding spatial or temporal embedding.</p> Source code in <code>dreem/models/embedding.py</code> <pre><code>def forward(self, seq_positions: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Get the sequence positional embeddings.\n\n    Args:\n        seq_positions:\n            * An (`N`, 1) tensor where seq_positions[i] represents the temporal position of instance_i in the sequence.\n            * An (`N`, n_anchors x 4) tensor where seq_positions[i, j, :] represents the [y1, x1, y2, x2] spatial locations of jth point of instance_i in the sequence.\n\n    Returns:\n        An `N` x `self.features` tensor representing the corresponding spatial or temporal embedding.\n    \"\"\"\n    emb = self._emb_func(seq_positions)\n\n    if emb.shape[-1] != self.features:\n        raise RuntimeError(\n            (\n                f\"Output embedding dimension is {emb.shape[-1]} but requested {self.features} dimensions! \\n\"\n                f\"hint: Try turning the MLP on by passing `mlp_cfg` to the constructor to project to the correct embedding dimensions.\"\n            )\n        )\n    return emb\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerDecoder","title":"<code>dreem.models.transformer.TransformerDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer Decoder Block composed of Transformer Decoder Layers.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class TransformerDecoder(nn.Module):\n    \"\"\"Transformer Decoder Block composed of Transformer Decoder Layers.\"\"\"\n\n    def __init__(\n        self,\n        decoder_layer: TransformerDecoderLayer,\n        num_layers: int,\n        return_intermediate: bool = False,\n        norm: nn.Module | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize transformer decoder block.\n\n        Args:\n            decoder_layer: An instance of TransformerDecoderLayer.\n            num_layers: The number of decoder layers to be stacked.\n            return_intermediate: Return intermediate layers from decoder.\n            norm: The normalization layer to be applied.\n        \"\"\"\n        super().__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.return_intermediate = return_intermediate\n        self.norm = norm if norm is not None else nn.Identity()\n\n    def forward(\n        self,\n        decoder_queries: torch.Tensor,\n        encoder_features: torch.Tensor,\n        ref_pos_emb: torch.Tensor | None = None,\n        query_pos_emb: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute a forward pass of the decoder block.\n\n        Args:\n            decoder_queries: Query sequence for decoder to generate (n_query, batch_size, embed_dim).\n            encoder_features: Output from encoder, that decoder uses to attend to relevant\n                parts of input sequence (total_instances, batch_size, embed_dim)\n            ref_pos_emb: The input positional embedding tensor of shape (total_instances, batch_size, embed_dim).\n            query_pos_emb: The query positional embedding of shape (n_query, batch_size, embed_dim)\n\n        Returns:\n            The output tensor of shape (L, n_query, batch_size, embed_dim).\n        \"\"\"\n        decoder_features = decoder_queries\n\n        intermediate = []\n\n        for layer in self.layers:\n            decoder_features = layer(\n                decoder_features,\n                encoder_features,\n                ref_pos_emb=ref_pos_emb,\n                query_pos_emb=query_pos_emb,\n            )\n            if self.return_intermediate:\n                intermediate.append(self.norm(decoder_features))\n\n        decoder_features = self.norm(decoder_features)\n        if self.return_intermediate:\n            intermediate.pop()\n            intermediate.append(decoder_features)\n\n            return torch.stack(intermediate)\n\n        return decoder_features.unsqueeze(0)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerDecoder.__init__","title":"<code>__init__(decoder_layer, num_layers, return_intermediate=False, norm=None)</code>","text":"<p>Initialize transformer decoder block.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_layer</code> <code>TransformerDecoderLayer</code> <p>An instance of TransformerDecoderLayer.</p> required <code>num_layers</code> <code>int</code> <p>The number of decoder layers to be stacked.</p> required <code>return_intermediate</code> <code>bool</code> <p>Return intermediate layers from decoder.</p> <code>False</code> <code>norm</code> <code>Module | None</code> <p>The normalization layer to be applied.</p> <code>None</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    decoder_layer: TransformerDecoderLayer,\n    num_layers: int,\n    return_intermediate: bool = False,\n    norm: nn.Module | None = None,\n) -&gt; None:\n    \"\"\"Initialize transformer decoder block.\n\n    Args:\n        decoder_layer: An instance of TransformerDecoderLayer.\n        num_layers: The number of decoder layers to be stacked.\n        return_intermediate: Return intermediate layers from decoder.\n        norm: The normalization layer to be applied.\n    \"\"\"\n    super().__init__()\n    self.layers = _get_clones(decoder_layer, num_layers)\n    self.num_layers = num_layers\n    self.return_intermediate = return_intermediate\n    self.norm = norm if norm is not None else nn.Identity()\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerDecoder.forward","title":"<code>forward(decoder_queries, encoder_features, ref_pos_emb=None, query_pos_emb=None)</code>","text":"<p>Execute a forward pass of the decoder block.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_queries</code> <code>Tensor</code> <p>Query sequence for decoder to generate (n_query, batch_size, embed_dim).</p> required <code>encoder_features</code> <code>Tensor</code> <p>Output from encoder, that decoder uses to attend to relevant parts of input sequence (total_instances, batch_size, embed_dim)</p> required <code>ref_pos_emb</code> <code>Tensor | None</code> <p>The input positional embedding tensor of shape (total_instances, batch_size, embed_dim).</p> <code>None</code> <code>query_pos_emb</code> <code>Tensor | None</code> <p>The query positional embedding of shape (n_query, batch_size, embed_dim)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (L, n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self,\n    decoder_queries: torch.Tensor,\n    encoder_features: torch.Tensor,\n    ref_pos_emb: torch.Tensor | None = None,\n    query_pos_emb: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Execute a forward pass of the decoder block.\n\n    Args:\n        decoder_queries: Query sequence for decoder to generate (n_query, batch_size, embed_dim).\n        encoder_features: Output from encoder, that decoder uses to attend to relevant\n            parts of input sequence (total_instances, batch_size, embed_dim)\n        ref_pos_emb: The input positional embedding tensor of shape (total_instances, batch_size, embed_dim).\n        query_pos_emb: The query positional embedding of shape (n_query, batch_size, embed_dim)\n\n    Returns:\n        The output tensor of shape (L, n_query, batch_size, embed_dim).\n    \"\"\"\n    decoder_features = decoder_queries\n\n    intermediate = []\n\n    for layer in self.layers:\n        decoder_features = layer(\n            decoder_features,\n            encoder_features,\n            ref_pos_emb=ref_pos_emb,\n            query_pos_emb=query_pos_emb,\n        )\n        if self.return_intermediate:\n            intermediate.append(self.norm(decoder_features))\n\n    decoder_features = self.norm(decoder_features)\n    if self.return_intermediate:\n        intermediate.pop()\n        intermediate.append(decoder_features)\n\n        return torch.stack(intermediate)\n\n    return decoder_features.unsqueeze(0)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerDecoderLayer","title":"<code>dreem.models.transformer.TransformerDecoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A single transformer decoder layer.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class TransformerDecoderLayer(nn.Module):\n    \"\"\"A single transformer decoder layer.\"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 1024,\n        nhead: int = 6,\n        dim_feedforward: int = 1024,\n        dropout: float = 0.1,\n        activation: str = \"relu\",\n        norm: bool = False,\n        decoder_self_attn: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize transformer decoder layer.\n\n        Args:\n            d_model: The number of features in the decoder inputs.\n            nhead: The number of heads for the decoder.\n            dim_feedforward: Dimension of the feedforward layers of decoder.\n            dropout: Dropout value applied to the output of decoder.\n            activation: Activation function to use.\n            norm: If True, normalize output of decoder.\n            decoder_self_attn: If True, use decoder self attention\n        \"\"\"\n        super().__init__()\n\n        self.decoder_self_attn = decoder_self_attn\n\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        if self.decoder_self_attn:\n            self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n\n        self.norm1 = nn.LayerNorm(d_model) if norm else nn.Identity()\n        self.norm2 = nn.LayerNorm(d_model) if norm else nn.Identity()\n        self.norm3 = nn.LayerNorm(d_model) if norm else nn.Identity()\n\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n\n    def forward(\n        self,\n        decoder_queries: torch.Tensor,\n        encoder_features: torch.Tensor,\n        ref_pos_emb: torch.Tensor | None = None,\n        query_pos_emb: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute forward pass of decoder layer.\n\n        Args:\n            decoder_queries: Target sequence for decoder to generate (n_query, batch_size, embed_dim).\n            encoder_features: Output from encoder, that decoder uses to attend to relevant\n                parts of input sequence (total_instances, batch_size, embed_dim)\n            ref_pos_emb: The input positional embedding tensor of shape (n_query, embed_dim).\n            query_pos_emb: The target positional embedding of shape (n_query, embed_dim)\n\n        Returns:\n            The output tensor of shape (n_query, batch_size, embed_dim).\n        \"\"\"\n        if query_pos_emb is None:\n            query_pos_emb = torch.zeros_like(decoder_queries)\n        if ref_pos_emb is None:\n            ref_pos_emb = torch.zeros_like(encoder_features)\n\n        decoder_queries = decoder_queries + query_pos_emb\n        encoder_features = encoder_features + ref_pos_emb\n\n        if self.decoder_self_attn:\n            self_attn_features = self.self_attn(\n                query=decoder_queries, key=decoder_queries, value=decoder_queries\n            )[0]\n            decoder_queries = decoder_queries + self.dropout1(self_attn_features)\n            decoder_queries = self.norm1(decoder_queries)\n\n        x_attn_features = self.multihead_attn(\n            query=decoder_queries,  # (n_query, batch_size, embed_dim)\n            key=encoder_features,  # (total_instances, batch_size, embed_dim)\n            value=encoder_features,  # (total_instances, batch_size, embed_dim)\n        )[\n            0\n        ]  # (n_query, batch_size, embed_dim)\n\n        decoder_queries = decoder_queries + self.dropout2(\n            x_attn_features\n        )  # (n_query, batch_size, embed_dim)\n        decoder_queries = self.norm2(\n            decoder_queries\n        )  # (n_query, batch_size, embed_dim)\n        projection = self.linear2(\n            self.dropout(self.activation(self.linear1(decoder_queries)))\n        )  # (n_query, batch_size, embed_dim)\n        decoder_queries = decoder_queries + self.dropout3(\n            projection\n        )  # (n_query, batch_size, embed_dim)\n        decoder_features = self.norm3(decoder_queries)\n\n        return decoder_features  # (n_query, batch_size, embed_dim)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerDecoderLayer.__init__","title":"<code>__init__(d_model=1024, nhead=6, dim_feedforward=1024, dropout=0.1, activation='relu', norm=False, decoder_self_attn=False)</code>","text":"<p>Initialize transformer decoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of features in the decoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads for the decoder.</p> <code>6</code> <code>dim_feedforward</code> <code>int</code> <p>Dimension of the feedforward layers of decoder.</p> <code>1024</code> <code>dropout</code> <code>float</code> <p>Dropout value applied to the output of decoder.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of decoder.</p> <code>False</code> <code>decoder_self_attn</code> <code>bool</code> <p>If True, use decoder self attention</p> <code>False</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    d_model: int = 1024,\n    nhead: int = 6,\n    dim_feedforward: int = 1024,\n    dropout: float = 0.1,\n    activation: str = \"relu\",\n    norm: bool = False,\n    decoder_self_attn: bool = False,\n) -&gt; None:\n    \"\"\"Initialize transformer decoder layer.\n\n    Args:\n        d_model: The number of features in the decoder inputs.\n        nhead: The number of heads for the decoder.\n        dim_feedforward: Dimension of the feedforward layers of decoder.\n        dropout: Dropout value applied to the output of decoder.\n        activation: Activation function to use.\n        norm: If True, normalize output of decoder.\n        decoder_self_attn: If True, use decoder self attention\n    \"\"\"\n    super().__init__()\n\n    self.decoder_self_attn = decoder_self_attn\n\n    self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n    self.linear1 = nn.Linear(d_model, dim_feedforward)\n    self.dropout = nn.Dropout(dropout)\n    self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n    if self.decoder_self_attn:\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n\n    self.norm1 = nn.LayerNorm(d_model) if norm else nn.Identity()\n    self.norm2 = nn.LayerNorm(d_model) if norm else nn.Identity()\n    self.norm3 = nn.LayerNorm(d_model) if norm else nn.Identity()\n\n    self.dropout1 = nn.Dropout(dropout)\n    self.dropout2 = nn.Dropout(dropout)\n    self.dropout3 = nn.Dropout(dropout)\n\n    self.activation = _get_activation_fn(activation)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerDecoderLayer.forward","title":"<code>forward(decoder_queries, encoder_features, ref_pos_emb=None, query_pos_emb=None)</code>","text":"<p>Execute forward pass of decoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_queries</code> <code>Tensor</code> <p>Target sequence for decoder to generate (n_query, batch_size, embed_dim).</p> required <code>encoder_features</code> <code>Tensor</code> <p>Output from encoder, that decoder uses to attend to relevant parts of input sequence (total_instances, batch_size, embed_dim)</p> required <code>ref_pos_emb</code> <code>Tensor | None</code> <p>The input positional embedding tensor of shape (n_query, embed_dim).</p> <code>None</code> <code>query_pos_emb</code> <code>Tensor | None</code> <p>The target positional embedding of shape (n_query, embed_dim)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self,\n    decoder_queries: torch.Tensor,\n    encoder_features: torch.Tensor,\n    ref_pos_emb: torch.Tensor | None = None,\n    query_pos_emb: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Execute forward pass of decoder layer.\n\n    Args:\n        decoder_queries: Target sequence for decoder to generate (n_query, batch_size, embed_dim).\n        encoder_features: Output from encoder, that decoder uses to attend to relevant\n            parts of input sequence (total_instances, batch_size, embed_dim)\n        ref_pos_emb: The input positional embedding tensor of shape (n_query, embed_dim).\n        query_pos_emb: The target positional embedding of shape (n_query, embed_dim)\n\n    Returns:\n        The output tensor of shape (n_query, batch_size, embed_dim).\n    \"\"\"\n    if query_pos_emb is None:\n        query_pos_emb = torch.zeros_like(decoder_queries)\n    if ref_pos_emb is None:\n        ref_pos_emb = torch.zeros_like(encoder_features)\n\n    decoder_queries = decoder_queries + query_pos_emb\n    encoder_features = encoder_features + ref_pos_emb\n\n    if self.decoder_self_attn:\n        self_attn_features = self.self_attn(\n            query=decoder_queries, key=decoder_queries, value=decoder_queries\n        )[0]\n        decoder_queries = decoder_queries + self.dropout1(self_attn_features)\n        decoder_queries = self.norm1(decoder_queries)\n\n    x_attn_features = self.multihead_attn(\n        query=decoder_queries,  # (n_query, batch_size, embed_dim)\n        key=encoder_features,  # (total_instances, batch_size, embed_dim)\n        value=encoder_features,  # (total_instances, batch_size, embed_dim)\n    )[\n        0\n    ]  # (n_query, batch_size, embed_dim)\n\n    decoder_queries = decoder_queries + self.dropout2(\n        x_attn_features\n    )  # (n_query, batch_size, embed_dim)\n    decoder_queries = self.norm2(\n        decoder_queries\n    )  # (n_query, batch_size, embed_dim)\n    projection = self.linear2(\n        self.dropout(self.activation(self.linear1(decoder_queries)))\n    )  # (n_query, batch_size, embed_dim)\n    decoder_queries = decoder_queries + self.dropout3(\n        projection\n    )  # (n_query, batch_size, embed_dim)\n    decoder_features = self.norm3(decoder_queries)\n\n    return decoder_features  # (n_query, batch_size, embed_dim)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.attention_head.ATTWeightHead","title":"<code>dreem.models.attention_head.ATTWeightHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Single attention head.</p> Source code in <code>dreem/models/attention_head.py</code> <pre><code>class ATTWeightHead(torch.nn.Module):\n    \"\"\"Single attention head.\"\"\"\n\n    def __init__(\n        self,\n        feature_dim: int,\n        num_layers: int,\n        dropout: float,\n    ):\n        \"\"\"Initialize an instance of ATTWeightHead.\n\n        Args:\n            feature_dim: The dimensionality of input features.\n            num_layers: The number of hidden layers in the MLP.\n            dropout: Dropout probability.\n        \"\"\"\n        super().__init__()\n\n        self.q_proj = MLP(feature_dim, feature_dim, feature_dim, num_layers, dropout)\n        self.k_proj = MLP(feature_dim, feature_dim, feature_dim, num_layers, dropout)\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the attention weights of a query tensor using the key tensor.\n\n        Args:\n            query: Input tensor of shape (batch_size, num_frame_instances, feature_dim).\n            key: Input tensor of shape (batch_size, num_window_instances, feature_dim).\n\n        Returns:\n            Output tensor of shape (batch_size, num_frame_instances, num_window_instances).\n        \"\"\"\n        k = self.k_proj(key)\n        q = self.q_proj(query)\n        attn_weights = torch.bmm(q, k.transpose(1, 2))\n\n        return attn_weights  # (B, N_t, N)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.attention_head.ATTWeightHead.__init__","title":"<code>__init__(feature_dim, num_layers, dropout)</code>","text":"<p>Initialize an instance of ATTWeightHead.</p> <p>Parameters:</p> Name Type Description Default <code>feature_dim</code> <code>int</code> <p>The dimensionality of input features.</p> required <code>num_layers</code> <code>int</code> <p>The number of hidden layers in the MLP.</p> required <code>dropout</code> <code>float</code> <p>Dropout probability.</p> required Source code in <code>dreem/models/attention_head.py</code> <pre><code>def __init__(\n    self,\n    feature_dim: int,\n    num_layers: int,\n    dropout: float,\n):\n    \"\"\"Initialize an instance of ATTWeightHead.\n\n    Args:\n        feature_dim: The dimensionality of input features.\n        num_layers: The number of hidden layers in the MLP.\n        dropout: Dropout probability.\n    \"\"\"\n    super().__init__()\n\n    self.q_proj = MLP(feature_dim, feature_dim, feature_dim, num_layers, dropout)\n    self.k_proj = MLP(feature_dim, feature_dim, feature_dim, num_layers, dropout)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.attention_head.ATTWeightHead.forward","title":"<code>forward(query, key)</code>","text":"<p>Compute the attention weights of a query tensor using the key tensor.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, num_frame_instances, feature_dim).</p> required <code>key</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, num_window_instances, feature_dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, num_frame_instances, num_window_instances).</p> Source code in <code>dreem/models/attention_head.py</code> <pre><code>def forward(\n    self,\n    query: torch.Tensor,\n    key: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the attention weights of a query tensor using the key tensor.\n\n    Args:\n        query: Input tensor of shape (batch_size, num_frame_instances, feature_dim).\n        key: Input tensor of shape (batch_size, num_window_instances, feature_dim).\n\n    Returns:\n        Output tensor of shape (batch_size, num_frame_instances, num_window_instances).\n    \"\"\"\n    k = self.k_proj(key)\n    q = self.q_proj(query)\n    attn_weights = torch.bmm(q, k.transpose(1, 2))\n\n    return attn_weights  # (B, N_t, N)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.mlp.MLP","title":"<code>dreem.models.mlp.MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>Multi-Layer Perceptron (MLP) module.</p> Source code in <code>dreem/models/mlp.py</code> <pre><code>class MLP(torch.nn.Module):\n    \"\"\"Multi-Layer Perceptron (MLP) module.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        output_dim: int,\n        num_layers: int,\n        dropout: float = 0.0,\n    ):\n        \"\"\"Initialize MLP.\n\n        Args:\n            input_dim: Dimensionality of the input features.\n            hidden_dim: Number of units in the hidden layers.\n            output_dim: Dimensionality of the output features.\n            num_layers: Number of hidden layers.\n            dropout: Dropout probability.\n        \"\"\"\n        super().__init__()\n\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        if self.num_layers &gt; 0:\n            h = [hidden_dim] * (num_layers - 1)\n            self.layers = torch.nn.ModuleList(\n                [\n                    torch.nn.Linear(n, k)\n                    for n, k in zip([input_dim] + h, h + [output_dim])\n                ]\n            )\n            if self.dropout &gt; 0.0:\n                self.dropouts = torch.nn.ModuleList(\n                    [torch.nn.Dropout(dropout) for _ in range(self.num_layers - 1)]\n                )\n        else:\n            self.layers = []\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of the MLP.\n\n        Args:\n            x: Input tensor of shape (batch_size, num_instances, input_dim).\n\n        Returns:\n            Output tensor of shape (batch_size, num_instances, output_dim).\n        \"\"\"\n        for i, layer in enumerate(self.layers):\n            x = F.relu(layer(x)) if i &lt; self.num_layers - 1 else layer(x)\n            if i &lt; self.num_layers - 1 and self.dropout &gt; 0.0:\n                x = self.dropouts[i](x)\n\n        return x\n</code></pre>"},{"location":"models/model_parts/#dreem.models.mlp.MLP.__init__","title":"<code>__init__(input_dim, hidden_dim, output_dim, num_layers, dropout=0.0)</code>","text":"<p>Initialize MLP.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of the input features.</p> required <code>hidden_dim</code> <code>int</code> <p>Number of units in the hidden layers.</p> required <code>output_dim</code> <code>int</code> <p>Dimensionality of the output features.</p> required <code>num_layers</code> <code>int</code> <p>Number of hidden layers.</p> required <code>dropout</code> <code>float</code> <p>Dropout probability.</p> <code>0.0</code> Source code in <code>dreem/models/mlp.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    hidden_dim: int,\n    output_dim: int,\n    num_layers: int,\n    dropout: float = 0.0,\n):\n    \"\"\"Initialize MLP.\n\n    Args:\n        input_dim: Dimensionality of the input features.\n        hidden_dim: Number of units in the hidden layers.\n        output_dim: Dimensionality of the output features.\n        num_layers: Number of hidden layers.\n        dropout: Dropout probability.\n    \"\"\"\n    super().__init__()\n\n    self.num_layers = num_layers\n    self.dropout = dropout\n\n    if self.num_layers &gt; 0:\n        h = [hidden_dim] * (num_layers - 1)\n        self.layers = torch.nn.ModuleList(\n            [\n                torch.nn.Linear(n, k)\n                for n, k in zip([input_dim] + h, h + [output_dim])\n            ]\n        )\n        if self.dropout &gt; 0.0:\n            self.dropouts = torch.nn.ModuleList(\n                [torch.nn.Dropout(dropout) for _ in range(self.num_layers - 1)]\n            )\n    else:\n        self.layers = []\n</code></pre>"},{"location":"models/model_parts/#dreem.models.mlp.MLP.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the MLP.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, num_instances, input_dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, num_instances, output_dim).</p> Source code in <code>dreem/models/mlp.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the MLP.\n\n    Args:\n        x: Input tensor of shape (batch_size, num_instances, input_dim).\n\n    Returns:\n        Output tensor of shape (batch_size, num_instances, output_dim).\n    \"\"\"\n    for i, layer in enumerate(self.layers):\n        x = F.relu(layer(x)) if i &lt; self.num_layers - 1 else layer(x)\n        if i &lt; self.num_layers - 1 and self.dropout &gt; 0.0:\n            x = self.dropouts[i](x)\n\n    return x\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>dreem<ul> <li>cli</li> <li>datasets<ul> <li>base_dataset</li> <li>cell_tracking_dataset</li> <li>data_utils</li> <li>eval_dataset</li> <li>microscopy_dataset</li> <li>sleap_dataset</li> <li>tracking_dataset</li> </ul> </li> <li>inference<ul> <li>boxes</li> <li>eval</li> <li>metrics</li> <li>post_processing</li> <li>track</li> <li>track_queue</li> <li>tracker</li> </ul> </li> <li>io<ul> <li>association_matrix</li> <li>config</li> <li>frame</li> <li>instance</li> <li>track</li> <li>visualize</li> </ul> </li> <li>models<ul> <li>attention_head</li> <li>embedding</li> <li>global_tracking_transformer</li> <li>gtr_runner</li> <li>mlp</li> <li>model_utils</li> <li>transformer</li> <li>visual_encoder</li> </ul> </li> <li>training<ul> <li>losses</li> <li>train</li> </ul> </li> <li>version</li> </ul> </li> </ul>"},{"location":"reference/dreem/","title":"dreem","text":""},{"location":"reference/dreem/#dreem","title":"<code>dreem</code>","text":"<p>Top-level package for dreem.</p>"},{"location":"reference/dreem/#dreem.setup_logging","title":"<code>setup_logging()</code>","text":"<p>Setup logging based on <code>logging.yaml</code>.</p> Source code in <code>dreem/__init__.py</code> <pre><code>def setup_logging():\n    \"\"\"Setup logging based on `logging.yaml`.\"\"\"\n    import logging\n    import yaml\n    import os\n\n    package_directory = os.path.dirname(os.path.abspath(__file__))\n\n    with open(os.path.join(package_directory, \"..\", \"logging.yaml\"), \"r\") as stream:\n        logging_cfg = yaml.load(stream, Loader=yaml.FullLoader)\n\n    logging.config.dictConfig(logging_cfg)\n    logger = logging.getLogger(\"dreem\")\n</code></pre>"},{"location":"reference/dreem/cli/","title":"cli","text":""},{"location":"reference/dreem/cli/#dreem.cli","title":"<code>dreem.cli</code>","text":"<p>This module contains the command line interfaces for the dreem package.</p>"},{"location":"reference/dreem/version/","title":"version","text":""},{"location":"reference/dreem/version/#dreem.version","title":"<code>dreem.version</code>","text":"<p>Central location for version information.</p>"},{"location":"reference/dreem/datasets/","title":"datasets","text":""},{"location":"reference/dreem/datasets/#dreem.datasets","title":"<code>dreem.datasets</code>","text":"<p>Data loading and preprocessing.</p>"},{"location":"reference/dreem/datasets/base_dataset/","title":"base_dataset","text":""},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset","title":"<code>dreem.datasets.base_dataset</code>","text":"<p>Module containing logic for loading datasets.</p>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset","title":"<code>BaseDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Base Dataset for microscopy and sleap datasets to override.</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>class BaseDataset(Dataset):\n    \"\"\"Base Dataset for microscopy and sleap datasets to override.\"\"\"\n\n    def __init__(\n        self,\n        label_files: list[str],\n        vid_files: list[str],\n        padding: int,\n        crop_size: int,\n        chunk: bool,\n        clip_length: int,\n        mode: str,\n        augmentations: dict | None = None,\n        n_chunks: int | float = 1.0,\n        seed: int | None = None,\n        gt_list: str | None = None,\n    ):\n        \"\"\"Initialize Dataset.\n\n        Args:\n            label_files: a list of paths to label files.\n                should at least contain detections for inference, detections + tracks for training.\n            vid_files: list of paths to video files.\n            padding: amount of padding around object crops\n            crop_size: the size of the object crops\n            chunk: whether or not to chunk the dataset into batches\n            clip_length: the number of frames in each chunk\n            mode: `train` or `val`. Determines whether this dataset is used for\n                training or validation. Currently doesn't affect dataset logic\n            augmentations: An optional dict mapping augmentations to parameters.\n                See subclasses for details.\n            n_chunks: Number of chunks to subsample from.\n                Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n            seed: set a seed for reproducibility\n            gt_list: An optional path to .txt file containing ground truth for\n                cell tracking challenge datasets.\n        \"\"\"\n        self.vid_files = vid_files\n        self.label_files = label_files\n        self.padding = padding\n        self.crop_size = crop_size\n        self.chunk = chunk\n        self.clip_length = clip_length\n        self.mode = mode\n        self.n_chunks = n_chunks\n        self.seed = seed\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n\n        if augmentations and self.mode == \"train\":\n            self.instance_dropout = augmentations.pop(\n                \"InstanceDropout\", {\"p\": 0.0, \"n\": 0}\n            )\n            self.node_dropout = data_utils.NodeDropout(\n                **augmentations.pop(\"NodeDropout\", {\"p\": 0.0, \"n\": 0})\n            )\n            self.augmentations = data_utils.build_augmentations(augmentations)\n        else:\n            self.instance_dropout = {\"p\": 0.0, \"n\": 0}\n            self.node_dropout = data_utils.NodeDropout(p=0.0, n=0)\n            self.augmentations = None\n\n        # Initialize in subclasses\n        self.frame_idx = None\n        self.labels = None\n        self.gt_list = None\n\n    def create_chunks(self) -&gt; None:\n        \"\"\"Get indexing for data.\n\n        Creates both indexes for selecting dataset (label_idx) and frame in\n        dataset (chunked_frame_idx). If chunking is false, we index directly\n        using the frame ids. Setting chunking to true creates a list of lists\n        containing chunk frames for indexing. This is useful for computational\n        efficiency and data shuffling. To be called by subclass __init__()\n        \"\"\"\n        if self.chunk:\n            self.chunked_frame_idx, self.label_idx = [], []\n            for i, frame_idx in enumerate(self.frame_idx):\n                frame_idx_split = torch.split(frame_idx, self.clip_length)\n                self.chunked_frame_idx.extend(frame_idx_split)\n                self.label_idx.extend(len(frame_idx_split) * [i])\n\n            if self.n_chunks &gt; 0 and self.n_chunks &lt;= 1.0:\n                n_chunks = int(self.n_chunks * len(self.chunked_frame_idx))\n\n            elif self.n_chunks &lt;= len(self.chunked_frame_idx):\n                n_chunks = int(self.n_chunks)\n\n            else:\n                n_chunks = len(self.chunked_frame_idx)\n\n            if n_chunks &gt; 0 and n_chunks &lt; len(self.chunked_frame_idx):\n                sample_idx = np.random.choice(\n                    np.arange(len(self.chunked_frame_idx)), n_chunks, replace=False\n                )\n\n                self.chunked_frame_idx = [self.chunked_frame_idx[i] for i in sample_idx]\n\n                self.label_idx = [self.label_idx[i] for i in sample_idx]\n\n        else:\n            self.chunked_frame_idx = self.frame_idx\n            self.label_idx = [i for i in range(len(self.labels))]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the size of the dataset.\n\n        Returns:\n            the size or the number of chunks in the dataset\n        \"\"\"\n        return len(self.chunked_frame_idx)\n\n    def no_batching_fn(self, batch: list[Frame]) -&gt; list[Frame]:\n        \"\"\"Collate function used to overwrite dataloader batching function.\n\n        Args:\n            batch: the chunk of frames to be returned\n\n        Returns:\n            The batch\n        \"\"\"\n        return batch\n\n    def __getitem__(self, idx: int) -&gt; list[Frame]:\n        \"\"\"Get an element of the dataset.\n\n        Args:\n            idx: the index of the batch. Note this is not the index of the video\n                or the frame.\n\n        Returns:\n            A list of `Frame`s in the chunk containing the metadata + instance features.\n        \"\"\"\n        label_idx, frame_idx = self.get_indices(idx)\n\n        return self.get_instances(label_idx, frame_idx)\n\n    def get_indices(self, idx: int):\n        \"\"\"Retrieve label and frame indices given batch index.\n\n        This method should be implemented in any subclass of the BaseDataset.\n\n        Args:\n            idx: the index of the batch.\n\n        Raises:\n            NotImplementedError: If this method is not overridden in a subclass.\n        \"\"\"\n        raise NotImplementedError(\"Must be implemented in subclass\")\n\n    def get_instances(self, label_idx: list[int], frame_idx: list[int]):\n        \"\"\"Build chunk of frames.\n\n        This method should be implemented in any subclass of the BaseDataset.\n\n        Args:\n            label_idx: The index of the labels.\n            frame_idx: The index of the frames.\n\n        Raises:\n            NotImplementedError: If this method is not overridden in a subclass.\n        \"\"\"\n        raise NotImplementedError(\"Must be implemented in subclass\")\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get an element of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the batch. Note this is not the index of the video or the frame.</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A list of <code>Frame</code>s in the chunk containing the metadata + instance features.</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; list[Frame]:\n    \"\"\"Get an element of the dataset.\n\n    Args:\n        idx: the index of the batch. Note this is not the index of the video\n            or the frame.\n\n    Returns:\n        A list of `Frame`s in the chunk containing the metadata + instance features.\n    \"\"\"\n    label_idx, frame_idx = self.get_indices(idx)\n\n    return self.get_instances(label_idx, frame_idx)\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset.__init__","title":"<code>__init__(label_files, vid_files, padding, crop_size, chunk, clip_length, mode, augmentations=None, n_chunks=1.0, seed=None, gt_list=None)</code>","text":"<p>Initialize Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>label_files</code> <code>list[str]</code> <p>a list of paths to label files. should at least contain detections for inference, detections + tracks for training.</p> required <code>vid_files</code> <code>list[str]</code> <p>list of paths to video files.</p> required <code>padding</code> <code>int</code> <p>amount of padding around object crops</p> required <code>crop_size</code> <code>int</code> <p>the size of the object crops</p> required <code>chunk</code> <code>bool</code> <p>whether or not to chunk the dataset into batches</p> required <code>clip_length</code> <code>int</code> <p>the number of frames in each chunk</p> required <code>mode</code> <code>str</code> <p><code>train</code> or <code>val</code>. Determines whether this dataset is used for training or validation. Currently doesn't affect dataset logic</p> required <code>augmentations</code> <code>dict | None</code> <p>An optional dict mapping augmentations to parameters. See subclasses for details.</p> <code>None</code> <code>n_chunks</code> <code>int | float</code> <p>Number of chunks to subsample from. Can either a fraction of the dataset (ie (0,1.0]) or number of chunks</p> <code>1.0</code> <code>seed</code> <code>int | None</code> <p>set a seed for reproducibility</p> <code>None</code> <code>gt_list</code> <code>str | None</code> <p>An optional path to .txt file containing ground truth for cell tracking challenge datasets.</p> <code>None</code> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def __init__(\n    self,\n    label_files: list[str],\n    vid_files: list[str],\n    padding: int,\n    crop_size: int,\n    chunk: bool,\n    clip_length: int,\n    mode: str,\n    augmentations: dict | None = None,\n    n_chunks: int | float = 1.0,\n    seed: int | None = None,\n    gt_list: str | None = None,\n):\n    \"\"\"Initialize Dataset.\n\n    Args:\n        label_files: a list of paths to label files.\n            should at least contain detections for inference, detections + tracks for training.\n        vid_files: list of paths to video files.\n        padding: amount of padding around object crops\n        crop_size: the size of the object crops\n        chunk: whether or not to chunk the dataset into batches\n        clip_length: the number of frames in each chunk\n        mode: `train` or `val`. Determines whether this dataset is used for\n            training or validation. Currently doesn't affect dataset logic\n        augmentations: An optional dict mapping augmentations to parameters.\n            See subclasses for details.\n        n_chunks: Number of chunks to subsample from.\n            Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n        seed: set a seed for reproducibility\n        gt_list: An optional path to .txt file containing ground truth for\n            cell tracking challenge datasets.\n    \"\"\"\n    self.vid_files = vid_files\n    self.label_files = label_files\n    self.padding = padding\n    self.crop_size = crop_size\n    self.chunk = chunk\n    self.clip_length = clip_length\n    self.mode = mode\n    self.n_chunks = n_chunks\n    self.seed = seed\n\n    if self.seed is not None:\n        np.random.seed(self.seed)\n\n    if augmentations and self.mode == \"train\":\n        self.instance_dropout = augmentations.pop(\n            \"InstanceDropout\", {\"p\": 0.0, \"n\": 0}\n        )\n        self.node_dropout = data_utils.NodeDropout(\n            **augmentations.pop(\"NodeDropout\", {\"p\": 0.0, \"n\": 0})\n        )\n        self.augmentations = data_utils.build_augmentations(augmentations)\n    else:\n        self.instance_dropout = {\"p\": 0.0, \"n\": 0}\n        self.node_dropout = data_utils.NodeDropout(p=0.0, n=0)\n        self.augmentations = None\n\n    # Initialize in subclasses\n    self.frame_idx = None\n    self.labels = None\n    self.gt_list = None\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset.__len__","title":"<code>__len__()</code>","text":"<p>Get the size of the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>the size or the number of chunks in the dataset</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the size of the dataset.\n\n    Returns:\n        the size or the number of chunks in the dataset\n    \"\"\"\n    return len(self.chunked_frame_idx)\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset.create_chunks","title":"<code>create_chunks()</code>","text":"<p>Get indexing for data.</p> <p>Creates both indexes for selecting dataset (label_idx) and frame in dataset (chunked_frame_idx). If chunking is false, we index directly using the frame ids. Setting chunking to true creates a list of lists containing chunk frames for indexing. This is useful for computational efficiency and data shuffling. To be called by subclass init()</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def create_chunks(self) -&gt; None:\n    \"\"\"Get indexing for data.\n\n    Creates both indexes for selecting dataset (label_idx) and frame in\n    dataset (chunked_frame_idx). If chunking is false, we index directly\n    using the frame ids. Setting chunking to true creates a list of lists\n    containing chunk frames for indexing. This is useful for computational\n    efficiency and data shuffling. To be called by subclass __init__()\n    \"\"\"\n    if self.chunk:\n        self.chunked_frame_idx, self.label_idx = [], []\n        for i, frame_idx in enumerate(self.frame_idx):\n            frame_idx_split = torch.split(frame_idx, self.clip_length)\n            self.chunked_frame_idx.extend(frame_idx_split)\n            self.label_idx.extend(len(frame_idx_split) * [i])\n\n        if self.n_chunks &gt; 0 and self.n_chunks &lt;= 1.0:\n            n_chunks = int(self.n_chunks * len(self.chunked_frame_idx))\n\n        elif self.n_chunks &lt;= len(self.chunked_frame_idx):\n            n_chunks = int(self.n_chunks)\n\n        else:\n            n_chunks = len(self.chunked_frame_idx)\n\n        if n_chunks &gt; 0 and n_chunks &lt; len(self.chunked_frame_idx):\n            sample_idx = np.random.choice(\n                np.arange(len(self.chunked_frame_idx)), n_chunks, replace=False\n            )\n\n            self.chunked_frame_idx = [self.chunked_frame_idx[i] for i in sample_idx]\n\n            self.label_idx = [self.label_idx[i] for i in sample_idx]\n\n    else:\n        self.chunked_frame_idx = self.frame_idx\n        self.label_idx = [i for i in range(len(self.labels))]\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset.get_indices","title":"<code>get_indices(idx)</code>","text":"<p>Retrieve label and frame indices given batch index.</p> <p>This method should be implemented in any subclass of the BaseDataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the batch.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If this method is not overridden in a subclass.</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def get_indices(self, idx: int):\n    \"\"\"Retrieve label and frame indices given batch index.\n\n    This method should be implemented in any subclass of the BaseDataset.\n\n    Args:\n        idx: the index of the batch.\n\n    Raises:\n        NotImplementedError: If this method is not overridden in a subclass.\n    \"\"\"\n    raise NotImplementedError(\"Must be implemented in subclass\")\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset.get_instances","title":"<code>get_instances(label_idx, frame_idx)</code>","text":"<p>Build chunk of frames.</p> <p>This method should be implemented in any subclass of the BaseDataset.</p> <p>Parameters:</p> Name Type Description Default <code>label_idx</code> <code>list[int]</code> <p>The index of the labels.</p> required <code>frame_idx</code> <code>list[int]</code> <p>The index of the frames.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If this method is not overridden in a subclass.</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def get_instances(self, label_idx: list[int], frame_idx: list[int]):\n    \"\"\"Build chunk of frames.\n\n    This method should be implemented in any subclass of the BaseDataset.\n\n    Args:\n        label_idx: The index of the labels.\n        frame_idx: The index of the frames.\n\n    Raises:\n        NotImplementedError: If this method is not overridden in a subclass.\n    \"\"\"\n    raise NotImplementedError(\"Must be implemented in subclass\")\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset.no_batching_fn","title":"<code>no_batching_fn(batch)</code>","text":"<p>Collate function used to overwrite dataloader batching function.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[Frame]</code> <p>the chunk of frames to be returned</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>The batch</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def no_batching_fn(self, batch: list[Frame]) -&gt; list[Frame]:\n    \"\"\"Collate function used to overwrite dataloader batching function.\n\n    Args:\n        batch: the chunk of frames to be returned\n\n    Returns:\n        The batch\n    \"\"\"\n    return batch\n</code></pre>"},{"location":"reference/dreem/datasets/cell_tracking_dataset/","title":"cell_tracking_dataset","text":""},{"location":"reference/dreem/datasets/cell_tracking_dataset/#dreem.datasets.cell_tracking_dataset","title":"<code>dreem.datasets.cell_tracking_dataset</code>","text":"<p>Module containing cell tracking challenge dataset.</p>"},{"location":"reference/dreem/datasets/cell_tracking_dataset/#dreem.datasets.cell_tracking_dataset.CellTrackingDataset","title":"<code>CellTrackingDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset for loading cell tracking challenge data.</p> Source code in <code>dreem/datasets/cell_tracking_dataset.py</code> <pre><code>class CellTrackingDataset(BaseDataset):\n    \"\"\"Dataset for loading cell tracking challenge data.\"\"\"\n\n    def __init__(\n        self,\n        raw_images: list[list[str]],\n        gt_images: list[list[str]],\n        padding: int = 5,\n        crop_size: int = 20,\n        chunk: bool = False,\n        clip_length: int = 10,\n        mode: str = \"train\",\n        augmentations: dict | None = None,\n        n_chunks: int | float = 1.0,\n        seed: int | None = None,\n        gt_list: list[str] | None = None,\n    ):\n        \"\"\"Initialize CellTrackingDataset.\n\n        Args:\n            raw_images: paths to raw microscopy images\n            gt_images: paths to gt label images\n            padding: amount of padding around object crops\n            crop_size: the size of the object crops\n            chunk: whether or not to chunk the dataset into batches\n            clip_length: the number of frames in each chunk\n            mode: `train` or `val`. Determines whether this dataset is used for\n                training or validation. Currently doesn't affect dataset logic\n            augmentations: An optional dict mapping augmentations to parameters. The keys\n                should map directly to augmentation classes in albumentations. Example:\n                    augs = {\n                        'Rotate': {'limit': [-90, 90]},\n                        'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},\n                        'RandomContrast': {'limit': 0.2}\n                    }\n            n_chunks: Number of chunks to subsample from.\n                Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n            seed: set a seed for reproducibility\n            gt_list: An optional path to .txt file containing gt ids stored in cell\n                tracking challenge format: \"track_id\", \"start_frame\",\n                \"end_frame\", \"parent_id\"\n        \"\"\"\n        super().__init__(\n            gt_images,\n            raw_images,\n            padding,\n            crop_size,\n            chunk,\n            clip_length,\n            mode,\n            augmentations,\n            n_chunks,\n            seed,\n            gt_list,\n        )\n\n        self.videos = raw_images\n        self.labels = gt_images\n        self.chunk = chunk\n        self.clip_length = clip_length\n        self.crop_size = crop_size\n        self.padding = padding\n        self.mode = mode.lower()\n        self.n_chunks = n_chunks\n        self.seed = seed\n\n        # if self.seed is not None:\n        #     np.random.seed(self.seed)\n\n        if augmentations and self.mode == \"train\":\n            self.augmentations = data_utils.build_augmentations(augmentations)\n        else:\n            self.augmentations = None\n\n        if gt_list is not None:\n            self.gt_list = [\n                pd.read_csv(\n                    gtf,\n                    delimiter=\" \",\n                    header=None,\n                    names=[\"track_id\", \"start_frame\", \"end_frame\", \"parent_id\"],\n                )\n                for gtf in gt_list\n            ]\n        else:\n            self.gt_list = None\n\n        self.frame_idx = [torch.arange(len(image)) for image in self.labels]\n\n        # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n        # used in call to get_instances()\n        self.create_chunks()\n\n    def get_indices(self, idx: int) -&gt; tuple:\n        \"\"\"Retrieve label and frame indices given batch index.\n\n        Args:\n            idx: the index of the batch.\n\n        Returns:\n            the label and frame indices corresponding to a batch,\n        \"\"\"\n        return self.label_idx[idx], self.chunked_frame_idx[idx]\n\n    def get_instances(self, label_idx: list[int], frame_idx: list[int]) -&gt; list[Frame]:\n        \"\"\"Get an element of the dataset.\n\n        Args:\n            label_idx: index of the labels\n            frame_idx: index of the frames\n\n        Returns:\n            a list of Frame objects containing frame metadata and Instance Objects.\n            See `dreem.io.data_structures` for more info.\n        \"\"\"\n        image = self.videos[label_idx]\n        gt = self.labels[label_idx]\n\n        if self.gt_list is not None:\n            gt_list = self.gt_list[label_idx]\n        else:\n            gt_list = None\n\n        frames = []\n\n        for i in frame_idx:\n            instances, gt_track_ids, centroids, bboxes = [], [], [], []\n\n            i = int(i)\n\n            img = image[i]\n            gt_sec = gt[i]\n\n            img = np.array(Image.open(img))\n            gt_sec = np.array(Image.open(gt_sec))\n\n            if img.dtype == np.uint16:\n                img = ((img - img.min()) * (1 / (img.max() - img.min()) * 255)).astype(\n                    np.uint8\n                )\n\n            if gt_list is None:\n                unique_instances = np.unique(gt_sec)\n            else:\n                unique_instances = gt_list[\"track_id\"].unique()\n\n            for instance in unique_instances:\n                # not all instances are in the frame, and they also label the\n                # background instance as zero\n                if instance in gt_sec and instance != 0:\n                    mask = gt_sec == instance\n                    center_of_mass = measurements.center_of_mass(mask)\n\n                    # scipy returns yx\n                    x, y = center_of_mass[::-1]\n\n                    bbox = data_utils.pad_bbox(\n                        data_utils.get_bbox([int(x), int(y)], self.crop_size),\n                        padding=self.padding,\n                    )\n\n                    gt_track_ids.append(int(instance))\n                    centroids.append([x, y])\n                    bboxes.append(bbox)\n\n            # albumentations wants (spatial, channels), ensure correct dims\n            if self.augmentations is not None:\n                for transform in self.augmentations:\n                    # for occlusion simulation, can remove if we don't want\n                    if isinstance(transform, A.CoarseDropout):\n                        transform.fill_value = random.randint(0, 255)\n\n                augmented = self.augmentations(\n                    image=img,\n                    keypoints=np.vstack(centroids),\n                )\n\n                img, centroids = augmented[\"image\"], augmented[\"keypoints\"]\n\n            img = torch.Tensor(img).unsqueeze(0)\n\n            for j in range(len(gt_track_ids)):\n                crop = data_utils.crop_bbox(img, bboxes[j])\n\n                instances.append(\n                    Instance(\n                        gt_track_id=gt_track_ids[j],\n                        pred_track_id=-1,\n                        bbox=bboxes[j],\n                        crop=crop,\n                    )\n                )\n\n            if self.mode == \"train\":\n                np.random.shuffle(instances)\n\n            frames.append(\n                Frame(\n                    video_id=label_idx,\n                    frame_id=i,\n                    img_shape=img.shape,\n                    instances=instances,\n                )\n            )\n\n        return frames\n</code></pre>"},{"location":"reference/dreem/datasets/cell_tracking_dataset/#dreem.datasets.cell_tracking_dataset.CellTrackingDataset.__init__","title":"<code>__init__(raw_images, gt_images, padding=5, crop_size=20, chunk=False, clip_length=10, mode='train', augmentations=None, n_chunks=1.0, seed=None, gt_list=None)</code>","text":"<p>Initialize CellTrackingDataset.</p> <p>Parameters:</p> Name Type Description Default <code>raw_images</code> <code>list[list[str]]</code> <p>paths to raw microscopy images</p> required <code>gt_images</code> <code>list[list[str]]</code> <p>paths to gt label images</p> required <code>padding</code> <code>int</code> <p>amount of padding around object crops</p> <code>5</code> <code>crop_size</code> <code>int</code> <p>the size of the object crops</p> <code>20</code> <code>chunk</code> <code>bool</code> <p>whether or not to chunk the dataset into batches</p> <code>False</code> <code>clip_length</code> <code>int</code> <p>the number of frames in each chunk</p> <code>10</code> <code>mode</code> <code>str</code> <p><code>train</code> or <code>val</code>. Determines whether this dataset is used for training or validation. Currently doesn't affect dataset logic</p> <code>'train'</code> <code>augmentations</code> <code>dict | None</code> <p>An optional dict mapping augmentations to parameters. The keys should map directly to augmentation classes in albumentations. Example:     augs = {         'Rotate': {'limit': [-90, 90]},         'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},         'RandomContrast': {'limit': 0.2}     }</p> <code>None</code> <code>n_chunks</code> <code>int | float</code> <p>Number of chunks to subsample from. Can either a fraction of the dataset (ie (0,1.0]) or number of chunks</p> <code>1.0</code> <code>seed</code> <code>int | None</code> <p>set a seed for reproducibility</p> <code>None</code> <code>gt_list</code> <code>list[str] | None</code> <p>An optional path to .txt file containing gt ids stored in cell tracking challenge format: \"track_id\", \"start_frame\", \"end_frame\", \"parent_id\"</p> <code>None</code> Source code in <code>dreem/datasets/cell_tracking_dataset.py</code> <pre><code>def __init__(\n    self,\n    raw_images: list[list[str]],\n    gt_images: list[list[str]],\n    padding: int = 5,\n    crop_size: int = 20,\n    chunk: bool = False,\n    clip_length: int = 10,\n    mode: str = \"train\",\n    augmentations: dict | None = None,\n    n_chunks: int | float = 1.0,\n    seed: int | None = None,\n    gt_list: list[str] | None = None,\n):\n    \"\"\"Initialize CellTrackingDataset.\n\n    Args:\n        raw_images: paths to raw microscopy images\n        gt_images: paths to gt label images\n        padding: amount of padding around object crops\n        crop_size: the size of the object crops\n        chunk: whether or not to chunk the dataset into batches\n        clip_length: the number of frames in each chunk\n        mode: `train` or `val`. Determines whether this dataset is used for\n            training or validation. Currently doesn't affect dataset logic\n        augmentations: An optional dict mapping augmentations to parameters. The keys\n            should map directly to augmentation classes in albumentations. Example:\n                augs = {\n                    'Rotate': {'limit': [-90, 90]},\n                    'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},\n                    'RandomContrast': {'limit': 0.2}\n                }\n        n_chunks: Number of chunks to subsample from.\n            Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n        seed: set a seed for reproducibility\n        gt_list: An optional path to .txt file containing gt ids stored in cell\n            tracking challenge format: \"track_id\", \"start_frame\",\n            \"end_frame\", \"parent_id\"\n    \"\"\"\n    super().__init__(\n        gt_images,\n        raw_images,\n        padding,\n        crop_size,\n        chunk,\n        clip_length,\n        mode,\n        augmentations,\n        n_chunks,\n        seed,\n        gt_list,\n    )\n\n    self.videos = raw_images\n    self.labels = gt_images\n    self.chunk = chunk\n    self.clip_length = clip_length\n    self.crop_size = crop_size\n    self.padding = padding\n    self.mode = mode.lower()\n    self.n_chunks = n_chunks\n    self.seed = seed\n\n    # if self.seed is not None:\n    #     np.random.seed(self.seed)\n\n    if augmentations and self.mode == \"train\":\n        self.augmentations = data_utils.build_augmentations(augmentations)\n    else:\n        self.augmentations = None\n\n    if gt_list is not None:\n        self.gt_list = [\n            pd.read_csv(\n                gtf,\n                delimiter=\" \",\n                header=None,\n                names=[\"track_id\", \"start_frame\", \"end_frame\", \"parent_id\"],\n            )\n            for gtf in gt_list\n        ]\n    else:\n        self.gt_list = None\n\n    self.frame_idx = [torch.arange(len(image)) for image in self.labels]\n\n    # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n    # used in call to get_instances()\n    self.create_chunks()\n</code></pre>"},{"location":"reference/dreem/datasets/cell_tracking_dataset/#dreem.datasets.cell_tracking_dataset.CellTrackingDataset.get_indices","title":"<code>get_indices(idx)</code>","text":"<p>Retrieve label and frame indices given batch index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the batch.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>the label and frame indices corresponding to a batch,</p> Source code in <code>dreem/datasets/cell_tracking_dataset.py</code> <pre><code>def get_indices(self, idx: int) -&gt; tuple:\n    \"\"\"Retrieve label and frame indices given batch index.\n\n    Args:\n        idx: the index of the batch.\n\n    Returns:\n        the label and frame indices corresponding to a batch,\n    \"\"\"\n    return self.label_idx[idx], self.chunked_frame_idx[idx]\n</code></pre>"},{"location":"reference/dreem/datasets/cell_tracking_dataset/#dreem.datasets.cell_tracking_dataset.CellTrackingDataset.get_instances","title":"<code>get_instances(label_idx, frame_idx)</code>","text":"<p>Get an element of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>label_idx</code> <code>list[int]</code> <p>index of the labels</p> required <code>frame_idx</code> <code>list[int]</code> <p>index of the frames</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>a list of Frame objects containing frame metadata and Instance Objects. See <code>dreem.io.data_structures</code> for more info.</p> Source code in <code>dreem/datasets/cell_tracking_dataset.py</code> <pre><code>def get_instances(self, label_idx: list[int], frame_idx: list[int]) -&gt; list[Frame]:\n    \"\"\"Get an element of the dataset.\n\n    Args:\n        label_idx: index of the labels\n        frame_idx: index of the frames\n\n    Returns:\n        a list of Frame objects containing frame metadata and Instance Objects.\n        See `dreem.io.data_structures` for more info.\n    \"\"\"\n    image = self.videos[label_idx]\n    gt = self.labels[label_idx]\n\n    if self.gt_list is not None:\n        gt_list = self.gt_list[label_idx]\n    else:\n        gt_list = None\n\n    frames = []\n\n    for i in frame_idx:\n        instances, gt_track_ids, centroids, bboxes = [], [], [], []\n\n        i = int(i)\n\n        img = image[i]\n        gt_sec = gt[i]\n\n        img = np.array(Image.open(img))\n        gt_sec = np.array(Image.open(gt_sec))\n\n        if img.dtype == np.uint16:\n            img = ((img - img.min()) * (1 / (img.max() - img.min()) * 255)).astype(\n                np.uint8\n            )\n\n        if gt_list is None:\n            unique_instances = np.unique(gt_sec)\n        else:\n            unique_instances = gt_list[\"track_id\"].unique()\n\n        for instance in unique_instances:\n            # not all instances are in the frame, and they also label the\n            # background instance as zero\n            if instance in gt_sec and instance != 0:\n                mask = gt_sec == instance\n                center_of_mass = measurements.center_of_mass(mask)\n\n                # scipy returns yx\n                x, y = center_of_mass[::-1]\n\n                bbox = data_utils.pad_bbox(\n                    data_utils.get_bbox([int(x), int(y)], self.crop_size),\n                    padding=self.padding,\n                )\n\n                gt_track_ids.append(int(instance))\n                centroids.append([x, y])\n                bboxes.append(bbox)\n\n        # albumentations wants (spatial, channels), ensure correct dims\n        if self.augmentations is not None:\n            for transform in self.augmentations:\n                # for occlusion simulation, can remove if we don't want\n                if isinstance(transform, A.CoarseDropout):\n                    transform.fill_value = random.randint(0, 255)\n\n            augmented = self.augmentations(\n                image=img,\n                keypoints=np.vstack(centroids),\n            )\n\n            img, centroids = augmented[\"image\"], augmented[\"keypoints\"]\n\n        img = torch.Tensor(img).unsqueeze(0)\n\n        for j in range(len(gt_track_ids)):\n            crop = data_utils.crop_bbox(img, bboxes[j])\n\n            instances.append(\n                Instance(\n                    gt_track_id=gt_track_ids[j],\n                    pred_track_id=-1,\n                    bbox=bboxes[j],\n                    crop=crop,\n                )\n            )\n\n        if self.mode == \"train\":\n            np.random.shuffle(instances)\n\n        frames.append(\n            Frame(\n                video_id=label_idx,\n                frame_id=i,\n                img_shape=img.shape,\n                instances=instances,\n            )\n        )\n\n    return frames\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/","title":"data_utils","text":""},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils","title":"<code>dreem.datasets.data_utils</code>","text":"<p>Module containing helper functions for datasets.</p>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.LazyTiffStack","title":"<code>LazyTiffStack</code>","text":"<p>Class used for loading tiffs without loading into memory.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>class LazyTiffStack:\n    \"\"\"Class used for loading tiffs without loading into memory.\"\"\"\n\n    def __init__(self, filename: str):\n        \"\"\"Initialize class.\n\n        Args:\n            filename: name of tif file to be opened\n        \"\"\"\n        # expects spatial, channels\n        self.image = Image.open(filename)\n\n    def __getitem__(self, section_idx: int) -&gt; Image:\n        \"\"\"Get frame.\n\n        Args:\n            section_idx: index of frame or z-slice to get.\n\n        Returns:\n            a PIL image of that frame/z-slice.\n        \"\"\"\n        self.image.seek(section_idx)\n        return self.image\n\n    def get_section(self, section_idx: int) -&gt; np.array:\n        \"\"\"Get frame as ndarray.\n\n        Args:\n            section_idx: index of frame or z-slice to get.\n\n        Returns:\n            an np.array of that frame/z-slice.\n        \"\"\"\n        section = self.__getitem__(section_idx)\n        return np.array(section)\n\n    def close(self):\n        \"\"\"Close tiff stack.\"\"\"\n        self.file.close()\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.LazyTiffStack.__getitem__","title":"<code>__getitem__(section_idx)</code>","text":"<p>Get frame.</p> <p>Parameters:</p> Name Type Description Default <code>section_idx</code> <code>int</code> <p>index of frame or z-slice to get.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>a PIL image of that frame/z-slice.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def __getitem__(self, section_idx: int) -&gt; Image:\n    \"\"\"Get frame.\n\n    Args:\n        section_idx: index of frame or z-slice to get.\n\n    Returns:\n        a PIL image of that frame/z-slice.\n    \"\"\"\n    self.image.seek(section_idx)\n    return self.image\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.LazyTiffStack.__init__","title":"<code>__init__(filename)</code>","text":"<p>Initialize class.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>name of tif file to be opened</p> required Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def __init__(self, filename: str):\n    \"\"\"Initialize class.\n\n    Args:\n        filename: name of tif file to be opened\n    \"\"\"\n    # expects spatial, channels\n    self.image = Image.open(filename)\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.LazyTiffStack.close","title":"<code>close()</code>","text":"<p>Close tiff stack.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def close(self):\n    \"\"\"Close tiff stack.\"\"\"\n    self.file.close()\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.LazyTiffStack.get_section","title":"<code>get_section(section_idx)</code>","text":"<p>Get frame as ndarray.</p> <p>Parameters:</p> Name Type Description Default <code>section_idx</code> <code>int</code> <p>index of frame or z-slice to get.</p> required <p>Returns:</p> Type Description <code>array</code> <p>an np.array of that frame/z-slice.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def get_section(self, section_idx: int) -&gt; np.array:\n    \"\"\"Get frame as ndarray.\n\n    Args:\n        section_idx: index of frame or z-slice to get.\n\n    Returns:\n        an np.array of that frame/z-slice.\n    \"\"\"\n    section = self.__getitem__(section_idx)\n    return np.array(section)\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.NodeDropout","title":"<code>NodeDropout</code>","text":"<p>Node dropout augmentation.</p> <p>Drop up to <code>n</code> nodes with probability <code>p</code>.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>class NodeDropout:\n    \"\"\"Node dropout augmentation.\n\n    Drop up to `n` nodes with probability `p`.\n    \"\"\"\n\n    def __init__(self, p: float, n: int) -&gt; None:\n        \"\"\"Initialize Node Dropout Augmentation.\n\n        Args:\n            p: the probability with which to drop the nodes\n            n: the maximum number of nodes to drop\n        \"\"\"\n        self.n = n\n        self.p = p\n\n    def __call__(self, nodes: list[str]) -&gt; list[str]:\n        \"\"\"Wrap `drop_nodes` to enable class call.\n\n        Args:\n            nodes: A list of available node names to drop.\n\n        Returns:\n            dropped_nodes: A list of up to `self.n` nodes to drop.\n        \"\"\"\n        return self.forward(nodes)\n\n    def forward(self, nodes: list[str]) -&gt; list[str]:\n        \"\"\"Drop up to `n` random nodes with probability p.\n\n        Args:\n            nodes: A list of available node names to drop.\n\n        Returns:\n            dropped_nodes: A list of up to `self.n` nodes to drop.\n        \"\"\"\n        if self.n == 0 or self.p == 0:\n            return []\n\n        nodes_to_drop = np.random.permutation(nodes)\n        node_dropout_p = np.random.uniform(size=len(nodes_to_drop))\n\n        dropped_node_inds = np.where(node_dropout_p &lt; self.p)\n        node_dropout_p = node_dropout_p[dropped_node_inds]\n\n        n_nodes_to_drop = min(self.n, len(node_dropout_p))\n\n        dropped_node_inds = np.argpartition(node_dropout_p, -n_nodes_to_drop)[\n            -n_nodes_to_drop:\n        ]\n\n        dropped_nodes = nodes_to_drop[dropped_node_inds]\n\n        return dropped_nodes\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.NodeDropout.__call__","title":"<code>__call__(nodes)</code>","text":"<p>Wrap <code>drop_nodes</code> to enable class call.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[str]</code> <p>A list of available node names to drop.</p> required <p>Returns:</p> Name Type Description <code>dropped_nodes</code> <code>list[str]</code> <p>A list of up to <code>self.n</code> nodes to drop.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def __call__(self, nodes: list[str]) -&gt; list[str]:\n    \"\"\"Wrap `drop_nodes` to enable class call.\n\n    Args:\n        nodes: A list of available node names to drop.\n\n    Returns:\n        dropped_nodes: A list of up to `self.n` nodes to drop.\n    \"\"\"\n    return self.forward(nodes)\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.NodeDropout.__init__","title":"<code>__init__(p, n)</code>","text":"<p>Initialize Node Dropout Augmentation.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>the probability with which to drop the nodes</p> required <code>n</code> <code>int</code> <p>the maximum number of nodes to drop</p> required Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def __init__(self, p: float, n: int) -&gt; None:\n    \"\"\"Initialize Node Dropout Augmentation.\n\n    Args:\n        p: the probability with which to drop the nodes\n        n: the maximum number of nodes to drop\n    \"\"\"\n    self.n = n\n    self.p = p\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.NodeDropout.forward","title":"<code>forward(nodes)</code>","text":"<p>Drop up to <code>n</code> random nodes with probability p.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[str]</code> <p>A list of available node names to drop.</p> required <p>Returns:</p> Name Type Description <code>dropped_nodes</code> <code>list[str]</code> <p>A list of up to <code>self.n</code> nodes to drop.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def forward(self, nodes: list[str]) -&gt; list[str]:\n    \"\"\"Drop up to `n` random nodes with probability p.\n\n    Args:\n        nodes: A list of available node names to drop.\n\n    Returns:\n        dropped_nodes: A list of up to `self.n` nodes to drop.\n    \"\"\"\n    if self.n == 0 or self.p == 0:\n        return []\n\n    nodes_to_drop = np.random.permutation(nodes)\n    node_dropout_p = np.random.uniform(size=len(nodes_to_drop))\n\n    dropped_node_inds = np.where(node_dropout_p &lt; self.p)\n    node_dropout_p = node_dropout_p[dropped_node_inds]\n\n    n_nodes_to_drop = min(self.n, len(node_dropout_p))\n\n    dropped_node_inds = np.argpartition(node_dropout_p, -n_nodes_to_drop)[\n        -n_nodes_to_drop:\n    ]\n\n    dropped_nodes = nodes_to_drop[dropped_node_inds]\n\n    return dropped_nodes\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.build_augmentations","title":"<code>build_augmentations(augmentations)</code>","text":"<p>Get augmentations for dataset.</p> <p>Parameters:</p> Name Type Description Default <code>augmentations</code> <code>dict</code> <p>a dict containing the name of the augmentations            and their parameters</p> required <p>Returns:</p> Type Description <code>Compose</code> <p>An Albumentations composition of different augmentations.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def build_augmentations(augmentations: dict) -&gt; A.Compose:\n    \"\"\"Get augmentations for dataset.\n\n    Args:\n        augmentations: a dict containing the name of the augmentations\n                       and their parameters\n\n    Returns:\n        An Albumentations composition of different augmentations.\n    \"\"\"\n    aug_list = []\n    for aug_name, aug_args in augmentations.items():\n        aug_class = getattr(A, aug_name)\n        aug = aug_class(**aug_args)\n        aug_list.append(aug)\n\n    augs = A.Compose(\n        aug_list,\n        p=1.0,\n        keypoint_params=A.KeypointParams(format=\"xy\", remove_invisible=False),\n    )\n\n    return augs\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.centroid_bbox","title":"<code>centroid_bbox(points, anchors, crop_size)</code>","text":"<p>Calculate bbox around instance centroid.</p> <p>This is useful for ensuring that crops are centered around each instance in the case of incorrect pose estimates.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ArrayLike</code> <p>2d array of centroid coordinates where each row corresponds to a different anchor point.</p> required <code>anchors</code> <code>list</code> <p>indices of a given anchor point to use as the centroid</p> required <code>crop_size</code> <code>int</code> <p>Integer specifying the crop height and width</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Bounding box in [y1, x1, y2, x2] format.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def centroid_bbox(points: ArrayLike, anchors: list, crop_size: int) -&gt; torch.Tensor:\n    \"\"\"Calculate bbox around instance centroid.\n\n    This is useful for ensuring that crops are centered around each instance\n    in the case of incorrect pose estimates.\n\n    Args:\n        points: 2d array of centroid coordinates where each row corresponds to a\n            different anchor point.\n        anchors: indices of a given anchor point to use as the centroid\n        crop_size: Integer specifying the crop height and width\n\n    Returns:\n        Bounding box in [y1, x1, y2, x2] format.\n    \"\"\"\n    for anchor in anchors:\n        cx, cy = points[anchor][0], points[anchor][1]\n        if not np.isnan(cx):\n            break\n\n    bbox = torch.Tensor(\n        [\n            -crop_size / 2 + cy,\n            -crop_size / 2 + cx,\n            crop_size / 2 + cy,\n            crop_size / 2 + cx,\n        ]\n    )\n\n    return bbox\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.crop_bbox","title":"<code>crop_bbox(img, bbox)</code>","text":"<p>Crop an image to a bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor</code> <p>Image as a tensor of shape (channels, height, width).</p> required <code>bbox</code> <code>ArrayLike</code> <p>Bounding box in [y1, x1, y2, x2] format.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Cropped pixels as tensor of shape (channels, height, width).</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def crop_bbox(img: torch.Tensor, bbox: ArrayLike) -&gt; torch.Tensor:\n    \"\"\"Crop an image to a bounding box.\n\n    Args:\n        img: Image as a tensor of shape (channels, height, width).\n        bbox: Bounding box in [y1, x1, y2, x2] format.\n\n    Returns:\n        Cropped pixels as tensor of shape (channels, height, width).\n    \"\"\"\n    # Crop to the bounding box.\n    y1, x1, y2, x2 = bbox\n    crop = tvf.crop(\n        img,\n        top=int(y1.round()),\n        left=int(x1.round()),\n        height=int((y2 - y1).round()),\n        width=int((x2 - x1).round()),\n    )\n\n    return crop\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.get_bbox","title":"<code>get_bbox(center, size)</code>","text":"<p>Get a square bbox around a centroid coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>center</code> <code>ArrayLike</code> <p>centroid coordinates in (x,y)</p> required <code>size</code> <code>int | tuple[int]</code> <p>size of the bounding box</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor in form y1, x1, y2, x2</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def get_bbox(center: ArrayLike, size: int | tuple[int]) -&gt; torch.Tensor:\n    \"\"\"Get a square bbox around a centroid coordinates.\n\n    Args:\n        center: centroid coordinates in (x,y)\n        size: size of the bounding box\n\n    Returns:\n        A torch tensor in form y1, x1, y2, x2\n    \"\"\"\n    if isinstance(size, int):\n        size = (size, size)\n    cx, cy = center[0], center[1]\n\n    y1 = max(0, -size[-1] // 2 + cy)\n    x1 = max(0, -size[0] // 2 + cx)\n    y2 = size[-1] // 2 + cy if y1 != 0 else size[1]\n    x2 = size[0] // 2 + cx if x1 != 0 else size[0]\n    bbox = torch.Tensor([y1, x1, y2, x2])\n\n    return bbox\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.get_max_padding","title":"<code>get_max_padding(height, width)</code>","text":"<p>Calculate maximum padding dimensions for a given height and width.</p> <p>Useful if padding is required for rotational augmentations, e.g when centroids lie on the borders of an image.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>The original height.</p> required <code>width</code> <code>int</code> <p>The original width.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the padded height and padded width.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def get_max_padding(height: int, width: int) -&gt; tuple:\n    \"\"\"Calculate maximum padding dimensions for a given height and width.\n\n    Useful if padding is required for rotational augmentations, e.g when\n    centroids lie on the borders of an image.\n\n    Args:\n        height: The original height.\n        width: The original width.\n\n    Returns:\n        A tuple containing the padded height and padded width.\n    \"\"\"\n    diagonal = math.ceil(math.sqrt(height**2 + width**2))\n\n    padded_height = height + (diagonal - height)\n    padded_width = width + (diagonal - width)\n\n    return padded_height, padded_width\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.pad_bbox","title":"<code>pad_bbox(bbox, padding=16)</code>","text":"<p>Pad bounding box coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>ArrayLike</code> <p>Bounding box in [y1, x1, y2, x2] format.</p> required <code>padding</code> <code>int</code> <p>Padding to add to each side in pixels.</p> <code>16</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Padded bounding box in [y1, x1, y2, x2] format.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def pad_bbox(bbox: ArrayLike, padding: int = 16) -&gt; torch.Tensor:\n    \"\"\"Pad bounding box coordinates.\n\n    Args:\n        bbox: Bounding box in [y1, x1, y2, x2] format.\n        padding: Padding to add to each side in pixels.\n\n    Returns:\n        Padded bounding box in [y1, x1, y2, x2] format.\n    \"\"\"\n    y1, x1, y2, x2 = bbox\n    y1, x1 = y1 - padding, x1 - padding\n    y2, x2 = y2 + padding, x2 + padding\n    return torch.Tensor([y1, x1, y2, x2])\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.parse_synthetic","title":"<code>parse_synthetic(xml_path, source='icy')</code>","text":"<p>Parse .xml labels from synthetic data generated by ICY or ISBI tracking challenge.</p> <p>Logic adapted from https://github.com/sylvainprigent/napari-tracks-reader/blob/main/napari_tracks_reader</p> <p>Parameters:</p> Name Type Description Default <code>xml_path</code> <code>str</code> <p>path to .xml file containing ICY or ISBI gt trajectory labels</p> required <code>source</code> <code>str</code> <p>synthetic dataset type. Should be either icy or isbi</p> <code>'icy'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas DataFrame containing frame idx, gt track id and centroid x,y coordinates in pixels</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def parse_synthetic(xml_path: str, source: str = \"icy\") -&gt; pd.DataFrame:\n    \"\"\"Parse .xml labels from synthetic data generated by ICY or ISBI tracking challenge.\n\n    Logic adapted from https://github.com/sylvainprigent/napari-tracks-reader/blob/main/napari_tracks_reader\n\n    Args:\n        xml_path: path to .xml file containing ICY or ISBI gt trajectory labels\n        source: synthetic dataset type. Should be either icy or isbi\n\n    Returns:\n        pandas DataFrame containing frame idx, gt track id\n        and centroid x,y coordinates in pixels\n    \"\"\"\n    if source.lower() == \"icy\":\n        root_tag = \"trackgroup\"\n    elif source.lower() == \"isbi\":\n        root_tag = \"TrackContestISBI2012\"\n    else:\n        raise ValueError(f\"{source} source mode not supported\")\n\n    tree = et.parse(xml_path)\n\n    root = tree.getroot()\n    tracks = np.empty((0, 4))\n\n    # get the trackgroup element\n    idx_trackgroup = 0\n    for i in range(len(root)):\n        if root[i].tag == root_tag:\n            idx_trackgroup = i\n            break\n\n    ids_map = {}\n    track_id = -1\n    for track_element in root[idx_trackgroup]:\n        track_id += 1\n\n        try:\n            ids_map[track_element.attrib[\"id\"]] = track_id\n        except:\n            pass\n\n        for detection_element in track_element:\n            row = [\n                float(track_id),\n                float(detection_element.attrib[\"t\"]),\n                float(detection_element.attrib[\"y\"]),\n                float(detection_element.attrib[\"x\"]),\n            ]\n            tracks = np.concatenate((tracks, [row]), axis=0)\n\n    tracks_df = pd.DataFrame(\n        tracks, columns=[\"TRACK_ID\", \"FRAME\", \"POSITION_Y\", \"POSITION_X\"]\n    )\n\n    tracks_df = tracks_df.apply(pd.to_numeric, errors=\"coerce\", downcast=\"integer\")\n\n    return tracks_df\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.parse_trackmate","title":"<code>parse_trackmate(data_path)</code>","text":"<p>Parse trackmate xml or csv labels file.</p> <p>Logic adapted from https://github.com/hadim/pytrackmate.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>string path to xml or csv file storing trackmate trajectory labels</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>pandas DataFrame</code> containing frame number, track_ids, and centroid x,y coordinates in pixels</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def parse_trackmate(data_path: str) -&gt; pd.DataFrame:\n    \"\"\"Parse trackmate xml or csv labels file.\n\n    Logic adapted from https://github.com/hadim/pytrackmate.\n\n    Args:\n        data_path: string path to xml or csv file storing trackmate trajectory labels\n\n    Returns:\n        `pandas DataFrame` containing frame number, track_ids,\n        and centroid x,y coordinates in pixels\n    \"\"\"\n    if data_path.endswith(\".xml\"):\n        root = et.fromstring(open(data_path).read())\n\n        objects = []\n        features = root.find(\"Model\").find(\"FeatureDeclarations\").find(\"SpotFeatures\")\n        features = [c.get(\"feature\") for c in list(features)] + [\"ID\"]\n\n        spots = root.find(\"Model\").find(\"AllSpots\")\n\n        objects = []\n\n        for frame in spots.findall(\"SpotsInFrame\"):\n            for spot in frame.findall(\"Spot\"):\n                single_object = []\n                for label in features:\n                    single_object.append(spot.get(label))\n                objects.append(single_object)\n\n        tracks_df = pd.DataFrame(objects, columns=features)\n        tracks_df = tracks_df.astype(np.float)\n\n        filtered_track_ids = [\n            int(track.get(\"TRACK_ID\"))\n            for track in root.find(\"Model\").find(\"FilteredTracks\").findall(\"TrackID\")\n        ]\n\n        label_id = 0\n        tracks_df[\"label\"] = np.nan\n\n        tracks = root.find(\"Model\").find(\"AllTracks\")\n        for track in tracks.findall(\"Track\"):\n            track_id = int(track.get(\"TRACK_ID\"))\n            if track_id in filtered_track_ids:\n                spot_ids = [\n                    (\n                        edge.get(\"SPOT_SOURCE_ID\"),\n                        edge.get(\"SPOT_TARGET_ID\"),\n                        edge.get(\"EDGE_TIME\"),\n                    )\n                    for edge in track.findall(\"Edge\")\n                ]\n                spot_ids = np.array(spot_ids).astype(\"float\")[:, :2]\n                spot_ids = set(spot_ids.flatten())\n\n                tracks_df.loc[tracks_df[\"ID\"].isin(spot_ids), \"TRACK_ID\"] = label_id\n                label_id += 1\n\n    elif data_path.endswith(\".csv\"):\n        tracks_df = pd.read_csv(data_path, encoding=\"ISO-8859-1\")\n\n    else:\n        raise ValueError(f\"Unsupported trackmate file extension: {data_path}\")\n\n    tracks_df = tracks_df.apply(pd.to_numeric, errors=\"coerce\", downcast=\"integer\")\n\n    posx_key = \"POSITION_X\"\n    posy_key = \"POSITION_Y\"\n    frame_key = \"FRAME\"\n    track_key = \"TRACK_ID\"\n\n    mapper = {\n        \"X\": posx_key,\n        \"Y\": posy_key,\n        \"x\": posx_key,\n        \"y\": posy_key,\n        \"Slice n\u00b0\": frame_key,\n        \"Track n\u00b0\": track_key,\n    }\n\n    if \"t\" in tracks_df:\n        mapper.update({\"t\": frame_key})\n\n    tracks_df = tracks_df.rename(mapper=mapper, axis=1)\n\n    if data_path.endswith(\".csv\"):\n        # 0 index track and frame ids\n        if min(tracks_df[frame_key]) == 1:\n            tracks_df[frame_key] = tracks_df[frame_key] - 1\n\n        if min(tracks_df[track_key] == 1):\n            tracks_df[track_key] = tracks_df[track_key] - 1\n\n    return tracks_df\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.pose_bbox","title":"<code>pose_bbox(points, bbox_size)</code>","text":"<p>Calculate bbox around instance pose.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>an np array of shape nodes x 2,</p> required <code>bbox_size</code> <code>tuple[int] | int</code> <p>size of bbox either an int indicating square bbox or in (x,y)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Bounding box in [y1, x1, y2, x2] format.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def pose_bbox(points: np.ndarray, bbox_size: tuple[int] | int) -&gt; torch.Tensor:\n    \"\"\"Calculate bbox around instance pose.\n\n    Args:\n        points: an np array of shape nodes x 2,\n        bbox_size: size of bbox either an int indicating square bbox or in (x,y)\n\n    Returns:\n        Bounding box in [y1, x1, y2, x2] format.\n    \"\"\"\n    if isinstance(bbox_size, int):\n        bbox_size = (bbox_size, bbox_size)\n\n    c = np.nanmean(points, axis=0)\n    bbox = torch.Tensor(\n        [\n            c[-1] - bbox_size[-1] / 2,\n            c[0] - bbox_size[0] / 2,\n            c[-1] + bbox_size[-1] / 2,\n            c[0] + bbox_size[0] / 2,\n        ]\n    )\n    return bbox\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.resize_and_pad","title":"<code>resize_and_pad(img, output_size)</code>","text":"<p>Resize and pad an image to fit a square output size.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor</code> <p>Image as a tensor of shape (channels, height, width).</p> required <code>output_size</code> <code>int</code> <p>Integer size of height and width of output.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The image zero padded to be of shape (channels, output_size, output_size).</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def resize_and_pad(img: torch.Tensor, output_size: int) -&gt; torch.Tensor:\n    \"\"\"Resize and pad an image to fit a square output size.\n\n    Args:\n        img: Image as a tensor of shape (channels, height, width).\n        output_size: Integer size of height and width of output.\n\n    Returns:\n        The image zero padded to be of shape (channels, output_size, output_size).\n    \"\"\"\n    # Figure out how to scale without breaking aspect ratio.\n    img_height, img_width = img.shape[-2:]\n    if img_width &lt; img_height:  # taller\n        crop_height = output_size\n        scale = crop_height / img_height\n        crop_width = int(img_width * scale)\n    else:  # wider\n        crop_width = output_size\n        scale = crop_width / img_width\n        crop_height = int(img_height * scale)\n\n    # Scale without breaking aspect ratio.\n    img = tvf.resize(img, size=[crop_height, crop_width])\n\n    # Pad to square.\n    img_height, img_width = img.shape[-2:]\n    hp1 = int((output_size - img_width) / 2)\n    vp1 = int((output_size - img_height) / 2)\n    hp2 = output_size - (img_width + hp1)\n    vp2 = output_size - (img_height + vp1)\n    padding = (hp1, vp1, hp2, vp2)\n    return tvf.pad(img, padding, 0, \"constant\")\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.sorted_anchors","title":"<code>sorted_anchors(labels)</code>","text":"<p>Sort anchor names from most instances with that node to least.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Labels</code> <p>a sleap_io.labels object containing all the labels for that video</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of anchor names sorted by most nodes to least nodes</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def sorted_anchors(labels: sio.Labels) -&gt; list[str]:\n    \"\"\"Sort anchor names from most instances with that node to least.\n\n    Args:\n        labels: a sleap_io.labels object containing all the labels for that video\n\n    Returns:\n        A list of anchor names sorted by most nodes to least nodes\n    \"\"\"\n    all_anchors = labels.skeletons[0].node_names\n\n    anchor_counts = {anchor: 0 for anchor in all_anchors}\n\n    for i in range(len(labels)):\n        lf = labels[i]\n        for instance in lf:\n            for anchor in all_anchors:\n                x, y = instance[anchor].x, instance[anchor].y\n                if np.isnan(x) or np.isnan(y):\n                    anchor_counts[anchor] += 1\n\n    sorted_anchors = sorted(anchor_counts.keys(), key=lambda k: anchor_counts[k])\n\n    return sorted_anchors\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.view_training_batch","title":"<code>view_training_batch(instances, num_frames=1, cmap=None)</code>","text":"<p>Display a grid of images from a batch of training instances.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[dict[str, list[ndarray]]]</code> <p>A list of training instances, where each instance is a dictionary containing the object crops.</p> required <code>num_frames</code> <code>int</code> <p>The number of frames to display per instance.</p> <code>1</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def view_training_batch(\n    instances: list[dict[str, list[np.ndarray]]], num_frames: int = 1, cmap=None\n) -&gt; None:\n    \"\"\"Display a grid of images from a batch of training instances.\n\n    Args:\n        instances: A list of training instances, where each instance is a\n            dictionary containing the object crops.\n        num_frames: The number of frames to display per instance.\n\n    Returns:\n        None\n    \"\"\"\n    num_crops = len(instances[0][\"crops\"])\n    num_columns = num_crops\n    num_rows = num_frames\n\n    base_size = 2\n    fig_size = (base_size * num_columns, base_size * num_rows)\n\n    fig, axes = plt.subplots(num_rows, num_columns, figsize=fig_size)\n\n    for i in range(num_frames):\n        for j, data in enumerate(instances[i][\"crops\"]):\n            try:\n                ax = (\n                    axes[j]\n                    if num_frames == 1\n                    else (axes[i] if num_crops == 1 else axes[i, j])\n                )\n\n                (ax.imshow(data.T) if cmap is None else ax.imshow(data.T, cmap=cmap))\n                ax.axis(\"off\")\n\n            except Exception as e:\n                print(e)\n                pass\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/dreem/datasets/eval_dataset/","title":"eval_dataset","text":""},{"location":"reference/dreem/datasets/eval_dataset/#dreem.datasets.eval_dataset","title":"<code>dreem.datasets.eval_dataset</code>","text":"<p>Module containing wrapper for merging gt and pred datasets for evaluation.</p>"},{"location":"reference/dreem/datasets/eval_dataset/#dreem.datasets.eval_dataset.EvalDataset","title":"<code>EvalDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Wrapper around gt and predicted dataset.</p> Source code in <code>dreem/datasets/eval_dataset.py</code> <pre><code>class EvalDataset(Dataset):\n    \"\"\"Wrapper around gt and predicted dataset.\"\"\"\n\n    def __init__(self, gt_dataset: Dataset, pred_dataset: Dataset) -&gt; None:\n        \"\"\"Initialize EvalDataset.\n\n        Args:\n            gt_dataset: A Dataset object containing ground truth track ids\n            pred_dataset: A dataset object containing predicted track ids\n        \"\"\"\n        self.gt_dataset = gt_dataset\n        self.pred_dataset = pred_dataset\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the size of the dataset.\n\n        Returns:\n            the size or the number of chunks in the dataset\n        \"\"\"\n        return len(self.gt_dataset)\n\n    def __getitem__(self, idx: int) -&gt; list[Frame]:\n        \"\"\"Get an element of the dataset.\n\n        Args:\n            idx: the index of the batch. Note this is not the index of the video\n                or the frame.\n\n        Returns:\n            A list of Frames where frames contain instances w gt and pred track ids + bboxes.\n        \"\"\"\n        gt_batch = self.gt_dataset[idx]\n        pred_batch = self.pred_dataset[idx]\n\n        eval_frames = []\n        for gt_frame, pred_frame in zip(gt_batch, pred_batch):\n            eval_instances = []\n            for i, gt_instance in enumerate(gt_frame.instances):\n\n                gt_track_id = gt_instance.gt_track_id\n\n                try:\n                    pred_track_id = pred_frame.instances[i].gt_track_id\n                    pred_bbox = pred_frame.instances[i].bbox\n                except IndexError:\n                    pred_track_id = -1\n                    pred_bbox = [-1, -1, -1, -1]\n                eval_instances.append(\n                    Instance(\n                        gt_track_id=gt_track_id,\n                        pred_track_id=pred_track_id,\n                        bbox=pred_bbox,\n                    )\n                )\n            eval_frames.append(\n                Frame(\n                    video_id=gt_frame.video_id,\n                    frame_id=gt_frame.frame_id,\n                    vid_file=gt_frame.video.filename,\n                    img_shape=gt_frame.img_shape,\n                    instances=eval_instances,\n                )\n            )\n\n        return eval_frames\n</code></pre>"},{"location":"reference/dreem/datasets/eval_dataset/#dreem.datasets.eval_dataset.EvalDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get an element of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the batch. Note this is not the index of the video or the frame.</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A list of Frames where frames contain instances w gt and pred track ids + bboxes.</p> Source code in <code>dreem/datasets/eval_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; list[Frame]:\n    \"\"\"Get an element of the dataset.\n\n    Args:\n        idx: the index of the batch. Note this is not the index of the video\n            or the frame.\n\n    Returns:\n        A list of Frames where frames contain instances w gt and pred track ids + bboxes.\n    \"\"\"\n    gt_batch = self.gt_dataset[idx]\n    pred_batch = self.pred_dataset[idx]\n\n    eval_frames = []\n    for gt_frame, pred_frame in zip(gt_batch, pred_batch):\n        eval_instances = []\n        for i, gt_instance in enumerate(gt_frame.instances):\n\n            gt_track_id = gt_instance.gt_track_id\n\n            try:\n                pred_track_id = pred_frame.instances[i].gt_track_id\n                pred_bbox = pred_frame.instances[i].bbox\n            except IndexError:\n                pred_track_id = -1\n                pred_bbox = [-1, -1, -1, -1]\n            eval_instances.append(\n                Instance(\n                    gt_track_id=gt_track_id,\n                    pred_track_id=pred_track_id,\n                    bbox=pred_bbox,\n                )\n            )\n        eval_frames.append(\n            Frame(\n                video_id=gt_frame.video_id,\n                frame_id=gt_frame.frame_id,\n                vid_file=gt_frame.video.filename,\n                img_shape=gt_frame.img_shape,\n                instances=eval_instances,\n            )\n        )\n\n    return eval_frames\n</code></pre>"},{"location":"reference/dreem/datasets/eval_dataset/#dreem.datasets.eval_dataset.EvalDataset.__init__","title":"<code>__init__(gt_dataset, pred_dataset)</code>","text":"<p>Initialize EvalDataset.</p> <p>Parameters:</p> Name Type Description Default <code>gt_dataset</code> <code>Dataset</code> <p>A Dataset object containing ground truth track ids</p> required <code>pred_dataset</code> <code>Dataset</code> <p>A dataset object containing predicted track ids</p> required Source code in <code>dreem/datasets/eval_dataset.py</code> <pre><code>def __init__(self, gt_dataset: Dataset, pred_dataset: Dataset) -&gt; None:\n    \"\"\"Initialize EvalDataset.\n\n    Args:\n        gt_dataset: A Dataset object containing ground truth track ids\n        pred_dataset: A dataset object containing predicted track ids\n    \"\"\"\n    self.gt_dataset = gt_dataset\n    self.pred_dataset = pred_dataset\n</code></pre>"},{"location":"reference/dreem/datasets/eval_dataset/#dreem.datasets.eval_dataset.EvalDataset.__len__","title":"<code>__len__()</code>","text":"<p>Get the size of the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>the size or the number of chunks in the dataset</p> Source code in <code>dreem/datasets/eval_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the size of the dataset.\n\n    Returns:\n        the size or the number of chunks in the dataset\n    \"\"\"\n    return len(self.gt_dataset)\n</code></pre>"},{"location":"reference/dreem/datasets/microscopy_dataset/","title":"microscopy_dataset","text":""},{"location":"reference/dreem/datasets/microscopy_dataset/#dreem.datasets.microscopy_dataset","title":"<code>dreem.datasets.microscopy_dataset</code>","text":"<p>Module containing microscopy dataset.</p>"},{"location":"reference/dreem/datasets/microscopy_dataset/#dreem.datasets.microscopy_dataset.MicroscopyDataset","title":"<code>MicroscopyDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset for loading Microscopy Data.</p> Source code in <code>dreem/datasets/microscopy_dataset.py</code> <pre><code>class MicroscopyDataset(BaseDataset):\n    \"\"\"Dataset for loading Microscopy Data.\"\"\"\n\n    def __init__(\n        self,\n        videos: list[str],\n        tracks: list[str],\n        source: str,\n        padding: int = 5,\n        crop_size: int = 20,\n        chunk: bool = False,\n        clip_length: int = 10,\n        mode: str = \"Train\",\n        augmentations: dict | None = None,\n        n_chunks: int | float = 1.0,\n        seed: int | None = None,\n    ):\n        \"\"\"Initialize MicroscopyDataset.\n\n        Args:\n            videos: paths to raw microscopy videos\n            tracks: paths to trackmate gt labels (either .xml or .csv)\n            source: file format of gt labels based on label generator\n            padding: amount of padding around object crops\n            crop_size: the size of the object crops\n            chunk: whether or not to chunk the dataset into batches\n            clip_length: the number of frames in each chunk\n            mode: `train` or `val`. Determines whether this dataset is used for\n                training or validation. Currently doesn't affect dataset logic\n            augmentations: An optional dict mapping augmentations to parameters. The keys\n                should map directly to augmentation classes in albumentations. Example:\n                    augs = {\n                        'Rotate': {'limit': [-90, 90]},\n                        'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},\n                        'RandomContrast': {'limit': 0.2}\n                    }\n            n_chunks: Number of chunks to subsample from.\n                Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n            seed: set a seed for reproducibility\n        \"\"\"\n        super().__init__(\n            tracks,\n            videos,\n            padding,\n            crop_size,\n            chunk,\n            clip_length,\n            mode,\n            augmentations,\n            n_chunks,\n            seed,\n        )\n\n        self.vid_files = videos\n        self.tracks = tracks\n        self.chunk = chunk\n        self.clip_length = clip_length\n        self.crop_size = crop_size\n        self.padding = padding\n        self.mode = mode.lower()\n        self.n_chunks = n_chunks\n        self.seed = seed\n\n        # if self.seed is not None:\n        #     np.random.seed(self.seed)\n        if augmentations and self.mode == \"train\":\n            self.augmentations = data_utils.build_augmentations(augmentations)\n        else:\n            self.augmentations = None\n\n        if source.lower() == \"trackmate\":\n            parser = data_utils.parse_trackmate\n        elif source.lower() in [\"icy\", \"isbi\"]:\n            parser = lambda x: data_utils.parse_synthetic(x, source=source)\n        else:\n            raise ValueError(\n                f\"{source} is unsupported! Must be one of [trackmate, icy, isbi]\"\n            )\n\n        self.labels = [\n            parser(self.tracks[video_idx]) for video_idx in range(len(self.tracks))\n        ]\n\n        self.videos = []\n        for vid_file in self.vid_files:\n            if not isinstance(vid_file, list):\n                self.videos.append(data_utils.LazyTiffStack(vid_file))\n            else:\n                self.videos.append([Image.open(frame_file) for frame_file in vid_file])\n        self.frame_idx = [\n            (\n                torch.arange(Image.open(video).n_frames)\n                if isinstance(video, str)\n                else torch.arange(len(video))\n            )\n            for video in self.vid_files\n        ]\n\n        # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n        # used in call to get_instances()\n        self.create_chunks()\n\n    def get_indices(self, idx: int) -&gt; tuple:\n        \"\"\"Retrieve label and frame indices given batch index.\n\n        Args:\n            idx: the index of the batch.\n        \"\"\"\n        return self.label_idx[idx], self.chunked_frame_idx[idx]\n\n    def get_instances(self, label_idx: list[int], frame_idx: list[int]) -&gt; list[Frame]:\n        \"\"\"Get an element of the dataset.\n\n        Args:\n            label_idx: index of the labels\n            frame_idx: index of the frames\n\n        Returns:\n            A list of Frames containing Instances to be tracked (See `dreem.io.data_structures for more info`)\n        \"\"\"\n        labels = self.labels[label_idx]\n        labels = labels.dropna(how=\"all\")\n\n        video = self.videos[label_idx]\n\n        frames = []\n        for frame_id in frame_idx:\n            instances, gt_track_ids, centroids = [], [], []\n\n            img = (\n                video.get_section(frame_id)\n                if not isinstance(video, list)\n                else np.array(video[frame_id])\n            )\n\n            lf = labels[labels[\"FRAME\"].astype(int) == frame_id.item()]\n\n            for instance in sorted(lf[\"TRACK_ID\"].unique()):\n                gt_track_ids.append(int(instance))\n\n                x = lf[lf[\"TRACK_ID\"] == instance][\"POSITION_X\"].iloc[0]\n                y = lf[lf[\"TRACK_ID\"] == instance][\"POSITION_Y\"].iloc[0]\n                centroids.append([x, y])\n\n            # albumentations wants (spatial, channels), ensure correct dims\n            if self.augmentations is not None:\n                for transform in self.augmentations:\n                    # for occlusion simulation, can remove if we don't want\n                    if isinstance(transform, A.CoarseDropout):\n                        transform.fill_value = random.randint(0, 255)\n\n                augmented = self.augmentations(\n                    image=img,\n                    keypoints=np.vstack(centroids),\n                )\n                img, centroids = augmented[\"image\"], augmented[\"keypoints\"]\n\n            img = torch.Tensor(img)\n\n            # torch wants (channels, spatial) - ensure correct dims\n            if len(img.shape) == 2:\n                img = img.unsqueeze(0)\n            elif len(img.shape) == 3:\n                if img.shape[2] == 3:\n                    img = img.T  # todo: check for edge cases\n\n            for gt_id in range(len(gt_track_ids)):\n                c = centroids[gt_id]\n                bbox = data_utils.pad_bbox(\n                    data_utils.get_bbox([int(c[0]), int(c[1])], self.crop_size),\n                    padding=self.padding,\n                )\n                crop = data_utils.crop_bbox(img, bbox)\n\n                instances.append(\n                    Instance(\n                        gt_track_id=gt_track_ids[gt_id],\n                        pred_track_id=-1,\n                        bbox=bbox,\n                        crop=crop,\n                    )\n                )\n\n            if self.mode == \"train\":\n                np.random.shuffle(instances)\n\n            frames.append(\n                Frame(\n                    video_id=label_idx,\n                    frame_id=frame_id,\n                    img_shape=img.shape,\n                    instances=instances,\n                )\n            )\n\n        return frames\n\n    def __del__(self):\n        \"\"\"Handle file closing before deletion.\"\"\"\n        for vid_reader in self.videos:\n            if not isinstance(vid_reader, list):\n                vid_reader.close()\n            else:\n                for frame_reader in vid_reader:\n                    frame_reader.close()\n</code></pre>"},{"location":"reference/dreem/datasets/microscopy_dataset/#dreem.datasets.microscopy_dataset.MicroscopyDataset.__del__","title":"<code>__del__()</code>","text":"<p>Handle file closing before deletion.</p> Source code in <code>dreem/datasets/microscopy_dataset.py</code> <pre><code>def __del__(self):\n    \"\"\"Handle file closing before deletion.\"\"\"\n    for vid_reader in self.videos:\n        if not isinstance(vid_reader, list):\n            vid_reader.close()\n        else:\n            for frame_reader in vid_reader:\n                frame_reader.close()\n</code></pre>"},{"location":"reference/dreem/datasets/microscopy_dataset/#dreem.datasets.microscopy_dataset.MicroscopyDataset.__init__","title":"<code>__init__(videos, tracks, source, padding=5, crop_size=20, chunk=False, clip_length=10, mode='Train', augmentations=None, n_chunks=1.0, seed=None)</code>","text":"<p>Initialize MicroscopyDataset.</p> <p>Parameters:</p> Name Type Description Default <code>videos</code> <code>list[str]</code> <p>paths to raw microscopy videos</p> required <code>tracks</code> <code>list[str]</code> <p>paths to trackmate gt labels (either .xml or .csv)</p> required <code>source</code> <code>str</code> <p>file format of gt labels based on label generator</p> required <code>padding</code> <code>int</code> <p>amount of padding around object crops</p> <code>5</code> <code>crop_size</code> <code>int</code> <p>the size of the object crops</p> <code>20</code> <code>chunk</code> <code>bool</code> <p>whether or not to chunk the dataset into batches</p> <code>False</code> <code>clip_length</code> <code>int</code> <p>the number of frames in each chunk</p> <code>10</code> <code>mode</code> <code>str</code> <p><code>train</code> or <code>val</code>. Determines whether this dataset is used for training or validation. Currently doesn't affect dataset logic</p> <code>'Train'</code> <code>augmentations</code> <code>dict | None</code> <p>An optional dict mapping augmentations to parameters. The keys should map directly to augmentation classes in albumentations. Example:     augs = {         'Rotate': {'limit': [-90, 90]},         'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},         'RandomContrast': {'limit': 0.2}     }</p> <code>None</code> <code>n_chunks</code> <code>int | float</code> <p>Number of chunks to subsample from. Can either a fraction of the dataset (ie (0,1.0]) or number of chunks</p> <code>1.0</code> <code>seed</code> <code>int | None</code> <p>set a seed for reproducibility</p> <code>None</code> Source code in <code>dreem/datasets/microscopy_dataset.py</code> <pre><code>def __init__(\n    self,\n    videos: list[str],\n    tracks: list[str],\n    source: str,\n    padding: int = 5,\n    crop_size: int = 20,\n    chunk: bool = False,\n    clip_length: int = 10,\n    mode: str = \"Train\",\n    augmentations: dict | None = None,\n    n_chunks: int | float = 1.0,\n    seed: int | None = None,\n):\n    \"\"\"Initialize MicroscopyDataset.\n\n    Args:\n        videos: paths to raw microscopy videos\n        tracks: paths to trackmate gt labels (either .xml or .csv)\n        source: file format of gt labels based on label generator\n        padding: amount of padding around object crops\n        crop_size: the size of the object crops\n        chunk: whether or not to chunk the dataset into batches\n        clip_length: the number of frames in each chunk\n        mode: `train` or `val`. Determines whether this dataset is used for\n            training or validation. Currently doesn't affect dataset logic\n        augmentations: An optional dict mapping augmentations to parameters. The keys\n            should map directly to augmentation classes in albumentations. Example:\n                augs = {\n                    'Rotate': {'limit': [-90, 90]},\n                    'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},\n                    'RandomContrast': {'limit': 0.2}\n                }\n        n_chunks: Number of chunks to subsample from.\n            Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n        seed: set a seed for reproducibility\n    \"\"\"\n    super().__init__(\n        tracks,\n        videos,\n        padding,\n        crop_size,\n        chunk,\n        clip_length,\n        mode,\n        augmentations,\n        n_chunks,\n        seed,\n    )\n\n    self.vid_files = videos\n    self.tracks = tracks\n    self.chunk = chunk\n    self.clip_length = clip_length\n    self.crop_size = crop_size\n    self.padding = padding\n    self.mode = mode.lower()\n    self.n_chunks = n_chunks\n    self.seed = seed\n\n    # if self.seed is not None:\n    #     np.random.seed(self.seed)\n    if augmentations and self.mode == \"train\":\n        self.augmentations = data_utils.build_augmentations(augmentations)\n    else:\n        self.augmentations = None\n\n    if source.lower() == \"trackmate\":\n        parser = data_utils.parse_trackmate\n    elif source.lower() in [\"icy\", \"isbi\"]:\n        parser = lambda x: data_utils.parse_synthetic(x, source=source)\n    else:\n        raise ValueError(\n            f\"{source} is unsupported! Must be one of [trackmate, icy, isbi]\"\n        )\n\n    self.labels = [\n        parser(self.tracks[video_idx]) for video_idx in range(len(self.tracks))\n    ]\n\n    self.videos = []\n    for vid_file in self.vid_files:\n        if not isinstance(vid_file, list):\n            self.videos.append(data_utils.LazyTiffStack(vid_file))\n        else:\n            self.videos.append([Image.open(frame_file) for frame_file in vid_file])\n    self.frame_idx = [\n        (\n            torch.arange(Image.open(video).n_frames)\n            if isinstance(video, str)\n            else torch.arange(len(video))\n        )\n        for video in self.vid_files\n    ]\n\n    # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n    # used in call to get_instances()\n    self.create_chunks()\n</code></pre>"},{"location":"reference/dreem/datasets/microscopy_dataset/#dreem.datasets.microscopy_dataset.MicroscopyDataset.get_indices","title":"<code>get_indices(idx)</code>","text":"<p>Retrieve label and frame indices given batch index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the batch.</p> required Source code in <code>dreem/datasets/microscopy_dataset.py</code> <pre><code>def get_indices(self, idx: int) -&gt; tuple:\n    \"\"\"Retrieve label and frame indices given batch index.\n\n    Args:\n        idx: the index of the batch.\n    \"\"\"\n    return self.label_idx[idx], self.chunked_frame_idx[idx]\n</code></pre>"},{"location":"reference/dreem/datasets/microscopy_dataset/#dreem.datasets.microscopy_dataset.MicroscopyDataset.get_instances","title":"<code>get_instances(label_idx, frame_idx)</code>","text":"<p>Get an element of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>label_idx</code> <code>list[int]</code> <p>index of the labels</p> required <code>frame_idx</code> <code>list[int]</code> <p>index of the frames</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A list of Frames containing Instances to be tracked (See <code>dreem.io.data_structures for more info</code>)</p> Source code in <code>dreem/datasets/microscopy_dataset.py</code> <pre><code>def get_instances(self, label_idx: list[int], frame_idx: list[int]) -&gt; list[Frame]:\n    \"\"\"Get an element of the dataset.\n\n    Args:\n        label_idx: index of the labels\n        frame_idx: index of the frames\n\n    Returns:\n        A list of Frames containing Instances to be tracked (See `dreem.io.data_structures for more info`)\n    \"\"\"\n    labels = self.labels[label_idx]\n    labels = labels.dropna(how=\"all\")\n\n    video = self.videos[label_idx]\n\n    frames = []\n    for frame_id in frame_idx:\n        instances, gt_track_ids, centroids = [], [], []\n\n        img = (\n            video.get_section(frame_id)\n            if not isinstance(video, list)\n            else np.array(video[frame_id])\n        )\n\n        lf = labels[labels[\"FRAME\"].astype(int) == frame_id.item()]\n\n        for instance in sorted(lf[\"TRACK_ID\"].unique()):\n            gt_track_ids.append(int(instance))\n\n            x = lf[lf[\"TRACK_ID\"] == instance][\"POSITION_X\"].iloc[0]\n            y = lf[lf[\"TRACK_ID\"] == instance][\"POSITION_Y\"].iloc[0]\n            centroids.append([x, y])\n\n        # albumentations wants (spatial, channels), ensure correct dims\n        if self.augmentations is not None:\n            for transform in self.augmentations:\n                # for occlusion simulation, can remove if we don't want\n                if isinstance(transform, A.CoarseDropout):\n                    transform.fill_value = random.randint(0, 255)\n\n            augmented = self.augmentations(\n                image=img,\n                keypoints=np.vstack(centroids),\n            )\n            img, centroids = augmented[\"image\"], augmented[\"keypoints\"]\n\n        img = torch.Tensor(img)\n\n        # torch wants (channels, spatial) - ensure correct dims\n        if len(img.shape) == 2:\n            img = img.unsqueeze(0)\n        elif len(img.shape) == 3:\n            if img.shape[2] == 3:\n                img = img.T  # todo: check for edge cases\n\n        for gt_id in range(len(gt_track_ids)):\n            c = centroids[gt_id]\n            bbox = data_utils.pad_bbox(\n                data_utils.get_bbox([int(c[0]), int(c[1])], self.crop_size),\n                padding=self.padding,\n            )\n            crop = data_utils.crop_bbox(img, bbox)\n\n            instances.append(\n                Instance(\n                    gt_track_id=gt_track_ids[gt_id],\n                    pred_track_id=-1,\n                    bbox=bbox,\n                    crop=crop,\n                )\n            )\n\n        if self.mode == \"train\":\n            np.random.shuffle(instances)\n\n        frames.append(\n            Frame(\n                video_id=label_idx,\n                frame_id=frame_id,\n                img_shape=img.shape,\n                instances=instances,\n            )\n        )\n\n    return frames\n</code></pre>"},{"location":"reference/dreem/datasets/sleap_dataset/","title":"sleap_dataset","text":""},{"location":"reference/dreem/datasets/sleap_dataset/#dreem.datasets.sleap_dataset","title":"<code>dreem.datasets.sleap_dataset</code>","text":"<p>Module containing logic for loading sleap datasets.</p>"},{"location":"reference/dreem/datasets/sleap_dataset/#dreem.datasets.sleap_dataset.SleapDataset","title":"<code>SleapDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset for loading animal behavior data from sleap.</p> Source code in <code>dreem/datasets/sleap_dataset.py</code> <pre><code>class SleapDataset(BaseDataset):\n    \"\"\"Dataset for loading animal behavior data from sleap.\"\"\"\n\n    def __init__(\n        self,\n        slp_files: list[str],\n        video_files: list[str],\n        padding: int = 5,\n        crop_size: int = 128,\n        anchors: int | list[str] | str = \"\",\n        chunk: bool = True,\n        clip_length: int = 500,\n        mode: str = \"train\",\n        handle_missing: str = \"centroid\",\n        augmentations: dict | None = None,\n        n_chunks: int | float = 1.0,\n        seed: int | None = None,\n        verbose: bool = False,\n    ):\n        \"\"\"Initialize SleapDataset.\n\n        Args:\n            slp_files: a list of .slp files storing tracking annotations\n            video_files: a list of paths to video files\n            padding: amount of padding around object crops\n            crop_size: the size of the object crops\n            anchors: One of:\n                        * a string indicating a single node to center crops around\n                        * a list of skeleton node names to be used as the center of crops\n                        * an int indicating the number of anchors to randomly select\n                    If unavailable then crop around the midpoint between all visible anchors.\n            chunk: whether or not to chunk the dataset into batches\n            clip_length: the number of frames in each chunk\n            mode: `train`, `val`, or `test`. Determines whether this dataset is used for\n                training, validation/testing/inference.\n            handle_missing: how to handle missing single nodes. one of `[\"drop\", \"ignore\", \"centroid\"]`.\n                            if \"drop\" then we dont include instances which are missing the `anchor`.\n                            if \"ignore\" then we use a mask instead of a crop and nan centroids/bboxes.\n                            if \"centroid\" then we default to the pose centroid as the node to crop around.\n            augmentations: An optional dict mapping augmentations to parameters. The keys\n                should map directly to augmentation classes in albumentations. Example:\n                    augmentations = {\n                        'Rotate': {'limit': [-90, 90], 'p': 0.5},\n                        'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0, 'p': 0.2},\n                        'RandomContrast': {'limit': 0.2, 'p': 0.6}\n                    }\n            n_chunks: Number of chunks to subsample from.\n                Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n            seed: set a seed for reproducibility\n            verbose: boolean representing whether to print\n        \"\"\"\n        super().__init__(\n            slp_files,\n            video_files,\n            padding,\n            crop_size,\n            chunk,\n            clip_length,\n            mode,\n            augmentations,\n            n_chunks,\n            seed,\n        )\n\n        self.slp_files = slp_files\n        self.video_files = video_files\n        self.padding = padding\n        self.crop_size = crop_size\n        self.chunk = chunk\n        self.clip_length = clip_length\n        self.mode = mode.lower()\n        self.handle_missing = handle_missing.lower()\n        self.n_chunks = n_chunks\n        self.seed = seed\n\n        if isinstance(anchors, int):\n            self.anchors = anchors\n        elif isinstance(anchors, str):\n            self.anchors = [anchors]\n        else:\n            self.anchors = anchors\n\n        if (\n            isinstance(self.anchors, list) and len(self.anchors) == 0\n        ) or self.anchors == 0:\n            raise ValueError(f\"Must provide at least one anchor but got {self.anchors}\")\n\n        self.verbose = verbose\n\n        # if self.seed is not None:\n        #     np.random.seed(self.seed)\n        self.labels = [sio.load_slp(slp_file) for slp_file in self.slp_files]\n        self.videos = [imageio.get_reader(vid_file) for vid_file in self.vid_files]\n        # do we need this? would need to update with sleap-io\n\n        # for label in self.labels:\n        # label.remove_empty_instances(keep_empty_frames=False)\n\n        self.frame_idx = [torch.arange(len(labels)) for labels in self.labels]\n        # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n        # used in call to get_instances()\n        self.create_chunks()\n\n    def get_indices(self, idx: int) -&gt; tuple:\n        \"\"\"Retrieve label and frame indices given batch index.\n\n        Args:\n            idx: the index of the batch.\n        \"\"\"\n        return self.label_idx[idx], self.chunked_frame_idx[idx]\n\n    def get_instances(self, label_idx: list[int], frame_idx: list[int]) -&gt; list[Frame]:\n        \"\"\"Get an element of the dataset.\n\n        Args:\n            label_idx: index of the labels\n            frame_idx: index of the frames\n\n        Returns:\n            A list of `dreem.io.Frame` objects containing metadata and instance data for the batch/clip.\n\n        \"\"\"\n        video = self.labels[label_idx]\n\n        video_name = self.video_files[label_idx]\n\n        vid_reader = self.videos[label_idx]\n\n        img = vid_reader.get_data(0)\n\n        skeleton = video.skeletons[-1]\n\n        frames = []\n        for i, frame_ind in enumerate(frame_idx):\n            (\n                instances,\n                gt_track_ids,\n                poses,\n                shown_poses,\n                point_scores,\n                instance_score,\n            ) = ([], [], [], [], [], [])\n\n            frame_ind = int(frame_ind)\n\n            lf = video[frame_ind]\n\n            try:\n                img = vid_reader.get_data(frame_ind)\n                if len(img.shape) == 2:\n                    img = np.expand_dims(img, 0)\n                h, w, c = img.shape\n            except IndexError as e:\n                logger.warning(\n                    f\"Could not read frame {frame_ind} from {video_name} due to {e}\"\n                )\n                continue\n\n            if len(img.shape) == 2:\n                img = img.expand_dims(-1)\n            h, w, c = img.shape\n\n            if c == 1:\n                img = np.concatenate(\n                    [img, img, img], axis=-1\n                )  # convert to grayscale to rgb\n\n            if np.issubdtype(img.dtype, np.integer):  # convert int to float\n                img = img.astype(np.float32) / 255\n\n            n_instances_dropped = 0\n\n            gt_instances = lf.instances\n            if self.mode == \"train\":\n                np.random.shuffle(gt_instances)\n\n            for instance in gt_instances:\n                if (\n                    np.random.uniform() &lt; self.instance_dropout[\"p\"]\n                    and n_instances_dropped &lt; self.instance_dropout[\"n\"]\n                ):\n                    n_instances_dropped += 1\n                    continue\n\n                if instance.track is not None:\n                    gt_track_id = video.tracks.index(instance.track)\n                else:\n                    gt_track_id = -1\n                gt_track_ids.append(gt_track_id)\n\n                poses.append(\n                    dict(\n                        zip(\n                            [n.name for n in instance.skeleton.nodes],\n                            [[p.x, p.y] for p in instance.points.values()],\n                        )\n                    )\n                )\n\n                shown_poses = [\n                    {\n                        key: val\n                        for key, val in instance.items()\n                        if not np.isnan(val).any()\n                    }\n                    for instance in poses\n                ]\n\n                point_scores.append(\n                    np.array(\n                        [\n                            (\n                                point.score\n                                if isinstance(point, sio.PredictedPoint)\n                                else 1.0\n                            )\n                            for point in instance.points.values()\n                        ]\n                    )\n                )\n                if isinstance(instance, sio.PredictedInstance):\n                    instance_score.append(instance.score)\n                else:\n                    instance_score.append(1.0)\n            # augmentations\n            if self.augmentations is not None:\n                for transform in self.augmentations:\n                    if isinstance(transform, A.CoarseDropout):\n                        transform.fill_value = random.randint(0, 255)\n\n                if shown_poses:\n                    keypoints = np.vstack([list(s.values()) for s in shown_poses])\n\n                else:\n                    keypoints = []\n\n                augmented = self.augmentations(image=img, keypoints=keypoints)\n\n                img, aug_poses = augmented[\"image\"], augmented[\"keypoints\"]\n\n                aug_poses = [\n                    arr\n                    for arr in np.split(\n                        np.array(aug_poses),\n                        np.array([len(s) for s in shown_poses]).cumsum(),\n                    )\n                    if arr.size != 0\n                ]\n\n                aug_poses = [\n                    dict(zip(list(pose_dict.keys()), aug_pose_arr.tolist()))\n                    for aug_pose_arr, pose_dict in zip(aug_poses, shown_poses)\n                ]\n\n                _ = [\n                    pose.update(aug_pose)\n                    for pose, aug_pose in zip(shown_poses, aug_poses)\n                ]\n\n            img = tvf.to_tensor(img)\n\n            for j in range(len(gt_track_ids)):\n                pose = shown_poses[j]\n\n                \"\"\"Check for anchor\"\"\"\n                crops = []\n                boxes = []\n                centroids = {}\n\n                if isinstance(self.anchors, int):\n                    anchors_to_choose = list(pose.keys()) + [\"midpoint\"]\n                    anchors = np.random.choice(anchors_to_choose, self.anchors)\n                else:\n                    anchors = self.anchors\n\n                dropped_anchors = self.node_dropout(anchors)\n\n                for anchor in anchors:\n                    if anchor in dropped_anchors:\n                        centroid = np.array([np.nan, np.nan])\n\n                    elif anchor == \"midpoint\" or anchor == \"centroid\":\n                        centroid = np.nanmean(np.array(list(pose.values())), axis=0)\n\n                    elif anchor in pose:\n                        centroid = np.array(pose[anchor])\n                        if np.isnan(centroid).any():\n                            centroid = np.array([np.nan, np.nan])\n\n                    elif (\n                        anchor not in pose\n                        and len(anchors) == 1\n                        and self.handle_missing == \"centroid\"\n                    ):\n                        anchor = \"midpoint\"\n                        centroid = np.nanmean(np.array(list(pose.values())), axis=0)\n\n                    else:\n                        centroid = np.array([np.nan, np.nan])\n\n                    if np.isnan(centroid).all():\n                        bbox = torch.tensor([np.nan, np.nan, np.nan, np.nan])\n\n                    else:\n                        bbox = data_utils.pad_bbox(\n                            data_utils.get_bbox(centroid, self.crop_size),\n                            padding=self.padding,\n                        )\n\n                    if bbox.isnan().all():\n                        crop = torch.zeros(\n                            c,\n                            self.crop_size + 2 * self.padding,\n                            self.crop_size + 2 * self.padding,\n                            dtype=img.dtype,\n                        )\n                    else:\n                        crop = data_utils.crop_bbox(img, bbox)\n\n                    crops.append(crop)\n                    centroids[anchor] = centroid\n                    boxes.append(bbox)\n\n                if len(crops) &gt; 0:\n                    crops = torch.concat(crops, dim=0)\n\n                if len(boxes) &gt; 0:\n                    boxes = torch.stack(boxes, dim=0)\n\n                if self.handle_missing == \"drop\" and boxes.isnan().any():\n                    continue\n\n                instance = Instance(\n                    gt_track_id=gt_track_ids[j],\n                    pred_track_id=-1,\n                    crop=crops,\n                    centroid=centroids,\n                    bbox=boxes,\n                    skeleton=skeleton,\n                    pose=poses[j],\n                    point_scores=point_scores[j],\n                    instance_score=instance_score[j],\n                )\n\n                instances.append(instance)\n\n            frame = Frame(\n                video_id=label_idx,\n                frame_id=frame_ind,\n                vid_file=video_name,\n                img_shape=img.shape,\n                instances=instances,\n            )\n            frames.append(frame)\n\n        return frames\n\n    def __del__(self):\n        \"\"\"Handle file closing before garbage collection.\"\"\"\n        for reader in self.videos:\n            reader.close()\n</code></pre>"},{"location":"reference/dreem/datasets/sleap_dataset/#dreem.datasets.sleap_dataset.SleapDataset.__del__","title":"<code>__del__()</code>","text":"<p>Handle file closing before garbage collection.</p> Source code in <code>dreem/datasets/sleap_dataset.py</code> <pre><code>def __del__(self):\n    \"\"\"Handle file closing before garbage collection.\"\"\"\n    for reader in self.videos:\n        reader.close()\n</code></pre>"},{"location":"reference/dreem/datasets/sleap_dataset/#dreem.datasets.sleap_dataset.SleapDataset.__init__","title":"<code>__init__(slp_files, video_files, padding=5, crop_size=128, anchors='', chunk=True, clip_length=500, mode='train', handle_missing='centroid', augmentations=None, n_chunks=1.0, seed=None, verbose=False)</code>","text":"<p>Initialize SleapDataset.</p> <p>Parameters:</p> Name Type Description Default <code>slp_files</code> <code>list[str]</code> <p>a list of .slp files storing tracking annotations</p> required <code>video_files</code> <code>list[str]</code> <p>a list of paths to video files</p> required <code>padding</code> <code>int</code> <p>amount of padding around object crops</p> <code>5</code> <code>crop_size</code> <code>int</code> <p>the size of the object crops</p> <code>128</code> <code>anchors</code> <code>int | list[str] | str</code> <p>One of:         * a string indicating a single node to center crops around         * a list of skeleton node names to be used as the center of crops         * an int indicating the number of anchors to randomly select     If unavailable then crop around the midpoint between all visible anchors.</p> <code>''</code> <code>chunk</code> <code>bool</code> <p>whether or not to chunk the dataset into batches</p> <code>True</code> <code>clip_length</code> <code>int</code> <p>the number of frames in each chunk</p> <code>500</code> <code>mode</code> <code>str</code> <p><code>train</code>, <code>val</code>, or <code>test</code>. Determines whether this dataset is used for training, validation/testing/inference.</p> <code>'train'</code> <code>handle_missing</code> <code>str</code> <p>how to handle missing single nodes. one of <code>[\"drop\", \"ignore\", \"centroid\"]</code>.             if \"drop\" then we dont include instances which are missing the <code>anchor</code>.             if \"ignore\" then we use a mask instead of a crop and nan centroids/bboxes.             if \"centroid\" then we default to the pose centroid as the node to crop around.</p> <code>'centroid'</code> <code>augmentations</code> <code>dict | None</code> <p>An optional dict mapping augmentations to parameters. The keys should map directly to augmentation classes in albumentations. Example:     augmentations = {         'Rotate': {'limit': [-90, 90], 'p': 0.5},         'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0, 'p': 0.2},         'RandomContrast': {'limit': 0.2, 'p': 0.6}     }</p> <code>None</code> <code>n_chunks</code> <code>int | float</code> <p>Number of chunks to subsample from. Can either a fraction of the dataset (ie (0,1.0]) or number of chunks</p> <code>1.0</code> <code>seed</code> <code>int | None</code> <p>set a seed for reproducibility</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>boolean representing whether to print</p> <code>False</code> Source code in <code>dreem/datasets/sleap_dataset.py</code> <pre><code>def __init__(\n    self,\n    slp_files: list[str],\n    video_files: list[str],\n    padding: int = 5,\n    crop_size: int = 128,\n    anchors: int | list[str] | str = \"\",\n    chunk: bool = True,\n    clip_length: int = 500,\n    mode: str = \"train\",\n    handle_missing: str = \"centroid\",\n    augmentations: dict | None = None,\n    n_chunks: int | float = 1.0,\n    seed: int | None = None,\n    verbose: bool = False,\n):\n    \"\"\"Initialize SleapDataset.\n\n    Args:\n        slp_files: a list of .slp files storing tracking annotations\n        video_files: a list of paths to video files\n        padding: amount of padding around object crops\n        crop_size: the size of the object crops\n        anchors: One of:\n                    * a string indicating a single node to center crops around\n                    * a list of skeleton node names to be used as the center of crops\n                    * an int indicating the number of anchors to randomly select\n                If unavailable then crop around the midpoint between all visible anchors.\n        chunk: whether or not to chunk the dataset into batches\n        clip_length: the number of frames in each chunk\n        mode: `train`, `val`, or `test`. Determines whether this dataset is used for\n            training, validation/testing/inference.\n        handle_missing: how to handle missing single nodes. one of `[\"drop\", \"ignore\", \"centroid\"]`.\n                        if \"drop\" then we dont include instances which are missing the `anchor`.\n                        if \"ignore\" then we use a mask instead of a crop and nan centroids/bboxes.\n                        if \"centroid\" then we default to the pose centroid as the node to crop around.\n        augmentations: An optional dict mapping augmentations to parameters. The keys\n            should map directly to augmentation classes in albumentations. Example:\n                augmentations = {\n                    'Rotate': {'limit': [-90, 90], 'p': 0.5},\n                    'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0, 'p': 0.2},\n                    'RandomContrast': {'limit': 0.2, 'p': 0.6}\n                }\n        n_chunks: Number of chunks to subsample from.\n            Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n        seed: set a seed for reproducibility\n        verbose: boolean representing whether to print\n    \"\"\"\n    super().__init__(\n        slp_files,\n        video_files,\n        padding,\n        crop_size,\n        chunk,\n        clip_length,\n        mode,\n        augmentations,\n        n_chunks,\n        seed,\n    )\n\n    self.slp_files = slp_files\n    self.video_files = video_files\n    self.padding = padding\n    self.crop_size = crop_size\n    self.chunk = chunk\n    self.clip_length = clip_length\n    self.mode = mode.lower()\n    self.handle_missing = handle_missing.lower()\n    self.n_chunks = n_chunks\n    self.seed = seed\n\n    if isinstance(anchors, int):\n        self.anchors = anchors\n    elif isinstance(anchors, str):\n        self.anchors = [anchors]\n    else:\n        self.anchors = anchors\n\n    if (\n        isinstance(self.anchors, list) and len(self.anchors) == 0\n    ) or self.anchors == 0:\n        raise ValueError(f\"Must provide at least one anchor but got {self.anchors}\")\n\n    self.verbose = verbose\n\n    # if self.seed is not None:\n    #     np.random.seed(self.seed)\n    self.labels = [sio.load_slp(slp_file) for slp_file in self.slp_files]\n    self.videos = [imageio.get_reader(vid_file) for vid_file in self.vid_files]\n    # do we need this? would need to update with sleap-io\n\n    # for label in self.labels:\n    # label.remove_empty_instances(keep_empty_frames=False)\n\n    self.frame_idx = [torch.arange(len(labels)) for labels in self.labels]\n    # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n    # used in call to get_instances()\n    self.create_chunks()\n</code></pre>"},{"location":"reference/dreem/datasets/sleap_dataset/#dreem.datasets.sleap_dataset.SleapDataset.get_indices","title":"<code>get_indices(idx)</code>","text":"<p>Retrieve label and frame indices given batch index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the batch.</p> required Source code in <code>dreem/datasets/sleap_dataset.py</code> <pre><code>def get_indices(self, idx: int) -&gt; tuple:\n    \"\"\"Retrieve label and frame indices given batch index.\n\n    Args:\n        idx: the index of the batch.\n    \"\"\"\n    return self.label_idx[idx], self.chunked_frame_idx[idx]\n</code></pre>"},{"location":"reference/dreem/datasets/sleap_dataset/#dreem.datasets.sleap_dataset.SleapDataset.get_instances","title":"<code>get_instances(label_idx, frame_idx)</code>","text":"<p>Get an element of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>label_idx</code> <code>list[int]</code> <p>index of the labels</p> required <code>frame_idx</code> <code>list[int]</code> <p>index of the frames</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A list of <code>dreem.io.Frame</code> objects containing metadata and instance data for the batch/clip.</p> Source code in <code>dreem/datasets/sleap_dataset.py</code> <pre><code>def get_instances(self, label_idx: list[int], frame_idx: list[int]) -&gt; list[Frame]:\n    \"\"\"Get an element of the dataset.\n\n    Args:\n        label_idx: index of the labels\n        frame_idx: index of the frames\n\n    Returns:\n        A list of `dreem.io.Frame` objects containing metadata and instance data for the batch/clip.\n\n    \"\"\"\n    video = self.labels[label_idx]\n\n    video_name = self.video_files[label_idx]\n\n    vid_reader = self.videos[label_idx]\n\n    img = vid_reader.get_data(0)\n\n    skeleton = video.skeletons[-1]\n\n    frames = []\n    for i, frame_ind in enumerate(frame_idx):\n        (\n            instances,\n            gt_track_ids,\n            poses,\n            shown_poses,\n            point_scores,\n            instance_score,\n        ) = ([], [], [], [], [], [])\n\n        frame_ind = int(frame_ind)\n\n        lf = video[frame_ind]\n\n        try:\n            img = vid_reader.get_data(frame_ind)\n            if len(img.shape) == 2:\n                img = np.expand_dims(img, 0)\n            h, w, c = img.shape\n        except IndexError as e:\n            logger.warning(\n                f\"Could not read frame {frame_ind} from {video_name} due to {e}\"\n            )\n            continue\n\n        if len(img.shape) == 2:\n            img = img.expand_dims(-1)\n        h, w, c = img.shape\n\n        if c == 1:\n            img = np.concatenate(\n                [img, img, img], axis=-1\n            )  # convert to grayscale to rgb\n\n        if np.issubdtype(img.dtype, np.integer):  # convert int to float\n            img = img.astype(np.float32) / 255\n\n        n_instances_dropped = 0\n\n        gt_instances = lf.instances\n        if self.mode == \"train\":\n            np.random.shuffle(gt_instances)\n\n        for instance in gt_instances:\n            if (\n                np.random.uniform() &lt; self.instance_dropout[\"p\"]\n                and n_instances_dropped &lt; self.instance_dropout[\"n\"]\n            ):\n                n_instances_dropped += 1\n                continue\n\n            if instance.track is not None:\n                gt_track_id = video.tracks.index(instance.track)\n            else:\n                gt_track_id = -1\n            gt_track_ids.append(gt_track_id)\n\n            poses.append(\n                dict(\n                    zip(\n                        [n.name for n in instance.skeleton.nodes],\n                        [[p.x, p.y] for p in instance.points.values()],\n                    )\n                )\n            )\n\n            shown_poses = [\n                {\n                    key: val\n                    for key, val in instance.items()\n                    if not np.isnan(val).any()\n                }\n                for instance in poses\n            ]\n\n            point_scores.append(\n                np.array(\n                    [\n                        (\n                            point.score\n                            if isinstance(point, sio.PredictedPoint)\n                            else 1.0\n                        )\n                        for point in instance.points.values()\n                    ]\n                )\n            )\n            if isinstance(instance, sio.PredictedInstance):\n                instance_score.append(instance.score)\n            else:\n                instance_score.append(1.0)\n        # augmentations\n        if self.augmentations is not None:\n            for transform in self.augmentations:\n                if isinstance(transform, A.CoarseDropout):\n                    transform.fill_value = random.randint(0, 255)\n\n            if shown_poses:\n                keypoints = np.vstack([list(s.values()) for s in shown_poses])\n\n            else:\n                keypoints = []\n\n            augmented = self.augmentations(image=img, keypoints=keypoints)\n\n            img, aug_poses = augmented[\"image\"], augmented[\"keypoints\"]\n\n            aug_poses = [\n                arr\n                for arr in np.split(\n                    np.array(aug_poses),\n                    np.array([len(s) for s in shown_poses]).cumsum(),\n                )\n                if arr.size != 0\n            ]\n\n            aug_poses = [\n                dict(zip(list(pose_dict.keys()), aug_pose_arr.tolist()))\n                for aug_pose_arr, pose_dict in zip(aug_poses, shown_poses)\n            ]\n\n            _ = [\n                pose.update(aug_pose)\n                for pose, aug_pose in zip(shown_poses, aug_poses)\n            ]\n\n        img = tvf.to_tensor(img)\n\n        for j in range(len(gt_track_ids)):\n            pose = shown_poses[j]\n\n            \"\"\"Check for anchor\"\"\"\n            crops = []\n            boxes = []\n            centroids = {}\n\n            if isinstance(self.anchors, int):\n                anchors_to_choose = list(pose.keys()) + [\"midpoint\"]\n                anchors = np.random.choice(anchors_to_choose, self.anchors)\n            else:\n                anchors = self.anchors\n\n            dropped_anchors = self.node_dropout(anchors)\n\n            for anchor in anchors:\n                if anchor in dropped_anchors:\n                    centroid = np.array([np.nan, np.nan])\n\n                elif anchor == \"midpoint\" or anchor == \"centroid\":\n                    centroid = np.nanmean(np.array(list(pose.values())), axis=0)\n\n                elif anchor in pose:\n                    centroid = np.array(pose[anchor])\n                    if np.isnan(centroid).any():\n                        centroid = np.array([np.nan, np.nan])\n\n                elif (\n                    anchor not in pose\n                    and len(anchors) == 1\n                    and self.handle_missing == \"centroid\"\n                ):\n                    anchor = \"midpoint\"\n                    centroid = np.nanmean(np.array(list(pose.values())), axis=0)\n\n                else:\n                    centroid = np.array([np.nan, np.nan])\n\n                if np.isnan(centroid).all():\n                    bbox = torch.tensor([np.nan, np.nan, np.nan, np.nan])\n\n                else:\n                    bbox = data_utils.pad_bbox(\n                        data_utils.get_bbox(centroid, self.crop_size),\n                        padding=self.padding,\n                    )\n\n                if bbox.isnan().all():\n                    crop = torch.zeros(\n                        c,\n                        self.crop_size + 2 * self.padding,\n                        self.crop_size + 2 * self.padding,\n                        dtype=img.dtype,\n                    )\n                else:\n                    crop = data_utils.crop_bbox(img, bbox)\n\n                crops.append(crop)\n                centroids[anchor] = centroid\n                boxes.append(bbox)\n\n            if len(crops) &gt; 0:\n                crops = torch.concat(crops, dim=0)\n\n            if len(boxes) &gt; 0:\n                boxes = torch.stack(boxes, dim=0)\n\n            if self.handle_missing == \"drop\" and boxes.isnan().any():\n                continue\n\n            instance = Instance(\n                gt_track_id=gt_track_ids[j],\n                pred_track_id=-1,\n                crop=crops,\n                centroid=centroids,\n                bbox=boxes,\n                skeleton=skeleton,\n                pose=poses[j],\n                point_scores=point_scores[j],\n                instance_score=instance_score[j],\n            )\n\n            instances.append(instance)\n\n        frame = Frame(\n            video_id=label_idx,\n            frame_id=frame_ind,\n            vid_file=video_name,\n            img_shape=img.shape,\n            instances=instances,\n        )\n        frames.append(frame)\n\n    return frames\n</code></pre>"},{"location":"reference/dreem/datasets/tracking_dataset/","title":"tracking_dataset","text":""},{"location":"reference/dreem/datasets/tracking_dataset/#dreem.datasets.tracking_dataset","title":"<code>dreem.datasets.tracking_dataset</code>","text":"<p>Module containing Lightning module wrapper around all other datasets.</p>"},{"location":"reference/dreem/datasets/tracking_dataset/#dreem.datasets.tracking_dataset.TrackingDataset","title":"<code>TrackingDataset</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>Lightning dataset used to load dataloaders for train, test and validation.</p> <p>Nice for wrapping around other data formats.</p> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>class TrackingDataset(LightningDataModule):\n    \"\"\"Lightning dataset used to load dataloaders for train, test and validation.\n\n    Nice for wrapping around other data formats.\n    \"\"\"\n\n    def __init__(\n        self,\n        train_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n        train_dl: DataLoader | None = None,\n        val_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n        val_dl: DataLoader | None = None,\n        test_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n        test_dl: DataLoader | None = None,\n    ):\n        \"\"\"Initialize tracking dataset.\n\n        Args:\n            train_ds: Sleap or Microscopy training Dataset\n            train_dl: Training dataloader. Only used for overriding `train_dataloader`.\n            val_ds: Sleap or Microscopy Validation set\n            val_dl : Validation dataloader. Only used for overriding `val_dataloader`.\n            test_ds: Sleap or Microscopy test set\n            test_dl : Test dataloader. Only used for overriding `test_dataloader`.\n        \"\"\"\n        super().__init__()\n        self.train_ds = train_ds\n        self.train_dl = train_dl\n        self.val_ds = val_ds\n        self.val_dl = val_dl\n        self.test_ds = test_ds\n        self.test_dl = test_dl\n\n    def setup(self, stage=None):\n        \"\"\"Set up lightning dataset.\n\n        UNUSED.\n        \"\"\"\n        pass\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"Get train_dataloader.\n\n        Returns: The Training Dataloader.\n        \"\"\"\n        if self.train_dl is None and self.train_ds is None:\n            return None\n        elif self.train_dl is None:\n            return DataLoader(\n                self.train_ds,\n                batch_size=1,\n                shuffle=True,\n                pin_memory=False,\n                collate_fn=self.train_ds.no_batching_fn,\n                num_workers=0,\n                generator=(\n                    torch.Generator(device=\"cuda\")\n                    if torch.cuda.is_available()\n                    else torch.Generator()\n                ),\n            )\n        else:\n            return self.train_dl\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"Get val dataloader.\n\n        Returns: The validation dataloader.\n        \"\"\"\n        if self.val_dl is None and self.val_ds is None:\n            return None\n        elif self.val_dl is None:\n            return DataLoader(\n                self.val_ds,\n                batch_size=1,\n                shuffle=False,\n                pin_memory=0,\n                collate_fn=self.train_ds.no_batching_fn,\n                num_workers=False,\n                generator=None,\n            )\n        else:\n            return self.val_dl\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"Get.\n\n        Returns: The test dataloader\n        \"\"\"\n        if self.test_dl is None and self.test_ds is None:\n            return None\n        elif self.test_dl is None:\n            return DataLoader(\n                self.test_ds,\n                batch_size=1,\n                shuffle=False,\n                pin_memory=0,\n                collate_fn=self.train_ds.no_batching_fn,\n                num_workers=False,\n                generator=None,\n            )\n        else:\n            return self.test_dl\n</code></pre>"},{"location":"reference/dreem/datasets/tracking_dataset/#dreem.datasets.tracking_dataset.TrackingDataset.__init__","title":"<code>__init__(train_ds=None, train_dl=None, val_ds=None, val_dl=None, test_ds=None, test_dl=None)</code>","text":"<p>Initialize tracking dataset.</p> <p>Parameters:</p> Name Type Description Default <code>train_ds</code> <code>SleapDataset | MicroscopyDataset | CellTrackingDataset | None</code> <p>Sleap or Microscopy training Dataset</p> <code>None</code> <code>train_dl</code> <code>DataLoader | None</code> <p>Training dataloader. Only used for overriding <code>train_dataloader</code>.</p> <code>None</code> <code>val_ds</code> <code>SleapDataset | MicroscopyDataset | CellTrackingDataset | None</code> <p>Sleap or Microscopy Validation set</p> <code>None</code> <code>val_dl</code> <p>Validation dataloader. Only used for overriding <code>val_dataloader</code>.</p> <code>None</code> <code>test_ds</code> <code>SleapDataset | MicroscopyDataset | CellTrackingDataset | None</code> <p>Sleap or Microscopy test set</p> <code>None</code> <code>test_dl</code> <p>Test dataloader. Only used for overriding <code>test_dataloader</code>.</p> <code>None</code> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>def __init__(\n    self,\n    train_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n    train_dl: DataLoader | None = None,\n    val_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n    val_dl: DataLoader | None = None,\n    test_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n    test_dl: DataLoader | None = None,\n):\n    \"\"\"Initialize tracking dataset.\n\n    Args:\n        train_ds: Sleap or Microscopy training Dataset\n        train_dl: Training dataloader. Only used for overriding `train_dataloader`.\n        val_ds: Sleap or Microscopy Validation set\n        val_dl : Validation dataloader. Only used for overriding `val_dataloader`.\n        test_ds: Sleap or Microscopy test set\n        test_dl : Test dataloader. Only used for overriding `test_dataloader`.\n    \"\"\"\n    super().__init__()\n    self.train_ds = train_ds\n    self.train_dl = train_dl\n    self.val_ds = val_ds\n    self.val_dl = val_dl\n    self.test_ds = test_ds\n    self.test_dl = test_dl\n</code></pre>"},{"location":"reference/dreem/datasets/tracking_dataset/#dreem.datasets.tracking_dataset.TrackingDataset.setup","title":"<code>setup(stage=None)</code>","text":"<p>Set up lightning dataset.</p> <p>UNUSED.</p> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>def setup(self, stage=None):\n    \"\"\"Set up lightning dataset.\n\n    UNUSED.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/dreem/datasets/tracking_dataset/#dreem.datasets.tracking_dataset.TrackingDataset.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Get.</p> <p>Returns: The test dataloader</p> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"Get.\n\n    Returns: The test dataloader\n    \"\"\"\n    if self.test_dl is None and self.test_ds is None:\n        return None\n    elif self.test_dl is None:\n        return DataLoader(\n            self.test_ds,\n            batch_size=1,\n            shuffle=False,\n            pin_memory=0,\n            collate_fn=self.train_ds.no_batching_fn,\n            num_workers=False,\n            generator=None,\n        )\n    else:\n        return self.test_dl\n</code></pre>"},{"location":"reference/dreem/datasets/tracking_dataset/#dreem.datasets.tracking_dataset.TrackingDataset.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Get train_dataloader.</p> <p>Returns: The Training Dataloader.</p> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"Get train_dataloader.\n\n    Returns: The Training Dataloader.\n    \"\"\"\n    if self.train_dl is None and self.train_ds is None:\n        return None\n    elif self.train_dl is None:\n        return DataLoader(\n            self.train_ds,\n            batch_size=1,\n            shuffle=True,\n            pin_memory=False,\n            collate_fn=self.train_ds.no_batching_fn,\n            num_workers=0,\n            generator=(\n                torch.Generator(device=\"cuda\")\n                if torch.cuda.is_available()\n                else torch.Generator()\n            ),\n        )\n    else:\n        return self.train_dl\n</code></pre>"},{"location":"reference/dreem/datasets/tracking_dataset/#dreem.datasets.tracking_dataset.TrackingDataset.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Get val dataloader.</p> <p>Returns: The validation dataloader.</p> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"Get val dataloader.\n\n    Returns: The validation dataloader.\n    \"\"\"\n    if self.val_dl is None and self.val_ds is None:\n        return None\n    elif self.val_dl is None:\n        return DataLoader(\n            self.val_ds,\n            batch_size=1,\n            shuffle=False,\n            pin_memory=0,\n            collate_fn=self.train_ds.no_batching_fn,\n            num_workers=False,\n            generator=None,\n        )\n    else:\n        return self.val_dl\n</code></pre>"},{"location":"reference/dreem/inference/","title":"inference","text":""},{"location":"reference/dreem/inference/#dreem.inference","title":"<code>dreem.inference</code>","text":"<p>Tracking Inference using GTR Model.</p>"},{"location":"reference/dreem/inference/boxes/","title":"boxes","text":""},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes","title":"<code>dreem.inference.boxes</code>","text":"<p>Module containing Boxes class.</p>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes","title":"<code>Boxes</code>","text":"<p>Adapted from https://github.com/facebookresearch/detectron2/blob/main/detectron2/structures/boxes.py.</p> <p>This structure stores a list of boxes as a Nx4 torch.Tensor. It supports some common methods about boxes (<code>area</code>, <code>clip</code>, <code>nonempty</code>, etc), and also behaves like a Tensor (support indexing, <code>to(device)</code>, <code>.device</code>, and iteration over all boxes)</p> <p>Attributes:</p> Name Type Description <code>tensor</code> <code>Tensor</code> <p>float matrix of Nx4. Each row is (x1, y1, x2, y2).</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>class Boxes:\n    \"\"\"Adapted from https://github.com/facebookresearch/detectron2/blob/main/detectron2/structures/boxes.py.\n\n    This structure stores a list of boxes as a Nx4 torch.Tensor.\n    It supports some common methods about boxes\n    (`area`, `clip`, `nonempty`, etc),\n    and also behaves like a Tensor\n    (support indexing, `to(device)`, `.device`, and iteration over all boxes)\n\n    Attributes:\n        tensor (torch.Tensor): float matrix of Nx4. Each row is (x1, y1, x2, y2).\n    \"\"\"\n\n    def __init__(self, tensor: torch.Tensor):\n        \"\"\"Initialize Boxes.\n\n        Args:\n            tensor (Tensor[float]): a Nx4 matrix.  Each row is (x1, y1, x2, y2).\n        \"\"\"\n        if not isinstance(tensor, torch.Tensor):\n            tensor = torch.as_tensor(\n                tensor, dtype=torch.float32, device=torch.device(\"cpu\")\n            )\n        else:\n            tensor = tensor.to(torch.float32)\n        if tensor.numel() == 0:\n            # Use reshape, so we don't end up creating a new tensor that does not depend on\n            # the inputs (and consequently confuses jit)\n            tensor = tensor.reshape((-1, 4)).to(dtype=torch.float32)\n        assert tensor.dim() == 3 and tensor.size(-1) == 4, tensor.size()\n\n        self.tensor = tensor\n\n    def clone(self) -&gt; Self:\n        \"\"\"Clone the Boxes.\n\n        Returns:\n            Boxes\n        \"\"\"\n        return Boxes(self.tensor.clone())\n\n    def to(self, device: torch.device) -&gt; Self:\n        \"\"\"Load boxes to gpu/cpu.\n\n        Args:\n            device: The device to load the boxes to\n\n        Returns: Boxes on device.\n        \"\"\"\n        # Boxes are assumed float32 and does not support to(dtype)\n        return Boxes(self.tensor.to(device=device))\n\n    def area(self) -&gt; torch.Tensor:\n        \"\"\"Compute the area of all the boxes.\n\n        Returns:\n            torch.Tensor: a vector with areas of each box.\n        \"\"\"\n        box = self.tensor\n        area = (box[:, :, 2] - box[:, :, 0]) * (box[:, :, 3] - box[:, :, 1])\n        return area\n\n    def clip(self, box_size: list[int, int]) -&gt; None:\n        \"\"\"Clip (in place) the boxes.\n\n        Limits x coordinates to the range [0, width]\n        and y coordinates to the range [0, height].\n\n        Args:\n            box_size (height, width): The clipping box's size.\n        \"\"\"\n        assert torch.isfinite(self.tensor).all(), \"Box tensor contains infinite or NaN!\"\n        h, w = box_size\n        x1 = self.tensor[:, :, 0].clamp(min=0, max=w)\n        y1 = self.tensor[:, :, 1].clamp(min=0, max=h)\n        x2 = self.tensor[:, :, 2].clamp(min=0, max=w)\n        y2 = self.tensor[:, :, 3].clamp(min=0, max=h)\n        self.tensor = torch.stack((x1, y1, x2, y2), dim=-1)\n\n    def nonempty(self, threshold: float = 0.0) -&gt; torch.Tensor:\n        \"\"\"Find boxes that are non-empty.\n\n        A box is considered empty, if either of its side is no larger than threshold.\n\n        Args:\n            threshold: the smallest a box can be.\n\n        Returns:\n            Tensor:\n                a binary vector which represents whether each box is empty\n                (False) or non-empty (True).\n        \"\"\"\n        box = self.tensor\n        widths = box[:, :, 2] - box[:, :, 0]\n        heights = box[:, :, 3] - box[:, :, 1]\n        keep = (widths &gt; threshold) &amp; (heights &gt; threshold)\n        return keep\n\n    def __getitem__(self, item: int | slice | torch.BoolTensor) -&gt; \"Boxes\":\n        \"\"\"Getter for boxes.\n\n        Args:\n            item: int, slice, or a BoolTensor\n\n        Returns:\n            Boxes: Create a new :class:`Boxes` by indexing.\n\n        Usage:\n            The following usage are allowed:\n            1. `new_boxes = boxes[3]`: return a `Boxes` which contains only one box.\n            2. `new_boxes = boxes[2:10]`: return a slice of boxes.\n            3. `new_boxes = boxes[vector]`, where vector is a torch.BoolTensor\n            with `length = len(boxes)`. Nonzero elements in the vector will be selected.\n\n        NOTE: that the returned Boxes might share storage with this Boxes,\n        subject to Pytorch's indexing semantics.\n        \"\"\"\n        if isinstance(item, int):\n            return Boxes(self.tensor[item])\n        b = self.tensor[item]\n        assert (\n            b.dim() == 3\n        ), \"Indexing on Boxes with {} failed to return a matrix!\".format(item)\n        return Boxes(b)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the number of boxes stored in this object.\n\n        Returns:\n            the number of boxes stored in this object\n        \"\"\"\n        return self.tensor.shape[0]\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Override representation for printing.\n\n        Returns:\n            'Boxes(tensor)'\n        \"\"\"\n        return \"Boxes(\" + str(self.tensor) + \")\"\n\n    def inside_box(\n        self, box_size: tuple[int, int], boundary_threshold: int = 0\n    ) -&gt; torch.Tensor:\n        \"\"\"Check if box is inside reference box.\n\n        Args:\n            box_size (height, width): Size of the reference box.\n            boundary_threshold (int): Boxes that extend beyond the reference box\n                boundary by more than boundary_threshold are considered \"outside\".\n\n        Returns:\n            a binary vector, indicating whether each box is inside the reference box.\n        \"\"\"\n        height, width = box_size\n        inds_inside = (\n            (self.tensor[..., 0] &gt;= -boundary_threshold)\n            &amp; (self.tensor[..., 1] &gt;= -boundary_threshold)\n            &amp; (self.tensor[..., 2] &lt; width + boundary_threshold)\n            &amp; (self.tensor[..., 3] &lt; height + boundary_threshold)\n        )\n        return inds_inside\n\n    def get_centers(self) -&gt; torch.Tensor:\n        \"\"\"Get the centroid of the bbox.\n\n        Returns:\n            The box centers in a Nx2 array of (x, y).\n        \"\"\"\n        return (self.tensor[:, :, :2] + self.tensor[:, :, 2:]) / 2\n\n    def scale(self, scale_x: float, scale_y: float) -&gt; None:\n        \"\"\"Scale the box with horizontal and vertical scaling factors.\"\"\"\n        self.tensor[:, :, 0::2] *= scale_x\n        self.tensor[:, :, 1::2] *= scale_y\n\n    @classmethod\n    def cat(cls, boxes_list: list[\"Boxes\"]) -&gt; \"Boxes\":\n        \"\"\"Concatenates a list of Boxes into a single Boxes.\n\n        Arguments:\n            boxes_list: list of `Boxes`\n\n        Returns:\n            Boxes: the concatenated Boxes\n        \"\"\"\n        assert isinstance(boxes_list, (list, tuple))\n        if len(boxes_list) == 0:\n            return cls(torch.empty(0))\n        assert all([isinstance(box, Boxes) for box in boxes_list])\n\n        # use torch.cat (v.s. layers.cat) so the returned boxes never share storage with input\n        cat_boxes = cls(torch.cat([b.tensor for b in boxes_list], dim=0))\n        return cat_boxes\n\n    @property\n    def device(self) -&gt; torch.device:\n        \"\"\"Get the device the box is on.\n\n        Returns: the device the box is on\n        \"\"\"\n        return self.tensor.device\n\n    # type \"Iterator[torch.Tensor]\", yield, and iter() not supported by torchscript\n    # https://github.com/pytorch/pytorch/issues/18627\n    @torch.jit.unused\n    def __iter__(self):\n        \"\"\"Yield a box as a Tensor of shape (4,) at a time.\"\"\"\n        yield from self.tensor\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.device","title":"<code>device: torch.device</code>  <code>property</code>","text":"<p>Get the device the box is on.</p> <p>Returns: the device the box is on</p>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.__getitem__","title":"<code>__getitem__(item)</code>","text":"<p>Getter for boxes.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>int | slice | BoolTensor</code> <p>int, slice, or a BoolTensor</p> required <p>Returns:</p> Name Type Description <code>Boxes</code> <code>Boxes</code> <p>Create a new :class:<code>Boxes</code> by indexing.</p> Usage <p>The following usage are allowed: 1. <code>new_boxes = boxes[3]</code>: return a <code>Boxes</code> which contains only one box. 2. <code>new_boxes = boxes[2:10]</code>: return a slice of boxes. 3. <code>new_boxes = boxes[vector]</code>, where vector is a torch.BoolTensor with <code>length = len(boxes)</code>. Nonzero elements in the vector will be selected.</p> <p>NOTE: that the returned Boxes might share storage with this Boxes, subject to Pytorch's indexing semantics.</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def __getitem__(self, item: int | slice | torch.BoolTensor) -&gt; \"Boxes\":\n    \"\"\"Getter for boxes.\n\n    Args:\n        item: int, slice, or a BoolTensor\n\n    Returns:\n        Boxes: Create a new :class:`Boxes` by indexing.\n\n    Usage:\n        The following usage are allowed:\n        1. `new_boxes = boxes[3]`: return a `Boxes` which contains only one box.\n        2. `new_boxes = boxes[2:10]`: return a slice of boxes.\n        3. `new_boxes = boxes[vector]`, where vector is a torch.BoolTensor\n        with `length = len(boxes)`. Nonzero elements in the vector will be selected.\n\n    NOTE: that the returned Boxes might share storage with this Boxes,\n    subject to Pytorch's indexing semantics.\n    \"\"\"\n    if isinstance(item, int):\n        return Boxes(self.tensor[item])\n    b = self.tensor[item]\n    assert (\n        b.dim() == 3\n    ), \"Indexing on Boxes with {} failed to return a matrix!\".format(item)\n    return Boxes(b)\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.__init__","title":"<code>__init__(tensor)</code>","text":"<p>Initialize Boxes.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor[float]</code> <p>a Nx4 matrix.  Each row is (x1, y1, x2, y2).</p> required Source code in <code>dreem/inference/boxes.py</code> <pre><code>def __init__(self, tensor: torch.Tensor):\n    \"\"\"Initialize Boxes.\n\n    Args:\n        tensor (Tensor[float]): a Nx4 matrix.  Each row is (x1, y1, x2, y2).\n    \"\"\"\n    if not isinstance(tensor, torch.Tensor):\n        tensor = torch.as_tensor(\n            tensor, dtype=torch.float32, device=torch.device(\"cpu\")\n        )\n    else:\n        tensor = tensor.to(torch.float32)\n    if tensor.numel() == 0:\n        # Use reshape, so we don't end up creating a new tensor that does not depend on\n        # the inputs (and consequently confuses jit)\n        tensor = tensor.reshape((-1, 4)).to(dtype=torch.float32)\n    assert tensor.dim() == 3 and tensor.size(-1) == 4, tensor.size()\n\n    self.tensor = tensor\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.__iter__","title":"<code>__iter__()</code>","text":"<p>Yield a box as a Tensor of shape (4,) at a time.</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>@torch.jit.unused\ndef __iter__(self):\n    \"\"\"Yield a box as a Tensor of shape (4,) at a time.\"\"\"\n    yield from self.tensor\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of boxes stored in this object.</p> <p>Returns:</p> Type Description <code>int</code> <p>the number of boxes stored in this object</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the number of boxes stored in this object.\n\n    Returns:\n        the number of boxes stored in this object\n    \"\"\"\n    return self.tensor.shape[0]\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.__repr__","title":"<code>__repr__()</code>","text":"<p>Override representation for printing.</p> <p>Returns:</p> Type Description <code>str</code> <p>'Boxes(tensor)'</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Override representation for printing.\n\n    Returns:\n        'Boxes(tensor)'\n    \"\"\"\n    return \"Boxes(\" + str(self.tensor) + \")\"\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.area","title":"<code>area()</code>","text":"<p>Compute the area of all the boxes.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: a vector with areas of each box.</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def area(self) -&gt; torch.Tensor:\n    \"\"\"Compute the area of all the boxes.\n\n    Returns:\n        torch.Tensor: a vector with areas of each box.\n    \"\"\"\n    box = self.tensor\n    area = (box[:, :, 2] - box[:, :, 0]) * (box[:, :, 3] - box[:, :, 1])\n    return area\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.cat","title":"<code>cat(boxes_list)</code>  <code>classmethod</code>","text":"<p>Concatenates a list of Boxes into a single Boxes.</p> <p>Parameters:</p> Name Type Description Default <code>boxes_list</code> <code>list[Boxes]</code> <p>list of <code>Boxes</code></p> required <p>Returns:</p> Name Type Description <code>Boxes</code> <code>Boxes</code> <p>the concatenated Boxes</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>@classmethod\ndef cat(cls, boxes_list: list[\"Boxes\"]) -&gt; \"Boxes\":\n    \"\"\"Concatenates a list of Boxes into a single Boxes.\n\n    Arguments:\n        boxes_list: list of `Boxes`\n\n    Returns:\n        Boxes: the concatenated Boxes\n    \"\"\"\n    assert isinstance(boxes_list, (list, tuple))\n    if len(boxes_list) == 0:\n        return cls(torch.empty(0))\n    assert all([isinstance(box, Boxes) for box in boxes_list])\n\n    # use torch.cat (v.s. layers.cat) so the returned boxes never share storage with input\n    cat_boxes = cls(torch.cat([b.tensor for b in boxes_list], dim=0))\n    return cat_boxes\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.clip","title":"<code>clip(box_size)</code>","text":"<p>Clip (in place) the boxes.</p> <p>Limits x coordinates to the range [0, width] and y coordinates to the range [0, height].</p> <p>Parameters:</p> Name Type Description Default <code>box_size</code> <code>(height, width)</code> <p>The clipping box's size.</p> required Source code in <code>dreem/inference/boxes.py</code> <pre><code>def clip(self, box_size: list[int, int]) -&gt; None:\n    \"\"\"Clip (in place) the boxes.\n\n    Limits x coordinates to the range [0, width]\n    and y coordinates to the range [0, height].\n\n    Args:\n        box_size (height, width): The clipping box's size.\n    \"\"\"\n    assert torch.isfinite(self.tensor).all(), \"Box tensor contains infinite or NaN!\"\n    h, w = box_size\n    x1 = self.tensor[:, :, 0].clamp(min=0, max=w)\n    y1 = self.tensor[:, :, 1].clamp(min=0, max=h)\n    x2 = self.tensor[:, :, 2].clamp(min=0, max=w)\n    y2 = self.tensor[:, :, 3].clamp(min=0, max=h)\n    self.tensor = torch.stack((x1, y1, x2, y2), dim=-1)\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.clone","title":"<code>clone()</code>","text":"<p>Clone the Boxes.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Boxes</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def clone(self) -&gt; Self:\n    \"\"\"Clone the Boxes.\n\n    Returns:\n        Boxes\n    \"\"\"\n    return Boxes(self.tensor.clone())\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.get_centers","title":"<code>get_centers()</code>","text":"<p>Get the centroid of the bbox.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The box centers in a Nx2 array of (x, y).</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def get_centers(self) -&gt; torch.Tensor:\n    \"\"\"Get the centroid of the bbox.\n\n    Returns:\n        The box centers in a Nx2 array of (x, y).\n    \"\"\"\n    return (self.tensor[:, :, :2] + self.tensor[:, :, 2:]) / 2\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.inside_box","title":"<code>inside_box(box_size, boundary_threshold=0)</code>","text":"<p>Check if box is inside reference box.</p> <p>Parameters:</p> Name Type Description Default <code>box_size</code> <code>(height, width)</code> <p>Size of the reference box.</p> required <code>boundary_threshold</code> <code>int</code> <p>Boxes that extend beyond the reference box boundary by more than boundary_threshold are considered \"outside\".</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>a binary vector, indicating whether each box is inside the reference box.</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def inside_box(\n    self, box_size: tuple[int, int], boundary_threshold: int = 0\n) -&gt; torch.Tensor:\n    \"\"\"Check if box is inside reference box.\n\n    Args:\n        box_size (height, width): Size of the reference box.\n        boundary_threshold (int): Boxes that extend beyond the reference box\n            boundary by more than boundary_threshold are considered \"outside\".\n\n    Returns:\n        a binary vector, indicating whether each box is inside the reference box.\n    \"\"\"\n    height, width = box_size\n    inds_inside = (\n        (self.tensor[..., 0] &gt;= -boundary_threshold)\n        &amp; (self.tensor[..., 1] &gt;= -boundary_threshold)\n        &amp; (self.tensor[..., 2] &lt; width + boundary_threshold)\n        &amp; (self.tensor[..., 3] &lt; height + boundary_threshold)\n    )\n    return inds_inside\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.nonempty","title":"<code>nonempty(threshold=0.0)</code>","text":"<p>Find boxes that are non-empty.</p> <p>A box is considered empty, if either of its side is no larger than threshold.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>the smallest a box can be.</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <pre><code>a binary vector which represents whether each box is empty\n(False) or non-empty (True).\n</code></pre> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def nonempty(self, threshold: float = 0.0) -&gt; torch.Tensor:\n    \"\"\"Find boxes that are non-empty.\n\n    A box is considered empty, if either of its side is no larger than threshold.\n\n    Args:\n        threshold: the smallest a box can be.\n\n    Returns:\n        Tensor:\n            a binary vector which represents whether each box is empty\n            (False) or non-empty (True).\n    \"\"\"\n    box = self.tensor\n    widths = box[:, :, 2] - box[:, :, 0]\n    heights = box[:, :, 3] - box[:, :, 1]\n    keep = (widths &gt; threshold) &amp; (heights &gt; threshold)\n    return keep\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.scale","title":"<code>scale(scale_x, scale_y)</code>","text":"<p>Scale the box with horizontal and vertical scaling factors.</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def scale(self, scale_x: float, scale_y: float) -&gt; None:\n    \"\"\"Scale the box with horizontal and vertical scaling factors.\"\"\"\n    self.tensor[:, :, 0::2] *= scale_x\n    self.tensor[:, :, 1::2] *= scale_y\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.to","title":"<code>to(device)</code>","text":"<p>Load boxes to gpu/cpu.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>The device to load the boxes to</p> required <p>Returns: Boxes on device.</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def to(self, device: torch.device) -&gt; Self:\n    \"\"\"Load boxes to gpu/cpu.\n\n    Args:\n        device: The device to load the boxes to\n\n    Returns: Boxes on device.\n    \"\"\"\n    # Boxes are assumed float32 and does not support to(dtype)\n    return Boxes(self.tensor.to(device=device))\n</code></pre>"},{"location":"reference/dreem/inference/eval/","title":"eval","text":""},{"location":"reference/dreem/inference/eval/#dreem.inference.eval","title":"<code>dreem.inference.eval</code>","text":"<p>Script to evaluate model.</p>"},{"location":"reference/dreem/inference/eval/#dreem.inference.eval.run","title":"<code>run(cfg)</code>","text":"<p>Run inference based on config file.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>A dictconfig loaded from hydra containing checkpoint path and data</p> required Source code in <code>dreem/inference/eval.py</code> <pre><code>@hydra.main(config_path=None, config_name=None, version_base=None)\ndef run(cfg: DictConfig) -&gt; dict[int, sio.Labels]:\n    \"\"\"Run inference based on config file.\n\n    Args:\n        cfg: A dictconfig loaded from hydra containing checkpoint path and data\n    \"\"\"\n    eval_cfg = Config(cfg)\n\n    if \"checkpoints\" in cfg.keys():\n        try:\n            index = int(os.environ[\"POD_INDEX\"])\n        # For testing without deploying a job on runai\n        except KeyError:\n            index = input(\"Pod Index Not found! Please choose a pod index: \")\n\n        logger.info(f\"Pod Index: {index}\")\n\n        checkpoints = pd.read_csv(cfg.checkpoints)\n        checkpoint = checkpoints.iloc[index]\n    else:\n        checkpoint = eval_cfg.cfg.ckpt_path\n\n    model = GTRRunner.load_from_checkpoint(checkpoint)\n    model.tracker_cfg = eval_cfg.cfg.tracker\n    model.tracker = Tracker(**model.tracker_cfg)\n    logger.info(f\"Using the following tracker:\")\n    print(model.tracker)\n    model.metrics[\"test\"] = eval_cfg.cfg.runner.metrics.test\n    logger.info(f\"Computing the following metrics:\")\n    logger.info(model.metrics.test)\n    model.test_results[\"save_path\"] = eval_cfg.cfg.runner.save_path\n    logger.info(f\"Saving results to {model.test_results['save_path']}\")\n\n    labels_files, vid_files = eval_cfg.get_data_paths(eval_cfg.cfg.dataset.test_dataset)\n    trainer = eval_cfg.get_trainer()\n    for label_file, vid_file in zip(labels_files, vid_files):\n        dataset = eval_cfg.get_dataset(\n            label_files=[label_file], vid_files=[vid_file], mode=\"test\"\n        )\n        dataloader = eval_cfg.get_dataloader(dataset, mode=\"test\")\n        metrics = trainer.test(model, dataloader)\n</code></pre>"},{"location":"reference/dreem/inference/metrics/","title":"metrics","text":""},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics","title":"<code>dreem.inference.metrics</code>","text":"<p>Helper functions for calculating mot metrics.</p>"},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics.get_matches","title":"<code>get_matches(frames)</code>","text":"<p>Get comparison between predicted and gt trajectory labels.</p> <p>Parameters:</p> Name Type Description Default <code>frames</code> <code>list[Frame]</code> <p>a list of Frames containing the video_id, frame_id, gt labels and predicted labels</p> required <p>Returns:</p> Name Type Description <code>matches</code> <code>tuple[dict, list, int]</code> <p>a dict containing predicted and gt trajectory labels indices: the frame indices being compared video_id: the video being</p> Source code in <code>dreem/inference/metrics.py</code> <pre><code>def get_matches(frames: list[\"dreem.io.Frame\"]) -&gt; tuple[dict, list, int]:\n    \"\"\"Get comparison between predicted and gt trajectory labels.\n\n    Args:\n        frames: a list of Frames containing the video_id, frame_id,\n            gt labels and predicted labels\n\n    Returns:\n        matches: a dict containing predicted and gt trajectory labels\n        indices: the frame indices being compared\n        video_id: the video being\n    \"\"\"\n    matches = {}\n    indices = []\n\n    video_id = frames[0].video_id.item()\n\n    if any([frame.has_instances() for frame in frames]):\n        for idx, frame in enumerate(frames):\n            indices.append(frame.frame_id.item())\n            for gt_track_id, pred_track_id in zip(\n                frame.get_gt_track_ids(), frame.get_pred_track_ids()\n            ):\n                match = f\"{gt_track_id} -&gt; {pred_track_id}\"\n\n                if match not in matches:\n                    matches[match] = np.full(len(frames), 0)\n\n                matches[match][idx] = 1\n    else:\n        logger.debug(\"No instances detected!\")\n    return matches, indices, video_id\n</code></pre>"},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics.get_pymotmetrics","title":"<code>get_pymotmetrics(data, metrics='all', key='tracker_ids', save=None)</code>","text":"<p>Given data and a key, evaluate the predictions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>A dictionary. Example provided below.</p> required <code>key</code> <code>str</code> <p>The key within instances to look for track_ids (can be \"gt_ids\" or \"tracker_ids\").</p> <code>'tracker_ids'</code> <p>Returns:</p> Name Type Description <code>summary</code> <code>DataFrame</code> <p>A pandas DataFrame of all the pymot-metrics.</p>"},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics.get_pymotmetrics---an-example-of-data-","title":"--------------------------- An example of data ---------------------------","text":"<p>*: number of ids for gt at every frame of the video ^: number of ids for tracker at every frame of the video L: length of video</p> <p>data = {     \"num_gt_ids\": total number of unique gt ids,     \"num_tracker_dets\": total number of detections by your detection algorithm,     \"num_gt_dets\": total number of gt detections,     \"gt_ids\": (L, *),  # Ragged np.array     \"tracker_ids\": (L, ^),  # Ragged np.array     \"similarity_scores\": (L, *, ^),  # Ragged np.array     \"num_timsteps\": L, }</p> Source code in <code>dreem/inference/metrics.py</code> <pre><code>def get_pymotmetrics(\n    data: dict,\n    metrics: str | tuple = \"all\",\n    key: str = \"tracker_ids\",\n    save: str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Given data and a key, evaluate the predictions.\n\n    Args:\n        data: A dictionary. Example provided below.\n        key: The key within instances to look for track_ids (can be \"gt_ids\" or \"tracker_ids\").\n\n    Returns:\n        summary: A pandas DataFrame of all the pymot-metrics.\n\n    # --------------------------- An example of data --------------------------- #\n\n    *: number of ids for gt at every frame of the video\n    ^: number of ids for tracker at every frame of the video\n    L: length of video\n\n    data = {\n        \"num_gt_ids\": total number of unique gt ids,\n        \"num_tracker_dets\": total number of detections by your detection algorithm,\n        \"num_gt_dets\": total number of gt detections,\n        \"gt_ids\": (L, *),  # Ragged np.array\n        \"tracker_ids\": (L, ^),  # Ragged np.array\n        \"similarity_scores\": (L, *, ^),  # Ragged np.array\n        \"num_timsteps\": L,\n    }\n    \"\"\"\n    if not isinstance(metrics, str):\n        metrics = [\n            \"num_switches\" if metric.lower() == \"sw_cnt\" else metric\n            for metric in metrics\n        ]  # backward compatibility\n    acc = mm.MOTAccumulator(auto_id=True)\n\n    for i in range(len(data[\"gt_ids\"])):\n        acc.update(\n            oids=data[\"gt_ids\"][i],\n            hids=data[key][i],\n            dists=data[\"similarity_scores\"][i],\n        )\n\n    mh = mm.metrics.create()\n\n    all_metrics = [\n        metric.split(\"|\")[0] for metric in mh.list_metrics_markdown().split(\"\\n\")[2:-1]\n    ]\n\n    if isinstance(metrics, str):\n        metrics_list = all_metrics\n\n    elif isinstance(metrics, Iterable):\n        metrics = [metric.lower() for metric in metrics]\n        metrics_list = [metric for metric in all_metrics if metric.lower() in metrics]\n\n    else:\n        raise TypeError(\n            f\"Metrics must either be an iterable of strings or `all` not: {type(metrics)}\"\n        )\n\n    summary = mh.compute(acc, metrics=metrics_list, name=\"acc\")\n    summary = summary.transpose()\n\n    if save is not None and save != \"\":\n        summary.to_csv(save)\n\n    return summary[\"acc\"]\n</code></pre>"},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics.get_switch_count","title":"<code>get_switch_count(switches)</code>","text":"<p>Get the number of mislabeled predicted trajectories.</p> <p>Parameters:</p> Name Type Description Default <code>switches</code> <code>dict</code> <p>a dict of dicts containing the mislabeled trajectories and the frames at which they occur</p> required <p>Returns:</p> Type Description <code>int</code> <p>the number of switched labels in the video chunk</p> Source code in <code>dreem/inference/metrics.py</code> <pre><code>def get_switch_count(switches: dict) -&gt; int:\n    \"\"\"Get the number of mislabeled predicted trajectories.\n\n    Args:\n        switches: a dict of dicts containing the mislabeled trajectories\n            and the frames at which they occur\n\n    Returns:\n        the number of switched labels in the video chunk\n    \"\"\"\n    only_switches = {k: v for k, v in switches.items() if v != {}}\n    sw_cnt = sum([len(i) for i in list(only_switches.values())])\n    return sw_cnt\n</code></pre>"},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics.get_switches","title":"<code>get_switches(matches, indices)</code>","text":"<p>Get misassigned predicted trajectory labels.</p> <p>Parameters:</p> Name Type Description Default <code>matches</code> <code>dict</code> <p>a dict containing the gt and predicted labels</p> required <code>indices</code> <code>list</code> <p>a list of frame indices being used</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dict of dicts containing the frame at which the switch occured and the change in labels</p> Source code in <code>dreem/inference/metrics.py</code> <pre><code>def get_switches(matches: dict, indices: list) -&gt; dict:\n    \"\"\"Get misassigned predicted trajectory labels.\n\n    Args:\n        matches: a dict containing the gt and predicted labels\n        indices: a list of frame indices being used\n\n    Returns:\n        A dict of dicts containing the frame at which the switch occured\n        and the change in labels\n    \"\"\"\n    track, switches = {}, {}\n    if len(matches) &gt; 0 and len(indices) &gt; 0:\n        matches_key = np.array(list(matches.keys()))\n        matches = np.array(list(matches.values()))\n        num_frames = matches.shape[1]\n\n        assert num_frames == len(indices)\n\n        for i, idx in zip(range(num_frames), indices):\n            switches[idx] = {}\n\n            col = matches[:, i]\n            match_indices = np.where(col == 1)[0]\n            match_i = [\n                (m.split(\" \")[0], m.split(\" \")[-1]) for m in matches_key[match_indices]\n            ]\n\n            for m in match_i:\n                gt, pred = m\n\n                if gt in track and track[gt] != pred:\n                    switches[idx][gt] = {\n                        \"frames\": (idx - 1, idx),\n                        \"pred tracks (from, to)\": (track[gt], pred),\n                    }\n\n                track[gt] = pred\n\n    return switches\n</code></pre>"},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics.get_track_evals","title":"<code>get_track_evals(data, metrics)</code>","text":"<p>Run track_eval and get mot metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>A dictionary. Example provided below.</p> required <code>metrics</code> <code>dict</code> <p>mot metrics to be computed</p> required <p>Returns:     A dictionary with key being the metric, and value being the metric value computed.</p>"},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics.get_track_evals---an-example-of-data-","title":"--------------------------- An example of data ---------------------------","text":"<p>*: number of ids for gt at every frame of the video ^: number of ids for tracker at every frame of the video L: length of video</p> <p>data = {     \"num_gt_ids\": total number of unique gt ids,     \"num_tracker_dets\": total number of detections by your detection algorithm,     \"num_gt_dets\": total number of gt detections,     \"gt_ids\": (L, *),  # Ragged np.array     \"tracker_ids\": (L, ^),  # Ragged np.array     \"similarity_scores\": (L, *, ^),  # Ragged np.array     \"num_timsteps\": L, }</p> Source code in <code>dreem/inference/metrics.py</code> <pre><code>def get_track_evals(data: dict, metrics: dict) -&gt; dict:\n    \"\"\"Run track_eval and get mot metrics.\n\n    Args:\n        data: A dictionary. Example provided below.\n        metrics: mot metrics to be computed\n    Returns:\n        A dictionary with key being the metric, and value being the metric value computed.\n    # --------------------------- An example of data --------------------------- #\n\n    *: number of ids for gt at every frame of the video\n    ^: number of ids for tracker at every frame of the video\n    L: length of video\n\n    data = {\n        \"num_gt_ids\": total number of unique gt ids,\n        \"num_tracker_dets\": total number of detections by your detection algorithm,\n        \"num_gt_dets\": total number of gt detections,\n        \"gt_ids\": (L, *),  # Ragged np.array\n        \"tracker_ids\": (L, ^),  # Ragged np.array\n        \"similarity_scores\": (L, *, ^),  # Ragged np.array\n        \"num_timsteps\": L,\n    }\n    \"\"\"\n    results = {}\n    for metric_name, metric in metrics.items():\n        result = metric.eval_sequence(data)\n        results.update(result)\n    return results\n</code></pre>"},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics.to_track_eval","title":"<code>to_track_eval(frames)</code>","text":"<p>Reformats frames the output from <code>sliding_inference</code> to be used by <code>TrackEval</code>.</p> <p>Parameters:</p> Name Type Description Default <code>frames</code> <code>list[Frame]</code> <p>A list of Frames. <code>See dreem.io.data_structures for more info</code>.</p> required <p>Returns:</p> Name Type Description <code>data</code> <code>dict</code> <p>A dictionary. Example provided below.</p>"},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics.to_track_eval---an-example-of-data-","title":"--------------------------- An example of data ---------------------------","text":"<p>*: number of ids for gt at every frame of the video ^: number of ids for tracker at every frame of the video L: length of video</p> <p>data = {     \"num_gt_ids\": total number of unique gt ids,     \"num_tracker_dets\": total number of detections by your detection algorithm,     \"num_gt_dets\": total number of gt detections,     \"gt_ids\": (L, *),  # Ragged np.array     \"tracker_ids\": (L, ^),  # Ragged np.array     \"similarity_scores\": (L, *, ^),  # Ragged np.array     \"num_timesteps\": L, }</p> Source code in <code>dreem/inference/metrics.py</code> <pre><code>def to_track_eval(frames: list[\"dreem.io.Frame\"]) -&gt; dict:\n    \"\"\"Reformats frames the output from `sliding_inference` to be used by `TrackEval`.\n\n    Args:\n        frames: A list of Frames. `See dreem.io.data_structures for more info`.\n\n    Returns:\n        data: A dictionary. Example provided below.\n\n    # --------------------------- An example of data --------------------------- #\n\n    *: number of ids for gt at every frame of the video\n    ^: number of ids for tracker at every frame of the video\n    L: length of video\n\n    data = {\n        \"num_gt_ids\": total number of unique gt ids,\n        \"num_tracker_dets\": total number of detections by your detection algorithm,\n        \"num_gt_dets\": total number of gt detections,\n        \"gt_ids\": (L, *),  # Ragged np.array\n        \"tracker_ids\": (L, ^),  # Ragged np.array\n        \"similarity_scores\": (L, *, ^),  # Ragged np.array\n        \"num_timesteps\": L,\n    }\n    \"\"\"\n    unique_gt_ids = []\n    num_tracker_dets = 0\n    num_gt_dets = 0\n    gt_ids = []\n    track_ids = []\n    similarity_scores = []\n\n    data = {}\n    cos_sim = torch.nn.CosineSimilarity()\n\n    for fidx, frame in enumerate(frames):\n        gt_track_ids = frame.get_gt_track_ids().cpu().numpy().tolist()\n        pred_track_ids = frame.get_pred_track_ids().cpu().numpy().tolist()\n        # boxes = Boxes(frame.get_bboxes().cpu())\n\n        gt_ids.append(np.array(gt_track_ids))\n        track_ids.append(np.array(pred_track_ids))\n\n        num_tracker_dets += len(pred_track_ids)\n        num_gt_dets += len(gt_track_ids)\n\n        if not set(gt_track_ids).issubset(set(unique_gt_ids)):\n            unique_gt_ids.extend(list(set(gt_track_ids).difference(set(unique_gt_ids))))\n\n        # eval_matrix = _pairwise_iou(boxes, boxes)\n        eval_matrix = np.full((len(gt_track_ids), len(pred_track_ids)), np.nan)\n\n        for i, feature_i in enumerate(frame.get_features()):\n            for j, feature_j in enumerate(frame.get_features()):\n                eval_matrix[i][j] = cos_sim(\n                    feature_i.unsqueeze(0), feature_j.unsqueeze(0)\n                )\n\n        # eval_matrix\n        #                      pred_track_ids\n        #                            0        1\n        #  gt_track_ids    1        ...      ...\n        #                  0        ...      ...\n        #\n        # Since the order of both gt_track_ids and pred_track_ids matter (maps from pred to gt),\n        # we know the diagonal is the important part. E.g. gt_track_ids=1 maps to pred_track_ids=0\n        # and gt_track_ids=0 maps to pred_track_ids=1 because they are ordered in that way.\n\n        # Based on assumption that eval_matrix is always a square matrix.\n        # This is true because we are using ground-truth detections.\n        #\n        # - The number of predicted tracks for a frame will always be the same number\n        # of ground truth tracks for a frame.\n        # - The number of predicted and ground truth detections will always be the same\n        # for any frame.\n        # - Because we map detections to features one-to-one, there will always be the same\n        # number of features for both predicted and ground truth for any frame.\n\n        # Mask upper and lower triangles of the square matrix (set to 0).\n        eval_matrix = np.triu(np.tril(eval_matrix))\n\n        # Replace the 0s with np.nans.\n        i, j = np.where(eval_matrix == 0)\n        eval_matrix[i, j] = np.nan\n\n        similarity_scores.append(eval_matrix)\n\n    data[\"num_gt_ids\"] = len(unique_gt_ids)\n    data[\"num_tracker_dets\"] = num_tracker_dets\n    data[\"num_gt_dets\"] = num_gt_dets\n    data[\"gt_ids\"] = gt_ids\n    data[\"tracker_ids\"] = track_ids\n    data[\"similarity_scores\"] = similarity_scores\n    data[\"num_timesteps\"] = len(frames)\n\n    return data\n</code></pre>"},{"location":"reference/dreem/inference/post_processing/","title":"post_processing","text":""},{"location":"reference/dreem/inference/post_processing/#dreem.inference.post_processing","title":"<code>dreem.inference.post_processing</code>","text":"<p>Helper functions for post-processing association matrix pre-tracking.</p>"},{"location":"reference/dreem/inference/post_processing/#dreem.inference.post_processing.filter_max_center_dist","title":"<code>filter_max_center_dist(asso_output, max_center_dist=0, k_boxes=None, nonk_boxes=None, id_inds=None)</code>","text":"<p>Filter trajectory score by distances between objects across frames.</p> <p>Parameters:</p> Name Type Description Default <code>asso_output</code> <code>Tensor</code> <p>An N_t x N association matrix</p> required <code>max_center_dist</code> <code>float</code> <p>The euclidean distance threshold between bboxes</p> <code>0</code> <code>k_boxes</code> <code>Tensor | None</code> <p>The bounding boxes in the current frame</p> <code>None</code> <code>nonk_boxes</code> <code>Tensor | None</code> <p>the boxes not in the current frame</p> <code>None</code> <code>id_inds</code> <code>Tensor | None</code> <p>track ids</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>An N_t x N association matrix</p> Source code in <code>dreem/inference/post_processing.py</code> <pre><code>def filter_max_center_dist(\n    asso_output: torch.Tensor,\n    max_center_dist: float = 0,\n    k_boxes: torch.Tensor | None = None,\n    nonk_boxes: torch.Tensor | None = None,\n    id_inds: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Filter trajectory score by distances between objects across frames.\n\n    Args:\n        asso_output: An N_t x N association matrix\n        max_center_dist: The euclidean distance threshold between bboxes\n        k_boxes: The bounding boxes in the current frame\n        nonk_boxes: the boxes not in the current frame\n        id_inds: track ids\n\n    Returns:\n        An N_t x N association matrix\n    \"\"\"\n    if max_center_dist is not None and max_center_dist &gt; 0:\n        assert (\n            k_boxes is not None and nonk_boxes is not None and id_inds is not None\n        ), \"Need `k_boxes`, `nonk_boxes`, and `id_ind` to filter by `max_center_dist`\"\n        k_ct = (k_boxes[:, :, :2] + k_boxes[:, :, 2:]) / 2\n        k_s = ((k_boxes[:, :, 2:] - k_boxes[:, :, :2]) ** 2).sum(dim=2)  # n_k\n\n        nonk_ct = (nonk_boxes[:, :, :2] + nonk_boxes[:, :, 2:]) / 2\n\n        dist = ((k_ct[:, None, :, :] - nonk_ct[None, :, :, :]) ** 2).sum(\n            dim=-1\n        )  # n_k x Np\n\n        norm_dist = dist / (k_s[:, None, :] + 1e-8)\n        norm_dist = dist.mean(axis=-1)  # n_k x Np\n\n        valid = norm_dist &lt; max_center_dist  # n_k x Np\n        valid_assn = (\n            torch.mm(valid.float(), id_inds.to(valid.device))\n            .clamp_(max=1.0)\n            .long()\n            .bool()\n        )  # n_k x M\n        asso_output_filtered = asso_output.clone()\n        asso_output_filtered[~valid_assn] = 0  # n_k x M\n        return asso_output_filtered\n    else:\n        return asso_output\n</code></pre>"},{"location":"reference/dreem/inference/post_processing/#dreem.inference.post_processing.weight_decay_time","title":"<code>weight_decay_time(asso_output, decay_time=0, reid_features=None, T=None, k=None)</code>","text":"<p>Weight association matrix by time.</p> <p>Weighs matrix by number of frames the ith object is from the jth object in the association matrix.</p> <p>Parameters:</p> Name Type Description Default <code>asso_output</code> <code>Tensor</code> <p>the association matrix to be reweighted</p> required <code>decay_time</code> <code>float</code> <p>the scale to weight the asso_output by</p> <code>0</code> <code>reid_features</code> <code>Tensor | None</code> <p>The n x d matrix of feature vectors for each object</p> <code>None</code> <code>T</code> <code>int | None</code> <p>The length of the window</p> <code>None</code> <code>k</code> <code>int | None</code> <p>an integer for the query frame within the window of instances</p> <code>None</code> <p>Returns: The N_t x N association matrix weighted by decay time</p> Source code in <code>dreem/inference/post_processing.py</code> <pre><code>def weight_decay_time(\n    asso_output: torch.Tensor,\n    decay_time: float = 0,\n    reid_features: torch.Tensor | None = None,\n    T: int | None = None,\n    k: int | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Weight association matrix by time.\n\n    Weighs matrix by number of frames the ith object is from the jth object\n    in the association matrix.\n\n    Args:\n        asso_output: the association matrix to be reweighted\n        decay_time: the scale to weight the asso_output by\n        reid_features: The n x d matrix of feature vectors for each object\n        T: The length of the window\n        k: an integer for the query frame within the window of instances\n    Returns: The N_t x N association matrix weighted by decay time\n    \"\"\"\n    if decay_time is not None and decay_time &gt; 0:\n        assert (\n            reid_features is not None and T is not None and k is not None\n        ), \"Need reid_features to weight traj_score by `decay_time`!\"\n        N_t = asso_output.shape[0]\n        dts = torch.cat(\n            [\n                x.new_full((N_t,), T - t - 2)\n                for t, x in enumerate(reid_features)\n                if t != k\n            ],\n            dim=0,\n        ).cpu()  # Np\n        # asso_output = asso_output.to(self.device) * (self.decay_time ** dts[None, :])\n        asso_output = asso_output * (decay_time ** dts[:, None])\n    return asso_output\n</code></pre>"},{"location":"reference/dreem/inference/post_processing/#dreem.inference.post_processing.weight_iou","title":"<code>weight_iou(asso_output, method=None, last_ious=None)</code>","text":"<p>Weight the association matrix by the IOU between object bboxes across frames.</p> <p>Parameters:</p> Name Type Description Default <code>asso_output</code> <code>Tensor</code> <p>An N_t x N association matrix</p> required <code>method</code> <code>str | None</code> <p>string indicating whether to use a max weighting or multiplicative weighting     Max weighting: take <code>max(traj_score, iou)</code>     multiplicative weighting: <code>iou*weight + traj_score</code></p> <code>None</code> <code>last_ious</code> <code>Tensor</code> <p>torch Tensor containing the ious between current and previous frames</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>An N_t x N association matrix weighted by the IOU</p> Source code in <code>dreem/inference/post_processing.py</code> <pre><code>def weight_iou(\n    asso_output: torch.Tensor, method: str | None = None, last_ious: torch.Tensor = None\n) -&gt; torch.Tensor:\n    \"\"\"Weight the association matrix by the IOU between object bboxes across frames.\n\n    Args:\n        asso_output: An N_t x N association matrix\n        method: string indicating whether to use a max weighting or multiplicative weighting\n                Max weighting: take `max(traj_score, iou)`\n                multiplicative weighting: `iou*weight + traj_score`\n        last_ious: torch Tensor containing the ious between current and previous frames\n\n    Returns:\n        An N_t x N association matrix weighted by the IOU\n    \"\"\"\n    if method is not None and method != \"\":\n        assert last_ious is not None, \"Need `last_ious` to weight traj_score by `IOU`\"\n        if method.lower() == \"mult\":\n            weights = torch.abs(last_ious - asso_output)\n            weighted_iou = weights * last_ious\n            weighted_iou = torch.nan_to_num(weighted_iou, 0)\n            asso_output = asso_output + weighted_iou\n        elif method.lower() == \"max\":\n            asso_output = torch.max(asso_output, last_ious)\n        else:\n            raise ValueError(\n                f\"`method` must be one of ['mult' or 'max'] got '{method.lower()}'\"\n            )\n    return asso_output\n</code></pre>"},{"location":"reference/dreem/inference/track/","title":"track","text":""},{"location":"reference/dreem/inference/track/#dreem.inference.track","title":"<code>dreem.inference.track</code>","text":"<p>Script to run inference and get out tracks.</p>"},{"location":"reference/dreem/inference/track/#dreem.inference.track.export_trajectories","title":"<code>export_trajectories(frames_pred, save_path=None)</code>","text":"<p>Convert trajectories to data frame and save as .csv.</p> <p>Parameters:</p> Name Type Description Default <code>frames_pred</code> <code>list[Frame]</code> <p>A list of Frames with predicted track ids.</p> required <code>save_path</code> <code>str | None</code> <p>The path to save the predicted trajectories to.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dictionary containing the predicted track id and centroid coordinates for each instance in the video.</p> Source code in <code>dreem/inference/track.py</code> <pre><code>def export_trajectories(\n    frames_pred: list[\"dreem.io.Frame\"], save_path: str | None = None\n) -&gt; pd.DataFrame:\n    \"\"\"Convert trajectories to data frame and save as .csv.\n\n    Args:\n        frames_pred: A list of Frames with predicted track ids.\n        save_path: The path to save the predicted trajectories to.\n\n    Returns:\n        A dictionary containing the predicted track id and centroid coordinates for each instance in the video.\n    \"\"\"\n    save_dict = {}\n    frame_ids = []\n    X, Y = [], []\n    pred_track_ids = []\n    track_scores = []\n    for frame in frames_pred:\n        for i, instance in enumerate(frame.instances):\n            frame_ids.append(frame.frame_id.item())\n            bbox = instance.bbox.squeeze()\n            y = (bbox[2] + bbox[0]) / 2\n            x = (bbox[3] + bbox[1]) / 2\n            X.append(x.item())\n            Y.append(y.item())\n            track_scores.append(instance.track_score)\n            pred_track_ids.append(instance.pred_track_id.item())\n\n    save_dict[\"Frame\"] = frame_ids\n    save_dict[\"X\"] = X\n    save_dict[\"Y\"] = Y\n    save_dict[\"Pred_track_id\"] = pred_track_ids\n    save_dict[\"Track_score\"] = track_scores\n    save_df = pd.DataFrame(save_dict)\n    if save_path:\n        save_df.to_csv(save_path, index=False)\n    return save_df\n</code></pre>"},{"location":"reference/dreem/inference/track/#dreem.inference.track.run","title":"<code>run(cfg)</code>","text":"<p>Run inference based on config file.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>A dictconfig loaded from hydra containing checkpoint path and data</p> required Source code in <code>dreem/inference/track.py</code> <pre><code>@hydra.main(config_path=None, config_name=None, version_base=None)\ndef run(cfg: DictConfig) -&gt; dict[int, sio.Labels]:\n    \"\"\"Run inference based on config file.\n\n    Args:\n        cfg: A dictconfig loaded from hydra containing checkpoint path and data\n    \"\"\"\n    pred_cfg = Config(cfg)\n\n    if \"checkpoints\" in cfg.keys():\n        try:\n            index = int(os.environ[\"POD_INDEX\"])\n        # For testing without deploying a job on runai\n        except KeyError:\n            index = input(\"Pod Index Not found! Please choose a pod index: \")\n\n        logger.info(f\"Pod Index: {index}\")\n\n        checkpoints = pd.read_csv(cfg.checkpoints)\n        checkpoint = checkpoints.iloc[index]\n    else:\n        checkpoint = pred_cfg.cfg.ckpt_path\n\n    model = GTRRunner.load_from_checkpoint(checkpoint)\n    tracker_cfg = pred_cfg.get_tracker_cfg()\n    logger.info(\"Updating tracker hparams\")\n    model.tracker_cfg = tracker_cfg\n    model.tracker = Tracker(**model.tracker_cfg)\n    logger.info(f\"Using the following tracker:\")\n    logger.info(model.tracker)\n\n    labels_files, vid_files = pred_cfg.get_data_paths(pred_cfg.cfg.dataset.test_dataset)\n    trainer = pred_cfg.get_trainer()\n    outdir = pred_cfg.cfg.outdir if \"outdir\" in pred_cfg.cfg else \"./results\"\n    os.makedirs(outdir, exist_ok=True)\n\n    for label_file, vid_file in zip(labels_files, vid_files):\n        dataset = pred_cfg.get_dataset(\n            label_files=[label_file], vid_files=[vid_file], mode=\"test\"\n        )\n        dataloader = pred_cfg.get_dataloader(dataset, mode=\"test\")\n        preds = track(model, trainer, dataloader)\n        outpath = os.path.join(outdir, f\"{Path(label_file).stem}.dreem_inference.slp\")\n        preds.save(outpath)\n\n    return preds\n</code></pre>"},{"location":"reference/dreem/inference/track/#dreem.inference.track.track","title":"<code>track(model, trainer, dataloader)</code>","text":"<p>Run Inference.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GTRRunner</code> <p>GTRRunner model loaded from checkpoint used for inference</p> required <code>trainer</code> <code>Trainer</code> <p>lighting Trainer object used for handling inference log.</p> required <code>dataloader</code> <code>DataLoader</code> <p>dataloader containing inference data</p> required Return <p>List of DataFrames containing prediction results for each video</p> Source code in <code>dreem/inference/track.py</code> <pre><code>def track(\n    model: GTRRunner, trainer: pl.Trainer, dataloader: torch.utils.data.DataLoader\n) -&gt; list[pd.DataFrame]:\n    \"\"\"Run Inference.\n\n    Args:\n        model: GTRRunner model loaded from checkpoint used for inference\n        trainer: lighting Trainer object used for handling inference log.\n        dataloader: dataloader containing inference data\n\n    Return:\n        List of DataFrames containing prediction results for each video\n    \"\"\"\n    preds = trainer.predict(model, dataloader)\n    pred_slp = []\n    tracks = {}\n    for batch in preds:\n        for frame in batch:\n            if frame.frame_id.item() == 0:\n                video = (\n                    sio.Video(frame.video)\n                    if isinstance(frame.video, str)\n                    else sio.Video\n                )\n            lf, tracks = frame.to_slp(tracks, video=video)\n            pred_slp.append(lf)\n    pred_slp = sio.Labels(pred_slp)\n    print(pred_slp)\n    return pred_slp\n</code></pre>"},{"location":"reference/dreem/inference/track_queue/","title":"track_queue","text":""},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue","title":"<code>dreem.inference.track_queue</code>","text":"<p>Module handling sliding window tracking.</p>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue","title":"<code>TrackQueue</code>","text":"<p>Class handling track local queue system for sliding window.</p> <p>Each trajectory has its own deque based queue of size <code>window_size - 1</code>. Elements of the queue are Instance objects that have already been tracked and will be compared against later frames for assignment.</p> Source code in <code>dreem/inference/track_queue.py</code> <pre><code>class TrackQueue:\n    \"\"\"Class handling track local queue system for sliding window.\n\n    Each trajectory has its own deque based queue of size `window_size - 1`.\n    Elements of the queue are Instance objects that have already been tracked\n    and will be compared against later frames for assignment.\n    \"\"\"\n\n    def __init__(\n        self, window_size: int, max_gap: int = np.inf, verbose: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize track queue.\n\n        Args:\n            window_size: The number of instances per trajectory allowed in the\n                queue to be compared against.\n            max_gap: The number of consecutive frames a trajectory can fail to\n                appear in before terminating the track.\n            verbose: Whether to print info during operations.\n        \"\"\"\n        self._window_size = window_size\n        self._queues = {}\n        self._max_gap = max_gap\n        self._curr_gap = {}\n        if self._max_gap &lt;= self._window_size:\n            self._max_gap = self._window_size\n        self._curr_track = -1\n        self._verbose = verbose\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get length of the queue.\n\n        Returns:\n            The total number of instances in every sub-queue.\n        \"\"\"\n        return sum([len(queue) for queue in self._queues.values()])\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the TrackQueue.\n\n        Returns:\n            The string representation of the current state of the queue.\n        \"\"\"\n        return (\n            \"TrackQueue(\"\n            f\"window_size={self.window_size}, \"\n            f\"max_gap={self.max_gap}, \"\n            f\"n_tracks={self.n_tracks}, \"\n            f\"curr_track={self.curr_track}, \"\n            f\"queues={[(key,len(queue)) for key, queue in self._queues.items()]}, \"\n            f\"curr_gap:{self._curr_gap}\"\n            \")\"\n        )\n\n    @property\n    def window_size(self) -&gt; int:\n        \"\"\"The maximum number of instances allowed in a sub-queue to be compared against.\n\n        Returns:\n            An int representing The maximum number of instances allowed in a\n                sub-queue to be compared against.\n        \"\"\"\n        return self._window_size\n\n    @window_size.setter\n    def window_size(self, window_size: int) -&gt; None:\n        \"\"\"Set the window size of the queue.\n\n        Args:\n            window_size: An int representing The maximum number of instances\n                allowed in a sub-queue to be compared against.\n        \"\"\"\n        self._window_size = window_size\n\n    @property\n    def max_gap(self) -&gt; int:\n        \"\"\"The maximum number of consecutive frames an trajectory can fail to appear before termination.\n\n        Returns:\n            An int representing the maximum number of consecutive frames an trajectory can fail to\n                appear before termination.\n        \"\"\"\n        return self._max_gap\n\n    @max_gap.setter\n    def max_gap(self, max_gap: int) -&gt; None:\n        \"\"\"Set the max consecutive frame gap allowed for a trajectory.\n\n        Args:\n            max_gap: An int representing the maximum number of consecutive frames an trajectory can fail to\n                appear before termination.\n        \"\"\"\n        self._max_gap = max_gap\n\n    @property\n    def curr_track(self) -&gt; int:\n        \"\"\"The newest *created* trajectory in the queue.\n\n        Returns:\n            The latest *created* trajectory in the queue.\n        \"\"\"\n        return self._curr_track\n\n    @curr_track.setter\n    def curr_track(self, curr_track: int) -&gt; None:\n        \"\"\"Set the newest *created* trajectory in the queue.\n\n        Args:\n            curr_track: The latest *created* trajectory in the queue.\n        \"\"\"\n        self._curr_track = curr_track\n\n    @property\n    def n_tracks(self) -&gt; int:\n        \"\"\"The current number of trajectories in the queue.\n\n        Returns:\n            An int representing the current number of trajectories in the queue.\n        \"\"\"\n        return len(self._queues.keys())\n\n    @property\n    def tracks(self) -&gt; list:\n        \"\"\"A list of the track ids currently in the queue.\n\n        Returns:\n            A list containing the track ids currently in the queue.\n        \"\"\"\n        return list(self._queues.keys())\n\n    @property\n    def verbose(self) -&gt; bool:\n        \"\"\"Indicate whether or not to print outputs along operations. Mostly used for debugging.\n\n        Returns:\n            A boolean representing whether or not printing is turned on.\n        \"\"\"\n        return self._verbose\n\n    @verbose.setter\n    def verbose(self, verbose: bool) -&gt; None:\n        \"\"\"Turn on/off printing.\n\n        Args:\n            verbose: A boolean representing whether printing should be on or off.\n        \"\"\"\n        self._verbose = verbose\n\n    def end_tracks(self, track_id: int | None = None) -&gt; bool:\n        \"\"\"Terminate tracks and removing them from the queue.\n\n        Args:\n            track_id: The index of the trajectory to be ended and removed.\n                If `None` then then every trajectory is removed and the track queue is reset.\n\n        Returns:\n            True if the track is successively removed, otherwise False.\n                (ie if the track doesn't exist in the queue.)\n        \"\"\"\n        if track_id is None:\n            self._queues = {}\n            self._curr_gap = {}\n            self.curr_track = -1\n        else:\n            try:\n                self._queues.pop(track_id)\n                self._curr_gap.pop(track_id)\n            except KeyError:\n                logger.exception(f\"Track ID {track_id} not found in queue!\")\n                return False\n        return True\n\n    def add_frame(self, frame: Frame) -&gt; None:\n        \"\"\"Add frames to the queue.\n\n        Each instance from the frame is added to the queue according to its pred_track_id.\n        If the corresponding trajectory is not already in the queue then create a new queue for the track.\n\n        Args:\n            frame: A Frame object containing instances that have already been tracked.\n        \"\"\"\n        if frame.num_detected == 0:  # only add frames with instances.\n            return\n        vid_id = frame.video_id.item()\n        frame_id = frame.frame_id.item()\n        img_shape = frame.img_shape\n        if isinstance(frame.video, str):\n            vid_name = frame.video\n        else:\n            vid_name = frame.video.filename\n        # traj_score = frame.get_traj_score()  TODO: figure out better way to save trajectory scores.\n        frame_meta = (vid_id, frame_id, vid_name, img_shape.cpu().tolist())\n\n        pred_tracks = []\n        for instance in frame.instances:\n            pred_track_id = instance.pred_track_id.item()\n            pred_tracks.append(pred_track_id)\n\n            if pred_track_id not in self._queues.keys():\n                self._queues[pred_track_id] = deque(\n                    [(*frame_meta, instance)], maxlen=self.window_size - 1\n                )  # dumb work around to retain `img_shape`\n                self.curr_track = pred_track_id\n\n                logger.debug(\n                    f\"New track = {pred_track_id} on frame {frame_id}! Current number of tracks = {self.n_tracks}\"\n                )\n\n            else:\n                self._queues[pred_track_id].append((*frame_meta, instance))\n        self.increment_gaps(\n            pred_tracks\n        )  # should this be done in the tracker or the queue?\n\n    def collate_tracks(\n        self,\n        track_ids: list[int] | None = None,\n        device: str | device | None = None,\n    ) -&gt; list[Frame]:\n        \"\"\"Merge queues into a single list of Frames containing corresponding instances.\n\n        Args:\n            track_ids: A list of trajectorys to merge. If None, then merge all\n                queues, otherwise filter queues by track_ids then merge.\n            device: A str representation of the device the frames should be on after merging\n                since all instances in the queue are kept on the cpu.\n\n        Returns:\n            A sorted list of Frame objects from which each instance came from,\n            containing the corresponding instances.\n        \"\"\"\n        if len(self._queues) == 0:\n            return []\n\n        frames = {}\n\n        tracks_to_convert = (\n            {track: queue for track, queue in self._queues if track in track_ids}\n            if track_ids is not None\n            else self._queues\n        )\n        for track, instances in tracks_to_convert.items():\n            for video_id, frame_id, vid_name, img_shape, instance in instances:\n                if (video_id, frame_id) not in frames.keys():\n                    frame = Frame(\n                        video_id,\n                        frame_id,\n                        img_shape=img_shape,\n                        instances=[instance],\n                        vid_file=vid_name,\n                    )\n                    frames[(video_id, frame_id)] = frame\n                else:\n                    frames[(video_id, frame_id)].instances.append(instance)\n        return [frames[frame].to(device) for frame in sorted(frames.keys())]\n\n    def increment_gaps(self, pred_track_ids: list[int]) -&gt; dict[int, bool]:\n        \"\"\"Keep track of number of consecutive frames each trajectory has been missing from the queue.\n\n        If a trajectory has exceeded the `max_gap` then terminate the track and remove it from the queue.\n\n        Args:\n            pred_track_ids: A list of track_ids to be matched against the trajectories in the queue.\n                If a trajectory is in `pred_track_ids` then its gap counter is reset,\n                otherwise its incremented by 1.\n\n        Returns:\n            A dictionary containing the trajectory id and a boolean value representing\n            whether or not it has exceeded the max allowed gap and been\n            terminated.\n        \"\"\"\n        exceeded_gap = {}\n\n        for track in pred_track_ids:\n            if track not in self._curr_gap:\n                self._curr_gap[track] = 0\n\n        for track in self._curr_gap:\n            if track not in pred_track_ids:\n                self._curr_gap[track] += 1\n                logger.debug(\n                    f\"Track {track} has not been seen for {self._curr_gap[track]} frames.\"\n                )\n            else:\n                self._curr_gap[track] = 0\n            if self._curr_gap[track] &gt;= self.max_gap:\n                exceeded_gap[track] = True\n            else:\n                exceeded_gap[track] = False\n\n        for track, gap_exceeded in exceeded_gap.items():\n            if gap_exceeded:\n                logger.debug(\n                    f\"Track {track} has not been seen for {self._curr_gap[track]} frames! Terminating Track...Current number of tracks = {self.n_tracks}.\"\n                )\n                self._queues.pop(track)\n                self._curr_gap.pop(track)\n\n        return exceeded_gap\n</code></pre>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.curr_track","title":"<code>curr_track: int</code>  <code>property</code> <code>writable</code>","text":"<p>The newest created trajectory in the queue.</p> <p>Returns:</p> Type Description <code>int</code> <p>The latest created trajectory in the queue.</p>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.max_gap","title":"<code>max_gap: int</code>  <code>property</code> <code>writable</code>","text":"<p>The maximum number of consecutive frames an trajectory can fail to appear before termination.</p> <p>Returns:</p> Type Description <code>int</code> <p>An int representing the maximum number of consecutive frames an trajectory can fail to     appear before termination.</p>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.n_tracks","title":"<code>n_tracks: int</code>  <code>property</code>","text":"<p>The current number of trajectories in the queue.</p> <p>Returns:</p> Type Description <code>int</code> <p>An int representing the current number of trajectories in the queue.</p>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.tracks","title":"<code>tracks: list</code>  <code>property</code>","text":"<p>A list of the track ids currently in the queue.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list containing the track ids currently in the queue.</p>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.verbose","title":"<code>verbose: bool</code>  <code>property</code> <code>writable</code>","text":"<p>Indicate whether or not to print outputs along operations. Mostly used for debugging.</p> <p>Returns:</p> Type Description <code>bool</code> <p>A boolean representing whether or not printing is turned on.</p>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.window_size","title":"<code>window_size: int</code>  <code>property</code> <code>writable</code>","text":"<p>The maximum number of instances allowed in a sub-queue to be compared against.</p> <p>Returns:</p> Type Description <code>int</code> <p>An int representing The maximum number of instances allowed in a     sub-queue to be compared against.</p>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.__init__","title":"<code>__init__(window_size, max_gap=np.inf, verbose=False)</code>","text":"<p>Initialize track queue.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>The number of instances per trajectory allowed in the queue to be compared against.</p> required <code>max_gap</code> <code>int</code> <p>The number of consecutive frames a trajectory can fail to appear in before terminating the track.</p> <code>inf</code> <code>verbose</code> <code>bool</code> <p>Whether to print info during operations.</p> <code>False</code> Source code in <code>dreem/inference/track_queue.py</code> <pre><code>def __init__(\n    self, window_size: int, max_gap: int = np.inf, verbose: bool = False\n) -&gt; None:\n    \"\"\"Initialize track queue.\n\n    Args:\n        window_size: The number of instances per trajectory allowed in the\n            queue to be compared against.\n        max_gap: The number of consecutive frames a trajectory can fail to\n            appear in before terminating the track.\n        verbose: Whether to print info during operations.\n    \"\"\"\n    self._window_size = window_size\n    self._queues = {}\n    self._max_gap = max_gap\n    self._curr_gap = {}\n    if self._max_gap &lt;= self._window_size:\n        self._max_gap = self._window_size\n    self._curr_track = -1\n    self._verbose = verbose\n</code></pre>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.__len__","title":"<code>__len__()</code>","text":"<p>Get length of the queue.</p> <p>Returns:</p> Type Description <code>int</code> <p>The total number of instances in every sub-queue.</p> Source code in <code>dreem/inference/track_queue.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get length of the queue.\n\n    Returns:\n        The total number of instances in every sub-queue.\n    \"\"\"\n    return sum([len(queue) for queue in self._queues.values()])\n</code></pre>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the TrackQueue.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the current state of the queue.</p> Source code in <code>dreem/inference/track_queue.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the TrackQueue.\n\n    Returns:\n        The string representation of the current state of the queue.\n    \"\"\"\n    return (\n        \"TrackQueue(\"\n        f\"window_size={self.window_size}, \"\n        f\"max_gap={self.max_gap}, \"\n        f\"n_tracks={self.n_tracks}, \"\n        f\"curr_track={self.curr_track}, \"\n        f\"queues={[(key,len(queue)) for key, queue in self._queues.items()]}, \"\n        f\"curr_gap:{self._curr_gap}\"\n        \")\"\n    )\n</code></pre>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.add_frame","title":"<code>add_frame(frame)</code>","text":"<p>Add frames to the queue.</p> <p>Each instance from the frame is added to the queue according to its pred_track_id. If the corresponding trajectory is not already in the queue then create a new queue for the track.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>Frame</code> <p>A Frame object containing instances that have already been tracked.</p> required Source code in <code>dreem/inference/track_queue.py</code> <pre><code>def add_frame(self, frame: Frame) -&gt; None:\n    \"\"\"Add frames to the queue.\n\n    Each instance from the frame is added to the queue according to its pred_track_id.\n    If the corresponding trajectory is not already in the queue then create a new queue for the track.\n\n    Args:\n        frame: A Frame object containing instances that have already been tracked.\n    \"\"\"\n    if frame.num_detected == 0:  # only add frames with instances.\n        return\n    vid_id = frame.video_id.item()\n    frame_id = frame.frame_id.item()\n    img_shape = frame.img_shape\n    if isinstance(frame.video, str):\n        vid_name = frame.video\n    else:\n        vid_name = frame.video.filename\n    # traj_score = frame.get_traj_score()  TODO: figure out better way to save trajectory scores.\n    frame_meta = (vid_id, frame_id, vid_name, img_shape.cpu().tolist())\n\n    pred_tracks = []\n    for instance in frame.instances:\n        pred_track_id = instance.pred_track_id.item()\n        pred_tracks.append(pred_track_id)\n\n        if pred_track_id not in self._queues.keys():\n            self._queues[pred_track_id] = deque(\n                [(*frame_meta, instance)], maxlen=self.window_size - 1\n            )  # dumb work around to retain `img_shape`\n            self.curr_track = pred_track_id\n\n            logger.debug(\n                f\"New track = {pred_track_id} on frame {frame_id}! Current number of tracks = {self.n_tracks}\"\n            )\n\n        else:\n            self._queues[pred_track_id].append((*frame_meta, instance))\n    self.increment_gaps(\n        pred_tracks\n    )  # should this be done in the tracker or the queue?\n</code></pre>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.collate_tracks","title":"<code>collate_tracks(track_ids=None, device=None)</code>","text":"<p>Merge queues into a single list of Frames containing corresponding instances.</p> <p>Parameters:</p> Name Type Description Default <code>track_ids</code> <code>list[int] | None</code> <p>A list of trajectorys to merge. If None, then merge all queues, otherwise filter queues by track_ids then merge.</p> <code>None</code> <code>device</code> <code>str | device | None</code> <p>A str representation of the device the frames should be on after merging since all instances in the queue are kept on the cpu.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A sorted list of Frame objects from which each instance came from, containing the corresponding instances.</p> Source code in <code>dreem/inference/track_queue.py</code> <pre><code>def collate_tracks(\n    self,\n    track_ids: list[int] | None = None,\n    device: str | device | None = None,\n) -&gt; list[Frame]:\n    \"\"\"Merge queues into a single list of Frames containing corresponding instances.\n\n    Args:\n        track_ids: A list of trajectorys to merge. If None, then merge all\n            queues, otherwise filter queues by track_ids then merge.\n        device: A str representation of the device the frames should be on after merging\n            since all instances in the queue are kept on the cpu.\n\n    Returns:\n        A sorted list of Frame objects from which each instance came from,\n        containing the corresponding instances.\n    \"\"\"\n    if len(self._queues) == 0:\n        return []\n\n    frames = {}\n\n    tracks_to_convert = (\n        {track: queue for track, queue in self._queues if track in track_ids}\n        if track_ids is not None\n        else self._queues\n    )\n    for track, instances in tracks_to_convert.items():\n        for video_id, frame_id, vid_name, img_shape, instance in instances:\n            if (video_id, frame_id) not in frames.keys():\n                frame = Frame(\n                    video_id,\n                    frame_id,\n                    img_shape=img_shape,\n                    instances=[instance],\n                    vid_file=vid_name,\n                )\n                frames[(video_id, frame_id)] = frame\n            else:\n                frames[(video_id, frame_id)].instances.append(instance)\n    return [frames[frame].to(device) for frame in sorted(frames.keys())]\n</code></pre>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.end_tracks","title":"<code>end_tracks(track_id=None)</code>","text":"<p>Terminate tracks and removing them from the queue.</p> <p>Parameters:</p> Name Type Description Default <code>track_id</code> <code>int | None</code> <p>The index of the trajectory to be ended and removed. If <code>None</code> then then every trajectory is removed and the track queue is reset.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the track is successively removed, otherwise False.     (ie if the track doesn't exist in the queue.)</p> Source code in <code>dreem/inference/track_queue.py</code> <pre><code>def end_tracks(self, track_id: int | None = None) -&gt; bool:\n    \"\"\"Terminate tracks and removing them from the queue.\n\n    Args:\n        track_id: The index of the trajectory to be ended and removed.\n            If `None` then then every trajectory is removed and the track queue is reset.\n\n    Returns:\n        True if the track is successively removed, otherwise False.\n            (ie if the track doesn't exist in the queue.)\n    \"\"\"\n    if track_id is None:\n        self._queues = {}\n        self._curr_gap = {}\n        self.curr_track = -1\n    else:\n        try:\n            self._queues.pop(track_id)\n            self._curr_gap.pop(track_id)\n        except KeyError:\n            logger.exception(f\"Track ID {track_id} not found in queue!\")\n            return False\n    return True\n</code></pre>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.increment_gaps","title":"<code>increment_gaps(pred_track_ids)</code>","text":"<p>Keep track of number of consecutive frames each trajectory has been missing from the queue.</p> <p>If a trajectory has exceeded the <code>max_gap</code> then terminate the track and remove it from the queue.</p> <p>Parameters:</p> Name Type Description Default <code>pred_track_ids</code> <code>list[int]</code> <p>A list of track_ids to be matched against the trajectories in the queue. If a trajectory is in <code>pred_track_ids</code> then its gap counter is reset, otherwise its incremented by 1.</p> required <p>Returns:</p> Type Description <code>dict[int, bool]</code> <p>A dictionary containing the trajectory id and a boolean value representing whether or not it has exceeded the max allowed gap and been terminated.</p> Source code in <code>dreem/inference/track_queue.py</code> <pre><code>def increment_gaps(self, pred_track_ids: list[int]) -&gt; dict[int, bool]:\n    \"\"\"Keep track of number of consecutive frames each trajectory has been missing from the queue.\n\n    If a trajectory has exceeded the `max_gap` then terminate the track and remove it from the queue.\n\n    Args:\n        pred_track_ids: A list of track_ids to be matched against the trajectories in the queue.\n            If a trajectory is in `pred_track_ids` then its gap counter is reset,\n            otherwise its incremented by 1.\n\n    Returns:\n        A dictionary containing the trajectory id and a boolean value representing\n        whether or not it has exceeded the max allowed gap and been\n        terminated.\n    \"\"\"\n    exceeded_gap = {}\n\n    for track in pred_track_ids:\n        if track not in self._curr_gap:\n            self._curr_gap[track] = 0\n\n    for track in self._curr_gap:\n        if track not in pred_track_ids:\n            self._curr_gap[track] += 1\n            logger.debug(\n                f\"Track {track} has not been seen for {self._curr_gap[track]} frames.\"\n            )\n        else:\n            self._curr_gap[track] = 0\n        if self._curr_gap[track] &gt;= self.max_gap:\n            exceeded_gap[track] = True\n        else:\n            exceeded_gap[track] = False\n\n    for track, gap_exceeded in exceeded_gap.items():\n        if gap_exceeded:\n            logger.debug(\n                f\"Track {track} has not been seen for {self._curr_gap[track]} frames! Terminating Track...Current number of tracks = {self.n_tracks}.\"\n            )\n            self._queues.pop(track)\n            self._curr_gap.pop(track)\n\n    return exceeded_gap\n</code></pre>"},{"location":"reference/dreem/inference/tracker/","title":"tracker","text":""},{"location":"reference/dreem/inference/tracker/#dreem.inference.tracker","title":"<code>dreem.inference.tracker</code>","text":"<p>Module containing logic for going from association -&gt; assignment.</p>"},{"location":"reference/dreem/inference/tracker/#dreem.inference.tracker.Tracker","title":"<code>Tracker</code>","text":"<p>Tracker class used for assignment based on sliding inference from GTR.</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>class Tracker:\n    \"\"\"Tracker class used for assignment based on sliding inference from GTR.\"\"\"\n\n    def __init__(\n        self,\n        window_size: int = 8,\n        use_vis_feats: bool = True,\n        overlap_thresh: float = 0.01,\n        mult_thresh: bool = True,\n        decay_time: float | None = None,\n        iou: str | None = None,\n        max_center_dist: float | None = None,\n        persistent_tracking: bool = False,\n        max_gap: int = inf,\n        max_tracks: int = inf,\n        verbose: bool = False,\n    ):\n        \"\"\"Initialize a tracker to run inference.\n\n        Args:\n            window_size: the size of the window used during sliding inference.\n            use_vis_feats: Whether or not to use visual feature extractor.\n            overlap_thresh: the trajectory overlap threshold to be used for assignment.\n            mult_thresh: Whether or not to use weight threshold.\n            decay_time: weight for `decay_time` postprocessing.\n            iou: Either [None, '', \"mult\" or \"max\"]\n                 Whether to use multiplicative or max iou reweighting.\n            max_center_dist: distance threshold for filtering trajectory score matrix.\n            persistent_tracking: whether to keep a buffer across chunks or not.\n            max_gap: the max number of frames a trajectory can be missing before termination.\n            max_tracks: the maximum number of tracks that can be created while tracking.\n                We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.\n            verbose: Whether or not to turn on debug printing after each operation.\n        \"\"\"\n        self.track_queue = TrackQueue(\n            window_size=window_size, max_gap=max_gap, verbose=verbose\n        )\n        self.use_vis_feats = use_vis_feats\n        self.overlap_thresh = overlap_thresh\n        self.mult_thresh = mult_thresh\n        self.decay_time = decay_time\n        self.iou = iou\n        self.max_center_dist = max_center_dist\n        self.persistent_tracking = persistent_tracking\n        self.verbose = verbose\n        self.max_tracks = max_tracks\n\n    def __call__(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; list[Frame]:\n        \"\"\"Wrap around `track` to enable `tracker()` instead of `tracker.track()`.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: list of Frames to run inference on\n\n        Returns:\n            List of frames containing association matrix scores and instances populated with pred track ids.\n        \"\"\"\n        return self.track(model, frames)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get string representation of tracker.\n\n        Returns: the string representation of the tracker\n        \"\"\"\n        return (\n            \"Tracker(\"\n            f\"persistent_tracking={self.persistent_tracking}, \"\n            f\"max_tracks={self.max_tracks}, \"\n            f\"use_vis_feats={self.use_vis_feats}, \"\n            f\"overlap_thresh={self.overlap_thresh}, \"\n            f\"mult_thresh={self.mult_thresh}, \"\n            f\"decay_time={self.decay_time}, \"\n            f\"max_center_dist={self.max_center_dist}, \"\n            f\"verbose={self.verbose}, \"\n            f\"queue={self.track_queue}\"\n        )\n\n    def track(\n        self, model: GlobalTrackingTransformer, frames: list[dict]\n    ) -&gt; list[Frame]:\n        \"\"\"Run tracker and get predicted trajectories.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: data dict to run inference on\n\n        Returns:\n            List of Frames populated with pred track ids and association matrix scores\n        \"\"\"\n        # Extract feature representations with pre-trained encoder.\n\n        _ = model.eval()\n\n        for frame in frames:\n            if frame.has_instances():\n                if not self.use_vis_feats:\n                    for instance in frame.instances:\n                        instance.features = torch.zeros(1, model.d_model)\n                    # frame[\"features\"] = torch.randn(\n                    #     num_frame_instances, self.model.d_model\n                    # )\n\n                # comment out to turn encoder off\n\n                # Assuming the encoder is already trained or train encoder jointly.\n                elif not frame.has_features():\n                    with torch.no_grad():\n                        crops = frame.get_crops()\n                        z = model.visual_encoder(crops)\n\n                        for i, z_i in enumerate(z):\n                            frame.instances[i].features = z_i\n\n        # I feel like this chunk is unnecessary:\n        # reid_features = torch.cat(\n        #     [frame[\"features\"] for frame in instances], dim=0\n        # ).unsqueeze(0)\n\n        # asso_preds, pred_boxes, pred_time, embeddings = self.model(\n        #     instances, reid_features\n        # )\n        instances_pred = self.sliding_inference(model, frames)\n\n        if not self.persistent_tracking:\n            logger.debug(f\"Clearing Queue after tracking\")\n            self.track_queue.end_tracks()\n\n        return instances_pred\n\n    def sliding_inference(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; list[Frame]:\n        \"\"\"Perform sliding inference on the input video (instances) with a given window size.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n        Returns:\n            frames: A list of Frames populated with pred_track_ids and asso_matrices\n        \"\"\"\n        # B: batch size.\n        # D: embedding dimension.\n        # nc: number of channels.\n        # H: height.\n        # W: width.\n\n        for batch_idx, frame_to_track in enumerate(frames):\n            tracked_frames = self.track_queue.collate_tracks(\n                device=frame_to_track.frame_id.device\n            )\n            logger.debug(f\"Current number of tracks is {self.track_queue.n_tracks}\")\n\n            if (\n                self.persistent_tracking and frame_to_track.frame_id == 0\n            ):  # check for new video and clear queue\n\n                logger.debug(\"New Video! Resetting Track Queue.\")\n                self.track_queue.end_tracks()\n\n            \"\"\"\n            Initialize tracks on first frame where detections appear.\n            \"\"\"\n            if len(self.track_queue) == 0:\n                if frame_to_track.has_instances():\n\n                    logger.debug(\n                        f\"Initializing track on clip ind {batch_idx} frame {frame_to_track.frame_id.item()}\"\n                    )\n\n                    curr_track_id = 0\n                    for i, instance in enumerate(frames[batch_idx].instances):\n                        instance.pred_track_id = instance.gt_track_id\n                        curr_track_id = max(curr_track_id, instance.pred_track_id)\n\n                    for i, instance in enumerate(frames[batch_idx].instances):\n                        if instance.pred_track_id == -1:\n                            curr_track += 1\n                            instance.pred_track_id = curr_track_id\n\n            else:\n                if (\n                    frame_to_track.has_instances()\n                ):  # Check if there are detections. If there are skip and increment gap count\n                    frames_to_track = tracked_frames + [\n                        frame_to_track\n                    ]  # better var name?\n\n                    query_ind = len(frames_to_track) - 1\n\n                    frame_to_track = self._run_global_tracker(\n                        model,\n                        frames_to_track,\n                        query_ind=query_ind,\n                    )\n\n            if frame_to_track.has_instances():\n                self.track_queue.add_frame(frame_to_track)\n            else:\n                self.track_queue.increment_gaps([])\n\n            frames[batch_idx] = frame_to_track\n        return frames\n\n    def _run_global_tracker(\n        self, model: GlobalTrackingTransformer, frames: list[Frame], query_ind: int\n    ) -&gt; Frame:\n        \"\"\"Run global tracker performs the actual tracking.\n\n        Uses Hungarian algorithm to do track assigning.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: A list of Frames containing reid features. See `dreem.io.data_structures` for more info.\n            query_ind: An integer for the query frame within the window of instances.\n\n        Returns:\n            query_frame: The query frame now populated with the pred_track_ids.\n        \"\"\"\n        # *: each item in frames is a frame in the window. So it follows\n        #    that each frame in the window has * detected instances.\n        # D: embedding dimension.\n        # total_instances: number of instances in the window.\n        # N_i: number of detected instances in i-th frame of window.\n        # instances_per_frame: a list of number of instances in each frame of the window.\n        # n_query: number of instances in current/query frame (rightmost frame of the window).\n        # n_nonquery: number of instances in the window excluding the current/query frame.\n        # window_size: length of window.\n        # L: number of decoder blocks.\n        # n_traj: number of existing tracks within the window so far.\n\n        # Number of instances in each frame of the window.\n        # E.g.: instances_per_frame: [4, 5, 6, 7]; window of length 4 with 4 detected instances in the first frame of the window.\n\n        _ = model.eval()\n\n        query_frame = frames[query_ind]\n\n        query_instances = query_frame.instances\n        all_instances = [instance for frame in frames for instance in frame.instances]\n\n        logger.debug(f\"Frame {query_frame.frame_id.item()}\")\n\n        instances_per_frame = [frame.num_detected for frame in frames]\n\n        total_instances, window_size = sum(instances_per_frame), len(\n            instances_per_frame\n        )  # Number of instances in window; length of window.\n\n        logger.debug(f\"total_instances: {total_instances}\")\n\n        overlap_thresh = self.overlap_thresh\n        mult_thresh = self.mult_thresh\n        n_traj = self.track_queue.n_tracks\n        curr_track = self.track_queue.curr_track\n\n        reid_features = torch.cat([frame.get_features() for frame in frames], dim=0)[\n            None\n        ]  # (1, total_instances, D=512)\n\n        # (L=1, n_query, total_instances)\n        with torch.no_grad():\n            asso_matrix = model(all_instances, query_instances)\n\n        asso_output = asso_matrix[-1].matrix.split(\n            instances_per_frame, dim=1\n        )  # (window_size, n_query, N_i)\n        asso_output = model_utils.softmax_asso(\n            asso_output\n        )  # (window_size, n_query, N_i)\n        asso_output = torch.cat(asso_output, dim=1).cpu()  # (n_query, total_instances)\n\n        asso_output_df = pd.DataFrame(\n            asso_output.clone().numpy(),\n            columns=[f\"Instance {i}\" for i in range(asso_output.shape[-1])],\n        )\n\n        asso_output_df.index.name = \"Instances\"\n        asso_output_df.columns.name = \"Instances\"\n\n        query_frame.add_traj_score(\"asso_output\", asso_output_df)\n        query_frame.asso_output = asso_matrix[-1]\n\n        n_query = (\n            query_frame.num_detected\n        )  # Number of instances in the current/query frame.\n\n        n_nonquery = (\n            total_instances - n_query\n        )  # Number of instances in the window not including the current/query frame.\n\n        logger.debug(f\"n_nonquery: {n_nonquery}\")\n        logger.debug(f\"n_query: {n_query}\")\n\n        instance_ids = torch.cat(\n            [\n                x.get_pred_track_ids()\n                for batch_idx, x in enumerate(frames)\n                if batch_idx != query_ind\n            ],\n            dim=0,\n        ).view(\n            n_nonquery\n        )  # (n_nonquery,)\n\n        query_inds = [\n            x\n            for x in range(\n                sum(instances_per_frame[:query_ind]),\n                sum(instances_per_frame[: query_ind + 1]),\n            )\n        ]\n        nonquery_inds = [i for i in range(total_instances) if i not in query_inds]\n\n        # instead should we do model(nonquery_instances, query_instances)?\n        asso_nonquery = asso_output[:, nonquery_inds]  # (n_query, n_nonquery)\n\n        asso_nonquery_df = pd.DataFrame(\n            asso_nonquery.clone().numpy(), columns=nonquery_inds\n        )\n\n        asso_nonquery_df.index.name = \"Current Frame Instances\"\n        asso_nonquery_df.columns.name = \"Nonquery Instances\"\n\n        query_frame.add_traj_score(\"asso_nonquery\", asso_nonquery_df)\n\n        pred_boxes = model_utils.get_boxes(all_instances)\n        query_boxes = pred_boxes[query_inds]  # n_k x 4\n        nonquery_boxes = pred_boxes[nonquery_inds]  # n_nonquery x 4\n\n        unique_ids = torch.unique(instance_ids)  # (n_nonquery,)\n\n        logger.debug(f\"Instance IDs: {instance_ids}\")\n        logger.debug(f\"unique ids: {unique_ids}\")\n\n        id_inds = (\n            unique_ids[None, :] == instance_ids[:, None]\n        ).float()  # (n_nonquery, n_traj)\n\n        ################################################################################\n\n        # reweighting hyper-parameters for association -&gt; they use 0.9\n\n        traj_score = post_processing.weight_decay_time(\n            asso_nonquery, self.decay_time, reid_features, window_size, query_ind\n        )\n\n        if self.decay_time is not None and self.decay_time &gt; 0:\n            decay_time_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=nonquery_inds\n            )\n\n            decay_time_traj_score.index.name = \"Query Instances\"\n            decay_time_traj_score.columns.name = \"Nonquery Instances\"\n\n            query_frame.add_traj_score(\"decay_time\", decay_time_traj_score)\n        ################################################################################\n\n        # (n_query x n_nonquery) x (n_nonquery x n_traj) --&gt; n_query x n_traj\n        traj_score = torch.mm(traj_score, id_inds.cpu())  # (n_query, n_traj)\n\n        traj_score_df = pd.DataFrame(\n            traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n        )\n\n        traj_score_df.index.name = \"Current Frame Instances\"\n        traj_score_df.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"traj_score\", traj_score_df)\n        ################################################################################\n\n        # with iou -&gt; combining with location in tracker, they set to True\n        # todo -&gt; should also work without pos_embed\n\n        if id_inds.numel() &gt; 0:\n            # this throws error, think we need to slice?\n            # last_inds = (id_inds * torch.arange(\n            #    n_nonquery, device=id_inds.device)[:, None]).max(dim=0)[1] # n_traj\n\n            last_inds = (\n                id_inds * torch.arange(n_nonquery, device=id_inds.device)[:, None]\n            ).max(dim=0)[\n                1\n            ]  # M\n\n            last_boxes = nonquery_boxes[last_inds]  # n_traj x 4\n            last_ious = post_processing._pairwise_iou(\n                Boxes(query_boxes), Boxes(last_boxes)\n            )  # n_k x M\n        else:\n            last_ious = traj_score.new_zeros(traj_score.shape)\n\n        traj_score = post_processing.weight_iou(traj_score, self.iou, last_ious.cpu())\n\n        if self.iou is not None and self.iou != \"\":\n            iou_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n            )\n\n            iou_traj_score.index.name = \"Current Frame Instances\"\n            iou_traj_score.columns.name = \"Unique IDs\"\n\n            query_frame.add_traj_score(\"weight_iou\", iou_traj_score)\n        ################################################################################\n\n        # threshold for continuing a tracking or starting a new track -&gt; they use 1.0\n        # todo -&gt; should also work without pos_embed\n        traj_score = post_processing.filter_max_center_dist(\n            traj_score, self.max_center_dist, query_boxes, nonquery_boxes, id_inds\n        )\n\n        if self.max_center_dist is not None and self.max_center_dist &gt; 0:\n            max_center_dist_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n            )\n\n            max_center_dist_traj_score.index.name = \"Current Frame Instances\"\n            max_center_dist_traj_score.columns.name = \"Unique IDs\"\n\n            query_frame.add_traj_score(\"max_center_dist\", max_center_dist_traj_score)\n\n        ################################################################################\n        scaled_traj_score = torch.softmax(traj_score, dim=1)\n        scaled_traj_score_df = pd.DataFrame(\n            scaled_traj_score.numpy(), columns=unique_ids.cpu().numpy()\n        )\n        scaled_traj_score_df.index.name = \"Current Frame Instances\"\n        scaled_traj_score_df.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"scaled\", scaled_traj_score_df)\n        ################################################################################\n\n        match_i, match_j = linear_sum_assignment((-traj_score))\n\n        track_ids = instance_ids.new_full((n_query,), -1)\n        for i, j in zip(match_i, match_j):\n            # The overlap threshold is multiplied by the number of times the unique track j is matched to an\n            # instance out of all instances in the window excluding the current frame.\n            #\n            # So if this is correct, the threshold is higher for matching an instance from the current frame\n            # to an existing track if that track has already been matched several times.\n            # So if an existing track in the window has been matched a lot, it gets harder to match to that track.\n            thresh = (\n                overlap_thresh * id_inds[:, j].sum() if mult_thresh else overlap_thresh\n            )\n            if n_traj &gt;= self.max_tracks or traj_score[i, j] &gt; thresh:\n                logger.debug(\n                    f\"Assigning instance {i} to track {j} with id {unique_ids[j]}\"\n                )\n                track_ids[i] = unique_ids[j]\n                query_frame.instances[i].track_score = scaled_traj_score[i, j].item()\n        logger.debug(f\"track_ids: {track_ids}\")\n        for i in range(n_query):\n            if track_ids[i] &lt; 0:\n                logger.debug(f\"Creating new track {curr_track}\")\n                curr_track += 1\n                track_ids[i] = curr_track\n\n        query_frame.matches = (match_i, match_j)\n\n        for instance, track_id in zip(query_frame.instances, track_ids):\n            instance.pred_track_id = track_id\n\n        final_traj_score = pd.DataFrame(\n            traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n        )\n        final_traj_score.index.name = \"Current Frame Instances\"\n        final_traj_score.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"final\", final_traj_score)\n        return query_frame\n</code></pre>"},{"location":"reference/dreem/inference/tracker/#dreem.inference.tracker.Tracker.__call__","title":"<code>__call__(model, frames)</code>","text":"<p>Wrap around <code>track</code> to enable <code>tracker()</code> instead of <code>tracker.track()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[Frame]</code> <p>list of Frames to run inference on</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>List of frames containing association matrix scores and instances populated with pred track ids.</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def __call__(\n    self, model: GlobalTrackingTransformer, frames: list[Frame]\n) -&gt; list[Frame]:\n    \"\"\"Wrap around `track` to enable `tracker()` instead of `tracker.track()`.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: list of Frames to run inference on\n\n    Returns:\n        List of frames containing association matrix scores and instances populated with pred track ids.\n    \"\"\"\n    return self.track(model, frames)\n</code></pre>"},{"location":"reference/dreem/inference/tracker/#dreem.inference.tracker.Tracker.__init__","title":"<code>__init__(window_size=8, use_vis_feats=True, overlap_thresh=0.01, mult_thresh=True, decay_time=None, iou=None, max_center_dist=None, persistent_tracking=False, max_gap=inf, max_tracks=inf, verbose=False)</code>","text":"<p>Initialize a tracker to run inference.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>the size of the window used during sliding inference.</p> <code>8</code> <code>use_vis_feats</code> <code>bool</code> <p>Whether or not to use visual feature extractor.</p> <code>True</code> <code>overlap_thresh</code> <code>float</code> <p>the trajectory overlap threshold to be used for assignment.</p> <code>0.01</code> <code>mult_thresh</code> <code>bool</code> <p>Whether or not to use weight threshold.</p> <code>True</code> <code>decay_time</code> <code>float | None</code> <p>weight for <code>decay_time</code> postprocessing.</p> <code>None</code> <code>iou</code> <code>str | None</code> <p>Either [None, '', \"mult\" or \"max\"]  Whether to use multiplicative or max iou reweighting.</p> <code>None</code> <code>max_center_dist</code> <code>float | None</code> <p>distance threshold for filtering trajectory score matrix.</p> <code>None</code> <code>persistent_tracking</code> <code>bool</code> <p>whether to keep a buffer across chunks or not.</p> <code>False</code> <code>max_gap</code> <code>int</code> <p>the max number of frames a trajectory can be missing before termination.</p> <code>inf</code> <code>max_tracks</code> <code>int</code> <p>the maximum number of tracks that can be created while tracking. We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.</p> <code>inf</code> <code>verbose</code> <code>bool</code> <p>Whether or not to turn on debug printing after each operation.</p> <code>False</code> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def __init__(\n    self,\n    window_size: int = 8,\n    use_vis_feats: bool = True,\n    overlap_thresh: float = 0.01,\n    mult_thresh: bool = True,\n    decay_time: float | None = None,\n    iou: str | None = None,\n    max_center_dist: float | None = None,\n    persistent_tracking: bool = False,\n    max_gap: int = inf,\n    max_tracks: int = inf,\n    verbose: bool = False,\n):\n    \"\"\"Initialize a tracker to run inference.\n\n    Args:\n        window_size: the size of the window used during sliding inference.\n        use_vis_feats: Whether or not to use visual feature extractor.\n        overlap_thresh: the trajectory overlap threshold to be used for assignment.\n        mult_thresh: Whether or not to use weight threshold.\n        decay_time: weight for `decay_time` postprocessing.\n        iou: Either [None, '', \"mult\" or \"max\"]\n             Whether to use multiplicative or max iou reweighting.\n        max_center_dist: distance threshold for filtering trajectory score matrix.\n        persistent_tracking: whether to keep a buffer across chunks or not.\n        max_gap: the max number of frames a trajectory can be missing before termination.\n        max_tracks: the maximum number of tracks that can be created while tracking.\n            We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.\n        verbose: Whether or not to turn on debug printing after each operation.\n    \"\"\"\n    self.track_queue = TrackQueue(\n        window_size=window_size, max_gap=max_gap, verbose=verbose\n    )\n    self.use_vis_feats = use_vis_feats\n    self.overlap_thresh = overlap_thresh\n    self.mult_thresh = mult_thresh\n    self.decay_time = decay_time\n    self.iou = iou\n    self.max_center_dist = max_center_dist\n    self.persistent_tracking = persistent_tracking\n    self.verbose = verbose\n    self.max_tracks = max_tracks\n</code></pre>"},{"location":"reference/dreem/inference/tracker/#dreem.inference.tracker.Tracker.__repr__","title":"<code>__repr__()</code>","text":"<p>Get string representation of tracker.</p> <p>Returns: the string representation of the tracker</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get string representation of tracker.\n\n    Returns: the string representation of the tracker\n    \"\"\"\n    return (\n        \"Tracker(\"\n        f\"persistent_tracking={self.persistent_tracking}, \"\n        f\"max_tracks={self.max_tracks}, \"\n        f\"use_vis_feats={self.use_vis_feats}, \"\n        f\"overlap_thresh={self.overlap_thresh}, \"\n        f\"mult_thresh={self.mult_thresh}, \"\n        f\"decay_time={self.decay_time}, \"\n        f\"max_center_dist={self.max_center_dist}, \"\n        f\"verbose={self.verbose}, \"\n        f\"queue={self.track_queue}\"\n    )\n</code></pre>"},{"location":"reference/dreem/inference/tracker/#dreem.inference.tracker.Tracker.sliding_inference","title":"<code>sliding_inference(model, frames)</code>","text":"<p>Perform sliding inference on the input video (instances) with a given window size.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[Frame]</code> <p>A list of Frames (See <code>dreem.io.Frame</code> for more info).</p> required <p>Returns:</p> Name Type Description <code>frames</code> <code>list[Frame]</code> <p>A list of Frames populated with pred_track_ids and asso_matrices</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def sliding_inference(\n    self, model: GlobalTrackingTransformer, frames: list[Frame]\n) -&gt; list[Frame]:\n    \"\"\"Perform sliding inference on the input video (instances) with a given window size.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n    Returns:\n        frames: A list of Frames populated with pred_track_ids and asso_matrices\n    \"\"\"\n    # B: batch size.\n    # D: embedding dimension.\n    # nc: number of channels.\n    # H: height.\n    # W: width.\n\n    for batch_idx, frame_to_track in enumerate(frames):\n        tracked_frames = self.track_queue.collate_tracks(\n            device=frame_to_track.frame_id.device\n        )\n        logger.debug(f\"Current number of tracks is {self.track_queue.n_tracks}\")\n\n        if (\n            self.persistent_tracking and frame_to_track.frame_id == 0\n        ):  # check for new video and clear queue\n\n            logger.debug(\"New Video! Resetting Track Queue.\")\n            self.track_queue.end_tracks()\n\n        \"\"\"\n        Initialize tracks on first frame where detections appear.\n        \"\"\"\n        if len(self.track_queue) == 0:\n            if frame_to_track.has_instances():\n\n                logger.debug(\n                    f\"Initializing track on clip ind {batch_idx} frame {frame_to_track.frame_id.item()}\"\n                )\n\n                curr_track_id = 0\n                for i, instance in enumerate(frames[batch_idx].instances):\n                    instance.pred_track_id = instance.gt_track_id\n                    curr_track_id = max(curr_track_id, instance.pred_track_id)\n\n                for i, instance in enumerate(frames[batch_idx].instances):\n                    if instance.pred_track_id == -1:\n                        curr_track += 1\n                        instance.pred_track_id = curr_track_id\n\n        else:\n            if (\n                frame_to_track.has_instances()\n            ):  # Check if there are detections. If there are skip and increment gap count\n                frames_to_track = tracked_frames + [\n                    frame_to_track\n                ]  # better var name?\n\n                query_ind = len(frames_to_track) - 1\n\n                frame_to_track = self._run_global_tracker(\n                    model,\n                    frames_to_track,\n                    query_ind=query_ind,\n                )\n\n        if frame_to_track.has_instances():\n            self.track_queue.add_frame(frame_to_track)\n        else:\n            self.track_queue.increment_gaps([])\n\n        frames[batch_idx] = frame_to_track\n    return frames\n</code></pre>"},{"location":"reference/dreem/inference/tracker/#dreem.inference.tracker.Tracker.track","title":"<code>track(model, frames)</code>","text":"<p>Run tracker and get predicted trajectories.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[dict]</code> <p>data dict to run inference on</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>List of Frames populated with pred track ids and association matrix scores</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def track(\n    self, model: GlobalTrackingTransformer, frames: list[dict]\n) -&gt; list[Frame]:\n    \"\"\"Run tracker and get predicted trajectories.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: data dict to run inference on\n\n    Returns:\n        List of Frames populated with pred track ids and association matrix scores\n    \"\"\"\n    # Extract feature representations with pre-trained encoder.\n\n    _ = model.eval()\n\n    for frame in frames:\n        if frame.has_instances():\n            if not self.use_vis_feats:\n                for instance in frame.instances:\n                    instance.features = torch.zeros(1, model.d_model)\n                # frame[\"features\"] = torch.randn(\n                #     num_frame_instances, self.model.d_model\n                # )\n\n            # comment out to turn encoder off\n\n            # Assuming the encoder is already trained or train encoder jointly.\n            elif not frame.has_features():\n                with torch.no_grad():\n                    crops = frame.get_crops()\n                    z = model.visual_encoder(crops)\n\n                    for i, z_i in enumerate(z):\n                        frame.instances[i].features = z_i\n\n    # I feel like this chunk is unnecessary:\n    # reid_features = torch.cat(\n    #     [frame[\"features\"] for frame in instances], dim=0\n    # ).unsqueeze(0)\n\n    # asso_preds, pred_boxes, pred_time, embeddings = self.model(\n    #     instances, reid_features\n    # )\n    instances_pred = self.sliding_inference(model, frames)\n\n    if not self.persistent_tracking:\n        logger.debug(f\"Clearing Queue after tracking\")\n        self.track_queue.end_tracks()\n\n    return instances_pred\n</code></pre>"},{"location":"reference/dreem/io/","title":"io","text":""},{"location":"reference/dreem/io/#dreem.io","title":"<code>dreem.io</code>","text":"<p>Module containing input/output data structures for easy storage and manipulation.</p>"},{"location":"reference/dreem/io/association_matrix/","title":"association_matrix","text":""},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix","title":"<code>dreem.io.association_matrix</code>","text":"<p>Module containing class for storing and looking up association scores.</p>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix","title":"<code>AssociationMatrix</code>","text":"<p>Class representing the associations between detections.</p> <p>Attributes:</p> Name Type Description <code>matrix</code> <code>ndarray | Tensor</code> <p>the <code>n_query x n_ref</code> association matrix`</p> <code>ref_instances</code> <code>list[Instance]</code> <p>all instances used to associate against.</p> <code>query_instances</code> <code>list[Instance]</code> <p>query instances that were associated against ref instances.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>@attrs.define\nclass AssociationMatrix:\n    \"\"\"Class representing the associations between detections.\n\n    Attributes:\n        matrix: the `n_query x n_ref` association matrix`\n        ref_instances: all instances used to associate against.\n        query_instances: query instances that were associated against ref instances.\n    \"\"\"\n\n    matrix: np.ndarray | torch.Tensor\n    ref_instances: list[Instance] = attrs.field()\n    query_instances: list[Instance] = attrs.field()\n\n    @ref_instances.validator\n    def _check_ref_instances(self, attribute, value):\n        \"\"\"Check to ensure that the number of association matrix columns and reference instances match.\n\n        Args:\n            attribute: The ref instances.\n            value: the list of ref instances.\n\n        Raises:\n            ValueError if the number of columns and reference instances don't match.\n        \"\"\"\n        if len(value) != self.matrix.shape[-1]:\n            raise ValueError(\n                (\n                    \"Ref instances must equal number of columns in Association matrix\"\n                    f\"Found {len(value)} ref instances but {self.matrix.shape[-1]} columns.\"\n                )\n            )\n\n    @query_instances.validator\n    def _check_query_instances(self, attribute, value):\n        \"\"\"Check to ensure that the number of association matrix rows and query instances match.\n\n        Args:\n            attribute: The query instances.\n            value: the list of query instances.\n\n        Raises:\n            ValueError if the number of rows and query instances don't match.\n        \"\"\"\n        if len(value) != self.matrix.shape[0]:\n            raise ValueError(\n                (\n                    \"Query instances must equal number of rows in Association matrix\"\n                    f\"Found {len(value)} query instances but {self.matrix.shape[0]} rows.\"\n                )\n            )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get the string representation of the Association Matrix.\n\n        Returns:\n            the string representation of the association matrix.\n        \"\"\"\n        return (\n            f\"AssociationMatrix({self.matrix},\"\n            f\"query_instances={len(self.query_instances)},\"\n            f\"ref_instances={len(self.ref_instances)})\"\n        )\n\n    def numpy(self) -&gt; np.ndarray:\n        \"\"\"Convert association matrix to a numpy array.\n\n        Returns:\n            The association matrix as a numpy array.\n        \"\"\"\n        if isinstance(self.matrix, torch.Tensor):\n            return self.matrix.detach().cpu().numpy()\n        return self.matrix\n\n    def to_dataframe(\n        self, row_labels: str = \"gt\", col_labels: str = \"gt\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Convert the association matrix to a pandas DataFrame.\n\n        Args:\n            row_labels: How to label the rows(queries).\n                If list, then must match # of rows/queries\n                If `\"gt\"` then label by gt track id.\n                If `\"pred\"` then label by pred track id.\n                Otherwise label by the query_instance indices\n            col_labels: How to label the columns(references).\n                If list, then must match # of columns/refs\n                If `\"gt\"` then label by gt track id.\n                If `\"pred\"` then label by pred track id.\n                Otherwise label by the ref_instance indices\n\n        Returns:\n            The association matrix as a pandas dataframe.\n        \"\"\"\n        matrix = self.numpy()\n\n        if not isinstance(row_labels, str):\n            if len(row_labels) == len(self.query_instances):\n                row_inds = row_labels\n\n            else:\n                raise ValueError(\n                    (\n                        f\"Mismatched # of rows and labels!\",\n                        f\"Found {len(row_labels)} with {len(self.query_instances)} rows\",\n                    )\n                )\n\n        else:\n            if row_labels == \"gt\":\n                row_inds = [\n                    instance.gt_track_id.item() for instance in self.query_instances\n                ]\n\n            elif row_labels == \"pred\":\n                row_inds = [\n                    instance.pred_track_id.item() for instance in self.query_instances\n                ]\n\n            else:\n                row_inds = np.arange(len(self.query_instances))\n\n        if not isinstance(col_labels, str):\n            if len(col_labels) == len(self.ref_instances):\n                col_inds = col_labels\n\n            else:\n                raise ValueError(\n                    (\n                        f\"Mismatched # of columns and labels!\",\n                        f\"Found {len(col_labels)} with {len(self.ref_instances)} columns\",\n                    )\n                )\n\n        else:\n            if col_labels == \"gt\":\n                col_inds = [\n                    instance.gt_track_id.item() for instance in self.ref_instances\n                ]\n\n            elif col_labels == \"pred\":\n                col_inds = [\n                    instance.pred_track_id.item() for instance in self.ref_instances\n                ]\n\n            else:\n                col_inds = np.arange(len(self.ref_instances))\n\n        asso_df = pd.DataFrame(matrix, index=row_inds, columns=col_inds)\n\n        return asso_df\n\n    def reduce(\n        self,\n        row_dims: str = \"instance\",\n        col_dims: str = \"track\",\n        row_grouping: str | None = None,\n        col_grouping: str = \"pred\",\n        reduce_method: callable = np.sum,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Aggregate the association matrix by specified dimensions and grouping.\n\n        Args:\n           row_dims: A str indicating how to what dimensions to reduce rows to.\n                Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).\n           col_dims: A str indicating how to dimensions to reduce rows to.\n                Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)\n           row_grouping: A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".\n           col_grouping: A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".\n           reduce_method: A callable function that operates on numpy matrices and can take an `axis` arg for reducing.\n\n        Returns:\n            The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.\n        \"\"\"\n        n_rows = len(self.query_instances)\n        n_cols = len(self.ref_instances)\n\n        col_tracks = {-1: self.ref_instances}\n        row_tracks = {-1: self.query_instances}\n\n        col_inds = [i for i in range(len(self.ref_instances))]\n        row_inds = [i for i in range(len(self.query_instances))]\n\n        if col_dims == \"track\":\n            col_tracks = self.get_tracks(self.ref_instances, col_grouping)\n            col_inds = list(col_tracks.keys())\n            n_cols = len(col_inds)\n\n        if row_dims == \"track\":\n            row_tracks = self.get_tracks(self.query_instances, row_grouping)\n            row_inds = list(row_tracks.keys())\n            n_rows = len(row_inds)\n\n        reduced_matrix = []\n        for row_track, row_instances in row_tracks.items():\n            for col_track, col_instances in col_tracks.items():\n\n                asso_matrix = self[row_instances, col_instances]\n\n                if col_dims == \"track\":\n                    asso_matrix = reduce_method(asso_matrix, axis=1)\n\n                if row_dims == \"track\":\n                    asso_matrix = reduce_method(asso_matrix, axis=0)\n\n                reduced_matrix.append(asso_matrix)\n\n        reduced_matrix = np.array(reduced_matrix).reshape(n_cols, n_rows).T\n\n        return pd.DataFrame(reduced_matrix, index=row_inds, columns=col_inds)\n\n    def __getitem__(\n        self, inds: tuple[int | Instance | list[int | Instance]]\n    ) -&gt; np.ndarray:\n        \"\"\"Get elements of the association matrix.\n\n        Args:\n            inds: A tuple of query indices and reference indices.\n                Indices can be either:\n                    A single instance or integer.\n                    A list of instances or integers.\n\n        Returns:\n            An np.ndarray containing the elements requested.\n        \"\"\"\n        query_inst, ref_inst = inds\n\n        query_ind = self.__getindices__(query_inst, self.query_instances)\n        ref_ind = self.__getindices__(ref_inst, self.ref_instances)\n\n        try:\n            return self.numpy()[query_ind[:, None], ref_ind].squeeze()\n        except IndexError as e:\n            logger.exception(f\"Query_insts: {type(query_inst)}\")\n            logger.exception(f\"Query_inds: {query_ind}\")\n            logger.exception(f\"Ref_insts: {type(ref_inst)}\")\n            logger.exception(f\"Ref_ind: {ref_ind}\")\n            logger.exception(e)\n            raise (e)\n\n    def __getindices__(\n        self,\n        instance: Instance | int | np.typing.ArrayLike,\n        instance_lookup: list[Instance],\n    ) -&gt; np.ndarray:\n        \"\"\"Get the indices of the instance for lookup.\n\n        Args:\n            instance: The instance(s) to be retrieved\n                Can either be a single int/instance or a list of int/instances\n            instance_lookup: A list of Instances to be used to retrieve indices\n\n        Returns:\n            A np array of indices.\n        \"\"\"\n        if isinstance(instance, Instance):\n            ind = np.array([instance_lookup.index(instance)])\n\n        elif instance is None:\n            ind = np.arange(len(instance_lookup))\n\n        elif np.isscalar(instance):\n            ind = np.array([instance])\n\n        else:\n            instances = instance\n            if not [isinstance(inst, (Instance, int)) for inst in instance]:\n                raise ValueError(\n                    f\"List of indices must be `int` or `Instance`. Found {set([type(inst) for inst in instance])}\"\n                )\n            ind = np.array(\n                [\n                    (\n                        instance_lookup.index(instance)\n                        if isinstance(instance, Instance)\n                        else instance\n                    )\n                    for instance in instances\n                ]\n            )\n\n        return ind\n\n    def get_tracks(\n        self, instances: list[\"Instance\"], label: str = \"pred\"\n    ) -&gt; dict[int, list[\"Instance\"]]:\n        \"\"\"Group instances by track.\n\n        Args:\n            instances: The list of instances to group\n            label: the track id type to group by. Either `pred` or `gt`.\n\n        Returns:\n            A dictionary of track_id:instances\n        \"\"\"\n        if label == \"pred\":\n            traj_ids = set([instance.pred_track_id.item() for instance in instances])\n            traj = {\n                track_id: [\n                    instance\n                    for instance in instances\n                    if instance.pred_track_id.item() == track_id\n                ]\n                for track_id in traj_ids\n            }\n\n        elif label == \"gt\":\n            traj_ids = set(\n                [instance.gt_track_id.item() for instance in self.ref_instances]\n            )\n            traj = {\n                track_id: [\n                    instance\n                    for instance in self.ref_instances\n                    if instance.gt_track_id.item() == track_id\n                ]\n                for track_id in traj_ids\n            }\n\n        else:\n            raise ValueError(f\"Unsupported label '{label}'. Expected 'pred' or 'gt'.\")\n\n        return traj\n</code></pre>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix.__getindices__","title":"<code>__getindices__(instance, instance_lookup)</code>","text":"<p>Get the indices of the instance for lookup.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>Instance | int | ArrayLike</code> <p>The instance(s) to be retrieved Can either be a single int/instance or a list of int/instances</p> required <code>instance_lookup</code> <code>list[Instance]</code> <p>A list of Instances to be used to retrieve indices</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A np array of indices.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __getindices__(\n    self,\n    instance: Instance | int | np.typing.ArrayLike,\n    instance_lookup: list[Instance],\n) -&gt; np.ndarray:\n    \"\"\"Get the indices of the instance for lookup.\n\n    Args:\n        instance: The instance(s) to be retrieved\n            Can either be a single int/instance or a list of int/instances\n        instance_lookup: A list of Instances to be used to retrieve indices\n\n    Returns:\n        A np array of indices.\n    \"\"\"\n    if isinstance(instance, Instance):\n        ind = np.array([instance_lookup.index(instance)])\n\n    elif instance is None:\n        ind = np.arange(len(instance_lookup))\n\n    elif np.isscalar(instance):\n        ind = np.array([instance])\n\n    else:\n        instances = instance\n        if not [isinstance(inst, (Instance, int)) for inst in instance]:\n            raise ValueError(\n                f\"List of indices must be `int` or `Instance`. Found {set([type(inst) for inst in instance])}\"\n            )\n        ind = np.array(\n            [\n                (\n                    instance_lookup.index(instance)\n                    if isinstance(instance, Instance)\n                    else instance\n                )\n                for instance in instances\n            ]\n        )\n\n    return ind\n</code></pre>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix.__getitem__","title":"<code>__getitem__(inds)</code>","text":"<p>Get elements of the association matrix.</p> <p>Parameters:</p> Name Type Description Default <code>inds</code> <code>tuple[int | Instance | list[int | Instance]]</code> <p>A tuple of query indices and reference indices. Indices can be either:     A single instance or integer.     A list of instances or integers.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>An np.ndarray containing the elements requested.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __getitem__(\n    self, inds: tuple[int | Instance | list[int | Instance]]\n) -&gt; np.ndarray:\n    \"\"\"Get elements of the association matrix.\n\n    Args:\n        inds: A tuple of query indices and reference indices.\n            Indices can be either:\n                A single instance or integer.\n                A list of instances or integers.\n\n    Returns:\n        An np.ndarray containing the elements requested.\n    \"\"\"\n    query_inst, ref_inst = inds\n\n    query_ind = self.__getindices__(query_inst, self.query_instances)\n    ref_ind = self.__getindices__(ref_inst, self.ref_instances)\n\n    try:\n        return self.numpy()[query_ind[:, None], ref_ind].squeeze()\n    except IndexError as e:\n        logger.exception(f\"Query_insts: {type(query_inst)}\")\n        logger.exception(f\"Query_inds: {query_ind}\")\n        logger.exception(f\"Ref_insts: {type(ref_inst)}\")\n        logger.exception(f\"Ref_ind: {ref_ind}\")\n        logger.exception(e)\n        raise (e)\n</code></pre>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix.__repr__","title":"<code>__repr__()</code>","text":"<p>Get the string representation of the Association Matrix.</p> <p>Returns:</p> Type Description <code>str</code> <p>the string representation of the association matrix.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get the string representation of the Association Matrix.\n\n    Returns:\n        the string representation of the association matrix.\n    \"\"\"\n    return (\n        f\"AssociationMatrix({self.matrix},\"\n        f\"query_instances={len(self.query_instances)},\"\n        f\"ref_instances={len(self.ref_instances)})\"\n    )\n</code></pre>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix.get_tracks","title":"<code>get_tracks(instances, label='pred')</code>","text":"<p>Group instances by track.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[Instance]</code> <p>The list of instances to group</p> required <code>label</code> <code>str</code> <p>the track id type to group by. Either <code>pred</code> or <code>gt</code>.</p> <code>'pred'</code> <p>Returns:</p> Type Description <code>dict[int, list[Instance]]</code> <p>A dictionary of track_id:instances</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def get_tracks(\n    self, instances: list[\"Instance\"], label: str = \"pred\"\n) -&gt; dict[int, list[\"Instance\"]]:\n    \"\"\"Group instances by track.\n\n    Args:\n        instances: The list of instances to group\n        label: the track id type to group by. Either `pred` or `gt`.\n\n    Returns:\n        A dictionary of track_id:instances\n    \"\"\"\n    if label == \"pred\":\n        traj_ids = set([instance.pred_track_id.item() for instance in instances])\n        traj = {\n            track_id: [\n                instance\n                for instance in instances\n                if instance.pred_track_id.item() == track_id\n            ]\n            for track_id in traj_ids\n        }\n\n    elif label == \"gt\":\n        traj_ids = set(\n            [instance.gt_track_id.item() for instance in self.ref_instances]\n        )\n        traj = {\n            track_id: [\n                instance\n                for instance in self.ref_instances\n                if instance.gt_track_id.item() == track_id\n            ]\n            for track_id in traj_ids\n        }\n\n    else:\n        raise ValueError(f\"Unsupported label '{label}'. Expected 'pred' or 'gt'.\")\n\n    return traj\n</code></pre>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix.numpy","title":"<code>numpy()</code>","text":"<p>Convert association matrix to a numpy array.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The association matrix as a numpy array.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def numpy(self) -&gt; np.ndarray:\n    \"\"\"Convert association matrix to a numpy array.\n\n    Returns:\n        The association matrix as a numpy array.\n    \"\"\"\n    if isinstance(self.matrix, torch.Tensor):\n        return self.matrix.detach().cpu().numpy()\n    return self.matrix\n</code></pre>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix.reduce","title":"<code>reduce(row_dims='instance', col_dims='track', row_grouping=None, col_grouping='pred', reduce_method=np.sum)</code>","text":"<p>Aggregate the association matrix by specified dimensions and grouping.</p> <p>Parameters:</p> Name Type Description Default <code>row_dims</code> <code>str</code> <p>A str indicating how to what dimensions to reduce rows to.   Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).</p> <code>'instance'</code> <code>col_dims</code> <code>str</code> <p>A str indicating how to dimensions to reduce rows to.   Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)</p> <code>'track'</code> <code>row_grouping</code> <code>str | None</code> <p>A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".</p> <code>None</code> <code>col_grouping</code> <code>str</code> <p>A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".</p> <code>'pred'</code> <code>reduce_method</code> <code>callable</code> <p>A callable function that operates on numpy matrices and can take an <code>axis</code> arg for reducing.</p> <code>sum</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def reduce(\n    self,\n    row_dims: str = \"instance\",\n    col_dims: str = \"track\",\n    row_grouping: str | None = None,\n    col_grouping: str = \"pred\",\n    reduce_method: callable = np.sum,\n) -&gt; pd.DataFrame:\n    \"\"\"Aggregate the association matrix by specified dimensions and grouping.\n\n    Args:\n       row_dims: A str indicating how to what dimensions to reduce rows to.\n            Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).\n       col_dims: A str indicating how to dimensions to reduce rows to.\n            Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)\n       row_grouping: A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".\n       col_grouping: A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".\n       reduce_method: A callable function that operates on numpy matrices and can take an `axis` arg for reducing.\n\n    Returns:\n        The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.\n    \"\"\"\n    n_rows = len(self.query_instances)\n    n_cols = len(self.ref_instances)\n\n    col_tracks = {-1: self.ref_instances}\n    row_tracks = {-1: self.query_instances}\n\n    col_inds = [i for i in range(len(self.ref_instances))]\n    row_inds = [i for i in range(len(self.query_instances))]\n\n    if col_dims == \"track\":\n        col_tracks = self.get_tracks(self.ref_instances, col_grouping)\n        col_inds = list(col_tracks.keys())\n        n_cols = len(col_inds)\n\n    if row_dims == \"track\":\n        row_tracks = self.get_tracks(self.query_instances, row_grouping)\n        row_inds = list(row_tracks.keys())\n        n_rows = len(row_inds)\n\n    reduced_matrix = []\n    for row_track, row_instances in row_tracks.items():\n        for col_track, col_instances in col_tracks.items():\n\n            asso_matrix = self[row_instances, col_instances]\n\n            if col_dims == \"track\":\n                asso_matrix = reduce_method(asso_matrix, axis=1)\n\n            if row_dims == \"track\":\n                asso_matrix = reduce_method(asso_matrix, axis=0)\n\n            reduced_matrix.append(asso_matrix)\n\n    reduced_matrix = np.array(reduced_matrix).reshape(n_cols, n_rows).T\n\n    return pd.DataFrame(reduced_matrix, index=row_inds, columns=col_inds)\n</code></pre>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix.to_dataframe","title":"<code>to_dataframe(row_labels='gt', col_labels='gt')</code>","text":"<p>Convert the association matrix to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>row_labels</code> <code>str</code> <p>How to label the rows(queries). If list, then must match # of rows/queries If <code>\"gt\"</code> then label by gt track id. If <code>\"pred\"</code> then label by pred track id. Otherwise label by the query_instance indices</p> <code>'gt'</code> <code>col_labels</code> <code>str</code> <p>How to label the columns(references). If list, then must match # of columns/refs If <code>\"gt\"</code> then label by gt track id. If <code>\"pred\"</code> then label by pred track id. Otherwise label by the ref_instance indices</p> <code>'gt'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The association matrix as a pandas dataframe.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def to_dataframe(\n    self, row_labels: str = \"gt\", col_labels: str = \"gt\"\n) -&gt; pd.DataFrame:\n    \"\"\"Convert the association matrix to a pandas DataFrame.\n\n    Args:\n        row_labels: How to label the rows(queries).\n            If list, then must match # of rows/queries\n            If `\"gt\"` then label by gt track id.\n            If `\"pred\"` then label by pred track id.\n            Otherwise label by the query_instance indices\n        col_labels: How to label the columns(references).\n            If list, then must match # of columns/refs\n            If `\"gt\"` then label by gt track id.\n            If `\"pred\"` then label by pred track id.\n            Otherwise label by the ref_instance indices\n\n    Returns:\n        The association matrix as a pandas dataframe.\n    \"\"\"\n    matrix = self.numpy()\n\n    if not isinstance(row_labels, str):\n        if len(row_labels) == len(self.query_instances):\n            row_inds = row_labels\n\n        else:\n            raise ValueError(\n                (\n                    f\"Mismatched # of rows and labels!\",\n                    f\"Found {len(row_labels)} with {len(self.query_instances)} rows\",\n                )\n            )\n\n    else:\n        if row_labels == \"gt\":\n            row_inds = [\n                instance.gt_track_id.item() for instance in self.query_instances\n            ]\n\n        elif row_labels == \"pred\":\n            row_inds = [\n                instance.pred_track_id.item() for instance in self.query_instances\n            ]\n\n        else:\n            row_inds = np.arange(len(self.query_instances))\n\n    if not isinstance(col_labels, str):\n        if len(col_labels) == len(self.ref_instances):\n            col_inds = col_labels\n\n        else:\n            raise ValueError(\n                (\n                    f\"Mismatched # of columns and labels!\",\n                    f\"Found {len(col_labels)} with {len(self.ref_instances)} columns\",\n                )\n            )\n\n    else:\n        if col_labels == \"gt\":\n            col_inds = [\n                instance.gt_track_id.item() for instance in self.ref_instances\n            ]\n\n        elif col_labels == \"pred\":\n            col_inds = [\n                instance.pred_track_id.item() for instance in self.ref_instances\n            ]\n\n        else:\n            col_inds = np.arange(len(self.ref_instances))\n\n    asso_df = pd.DataFrame(matrix, index=row_inds, columns=col_inds)\n\n    return asso_df\n</code></pre>"},{"location":"reference/dreem/io/config/","title":"config","text":""},{"location":"reference/dreem/io/config/#dreem.io.config","title":"<code>dreem.io.config</code>","text":"<p>Data structures for handling config parsing.</p>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config","title":"<code>Config</code>","text":"<p>Class handling loading components based on config params.</p> Source code in <code>dreem/io/config.py</code> <pre><code>class Config:\n    \"\"\"Class handling loading components based on config params.\"\"\"\n\n    def __init__(self, cfg: DictConfig, params_cfg: DictConfig | None = None):\n        \"\"\"Initialize the class with config from hydra/omega conf.\n\n        First uses `base_param` file then overwrites with specific `params_config`.\n\n        Args:\n            cfg: The `DictConfig` containing all the hyperparameters needed for\n                training/evaluation.\n            params_cfg: The `DictConfig` containing subset of hyperparameters to override.\n                training/evaluation\n        \"\"\"\n        base_cfg = cfg\n        logger.info(f\"Base Config: {cfg}\")\n\n        if \"params_config\" in cfg:\n            params_cfg = OmegaConf.load(cfg.params_config)\n\n        if params_cfg:\n            logger.info(f\"Overwriting base config with {params_cfg}\")\n            with open_dict(base_cfg):\n                self.cfg = OmegaConf.merge(base_cfg, params_cfg)  # merge configs\n        else:\n            self.cfg = cfg\n\n    def __repr__(self):\n        \"\"\"Object representation of config class.\"\"\"\n        return f\"Config({self.cfg})\"\n\n    def __str__(self):\n        \"\"\"Return a string representation of config class.\"\"\"\n        return f\"Config({self.cfg})\"\n\n    @classmethod\n    def from_yaml(cls, base_cfg_path: str, params_cfg_path: str | None = None) -&gt; None:\n        \"\"\"Load config directly from yaml.\n\n        Args:\n            base_cfg_path: path to base config file.\n            params_cfg_path: path to override params.\n        \"\"\"\n        base_cfg = OmegaConf.load(base_cfg_path)\n        params_cfg = OmegaConf.load(params_cfg_path) if params_cfg else None\n        return cls(base_cfg, params_cfg)\n\n    def set_hparams(self, hparams: dict) -&gt; bool:\n        \"\"\"Setter function for overwriting specific hparams.\n\n        Useful for changing 1 or 2 hyperparameters such as dataset.\n\n        Args:\n            hparams: A dict containing the hyperparameter to be overwritten and\n                the value to be changed\n\n        Returns:\n            `True` if config is successfully updated, `False` otherwise\n        \"\"\"\n        if hparams == {} or hparams is None:\n            logger.warning(\"Nothing to update!\")\n            return False\n        for hparam, val in hparams.items():\n            try:\n                OmegaConf.update(self.cfg, hparam, val)\n            except Exception as e:\n                logger.exception(f\"Failed to update {hparam} to {val} due to {e}\")\n                return False\n        return True\n\n    def get_model(self) -&gt; \"GlobalTrackingTransformer\":\n        \"\"\"Getter for gtr model.\n\n        Returns:\n            A global tracking transformer with parameters indicated by cfg\n        \"\"\"\n        from dreem.models import GlobalTrackingTransformer\n\n        model_params = self.cfg.model\n        with open_dict(model_params):\n            ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n        if ckpt_path is not None and len(ckpt_path) &gt; 0:\n            return GTRRunner.load_from_checkpoint(ckpt_path).model\n\n        return GlobalTrackingTransformer(**model_params)\n\n    def get_tracker_cfg(self) -&gt; dict:\n        \"\"\"Getter for tracker config params.\n\n        Returns:\n            A dict containing the init params for `Tracker`.\n        \"\"\"\n        tracker_params = self.cfg.tracker\n        tracker_cfg = {}\n        for key, val in tracker_params.items():\n            tracker_cfg[key] = val\n        return tracker_cfg\n\n    def get_gtr_runner(self, ckpt_path=None) -&gt; \"GTRRunner\":\n        \"\"\"Get lightning module for training, validation, and inference.\n\n        Args:\n            ckpt_path: path to checkpoint for override\n\n        Returns:\n            a gtr runner model\n        \"\"\"\n        from dreem.models import GTRRunner\n\n        tracker_params = self.cfg.tracker\n        optimizer_params = self.cfg.optimizer\n        scheduler_params = self.cfg.scheduler\n        loss_params = self.cfg.loss\n        gtr_runner_params = self.cfg.runner\n        model_params = self.cfg.model\n\n        if ckpt_path is None:\n            with open_dict(model_params):\n                ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n        if ckpt_path is not None and ckpt_path != \"\":\n            model = GTRRunner.load_from_checkpoint(\n                ckpt_path,\n                tracker_cfg=tracker_params,\n                train_metrics=self.cfg.runner.metrics.train,\n                val_metrics=self.cfg.runner.metrics.val,\n                test_metrics=self.cfg.runner.metrics.test,\n                test_save_path=self.cfg.runner.save_path,\n            )\n\n        else:\n            model = GTRRunner(\n                model_params,\n                tracker_params,\n                loss_params,\n                optimizer_params,\n                scheduler_params,\n                **gtr_runner_params,\n            )\n\n        return model\n\n    def get_data_paths(self, data_cfg: dict) -&gt; tuple[list[str], list[str]]:\n        \"\"\"Get file paths from directory.\n\n        Args:\n            data_cfg: Config for the dataset containing \"dir\" key.\n\n        Returns:\n            lists of labels file paths and video file paths respectively\n        \"\"\"\n        with open_dict(data_cfg):\n            dir_cfg = data_cfg.pop(\"dir\", None)\n        label_files = vid_files = None\n        if dir_cfg:\n            labels_suff = dir_cfg.labels_suffix\n            vid_suff = dir_cfg.vid_suffix\n            labels_path = f\"{dir_cfg.path}/*{labels_suff}\"\n            vid_path = f\"{dir_cfg.path}/*{vid_suff}\"\n            logger.debug(f\"Searching for labels matching {labels_path}\")\n            label_files = glob.glob(labels_path)\n            logger.debug(f\"Searching for videos matching {vid_path}\")\n            vid_files = glob.glob(vid_path)\n            logger.debug(f\"Found {len(label_files)} labels and {len(vid_files)} videos\")\n\n        else:\n            if \"slp_files\" in data_cfg:\n                label_files = data_cfg.slp_files\n                vid_files = data_cfg.video_files\n            elif \"tracks\" in data_cfg or \"source\" in data_cfg:\n                label_files = data_cfg.tracks\n                vid_files = data_cfg.videos\n            elif \"raw_images\" in data_cfg:\n                label_files = data_cfg.gt_images\n                vid_files = data_cfg.raw_images\n\n        return label_files, vid_files\n\n    def get_dataset(\n        self,\n        mode: str,\n        label_files: list[str] | None = None,\n        vid_files: list[str | list[str]] = None,\n    ) -&gt; \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\":\n        \"\"\"Getter for datasets.\n\n        Args:\n            mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n                train, val, or test params for dataset\n            label_files: path to label_files for override\n            vid_files: path to vid_files for override\n\n        Returns:\n            Either a `SleapDataset` or `MicroscopyDataset` with params indicated by cfg\n        \"\"\"\n        from dreem.datasets import MicroscopyDataset, SleapDataset, CellTrackingDataset\n\n        if mode.lower() == \"train\":\n            dataset_params = self.cfg.dataset.train_dataset\n        elif mode.lower() == \"val\":\n            dataset_params = self.cfg.dataset.val_dataset\n        elif mode.lower() == \"test\":\n            dataset_params = self.cfg.dataset.test_dataset\n        else:\n            raise ValueError(\n                \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n            )\n        if label_files is None or vid_files is None:\n            with open_dict(dataset_params):\n                label_files, vid_files = self.get_data_paths(dataset_params)\n        # todo: handle this better\n        if \"slp_files\" in dataset_params:\n            if label_files is not None:\n                dataset_params.slp_files = label_files\n            if vid_files is not None:\n                dataset_params.video_files = vid_files\n            return SleapDataset(**dataset_params)\n\n        elif \"tracks\" in dataset_params or \"source\" in dataset_params:\n            if label_files is not None:\n                dataset_params.tracks = label_files\n            if vid_files is not None:\n                dataset_params.videos = vid_files\n            return MicroscopyDataset(**dataset_params)\n\n        elif \"raw_images\" in dataset_params:\n            if label_files is not None:\n                dataset_params.gt_images = label_files\n            if vid_files is not None:\n                dataset_params.raw_images = vid_files\n            return CellTrackingDataset(**dataset_params)\n\n        else:\n            raise ValueError(\n                \"Could not resolve dataset type from Config! Please include \\\n                either `slp_files` or `tracks`/`source`\"\n            )\n\n    def get_dataloader(\n        self,\n        dataset: \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\",\n        mode: str,\n    ) -&gt; torch.utils.data.DataLoader:\n        \"\"\"Getter for dataloader.\n\n        Args:\n            dataset: the Sleap or Microscopy Dataset used to initialize the dataloader\n            mode: either [\"train\", \"val\", or \"test\"] indicates which dataset\n                config to use\n\n        Returns:\n            A torch dataloader for `dataset` with parameters configured as specified\n        \"\"\"\n        if mode.lower() == \"train\":\n            dataloader_params = self.cfg.dataloader.train_dataloader\n        elif mode.lower() == \"val\":\n            dataloader_params = self.cfg.dataloader.val_dataloader\n        elif mode.lower() == \"test\":\n            dataloader_params = self.cfg.dataloader.test_dataloader\n        else:\n            raise ValueError(\n                \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n            )\n        if dataloader_params.num_workers &gt; 0:\n            # prevent too many open files error\n            pin_memory = True\n            torch.multiprocessing.set_sharing_strategy(\"file_system\")\n        else:\n            pin_memory = False\n\n        return torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size=1,\n            pin_memory=pin_memory,\n            collate_fn=dataset.no_batching_fn,\n            **dataloader_params,\n        )\n\n    def get_optimizer(self, params: Iterable) -&gt; torch.optim.Optimizer:\n        \"\"\"Getter for optimizer.\n\n        Args:\n            params: iterable of model parameters to optimize or dicts defining\n                parameter groups\n\n        Returns:\n            A torch Optimizer with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_optimizer\n\n        optimizer_params = self.cfg.optimizer\n\n        return init_optimizer(params, optimizer_params)\n\n    def get_scheduler(\n        self, optimizer: torch.optim.Optimizer\n    ) -&gt; torch.optim.lr_scheduler.LRScheduler:\n        \"\"\"Getter for lr scheduler.\n\n        Args:\n            optimizer: The optimizer to wrap the scheduler around\n\n        Returns:\n            A torch learning rate scheduler with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_scheduler\n\n        lr_scheduler_params = self.cfg.scheduler\n\n        return init_scheduler(optimizer, lr_scheduler_params)\n\n    def get_loss(self) -&gt; \"dreem.training.losses.AssoLoss\":\n        \"\"\"Getter for loss functions.\n\n        Returns:\n            An AssoLoss with specified params\n        \"\"\"\n        from dreem.training.losses import AssoLoss\n\n        loss_params = self.cfg.loss\n\n        return AssoLoss(**loss_params)\n\n    def get_logger(self) -&gt; pl.loggers.Logger:\n        \"\"\"Getter for logging callback.\n\n        Returns:\n            A Logger with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_logger\n\n        logger_params = OmegaConf.to_container(self.cfg.logging, resolve=True)\n\n        return init_logger(\n            logger_params, OmegaConf.to_container(self.cfg, resolve=True)\n        )\n\n    def get_early_stopping(self) -&gt; pl.callbacks.EarlyStopping:\n        \"\"\"Getter for lightning early stopping callback.\n\n        Returns:\n            A lightning early stopping callback with specified params\n        \"\"\"\n        early_stopping_params = self.cfg.early_stopping\n        return pl.callbacks.EarlyStopping(**early_stopping_params)\n\n    def get_checkpointing(self) -&gt; pl.callbacks.ModelCheckpoint:\n        \"\"\"Getter for lightning checkpointing callback.\n\n        Returns:\n            A lightning checkpointing callback with specified params\n        \"\"\"\n        # convert to dict to enable extracting/removing params\n        checkpoint_params = self.cfg.checkpointing\n        logging_params = self.cfg.logging\n        dirpath = checkpoint_params.pop(\"dirpath\", None)\n        if dirpath is None:\n            if \"group\" in logging_params:\n                dirpath = f\"./models/{logging_params.group}/{logging_params.name}\"\n            else:\n                dirpath = f\"./models/{logging_params.name}\"\n\n        dirpath = Path(dirpath).resolve()\n        if not Path(dirpath).exists():\n            try:\n                Path(dirpath).mkdir(parents=True, exist_ok=True)\n            except OSError as e:\n                logger.exception(\n                    f\"Cannot create a new folder. Check the permissions to the given Checkpoint directory. \\n {e}\"\n                )\n        with open_dict(checkpoint_params):\n            _ = checkpoint_params.pop(\"dirpath\", None)\n            monitor = checkpoint_params.pop(\"monitor\", [\"val_loss\"])\n        checkpointers = []\n\n        for metric in monitor:\n            checkpointer = pl.callbacks.ModelCheckpoint(\n                monitor=metric,\n                dirpath=dirpath,\n                filename=f\"{{epoch}}-{{{metric}}}\",\n                **checkpoint_params,\n            )\n            checkpointer.CHECKPOINT_NAME_LAST = f\"{{epoch}}-best-{{{metric}}}\"\n            checkpointers.append(checkpointer)\n        return checkpointers\n\n    def get_trainer(\n        self,\n        callbacks: list[pl.callbacks.Callback] | None = None,\n        logger: pl.loggers.WandbLogger | None = None,\n        devices: int = 1,\n        accelerator: str = \"auto\",\n    ) -&gt; pl.Trainer:\n        \"\"\"Getter for the lightning trainer.\n\n        Args:\n            callbacks: a list of lightning callbacks preconfigured to be used\n                for training\n            logger: the Wandb logger used for logging during training\n            devices: The number of gpus to be used. 0 means cpu\n            accelerator: either \"gpu\" or \"cpu\" specifies which device to use\n\n        Returns:\n            A lightning Trainer with specified params\n        \"\"\"\n        if \"trainer\" in self.cfg:\n            trainer_params = OmegaConf.to_container(self.cfg.trainer, resolve=True)\n\n        else:\n            trainer_params = {}\n\n        profiler = trainer_params.pop(\"profiler\", None)\n        if \"accelerator\" not in trainer_params:\n            trainer_params[\"accelerator\"] = accelerator\n        if \"devices\" not in trainer_params:\n            trainer_params[\"devices\"] = devices\n\n        if \"profiler\":\n            profiler = pl.profilers.AdvancedProfiler(filename=\"profile.txt\")\n        else:\n            profiler = None\n\n        return pl.Trainer(\n            callbacks=callbacks,\n            logger=logger,\n            profiler=profiler,\n            **trainer_params,\n        )\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.__init__","title":"<code>__init__(cfg, params_cfg=None)</code>","text":"<p>Initialize the class with config from hydra/omega conf.</p> <p>First uses <code>base_param</code> file then overwrites with specific <code>params_config</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>The <code>DictConfig</code> containing all the hyperparameters needed for training/evaluation.</p> required <code>params_cfg</code> <code>DictConfig | None</code> <p>The <code>DictConfig</code> containing subset of hyperparameters to override. training/evaluation</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>def __init__(self, cfg: DictConfig, params_cfg: DictConfig | None = None):\n    \"\"\"Initialize the class with config from hydra/omega conf.\n\n    First uses `base_param` file then overwrites with specific `params_config`.\n\n    Args:\n        cfg: The `DictConfig` containing all the hyperparameters needed for\n            training/evaluation.\n        params_cfg: The `DictConfig` containing subset of hyperparameters to override.\n            training/evaluation\n    \"\"\"\n    base_cfg = cfg\n    logger.info(f\"Base Config: {cfg}\")\n\n    if \"params_config\" in cfg:\n        params_cfg = OmegaConf.load(cfg.params_config)\n\n    if params_cfg:\n        logger.info(f\"Overwriting base config with {params_cfg}\")\n        with open_dict(base_cfg):\n            self.cfg = OmegaConf.merge(base_cfg, params_cfg)  # merge configs\n    else:\n        self.cfg = cfg\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.__repr__","title":"<code>__repr__()</code>","text":"<p>Object representation of config class.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def __repr__(self):\n    \"\"\"Object representation of config class.\"\"\"\n    return f\"Config({self.cfg})\"\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of config class.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def __str__(self):\n    \"\"\"Return a string representation of config class.\"\"\"\n    return f\"Config({self.cfg})\"\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.from_yaml","title":"<code>from_yaml(base_cfg_path, params_cfg_path=None)</code>  <code>classmethod</code>","text":"<p>Load config directly from yaml.</p> <p>Parameters:</p> Name Type Description Default <code>base_cfg_path</code> <code>str</code> <p>path to base config file.</p> required <code>params_cfg_path</code> <code>str | None</code> <p>path to override params.</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>@classmethod\ndef from_yaml(cls, base_cfg_path: str, params_cfg_path: str | None = None) -&gt; None:\n    \"\"\"Load config directly from yaml.\n\n    Args:\n        base_cfg_path: path to base config file.\n        params_cfg_path: path to override params.\n    \"\"\"\n    base_cfg = OmegaConf.load(base_cfg_path)\n    params_cfg = OmegaConf.load(params_cfg_path) if params_cfg else None\n    return cls(base_cfg, params_cfg)\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_checkpointing","title":"<code>get_checkpointing()</code>","text":"<p>Getter for lightning checkpointing callback.</p> <p>Returns:</p> Type Description <code>ModelCheckpoint</code> <p>A lightning checkpointing callback with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_checkpointing(self) -&gt; pl.callbacks.ModelCheckpoint:\n    \"\"\"Getter for lightning checkpointing callback.\n\n    Returns:\n        A lightning checkpointing callback with specified params\n    \"\"\"\n    # convert to dict to enable extracting/removing params\n    checkpoint_params = self.cfg.checkpointing\n    logging_params = self.cfg.logging\n    dirpath = checkpoint_params.pop(\"dirpath\", None)\n    if dirpath is None:\n        if \"group\" in logging_params:\n            dirpath = f\"./models/{logging_params.group}/{logging_params.name}\"\n        else:\n            dirpath = f\"./models/{logging_params.name}\"\n\n    dirpath = Path(dirpath).resolve()\n    if not Path(dirpath).exists():\n        try:\n            Path(dirpath).mkdir(parents=True, exist_ok=True)\n        except OSError as e:\n            logger.exception(\n                f\"Cannot create a new folder. Check the permissions to the given Checkpoint directory. \\n {e}\"\n            )\n    with open_dict(checkpoint_params):\n        _ = checkpoint_params.pop(\"dirpath\", None)\n        monitor = checkpoint_params.pop(\"monitor\", [\"val_loss\"])\n    checkpointers = []\n\n    for metric in monitor:\n        checkpointer = pl.callbacks.ModelCheckpoint(\n            monitor=metric,\n            dirpath=dirpath,\n            filename=f\"{{epoch}}-{{{metric}}}\",\n            **checkpoint_params,\n        )\n        checkpointer.CHECKPOINT_NAME_LAST = f\"{{epoch}}-best-{{{metric}}}\"\n        checkpointers.append(checkpointer)\n    return checkpointers\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_data_paths","title":"<code>get_data_paths(data_cfg)</code>","text":"<p>Get file paths from directory.</p> <p>Parameters:</p> Name Type Description Default <code>data_cfg</code> <code>dict</code> <p>Config for the dataset containing \"dir\" key.</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str]]</code> <p>lists of labels file paths and video file paths respectively</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_data_paths(self, data_cfg: dict) -&gt; tuple[list[str], list[str]]:\n    \"\"\"Get file paths from directory.\n\n    Args:\n        data_cfg: Config for the dataset containing \"dir\" key.\n\n    Returns:\n        lists of labels file paths and video file paths respectively\n    \"\"\"\n    with open_dict(data_cfg):\n        dir_cfg = data_cfg.pop(\"dir\", None)\n    label_files = vid_files = None\n    if dir_cfg:\n        labels_suff = dir_cfg.labels_suffix\n        vid_suff = dir_cfg.vid_suffix\n        labels_path = f\"{dir_cfg.path}/*{labels_suff}\"\n        vid_path = f\"{dir_cfg.path}/*{vid_suff}\"\n        logger.debug(f\"Searching for labels matching {labels_path}\")\n        label_files = glob.glob(labels_path)\n        logger.debug(f\"Searching for videos matching {vid_path}\")\n        vid_files = glob.glob(vid_path)\n        logger.debug(f\"Found {len(label_files)} labels and {len(vid_files)} videos\")\n\n    else:\n        if \"slp_files\" in data_cfg:\n            label_files = data_cfg.slp_files\n            vid_files = data_cfg.video_files\n        elif \"tracks\" in data_cfg or \"source\" in data_cfg:\n            label_files = data_cfg.tracks\n            vid_files = data_cfg.videos\n        elif \"raw_images\" in data_cfg:\n            label_files = data_cfg.gt_images\n            vid_files = data_cfg.raw_images\n\n    return label_files, vid_files\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_dataloader","title":"<code>get_dataloader(dataset, mode)</code>","text":"<p>Getter for dataloader.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>'SleapDataset' | 'MicroscopyDataset' | 'CellTrackingDataset'</code> <p>the Sleap or Microscopy Dataset used to initialize the dataloader</p> required <code>mode</code> <code>str</code> <p>either [\"train\", \"val\", or \"test\"] indicates which dataset config to use</p> required <p>Returns:</p> Type Description <code>DataLoader</code> <p>A torch dataloader for <code>dataset</code> with parameters configured as specified</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_dataloader(\n    self,\n    dataset: \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\",\n    mode: str,\n) -&gt; torch.utils.data.DataLoader:\n    \"\"\"Getter for dataloader.\n\n    Args:\n        dataset: the Sleap or Microscopy Dataset used to initialize the dataloader\n        mode: either [\"train\", \"val\", or \"test\"] indicates which dataset\n            config to use\n\n    Returns:\n        A torch dataloader for `dataset` with parameters configured as specified\n    \"\"\"\n    if mode.lower() == \"train\":\n        dataloader_params = self.cfg.dataloader.train_dataloader\n    elif mode.lower() == \"val\":\n        dataloader_params = self.cfg.dataloader.val_dataloader\n    elif mode.lower() == \"test\":\n        dataloader_params = self.cfg.dataloader.test_dataloader\n    else:\n        raise ValueError(\n            \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n        )\n    if dataloader_params.num_workers &gt; 0:\n        # prevent too many open files error\n        pin_memory = True\n        torch.multiprocessing.set_sharing_strategy(\"file_system\")\n    else:\n        pin_memory = False\n\n    return torch.utils.data.DataLoader(\n        dataset=dataset,\n        batch_size=1,\n        pin_memory=pin_memory,\n        collate_fn=dataset.no_batching_fn,\n        **dataloader_params,\n    )\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_dataset","title":"<code>get_dataset(mode, label_files=None, vid_files=None)</code>","text":"<p>Getter for datasets.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>[None, \"train\", \"test\", \"val\"]. Indicates whether to use train, val, or test params for dataset</p> required <code>label_files</code> <code>list[str] | None</code> <p>path to label_files for override</p> <code>None</code> <code>vid_files</code> <code>list[str | list[str]]</code> <p>path to vid_files for override</p> <code>None</code> <p>Returns:</p> Type Description <code>'SleapDataset' | 'MicroscopyDataset' | 'CellTrackingDataset'</code> <p>Either a <code>SleapDataset</code> or <code>MicroscopyDataset</code> with params indicated by cfg</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_dataset(\n    self,\n    mode: str,\n    label_files: list[str] | None = None,\n    vid_files: list[str | list[str]] = None,\n) -&gt; \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\":\n    \"\"\"Getter for datasets.\n\n    Args:\n        mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n            train, val, or test params for dataset\n        label_files: path to label_files for override\n        vid_files: path to vid_files for override\n\n    Returns:\n        Either a `SleapDataset` or `MicroscopyDataset` with params indicated by cfg\n    \"\"\"\n    from dreem.datasets import MicroscopyDataset, SleapDataset, CellTrackingDataset\n\n    if mode.lower() == \"train\":\n        dataset_params = self.cfg.dataset.train_dataset\n    elif mode.lower() == \"val\":\n        dataset_params = self.cfg.dataset.val_dataset\n    elif mode.lower() == \"test\":\n        dataset_params = self.cfg.dataset.test_dataset\n    else:\n        raise ValueError(\n            \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n        )\n    if label_files is None or vid_files is None:\n        with open_dict(dataset_params):\n            label_files, vid_files = self.get_data_paths(dataset_params)\n    # todo: handle this better\n    if \"slp_files\" in dataset_params:\n        if label_files is not None:\n            dataset_params.slp_files = label_files\n        if vid_files is not None:\n            dataset_params.video_files = vid_files\n        return SleapDataset(**dataset_params)\n\n    elif \"tracks\" in dataset_params or \"source\" in dataset_params:\n        if label_files is not None:\n            dataset_params.tracks = label_files\n        if vid_files is not None:\n            dataset_params.videos = vid_files\n        return MicroscopyDataset(**dataset_params)\n\n    elif \"raw_images\" in dataset_params:\n        if label_files is not None:\n            dataset_params.gt_images = label_files\n        if vid_files is not None:\n            dataset_params.raw_images = vid_files\n        return CellTrackingDataset(**dataset_params)\n\n    else:\n        raise ValueError(\n            \"Could not resolve dataset type from Config! Please include \\\n            either `slp_files` or `tracks`/`source`\"\n        )\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_early_stopping","title":"<code>get_early_stopping()</code>","text":"<p>Getter for lightning early stopping callback.</p> <p>Returns:</p> Type Description <code>EarlyStopping</code> <p>A lightning early stopping callback with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_early_stopping(self) -&gt; pl.callbacks.EarlyStopping:\n    \"\"\"Getter for lightning early stopping callback.\n\n    Returns:\n        A lightning early stopping callback with specified params\n    \"\"\"\n    early_stopping_params = self.cfg.early_stopping\n    return pl.callbacks.EarlyStopping(**early_stopping_params)\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_gtr_runner","title":"<code>get_gtr_runner(ckpt_path=None)</code>","text":"<p>Get lightning module for training, validation, and inference.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_path</code> <p>path to checkpoint for override</p> <code>None</code> <p>Returns:</p> Type Description <code>'GTRRunner'</code> <p>a gtr runner model</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_gtr_runner(self, ckpt_path=None) -&gt; \"GTRRunner\":\n    \"\"\"Get lightning module for training, validation, and inference.\n\n    Args:\n        ckpt_path: path to checkpoint for override\n\n    Returns:\n        a gtr runner model\n    \"\"\"\n    from dreem.models import GTRRunner\n\n    tracker_params = self.cfg.tracker\n    optimizer_params = self.cfg.optimizer\n    scheduler_params = self.cfg.scheduler\n    loss_params = self.cfg.loss\n    gtr_runner_params = self.cfg.runner\n    model_params = self.cfg.model\n\n    if ckpt_path is None:\n        with open_dict(model_params):\n            ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n    if ckpt_path is not None and ckpt_path != \"\":\n        model = GTRRunner.load_from_checkpoint(\n            ckpt_path,\n            tracker_cfg=tracker_params,\n            train_metrics=self.cfg.runner.metrics.train,\n            val_metrics=self.cfg.runner.metrics.val,\n            test_metrics=self.cfg.runner.metrics.test,\n            test_save_path=self.cfg.runner.save_path,\n        )\n\n    else:\n        model = GTRRunner(\n            model_params,\n            tracker_params,\n            loss_params,\n            optimizer_params,\n            scheduler_params,\n            **gtr_runner_params,\n        )\n\n    return model\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_logger","title":"<code>get_logger()</code>","text":"<p>Getter for logging callback.</p> <p>Returns:</p> Type Description <code>Logger</code> <p>A Logger with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_logger(self) -&gt; pl.loggers.Logger:\n    \"\"\"Getter for logging callback.\n\n    Returns:\n        A Logger with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_logger\n\n    logger_params = OmegaConf.to_container(self.cfg.logging, resolve=True)\n\n    return init_logger(\n        logger_params, OmegaConf.to_container(self.cfg, resolve=True)\n    )\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_loss","title":"<code>get_loss()</code>","text":"<p>Getter for loss functions.</p> <p>Returns:</p> Type Description <code>'dreem.training.losses.AssoLoss'</code> <p>An AssoLoss with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_loss(self) -&gt; \"dreem.training.losses.AssoLoss\":\n    \"\"\"Getter for loss functions.\n\n    Returns:\n        An AssoLoss with specified params\n    \"\"\"\n    from dreem.training.losses import AssoLoss\n\n    loss_params = self.cfg.loss\n\n    return AssoLoss(**loss_params)\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_model","title":"<code>get_model()</code>","text":"<p>Getter for gtr model.</p> <p>Returns:</p> Type Description <code>'GlobalTrackingTransformer'</code> <p>A global tracking transformer with parameters indicated by cfg</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_model(self) -&gt; \"GlobalTrackingTransformer\":\n    \"\"\"Getter for gtr model.\n\n    Returns:\n        A global tracking transformer with parameters indicated by cfg\n    \"\"\"\n    from dreem.models import GlobalTrackingTransformer\n\n    model_params = self.cfg.model\n    with open_dict(model_params):\n        ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n    if ckpt_path is not None and len(ckpt_path) &gt; 0:\n        return GTRRunner.load_from_checkpoint(ckpt_path).model\n\n    return GlobalTrackingTransformer(**model_params)\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_optimizer","title":"<code>get_optimizer(params)</code>","text":"<p>Getter for optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable</code> <p>iterable of model parameters to optimize or dicts defining parameter groups</p> required <p>Returns:</p> Type Description <code>Optimizer</code> <p>A torch Optimizer with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_optimizer(self, params: Iterable) -&gt; torch.optim.Optimizer:\n    \"\"\"Getter for optimizer.\n\n    Args:\n        params: iterable of model parameters to optimize or dicts defining\n            parameter groups\n\n    Returns:\n        A torch Optimizer with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_optimizer\n\n    optimizer_params = self.cfg.optimizer\n\n    return init_optimizer(params, optimizer_params)\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_scheduler","title":"<code>get_scheduler(optimizer)</code>","text":"<p>Getter for lr scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to wrap the scheduler around</p> required <p>Returns:</p> Type Description <code>LRScheduler</code> <p>A torch learning rate scheduler with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_scheduler(\n    self, optimizer: torch.optim.Optimizer\n) -&gt; torch.optim.lr_scheduler.LRScheduler:\n    \"\"\"Getter for lr scheduler.\n\n    Args:\n        optimizer: The optimizer to wrap the scheduler around\n\n    Returns:\n        A torch learning rate scheduler with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_scheduler\n\n    lr_scheduler_params = self.cfg.scheduler\n\n    return init_scheduler(optimizer, lr_scheduler_params)\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_tracker_cfg","title":"<code>get_tracker_cfg()</code>","text":"<p>Getter for tracker config params.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict containing the init params for <code>Tracker</code>.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_tracker_cfg(self) -&gt; dict:\n    \"\"\"Getter for tracker config params.\n\n    Returns:\n        A dict containing the init params for `Tracker`.\n    \"\"\"\n    tracker_params = self.cfg.tracker\n    tracker_cfg = {}\n    for key, val in tracker_params.items():\n        tracker_cfg[key] = val\n    return tracker_cfg\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_trainer","title":"<code>get_trainer(callbacks=None, logger=None, devices=1, accelerator='auto')</code>","text":"<p>Getter for the lightning trainer.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[Callback] | None</code> <p>a list of lightning callbacks preconfigured to be used for training</p> <code>None</code> <code>logger</code> <code>WandbLogger | None</code> <p>the Wandb logger used for logging during training</p> <code>None</code> <code>devices</code> <code>int</code> <p>The number of gpus to be used. 0 means cpu</p> <code>1</code> <code>accelerator</code> <code>str</code> <p>either \"gpu\" or \"cpu\" specifies which device to use</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>Trainer</code> <p>A lightning Trainer with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_trainer(\n    self,\n    callbacks: list[pl.callbacks.Callback] | None = None,\n    logger: pl.loggers.WandbLogger | None = None,\n    devices: int = 1,\n    accelerator: str = \"auto\",\n) -&gt; pl.Trainer:\n    \"\"\"Getter for the lightning trainer.\n\n    Args:\n        callbacks: a list of lightning callbacks preconfigured to be used\n            for training\n        logger: the Wandb logger used for logging during training\n        devices: The number of gpus to be used. 0 means cpu\n        accelerator: either \"gpu\" or \"cpu\" specifies which device to use\n\n    Returns:\n        A lightning Trainer with specified params\n    \"\"\"\n    if \"trainer\" in self.cfg:\n        trainer_params = OmegaConf.to_container(self.cfg.trainer, resolve=True)\n\n    else:\n        trainer_params = {}\n\n    profiler = trainer_params.pop(\"profiler\", None)\n    if \"accelerator\" not in trainer_params:\n        trainer_params[\"accelerator\"] = accelerator\n    if \"devices\" not in trainer_params:\n        trainer_params[\"devices\"] = devices\n\n    if \"profiler\":\n        profiler = pl.profilers.AdvancedProfiler(filename=\"profile.txt\")\n    else:\n        profiler = None\n\n    return pl.Trainer(\n        callbacks=callbacks,\n        logger=logger,\n        profiler=profiler,\n        **trainer_params,\n    )\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.set_hparams","title":"<code>set_hparams(hparams)</code>","text":"<p>Setter function for overwriting specific hparams.</p> <p>Useful for changing 1 or 2 hyperparameters such as dataset.</p> <p>Parameters:</p> Name Type Description Default <code>hparams</code> <code>dict</code> <p>A dict containing the hyperparameter to be overwritten and the value to be changed</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if config is successfully updated, <code>False</code> otherwise</p> Source code in <code>dreem/io/config.py</code> <pre><code>def set_hparams(self, hparams: dict) -&gt; bool:\n    \"\"\"Setter function for overwriting specific hparams.\n\n    Useful for changing 1 or 2 hyperparameters such as dataset.\n\n    Args:\n        hparams: A dict containing the hyperparameter to be overwritten and\n            the value to be changed\n\n    Returns:\n        `True` if config is successfully updated, `False` otherwise\n    \"\"\"\n    if hparams == {} or hparams is None:\n        logger.warning(\"Nothing to update!\")\n        return False\n    for hparam, val in hparams.items():\n        try:\n            OmegaConf.update(self.cfg, hparam, val)\n        except Exception as e:\n            logger.exception(f\"Failed to update {hparam} to {val} due to {e}\")\n            return False\n    return True\n</code></pre>"},{"location":"reference/dreem/io/frame/","title":"frame","text":""},{"location":"reference/dreem/io/frame/#dreem.io.frame","title":"<code>dreem.io.frame</code>","text":"<p>Module containing data classes such as Instances and Frames.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame","title":"<code>Frame</code>","text":"<p>Data structure containing metadata for a single frame of a video.</p> <p>Attributes:</p> Name Type Description <code>video_id</code> <code>Tensor</code> <p>The video index in the dataset.</p> <code>frame_id</code> <code>Tensor</code> <p>The index of the frame in a video.</p> <code>vid_file</code> <code>Tensor</code> <p>The path to the video the frame is from.</p> <code>img_shape</code> <code>Tensor</code> <p>The shape of the original frame (not the crop).</p> <code>instances</code> <code>list['Instance']</code> <p>A list of Instance objects that appear in the frame.</p> <code>asso_output</code> <code>'AssociationMatrix'</code> <p>The association matrix between instances output directly from the transformer.</p> <code>matches</code> <code>tuple</code> <p>matches from LSA algorithm between the instances and available trajectories during tracking.</p> <code>traj_score</code> <code>tuple</code> <p>Either a dict containing the association matrix between instances and trajectories along postprocessing pipeline or a single association matrix.</p> <code>device</code> <code>str</code> <p>The device the frame should be moved to.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>@attrs.define(eq=False)\nclass Frame:\n    \"\"\"Data structure containing metadata for a single frame of a video.\n\n    Attributes:\n        video_id: The video index in the dataset.\n        frame_id: The index of the frame in a video.\n        vid_file: The path to the video the frame is from.\n        img_shape: The shape of the original frame (not the crop).\n        instances: A list of Instance objects that appear in the frame.\n        asso_output: The association matrix between instances\n            output directly from the transformer.\n        matches: matches from LSA algorithm between the instances and\n            available trajectories during tracking.\n        traj_score: Either a dict containing the association matrix\n            between instances and trajectories along postprocessing pipeline\n            or a single association matrix.\n        device: The device the frame should be moved to.\n    \"\"\"\n\n    _video_id: int = attrs.field(alias=\"video_id\", converter=_to_tensor)\n    _frame_id: int = attrs.field(alias=\"frame_id\", converter=_to_tensor)\n    _video: str = attrs.field(alias=\"vid_file\", default=\"\")\n    _img_shape: ArrayLike = attrs.field(\n        alias=\"img_shape\", converter=_to_tensor, factory=list\n    )\n\n    _instances: list[\"Instance\"] = attrs.field(alias=\"instances\", factory=list)\n    _asso_output: \"AssociationMatrix\" | None = attrs.field(\n        alias=\"asso_output\", default=None\n    )\n    _matches: tuple = attrs.field(alias=\"matches\", factory=tuple)\n    _traj_score: dict = attrs.field(alias=\"traj_score\", factory=dict)\n    _device: str | torch.device | None = attrs.field(alias=\"device\", default=None)\n\n    def __attrs_post_init__(self) -&gt; None:\n        \"\"\"Handle more intricate default initializations and moving to device.\"\"\"\n        if len(self.img_shape) == 0:\n            self.img_shape = torch.tensor([0, 0, 0])\n\n        for instance in self.instances:\n            instance.frame = self\n\n        self.to(self.device)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return String representation of the Frame.\n\n        Returns:\n            The string representation of the frame.\n        \"\"\"\n        return (\n            \"Frame(\"\n            f\"video={self._video.filename if isinstance(self._video, sio.Video) else self._video}, \"\n            f\"video_id={self._video_id.item()}, \"\n            f\"frame_id={self._frame_id.item()}, \"\n            f\"img_shape={self._img_shape}, \"\n            f\"num_detected={self.num_detected}, \"\n            f\"asso_output={self._asso_output}, \"\n            f\"traj_score={self._traj_score}, \"\n            f\"matches={self._matches}, \"\n            f\"instances={self._instances}, \"\n            f\"device={self._device}\"\n            \")\"\n        )\n\n    def to(self, map_location: str | torch.device) -&gt; Self:\n        \"\"\"Move frame to different device or dtype (See `torch.to` for more info).\n\n        Args:\n            map_location: A string representing the device to move to.\n\n        Returns:\n            The frame moved to a different device/dtype.\n        \"\"\"\n        self._video_id = self._video_id.to(map_location)\n        self._frame_id = self._frame_id.to(map_location)\n        self._img_shape = self._img_shape.to(map_location)\n\n        if isinstance(self._asso_output, torch.Tensor):\n            self._asso_output = self._asso_output.to(map_location)\n\n        if isinstance(self._matches, torch.Tensor):\n            self._matches = self._matches.to(map_location)\n\n        for key, val in self._traj_score.items():\n            if isinstance(val, torch.Tensor):\n                self._traj_score[key] = val.to(map_location)\n        for instance in self.instances:\n            instance = instance.to(map_location)\n\n        if isinstance(map_location, (str, torch.device)):\n            self._device = map_location\n\n        return self\n\n    @classmethod\n    def from_slp(\n        cls,\n        lf: sio.LabeledFrame,\n        video_id: int = 0,\n        device: str | None = None,\n        **kwargs,\n    ) -&gt; Self:\n        \"\"\"Convert `sio.LabeledFrame` to `dreem.io.Frame`.\n\n        Args:\n            lf: A sio.LabeledFrame object\n\n        Returns:\n            A dreem.io.Frame object\n        \"\"\"\n        from dreem.io import Instance\n\n        img_shape = lf.image.shape\n        if len(img_shape) == 2:\n            img_shape = (1, *img_shape)\n        elif len(img_shape) &gt; 2 and img_shape[-1] &lt;= 3:\n            img_shape = (lf.image.shape[-1], lf.image.shape[0], lf.image.shape[1])\n        return cls(\n            video_id=video_id,\n            frame_id=(\n                lf.frame_idx.astype(np.int32)\n                if isinstance(lf.frame_idx, np.number)\n                else lf.frame_idx\n            ),\n            vid_file=lf.video.filename,\n            img_shape=img_shape,\n            instances=[Instance.from_slp(instance, **kwargs) for instance in lf],\n            device=device,\n        )\n\n    def to_slp(\n        self,\n        track_lookup: dict[int, sio.Track] | None = None,\n        video: sio.Video | None = None,\n    ) -&gt; tuple[sio.LabeledFrame, dict[int, sio.Track]]:\n        \"\"\"Convert Frame to sleap_io.LabeledFrame object.\n\n        Args:\n            track_lookup: A lookup dictionary containing the track_id and sio.Track for persistence\n            video: An sio.Video object used for overriding.\n\n        Returns: A tuple containing a LabeledFrame object with necessary metadata and\n        a lookup dictionary containing the track_id and sio.Track for persistence\n        \"\"\"\n        if track_lookup is None:\n            track_lookup = {}\n\n        slp_instances = []\n        for instance in self.instances:\n            slp_instance, track_lookup = instance.to_slp(track_lookup=track_lookup)\n            slp_instances.append(slp_instance)\n\n        if video is None:\n            video = (\n                self.video\n                if isinstance(self.video, sio.Video)\n                else sio.load_video(self.video)\n            )\n\n        return (\n            sio.LabeledFrame(\n                video=video,\n                frame_idx=self.frame_id.item(),\n                instances=slp_instances,\n            ),\n            track_lookup,\n        )\n\n    def to_h5(\n        self,\n        clip_group: h5py.Group,\n        instance_labels: list | None = None,\n        save: dict[str, bool] | None = None,\n    ) -&gt; h5py.Group:\n        \"\"\"Convert frame to h5py group.\n\n        Args:\n            clip_group: the h5py group representing the clip (e.g batch/video) the frame belongs to\n            instance_labels: the labels used to create instance group names\n            save: whether to save crops, features and embeddings for the instance\n        Returns:\n            An h5py group containing the frame\n        \"\"\"\n        if save is None:\n            save = {\"crop\": False, \"features\": False, \"embeddings\": False}\n        frame_group = clip_group.require_group(f\"frame_{self.frame_id.item()}\")\n        frame_group.attrs.create(\"frame_id\", self.frame_id.item())\n        frame_group.attrs.create(\"vid_id\", self.video_id.item())\n        frame_group.attrs.create(\"vid_name\", self.vid_name)\n\n        frame_group.create_dataset(\n            \"asso_matrix\",\n            data=self.asso_output.numpy() if self.asso_output is not None else [],\n        )\n        asso_group = frame_group.require_group(\"traj_scores\")\n        for key, value in self.get_traj_score().items():\n            asso_group.create_dataset(\n                key, data=value.to_numpy() if value is not None else []\n            )\n\n        if instance_labels is None:\n            instance_labels = self.get_gt_track_ids.cpu().numpy()\n        for instance_label, instance in zip(instance_labels, self.instances):\n            kwargs = {}\n            if save.get(\"crop\", False):\n                kwargs[\"crop\"] = instance.crop.cpu().numpy()\n            if save.get(\"features\", False):\n                kwargs[\"features\"] = instance.features.cpu().numpy()\n            if save.get(\"embeddings\", False):\n                for key, val in instance.get_embedding().items():\n                    kwargs[f\"{key}_emb\"] = val.cpu().numpy()\n            _ = instance.to_h5(frame_group, f\"instance_{instance_label}\", **kwargs)\n\n        return frame_group\n\n    @property\n    def device(self) -&gt; str:\n        \"\"\"The device the frame is on.\n\n        Returns:\n            The string representation of the device the frame is on.\n        \"\"\"\n        return self._device\n\n    @device.setter\n    def device(self, device: str) -&gt; None:\n        \"\"\"Set the device.\n\n        Note: Do not set `frame.device = device` normally. Use `frame.to(device)` instead.\n\n        Args:\n            device: the device the function should be on.\n        \"\"\"\n        self._device = device\n\n    @property\n    def video_id(self) -&gt; torch.Tensor:\n        \"\"\"The index of the video the frame comes from.\n\n        Returns:\n            A tensor containing the video index.\n        \"\"\"\n        return self._video_id\n\n    @video_id.setter\n    def video_id(self, video_id: int) -&gt; None:\n        \"\"\"Set the video index.\n\n        Note: Generally the video_id should be immutable after initialization.\n\n        Args:\n            video_id: an int representing the index of the video that the frame came from.\n        \"\"\"\n        self._video_id = torch.tensor([video_id])\n\n    @property\n    def frame_id(self) -&gt; torch.Tensor:\n        \"\"\"The index of the frame in a full video.\n\n        Returns:\n            A torch tensor containing the index of the frame in the video.\n        \"\"\"\n        return self._frame_id\n\n    @frame_id.setter\n    def frame_id(self, frame_id: int) -&gt; None:\n        \"\"\"Set the frame index of the frame.\n\n        Note: The frame_id should generally be immutable after initialization.\n\n        Args:\n            frame_id: The int index of the frame in the full video.\n        \"\"\"\n        self._frame_id = torch.tensor([frame_id])\n\n    @property\n    def video(self) -&gt; sio.Video | str:\n        \"\"\"Get the video associated with the frame.\n\n        Returns: An sio.Video object representing the video or a placeholder string\n        if it is not possible to create the sio.Video\n        \"\"\"\n        return self._video\n\n    @video.setter\n    def video(self, video: sio.Video | str) -&gt; None:\n        \"\"\"Set the video associated with the frame.\n\n        Note: we try to store the video in an sio.Video object.\n        However, if this is not possible (e.g. incompatible format or missing filepath)\n        then we simply store the string.\n\n        Args:\n            video: sio.Video containing the vid reader or string path to video_file\n        \"\"\"\n        if isinstance(video, sio.Video):\n            self._video = video\n        else:\n            try:\n                self._video = sio.load_video(video)\n            except ValueError:\n                self._video = video\n\n    @property\n    def vid_name(self) -&gt; str:\n        \"\"\"Get the path to the video corresponding to this frame.\n\n        Returns: A str file path corresponding to the frame.\n        \"\"\"\n        if isinstance(self.video, str):\n            return self.video\n        else:\n            return self.video.name\n\n    @property\n    def img_shape(self) -&gt; torch.Tensor:\n        \"\"\"The shape of the pre-cropped frame.\n\n        Returns:\n            A torch tensor containing the shape of the frame. Should generally be (c, h, w)\n        \"\"\"\n        return self._img_shape\n\n    @img_shape.setter\n    def img_shape(self, img_shape: ArrayLike) -&gt; None:\n        \"\"\"Set the shape of the frame image.\n\n        Note: the img_shape should generally be immutable after initialization.\n\n        Args:\n            img_shape: an ArrayLike object containing the shape of the frame image.\n        \"\"\"\n        self._img_shape = _to_tensor(img_shape)\n\n    @property\n    def instances(self) -&gt; list[\"Instance\"]:\n        \"\"\"A list of instances in the frame.\n\n        Returns:\n            The list of instances that appear in the frame.\n        \"\"\"\n        return self._instances\n\n    @instances.setter\n    def instances(self, instances: list[\"Instance\"]) -&gt; None:\n        \"\"\"Set the frame's instance.\n\n        Args:\n            instances: A list of Instances that appear in the frame.\n        \"\"\"\n        for instance in instances:\n            instance.frame = self\n        self._instances = instances\n\n    def has_instances(self) -&gt; bool:\n        \"\"\"Determine whether there are instances in the frame.\n\n        Returns:\n            True if there are instances in the frame, otherwise False.\n        \"\"\"\n        if self.num_detected == 0:\n            return False\n        return True\n\n    @property\n    def num_detected(self) -&gt; int:\n        \"\"\"The number of instances in the frame.\n\n        Returns:\n            the number of instances in the frame.\n        \"\"\"\n        return len(self.instances)\n\n    @property\n    def asso_output(self) -&gt; \"AssociationMatrix\":\n        \"\"\"The association matrix between instances outputed directly by transformer.\n\n        Returns:\n            An arraylike (n_query, n_nonquery) association matrix between instances.\n        \"\"\"\n        return self._asso_output\n\n    def has_asso_output(self) -&gt; bool:\n        \"\"\"Determine whether the frame has an association matrix computed.\n\n        Returns:\n            True if the frame has an association matrix otherwise, False.\n        \"\"\"\n        if self._asso_output is None or len(self._asso_output.matrix) == 0:\n            return False\n        return True\n\n    @asso_output.setter\n    def asso_output(self, asso_output: \"AssociationMatrix\") -&gt; None:\n        \"\"\"Set the association matrix of a frame.\n\n        Args:\n            asso_output: An arraylike (n_query, n_nonquery) association matrix between instances.\n        \"\"\"\n        self._asso_output = asso_output\n\n    @property\n    def matches(self) -&gt; tuple:\n        \"\"\"Matches between frame instances and availabel trajectories.\n\n        Returns:\n            A tuple containing the instance idx and trajectory idx for the matched instance.\n        \"\"\"\n        return self._matches\n\n    @matches.setter\n    def matches(self, matches: tuple) -&gt; None:\n        \"\"\"Set the frame matches.\n\n        Args:\n            matches: A tuple containing the instance idx and trajectory idx for the matched instance.\n        \"\"\"\n        self._matches = matches\n\n    def has_matches(self) -&gt; bool:\n        \"\"\"Check whether or not matches have been computed for frame.\n\n        Returns:\n            True if frame contains matches otherwise False.\n        \"\"\"\n        if self._matches is not None and len(self._matches) &gt; 0:\n            return True\n        return False\n\n    def get_traj_score(self, key: str | None = None) -&gt; dict | ArrayLike | None:\n        \"\"\"Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.\n\n        Args:\n            key: The key of the trajectory score to be accessed.\n                Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}\n\n        Returns:\n            - dictionary containing all trajectory scores if key is None\n            - trajectory score associated with key\n            - None if the key is not found\n        \"\"\"\n        if key is None:\n            return self._traj_score\n        else:\n            try:\n                return self._traj_score[key]\n            except KeyError as e:\n                logger.exception(f\"Could not access {key} traj_score due to {e}\")\n                return None\n\n    def add_traj_score(self, key: str, traj_score: ArrayLike) -&gt; None:\n        \"\"\"Add trajectory score to dictionary.\n\n        Args:\n            key: key associated with traj score to be used in dictionary\n            traj_score: association matrix between instances and trajectories\n        \"\"\"\n        self._traj_score[key] = traj_score\n\n    def has_traj_score(self) -&gt; bool:\n        \"\"\"Check if any trajectory association matrix has been saved.\n\n        Returns:\n            True there is at least one association matrix otherwise, false.\n        \"\"\"\n        if len(self._traj_score) == 0:\n            return False\n        return True\n\n    def has_gt_track_ids(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a gt track id.\n\n        Returns:\n            True if at least 1 instance has a gt track id otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_gt_track_id() for instance in self.instances])\n        return False\n\n    def get_gt_track_ids(self) -&gt; torch.Tensor:\n        \"\"\"Get the gt track ids of all instances in the frame.\n\n        Returns:\n            an (N,) shaped tensor with the gt track ids of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.gt_track_id for instance in self.instances])\n\n    def has_pred_track_ids(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a pred track id.\n\n        Returns:\n            True if at least 1 instance has a pred track id otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_pred_track_id() for instance in self.instances])\n        return False\n\n    def get_pred_track_ids(self) -&gt; torch.Tensor:\n        \"\"\"Get the pred track ids of all instances in the frame.\n\n        Returns:\n            an (N,) shaped tensor with the pred track ids of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.pred_track_id for instance in self.instances])\n\n    def has_bboxes(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a bounding box.\n\n        Returns:\n            True if at least 1 instance has a bounding box otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_bboxes() for instance in self.instances])\n        return False\n\n    def get_bboxes(self) -&gt; torch.Tensor:\n        \"\"\"Get the bounding boxes of all instances in the frame.\n\n        Returns:\n            an (N,4) shaped tensor with bounding boxes of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.empty(0, 4)\n        return torch.cat([instance.bbox for instance in self.instances], dim=0)\n\n    def has_crops(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a crop.\n\n        Returns:\n            True if at least 1 instance has a crop otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_crop() for instance in self.instances])\n        return False\n\n    def get_crops(self) -&gt; torch.Tensor:\n        \"\"\"Get the crops of all instances in the frame.\n\n        Returns:\n            an (N, C, H, W) shaped tensor with crops of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n\n        return torch.cat([instance.crop for instance in self.instances], dim=0)\n\n    def has_features(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has reid features already computed.\n\n        Returns:\n            True if at least 1 instance have reid features otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_features() for instance in self.instances])\n        return False\n\n    def get_features(self) -&gt; torch.Tensor:\n        \"\"\"Get the reid feature vectors of all instances in the frame.\n\n        Returns:\n            an (N, D) shaped tensor with reid feature vectors of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.features for instance in self.instances], dim=0)\n\n    def get_anchors(self) -&gt; list[str]:\n        \"\"\"Get the anchor names of instances in the frame.\n\n        Returns:\n            A list of anchor names used by the instances to get the crop.\n        \"\"\"\n        return [instance.anchor for instance in self.instances]\n\n    def get_centroids(self) -&gt; tuple[list[str], ArrayLike]:\n        \"\"\"Get the centroids around which each instance's crop was formed.\n\n        Returns:\n            anchors: the node names for the corresponding point\n            points: an n_instances x 2 array containing the centroids\n        \"\"\"\n        anchors = [\n            anchor for instance in self.instances for anchor in instance.centroid.keys()\n        ]\n\n        points = np.array(\n            [\n                point\n                for instance in self.instances\n                for point in instance.centroid.values()\n            ]\n        )\n\n        return (anchors, points)\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.asso_output","title":"<code>asso_output: 'AssociationMatrix'</code>  <code>property</code> <code>writable</code>","text":"<p>The association matrix between instances outputed directly by transformer.</p> <p>Returns:</p> Type Description <code>'AssociationMatrix'</code> <p>An arraylike (n_query, n_nonquery) association matrix between instances.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.device","title":"<code>device: str</code>  <code>property</code> <code>writable</code>","text":"<p>The device the frame is on.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the device the frame is on.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.frame_id","title":"<code>frame_id: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>The index of the frame in a full video.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor containing the index of the frame in the video.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.img_shape","title":"<code>img_shape: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>The shape of the pre-cropped frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor containing the shape of the frame. Should generally be (c, h, w)</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.instances","title":"<code>instances: list['Instance']</code>  <code>property</code> <code>writable</code>","text":"<p>A list of instances in the frame.</p> <p>Returns:</p> Type Description <code>list['Instance']</code> <p>The list of instances that appear in the frame.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.matches","title":"<code>matches: tuple</code>  <code>property</code> <code>writable</code>","text":"<p>Matches between frame instances and availabel trajectories.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the instance idx and trajectory idx for the matched instance.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.num_detected","title":"<code>num_detected: int</code>  <code>property</code>","text":"<p>The number of instances in the frame.</p> <p>Returns:</p> Type Description <code>int</code> <p>the number of instances in the frame.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.vid_name","title":"<code>vid_name: str</code>  <code>property</code>","text":"<p>Get the path to the video corresponding to this frame.</p> <p>Returns: A str file path corresponding to the frame.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.video","title":"<code>video: sio.Video | str</code>  <code>property</code> <code>writable</code>","text":"<p>Get the video associated with the frame.</p> <p>Returns: An sio.Video object representing the video or a placeholder string if it is not possible to create the sio.Video</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.video_id","title":"<code>video_id: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>The index of the video the frame comes from.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the video index.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.__attrs_post_init__","title":"<code>__attrs_post_init__()</code>","text":"<p>Handle more intricate default initializations and moving to device.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def __attrs_post_init__(self) -&gt; None:\n    \"\"\"Handle more intricate default initializations and moving to device.\"\"\"\n    if len(self.img_shape) == 0:\n        self.img_shape = torch.tensor([0, 0, 0])\n\n    for instance in self.instances:\n        instance.frame = self\n\n    self.to(self.device)\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.__repr__","title":"<code>__repr__()</code>","text":"<p>Return String representation of the Frame.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return String representation of the Frame.\n\n    Returns:\n        The string representation of the frame.\n    \"\"\"\n    return (\n        \"Frame(\"\n        f\"video={self._video.filename if isinstance(self._video, sio.Video) else self._video}, \"\n        f\"video_id={self._video_id.item()}, \"\n        f\"frame_id={self._frame_id.item()}, \"\n        f\"img_shape={self._img_shape}, \"\n        f\"num_detected={self.num_detected}, \"\n        f\"asso_output={self._asso_output}, \"\n        f\"traj_score={self._traj_score}, \"\n        f\"matches={self._matches}, \"\n        f\"instances={self._instances}, \"\n        f\"device={self._device}\"\n        \")\"\n    )\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.add_traj_score","title":"<code>add_traj_score(key, traj_score)</code>","text":"<p>Add trajectory score to dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key associated with traj score to be used in dictionary</p> required <code>traj_score</code> <code>ArrayLike</code> <p>association matrix between instances and trajectories</p> required Source code in <code>dreem/io/frame.py</code> <pre><code>def add_traj_score(self, key: str, traj_score: ArrayLike) -&gt; None:\n    \"\"\"Add trajectory score to dictionary.\n\n    Args:\n        key: key associated with traj score to be used in dictionary\n        traj_score: association matrix between instances and trajectories\n    \"\"\"\n    self._traj_score[key] = traj_score\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.from_slp","title":"<code>from_slp(lf, video_id=0, device=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Convert <code>sio.LabeledFrame</code> to <code>dreem.io.Frame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>lf</code> <code>LabeledFrame</code> <p>A sio.LabeledFrame object</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A dreem.io.Frame object</p> Source code in <code>dreem/io/frame.py</code> <pre><code>@classmethod\ndef from_slp(\n    cls,\n    lf: sio.LabeledFrame,\n    video_id: int = 0,\n    device: str | None = None,\n    **kwargs,\n) -&gt; Self:\n    \"\"\"Convert `sio.LabeledFrame` to `dreem.io.Frame`.\n\n    Args:\n        lf: A sio.LabeledFrame object\n\n    Returns:\n        A dreem.io.Frame object\n    \"\"\"\n    from dreem.io import Instance\n\n    img_shape = lf.image.shape\n    if len(img_shape) == 2:\n        img_shape = (1, *img_shape)\n    elif len(img_shape) &gt; 2 and img_shape[-1] &lt;= 3:\n        img_shape = (lf.image.shape[-1], lf.image.shape[0], lf.image.shape[1])\n    return cls(\n        video_id=video_id,\n        frame_id=(\n            lf.frame_idx.astype(np.int32)\n            if isinstance(lf.frame_idx, np.number)\n            else lf.frame_idx\n        ),\n        vid_file=lf.video.filename,\n        img_shape=img_shape,\n        instances=[Instance.from_slp(instance, **kwargs) for instance in lf],\n        device=device,\n    )\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.get_anchors","title":"<code>get_anchors()</code>","text":"<p>Get the anchor names of instances in the frame.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of anchor names used by the instances to get the crop.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_anchors(self) -&gt; list[str]:\n    \"\"\"Get the anchor names of instances in the frame.\n\n    Returns:\n        A list of anchor names used by the instances to get the crop.\n    \"\"\"\n    return [instance.anchor for instance in self.instances]\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.get_bboxes","title":"<code>get_bboxes()</code>","text":"<p>Get the bounding boxes of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,4) shaped tensor with bounding boxes of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_bboxes(self) -&gt; torch.Tensor:\n    \"\"\"Get the bounding boxes of all instances in the frame.\n\n    Returns:\n        an (N,4) shaped tensor with bounding boxes of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.empty(0, 4)\n    return torch.cat([instance.bbox for instance in self.instances], dim=0)\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.get_centroids","title":"<code>get_centroids()</code>","text":"<p>Get the centroids around which each instance's crop was formed.</p> <p>Returns:</p> Name Type Description <code>anchors</code> <code>tuple[list[str], ArrayLike]</code> <p>the node names for the corresponding point points: an n_instances x 2 array containing the centroids</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_centroids(self) -&gt; tuple[list[str], ArrayLike]:\n    \"\"\"Get the centroids around which each instance's crop was formed.\n\n    Returns:\n        anchors: the node names for the corresponding point\n        points: an n_instances x 2 array containing the centroids\n    \"\"\"\n    anchors = [\n        anchor for instance in self.instances for anchor in instance.centroid.keys()\n    ]\n\n    points = np.array(\n        [\n            point\n            for instance in self.instances\n            for point in instance.centroid.values()\n        ]\n    )\n\n    return (anchors, points)\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.get_crops","title":"<code>get_crops()</code>","text":"<p>Get the crops of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N, C, H, W) shaped tensor with crops of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_crops(self) -&gt; torch.Tensor:\n    \"\"\"Get the crops of all instances in the frame.\n\n    Returns:\n        an (N, C, H, W) shaped tensor with crops of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n\n    return torch.cat([instance.crop for instance in self.instances], dim=0)\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.get_features","title":"<code>get_features()</code>","text":"<p>Get the reid feature vectors of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N, D) shaped tensor with reid feature vectors of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_features(self) -&gt; torch.Tensor:\n    \"\"\"Get the reid feature vectors of all instances in the frame.\n\n    Returns:\n        an (N, D) shaped tensor with reid feature vectors of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.features for instance in self.instances], dim=0)\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.get_gt_track_ids","title":"<code>get_gt_track_ids()</code>","text":"<p>Get the gt track ids of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,) shaped tensor with the gt track ids of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_gt_track_ids(self) -&gt; torch.Tensor:\n    \"\"\"Get the gt track ids of all instances in the frame.\n\n    Returns:\n        an (N,) shaped tensor with the gt track ids of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.gt_track_id for instance in self.instances])\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.get_pred_track_ids","title":"<code>get_pred_track_ids()</code>","text":"<p>Get the pred track ids of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,) shaped tensor with the pred track ids of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_pred_track_ids(self) -&gt; torch.Tensor:\n    \"\"\"Get the pred track ids of all instances in the frame.\n\n    Returns:\n        an (N,) shaped tensor with the pred track ids of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.pred_track_id for instance in self.instances])\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.get_traj_score","title":"<code>get_traj_score(key=None)</code>","text":"<p>Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str | None</code> <p>The key of the trajectory score to be accessed. Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}</p> <code>None</code> <p>Returns:</p> Type Description <code>dict | ArrayLike | None</code> <ul> <li>dictionary containing all trajectory scores if key is None</li> <li>trajectory score associated with key</li> <li>None if the key is not found</li> </ul> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_traj_score(self, key: str | None = None) -&gt; dict | ArrayLike | None:\n    \"\"\"Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.\n\n    Args:\n        key: The key of the trajectory score to be accessed.\n            Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}\n\n    Returns:\n        - dictionary containing all trajectory scores if key is None\n        - trajectory score associated with key\n        - None if the key is not found\n    \"\"\"\n    if key is None:\n        return self._traj_score\n    else:\n        try:\n            return self._traj_score[key]\n        except KeyError as e:\n            logger.exception(f\"Could not access {key} traj_score due to {e}\")\n            return None\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_asso_output","title":"<code>has_asso_output()</code>","text":"<p>Determine whether the frame has an association matrix computed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the frame has an association matrix otherwise, False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_asso_output(self) -&gt; bool:\n    \"\"\"Determine whether the frame has an association matrix computed.\n\n    Returns:\n        True if the frame has an association matrix otherwise, False.\n    \"\"\"\n    if self._asso_output is None or len(self._asso_output.matrix) == 0:\n        return False\n    return True\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_bboxes","title":"<code>has_bboxes()</code>","text":"<p>Check if any of frames instances has a bounding box.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a bounding box otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_bboxes(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a bounding box.\n\n    Returns:\n        True if at least 1 instance has a bounding box otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_bboxes() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_crops","title":"<code>has_crops()</code>","text":"<p>Check if any of frames instances has a crop.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a crop otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_crops(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a crop.\n\n    Returns:\n        True if at least 1 instance has a crop otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_crop() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_features","title":"<code>has_features()</code>","text":"<p>Check if any of frames instances has reid features already computed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance have reid features otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_features(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has reid features already computed.\n\n    Returns:\n        True if at least 1 instance have reid features otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_features() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_gt_track_ids","title":"<code>has_gt_track_ids()</code>","text":"<p>Check if any of frames instances has a gt track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a gt track id otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_gt_track_ids(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a gt track id.\n\n    Returns:\n        True if at least 1 instance has a gt track id otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_gt_track_id() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_instances","title":"<code>has_instances()</code>","text":"<p>Determine whether there are instances in the frame.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if there are instances in the frame, otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_instances(self) -&gt; bool:\n    \"\"\"Determine whether there are instances in the frame.\n\n    Returns:\n        True if there are instances in the frame, otherwise False.\n    \"\"\"\n    if self.num_detected == 0:\n        return False\n    return True\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_matches","title":"<code>has_matches()</code>","text":"<p>Check whether or not matches have been computed for frame.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if frame contains matches otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_matches(self) -&gt; bool:\n    \"\"\"Check whether or not matches have been computed for frame.\n\n    Returns:\n        True if frame contains matches otherwise False.\n    \"\"\"\n    if self._matches is not None and len(self._matches) &gt; 0:\n        return True\n    return False\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_pred_track_ids","title":"<code>has_pred_track_ids()</code>","text":"<p>Check if any of frames instances has a pred track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a pred track id otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_pred_track_ids(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a pred track id.\n\n    Returns:\n        True if at least 1 instance has a pred track id otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_pred_track_id() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_traj_score","title":"<code>has_traj_score()</code>","text":"<p>Check if any trajectory association matrix has been saved.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True there is at least one association matrix otherwise, false.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_traj_score(self) -&gt; bool:\n    \"\"\"Check if any trajectory association matrix has been saved.\n\n    Returns:\n        True there is at least one association matrix otherwise, false.\n    \"\"\"\n    if len(self._traj_score) == 0:\n        return False\n    return True\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.to","title":"<code>to(map_location)</code>","text":"<p>Move frame to different device or dtype (See <code>torch.to</code> for more info).</p> <p>Parameters:</p> Name Type Description Default <code>map_location</code> <code>str | device</code> <p>A string representing the device to move to.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The frame moved to a different device/dtype.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to(self, map_location: str | torch.device) -&gt; Self:\n    \"\"\"Move frame to different device or dtype (See `torch.to` for more info).\n\n    Args:\n        map_location: A string representing the device to move to.\n\n    Returns:\n        The frame moved to a different device/dtype.\n    \"\"\"\n    self._video_id = self._video_id.to(map_location)\n    self._frame_id = self._frame_id.to(map_location)\n    self._img_shape = self._img_shape.to(map_location)\n\n    if isinstance(self._asso_output, torch.Tensor):\n        self._asso_output = self._asso_output.to(map_location)\n\n    if isinstance(self._matches, torch.Tensor):\n        self._matches = self._matches.to(map_location)\n\n    for key, val in self._traj_score.items():\n        if isinstance(val, torch.Tensor):\n            self._traj_score[key] = val.to(map_location)\n    for instance in self.instances:\n        instance = instance.to(map_location)\n\n    if isinstance(map_location, (str, torch.device)):\n        self._device = map_location\n\n    return self\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.to_h5","title":"<code>to_h5(clip_group, instance_labels=None, save=None)</code>","text":"<p>Convert frame to h5py group.</p> <p>Parameters:</p> Name Type Description Default <code>clip_group</code> <code>Group</code> <p>the h5py group representing the clip (e.g batch/video) the frame belongs to</p> required <code>instance_labels</code> <code>list | None</code> <p>the labels used to create instance group names</p> <code>None</code> <code>save</code> <code>dict[str, bool] | None</code> <p>whether to save crops, features and embeddings for the instance</p> <code>None</code> <p>Returns:     An h5py group containing the frame</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to_h5(\n    self,\n    clip_group: h5py.Group,\n    instance_labels: list | None = None,\n    save: dict[str, bool] | None = None,\n) -&gt; h5py.Group:\n    \"\"\"Convert frame to h5py group.\n\n    Args:\n        clip_group: the h5py group representing the clip (e.g batch/video) the frame belongs to\n        instance_labels: the labels used to create instance group names\n        save: whether to save crops, features and embeddings for the instance\n    Returns:\n        An h5py group containing the frame\n    \"\"\"\n    if save is None:\n        save = {\"crop\": False, \"features\": False, \"embeddings\": False}\n    frame_group = clip_group.require_group(f\"frame_{self.frame_id.item()}\")\n    frame_group.attrs.create(\"frame_id\", self.frame_id.item())\n    frame_group.attrs.create(\"vid_id\", self.video_id.item())\n    frame_group.attrs.create(\"vid_name\", self.vid_name)\n\n    frame_group.create_dataset(\n        \"asso_matrix\",\n        data=self.asso_output.numpy() if self.asso_output is not None else [],\n    )\n    asso_group = frame_group.require_group(\"traj_scores\")\n    for key, value in self.get_traj_score().items():\n        asso_group.create_dataset(\n            key, data=value.to_numpy() if value is not None else []\n        )\n\n    if instance_labels is None:\n        instance_labels = self.get_gt_track_ids.cpu().numpy()\n    for instance_label, instance in zip(instance_labels, self.instances):\n        kwargs = {}\n        if save.get(\"crop\", False):\n            kwargs[\"crop\"] = instance.crop.cpu().numpy()\n        if save.get(\"features\", False):\n            kwargs[\"features\"] = instance.features.cpu().numpy()\n        if save.get(\"embeddings\", False):\n            for key, val in instance.get_embedding().items():\n                kwargs[f\"{key}_emb\"] = val.cpu().numpy()\n        _ = instance.to_h5(frame_group, f\"instance_{instance_label}\", **kwargs)\n\n    return frame_group\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.to_slp","title":"<code>to_slp(track_lookup=None, video=None)</code>","text":"<p>Convert Frame to sleap_io.LabeledFrame object.</p> <p>Parameters:</p> Name Type Description Default <code>track_lookup</code> <code>dict[int, Track] | None</code> <p>A lookup dictionary containing the track_id and sio.Track for persistence</p> <code>None</code> <code>video</code> <code>Video | None</code> <p>An sio.Video object used for overriding.</p> <code>None</code> <p>Returns: A tuple containing a LabeledFrame object with necessary metadata and a lookup dictionary containing the track_id and sio.Track for persistence</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to_slp(\n    self,\n    track_lookup: dict[int, sio.Track] | None = None,\n    video: sio.Video | None = None,\n) -&gt; tuple[sio.LabeledFrame, dict[int, sio.Track]]:\n    \"\"\"Convert Frame to sleap_io.LabeledFrame object.\n\n    Args:\n        track_lookup: A lookup dictionary containing the track_id and sio.Track for persistence\n        video: An sio.Video object used for overriding.\n\n    Returns: A tuple containing a LabeledFrame object with necessary metadata and\n    a lookup dictionary containing the track_id and sio.Track for persistence\n    \"\"\"\n    if track_lookup is None:\n        track_lookup = {}\n\n    slp_instances = []\n    for instance in self.instances:\n        slp_instance, track_lookup = instance.to_slp(track_lookup=track_lookup)\n        slp_instances.append(slp_instance)\n\n    if video is None:\n        video = (\n            self.video\n            if isinstance(self.video, sio.Video)\n            else sio.load_video(self.video)\n        )\n\n    return (\n        sio.LabeledFrame(\n            video=video,\n            frame_idx=self.frame_id.item(),\n            instances=slp_instances,\n        ),\n        track_lookup,\n    )\n</code></pre>"},{"location":"reference/dreem/io/instance/","title":"instance","text":""},{"location":"reference/dreem/io/instance/#dreem.io.instance","title":"<code>dreem.io.instance</code>","text":"<p>Module containing data class for storing detections.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance","title":"<code>Instance</code>","text":"<p>Class representing a single instance to be tracked.</p> <p>Attributes:</p> Name Type Description <code>gt_track_id</code> <code>Tensor</code> <p>Ground truth track id - only used for train/eval.</p> <code>pred_track_id</code> <code>Tensor</code> <p>Predicted track id. Untracked instance is represented by -1.</p> <code>bbox</code> <code>Tensor</code> <p>The bounding box coordinate of the instance. Defaults to an empty tensor.</p> <code>crop</code> <code>Tensor</code> <p>The crop of the instance.</p> <code>centroid</code> <code>dict[str, ArrayLike]</code> <p>the centroid around which the bbox was cropped.</p> <code>features</code> <code>Tensor</code> <p>The reid features extracted from the CNN backbone used in the transformer.</p> <code>track_score</code> <code>float</code> <p>The track score output from the association matrix.</p> <code>point_scores</code> <code>ArrayLike</code> <p>The point scores from sleap.</p> <code>instance_score</code> <code>float</code> <p>The instance scores from sleap.</p> <code>skeleton</code> <code>Skeleton</code> <p>The sleap skeleton used for the instance.</p> <code>pose</code> <code>dict[str, ArrayLike]</code> <p>A dictionary containing the node name and corresponding point.</p> <code>device</code> <code>str</code> <p>String representation of the device the instance should be on.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>@attrs.define(eq=False)\nclass Instance:\n    \"\"\"Class representing a single instance to be tracked.\n\n    Attributes:\n        gt_track_id: Ground truth track id - only used for train/eval.\n        pred_track_id: Predicted track id. Untracked instance is represented by -1.\n        bbox: The bounding box coordinate of the instance. Defaults to an empty tensor.\n        crop: The crop of the instance.\n        centroid: the centroid around which the bbox was cropped.\n        features: The reid features extracted from the CNN backbone used in the transformer.\n        track_score: The track score output from the association matrix.\n        point_scores: The point scores from sleap.\n        instance_score: The instance scores from sleap.\n        skeleton: The sleap skeleton used for the instance.\n        pose: A dictionary containing the node name and corresponding point.\n        device: String representation of the device the instance should be on.\n    \"\"\"\n\n    _gt_track_id: int = attrs.field(\n        alias=\"gt_track_id\", default=-1, converter=_to_tensor\n    )\n    _pred_track_id: int = attrs.field(\n        alias=\"pred_track_id\", default=-1, converter=_to_tensor\n    )\n    _bbox: ArrayLike = attrs.field(alias=\"bbox\", factory=list, converter=_to_tensor)\n    _crop: ArrayLike = attrs.field(alias=\"crop\", factory=list, converter=_to_tensor)\n    _centroid: dict[str, ArrayLike] = attrs.field(alias=\"centroid\", factory=dict)\n    _features: ArrayLike = attrs.field(\n        alias=\"features\", factory=list, converter=_to_tensor\n    )\n    _embeddings: dict = attrs.field(alias=\"embeddings\", factory=dict)\n    _track_score: float = attrs.field(alias=\"track_score\", default=-1.0)\n    _instance_score: float = attrs.field(alias=\"instance_score\", default=-1.0)\n    _point_scores: ArrayLike | None = attrs.field(alias=\"point_scores\", default=None)\n    _skeleton: sio.Skeleton | None = attrs.field(alias=\"skeleton\", default=None)\n    _pose: dict[str, ArrayLike] = attrs.field(alias=\"pose\", factory=dict)\n    _device: str | torch.device | None = attrs.field(alias=\"device\", default=None)\n    _frame: \"Frame\" = None\n\n    def __attrs_post_init__(self) -&gt; None:\n        \"\"\"Handle dimensionality and more intricate default initializations post-init.\"\"\"\n        self.bbox = _expand_to_rank(self.bbox, 3)\n        self.crop = _expand_to_rank(self.crop, 4)\n        self.features = _expand_to_rank(self.features, 2)\n\n        if self.skeleton is None:\n            self.skeleton = sio.Skeleton([\"centroid\"])\n\n        if self.bbox.shape[-1] == 0:\n            self.bbox = torch.empty([1, 0, 4])\n\n        if self.crop.shape[-1] == 0 and self.bbox.shape[1] != 0:\n            y1, x1, y2, x2 = self.bbox.squeeze(dim=0).nanmean(dim=0)\n            self.centroid = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        if len(self.pose) == 0 and self.bbox.shape[1]:\n            y1, x1, y2, x2 = self.bbox.squeeze(dim=0).mean(dim=0)\n            self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        if self.point_scores is None and len(self.pose) != 0:\n            self._point_scores = np.zeros((len(self.pose), 2))\n\n        self.to(self.device)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation of the Instance.\"\"\"\n        return (\n            \"Instance(\"\n            f\"gt_track_id={self._gt_track_id.item()}, \"\n            f\"pred_track_id={self._pred_track_id.item()}, \"\n            f\"bbox={self._bbox}, \"\n            f\"centroid={self._centroid}, \"\n            f\"crop={self._crop.shape}, \"\n            f\"features={self._features.shape}, \"\n            f\"device={self._device}\"\n            \")\"\n        )\n\n    def to(self, map_location: str | torch.device) -&gt; Self:\n        \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n        Args:\n            map_location: Either the device or dtype for the instance to be moved.\n\n        Returns:\n            self: reference to the instance moved to correct device/dtype.\n        \"\"\"\n        if map_location is not None and map_location != \"\":\n            self._gt_track_id = self._gt_track_id.to(map_location)\n            self._pred_track_id = self._pred_track_id.to(map_location)\n            self._bbox = self._bbox.to(map_location)\n            self._crop = self._crop.to(map_location)\n            self._features = self._features.to(map_location)\n            if isinstance(map_location, (str, torch.device)):\n                self.device = map_location\n\n        return self\n\n    @classmethod\n    def from_slp(\n        cls,\n        slp_instance: sio.PredictedInstance | sio.Instance,\n        bbox_size: int | tuple[int, int] = 64,\n        crop: ArrayLike | None = None,\n        device: str | None = None,\n    ) -&gt; Self:\n        \"\"\"Convert a slp instance to a dreem instance.\n\n        Args:\n            slp_instance: A `sleap_io.Instance` object representing a detection\n            bbox_size: size of the pose-centered bbox to form.\n            crop: The corresponding crop of the bbox\n            device: which device to keep the instance on\n        Returns:\n            A dreem.Instance object with a pose-centered bbox and no crop.\n        \"\"\"\n        try:\n            track_id = int(slp_instance.track.name)\n        except ValueError:\n            track_id = int(\n                \"\".join([str(ord(c)) for c in slp_instance.track.name])\n            )  # better way to handle this?\n        if isinstance(bbox_size, int):\n            bbox_size = (bbox_size, bbox_size)\n\n        track_score = -1.0\n        point_scores = np.full(len(slp_instance.points), -1)\n        instance_score = -1\n        if isinstance(slp_instance, sio.PredictedInstance):\n            track_score = slp_instance.tracking_score\n            point_scores = slp_instance.numpy()[:, -1]\n            instance_score = slp_instance.score\n\n        centroid = np.nanmean(slp_instance.numpy(), axis=1)\n        bbox = [\n            centroid[1] - bbox_size[1],\n            centroid[0] - bbox_size[0],\n            centroid[1] + bbox_size[1],\n            centroid[0] + bbox_size[0],\n        ]\n        return cls(\n            gt_track_id=track_id,\n            bbox=bbox,\n            crop=crop,\n            centroid={\"centroid\": centroid},\n            track_score=track_score,\n            point_scores=point_scores,\n            instance_score=instance_score,\n            skeleton=slp_instance.skeleton,\n            pose={\n                node.name: point.numpy() for node, point in slp_instance.points.items()\n            },\n            device=device,\n        )\n\n    def to_slp(\n        self, track_lookup: dict[int, sio.Track] = {}\n    ) -&gt; tuple[sio.PredictedInstance, dict[int, sio.Track]]:\n        \"\"\"Convert instance to sleap_io.PredictedInstance object.\n\n        Args:\n            track_lookup: A track look up dictionary containing track_id:sio.Track.\n        Returns: A sleap_io.PredictedInstance with necessary metadata\n            and a track_lookup dictionary to persist tracks.\n        \"\"\"\n        try:\n            track_id = self.pred_track_id.item()\n            if track_id not in track_lookup:\n                track_lookup[track_id] = sio.Track(name=self.pred_track_id.item())\n\n            track = track_lookup[track_id]\n\n            return (\n                sio.PredictedInstance.from_numpy(\n                    points=np.array(list(self.pose.values())),\n                    skeleton=self.skeleton,\n                    point_scores=self.point_scores,\n                    instance_score=self.instance_score,\n                    tracking_score=self.track_score,\n                    track=track,\n                ),\n                track_lookup,\n            )\n        except Exception as e:\n            logger.exception(\n                f\"Pose: {np.array(list(self.pose.values())).shape}, Pose score shape {self.point_scores.shape}\"\n            )\n            raise RuntimeError(f\"Failed to convert to sio.PredictedInstance: {e}\")\n\n    def to_h5(self, frame_group=h5py.Group, label=None, **kwargs: dict) -&gt; h5py.Group:\n        \"\"\"Convert instance to an h5 group\".\n\n        By default we always save:\n            - the gt/pred track id\n            - bbox\n            - centroid\n            - pose\n            - instance/traj/points score\n        Larger arrays (crops/features/embeddings) can be saved by passing as kwargs\n\n        Args:\n            frame_group: the h5py group representing the frame the instance appears on\n            label: the name of the instance group that will be created\n            **kwargs: additional key:value pairs to be saved as datasets.\n\n        Returns:\n            The h5 group representing this instance.\n        \"\"\"\n        if label is None:\n            if pred_track_id != -1:\n                label = f\"instance_{self.pred_track_id.item()}\"\n            else:\n                label = f\"instance_{self.gt_track_id.item()}\"\n        instance_group = frame_group.create_group(label)\n        instance_group.attrs.create(\"gt_track_id\", self.gt_track_id.item())\n        instance_group.attrs.create(\"pred_track_id\", self.pred_track_id.item())\n        instance_group.attrs.create(\"track_score\", self.track_score)\n        instance_group.attrs.create(\"instance_score\", self.instance_score)\n\n        instance_group.create_dataset(\"bbox\", data=self.bbox.cpu().numpy())\n\n        pose_group = instance_group.create_group(\"pose\")\n        pose_group.create_dataset(\"points\", data=np.array(list(self.pose.values())))\n        pose_group.attrs.create(\"nodes\", list(self.pose.keys()))\n        pose_group.create_dataset(\"scores\", data=self.point_scores)\n\n        for key, value in kwargs.items():\n            if \"emb\" in key:\n                emb_group = instance_group.require_group(\"emb\")\n                emb_group.create_dataset(key, data=value)\n            else:\n                instance_group.create_dataset(key, data=value)\n\n        return instance_group\n\n    @property\n    def device(self) -&gt; str:\n        \"\"\"The device the instance is on.\n\n        Returns:\n            The str representation of the device the gpu is on.\n        \"\"\"\n        return self._device\n\n    @device.setter\n    def device(self, device) -&gt; None:\n        \"\"\"Set for the device property.\n\n        Args:\n            device: The str representation of the device.\n        \"\"\"\n        self._device = device\n\n    @property\n    def gt_track_id(self) -&gt; torch.Tensor:\n        \"\"\"The ground truth track id of the instance.\n\n        Returns:\n            A tensor containing the ground truth track id\n        \"\"\"\n        return self._gt_track_id\n\n    @gt_track_id.setter\n    def gt_track_id(self, track: int):\n        \"\"\"Set the instance ground-truth track id.\n\n        Args:\n           track: An int representing the ground-truth track id.\n        \"\"\"\n        if track is not None:\n            self._gt_track_id = torch.tensor([track])\n        else:\n            self._gt_track_id = torch.tensor([])\n\n    def has_gt_track_id(self) -&gt; bool:\n        \"\"\"Determine if instance has a gt track assignment.\n\n        Returns:\n            True if the gt track id is set, otherwise False.\n        \"\"\"\n        if self._gt_track_id.shape[0] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def pred_track_id(self) -&gt; torch.Tensor:\n        \"\"\"The track id predicted by the tracker using asso_output from model.\n\n        Returns:\n            A tensor containing the predicted track id.\n        \"\"\"\n        return self._pred_track_id\n\n    @pred_track_id.setter\n    def pred_track_id(self, track: int) -&gt; None:\n        \"\"\"Set predicted track id.\n\n        Args:\n            track: an int representing the predicted track id.\n        \"\"\"\n        if track is not None:\n            self._pred_track_id = torch.tensor([track])\n        else:\n            self._pred_track_id = torch.tensor([])\n\n    def has_pred_track_id(self) -&gt; bool:\n        \"\"\"Determine whether instance has predicted track id.\n\n        Returns:\n            True if instance has a pred track id, False otherwise.\n        \"\"\"\n        if self._pred_track_id.item() == -1 or self._pred_track_id.shape[0] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def bbox(self) -&gt; torch.Tensor:\n        \"\"\"The bounding box coordinates of the instance in the original frame.\n\n        Returns:\n            A (1,4) tensor containing the bounding box coordinates.\n        \"\"\"\n        return self._bbox\n\n    @bbox.setter\n    def bbox(self, bbox: ArrayLike) -&gt; None:\n        \"\"\"Set the instance bounding box.\n\n        Args:\n            bbox: an arraylike object containing the bounding box coordinates.\n        \"\"\"\n        if bbox is None or len(bbox) == 0:\n            self._bbox = torch.empty((0, 4))\n        else:\n            if not isinstance(bbox, torch.Tensor):\n                self._bbox = torch.tensor(bbox)\n            else:\n                self._bbox = bbox\n\n        if self._bbox.shape[0] and len(self._bbox.shape) == 1:\n            self._bbox = self._bbox.unsqueeze(0)\n        if self._bbox.shape[1] and len(self._bbox.shape) == 2:\n            self._bbox = self._bbox.unsqueeze(0)\n\n    def has_bbox(self) -&gt; bool:\n        \"\"\"Determine if the instance has a bbox.\n\n        Returns:\n            True if the instance has a bounding box, false otherwise.\n        \"\"\"\n        if self._bbox.shape[1] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def centroid(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"The centroid around which the crop was formed.\n\n        Returns:\n            A dict containing the anchor name and the x, y bbox midpoint.\n        \"\"\"\n        return self._centroid\n\n    @centroid.setter\n    def centroid(self, centroid: dict[str, ArrayLike]) -&gt; None:\n        \"\"\"Set the centroid of the instance.\n\n        Args:\n            centroid: A dict containing the anchor name and points.\n        \"\"\"\n        self._centroid = centroid\n\n    @property\n    def anchor(self) -&gt; list[str]:\n        \"\"\"The anchor node name around which the crop was formed.\n\n        Returns:\n            the list of anchors around which each crop was formed\n            the list of anchors around which each crop was formed\n        \"\"\"\n        if self.centroid:\n            return list(self.centroid.keys())\n        return \"\"\n\n    @property\n    def crop(self) -&gt; torch.Tensor:\n        \"\"\"The crop of the instance.\n\n        Returns:\n            A (1, c, h , w) tensor containing the cropped image centered around the instance.\n        \"\"\"\n        return self._crop\n\n    @crop.setter\n    def crop(self, crop: ArrayLike) -&gt; None:\n        \"\"\"Set the crop of the instance.\n\n        Args:\n            crop: an arraylike object containing the cropped image of the centered instance.\n        \"\"\"\n        if crop is None or len(crop) == 0:\n            self._crop = torch.tensor([])\n        else:\n            if not isinstance(crop, torch.Tensor):\n                self._crop = torch.tensor(crop)\n            else:\n                self._crop = crop\n\n        if len(self._crop.shape) == 2:\n            self._crop = self._crop.unsqueeze(0)\n        if len(self._crop.shape) == 3:\n            self._crop = self._crop.unsqueeze(0)\n\n    def has_crop(self) -&gt; bool:\n        \"\"\"Determine if the instance has a crop.\n\n        Returns:\n            True if the instance has an image otherwise False.\n        \"\"\"\n        if self._crop.shape[-1] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def features(self) -&gt; torch.Tensor:\n        \"\"\"Re-ID feature vector from backbone model to be used as input to transformer.\n\n        Returns:\n            a (1, d) tensor containing the reid feature vector.\n        \"\"\"\n        return self._features\n\n    @features.setter\n    def features(self, features: ArrayLike) -&gt; None:\n        \"\"\"Set the reid feature vector of the instance.\n\n        Args:\n            features: a (1,d) array like object containing the reid features for the instance.\n        \"\"\"\n        if features is None or len(features) == 0:\n            self._features = torch.tensor([])\n\n        elif not isinstance(features, torch.Tensor):\n            self._features = torch.tensor(features)\n        else:\n            self._features = features\n\n        if self._features.shape[0] and len(self._features.shape) == 1:\n            self._features = self._features.unsqueeze(0)\n\n    def has_features(self) -&gt; bool:\n        \"\"\"Determine if the instance has computed reid features.\n\n        Returns:\n            True if the instance has reid features, False otherwise.\n        \"\"\"\n        if self._features.shape[-1] == 0:\n            return False\n        else:\n            return True\n\n    def has_embedding(self, emb_type: str | None = None) -&gt; bool:\n        \"\"\"Determine if the instance has embedding type requested.\n\n        Args:\n            emb_type: The key to check in the embedding dictionary.\n\n        Returns:\n            True if `emb_type` in embedding_dict else false\n        \"\"\"\n        return emb_type in self._embeddings\n\n    def get_embedding(\n        self, emb_type: str = \"all\"\n    ) -&gt; dict[str, torch.Tensor] | torch.Tensor | None:\n        \"\"\"Retrieve instance's spatial/temporal embedding.\n\n        Args:\n            emb_type: The string key of the embedding to retrieve. Should be \"pos\", \"temp\"\n\n        Returns:\n            * A torch tensor representing the spatial/temporal location of the instance.\n            * None if the embedding is not stored\n        \"\"\"\n        if emb_type.lower() == \"all\":\n            return self._embeddings\n        else:\n            try:\n                return self._embeddings[emb_type]\n            except KeyError:\n                logger.exception(\n                    f\"{emb_type} not saved! Only {list(self._embeddings.keys())} are available\"\n                )\n        return None\n\n    def add_embedding(self, emb_type: str, embedding: torch.Tensor) -&gt; None:\n        \"\"\"Save embedding to instance embedding dictionary.\n\n        Args:\n            emb_type: Key/embedding type to be saved to dictionary\n            embedding: The actual torch tensor embedding.\n        \"\"\"\n        embedding = _expand_to_rank(embedding, 2)\n        self._embeddings[emb_type] = embedding\n\n    @property\n    def frame(self) -&gt; \"Frame\":\n        \"\"\"Get the frame the instance belongs to.\n\n        Returns:\n            The back reference to the `Frame` that this `Instance` belongs to.\n        \"\"\"\n        return self._frame\n\n    @frame.setter\n    def frame(self, frame: \"Frame\") -&gt; None:\n        \"\"\"Set the back reference to the `Frame` that this `Instance` belongs to.\n\n        This field is set when instances are added to `Frame` object.\n\n        Args:\n            frame: A `Frame` object containing the metadata for the frame that the instance belongs to\n        \"\"\"\n        self._frame = frame\n\n    @property\n    def pose(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"Get the pose of the instance.\n\n        Returns:\n            A dictionary containing the node and corresponding x,y points\n        \"\"\"\n        return self._pose\n\n    @pose.setter\n    def pose(self, pose: dict[str, ArrayLike]) -&gt; None:\n        \"\"\"Set the pose of the instance.\n\n        Args:\n            pose: A nodes x 2 array containing the pose coordinates.\n        \"\"\"\n        if pose is not None:\n            self._pose = pose\n\n        elif self.bbox.shape[0]:\n            y1, x1, y2, x2 = self.bbox.squeeze()\n            self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        else:\n            self._pose = {}\n\n    def has_pose(self) -&gt; bool:\n        \"\"\"Check if the instance has a pose.\n\n        Returns True if the instance has a pose.\n        \"\"\"\n        if len(self.pose):\n            return True\n        return False\n\n    @property\n    def shown_pose(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"Get the pose with shown nodes only.\n\n        Returns: A dictionary filtered by nodes that are shown (points are not nan).\n        \"\"\"\n        pose = self.pose\n        return {node: point for node, point in pose.items() if not np.isna(point).any()}\n\n    @property\n    def skeleton(self) -&gt; sio.Skeleton:\n        \"\"\"Get the skeleton associated with the instance.\n\n        Returns: The sio.Skeleton associated with the instance.\n        \"\"\"\n        return self._skeleton\n\n    @skeleton.setter\n    def skeleton(self, skeleton: sio.Skeleton) -&gt; None:\n        \"\"\"Set the skeleton associated with the instance.\n\n        Args:\n            skeleton: The sio.Skeleton associated with the instance.\n        \"\"\"\n        self._skeleton = skeleton\n\n    @property\n    def point_scores(self) -&gt; ArrayLike:\n        \"\"\"Get the point scores associated with the pose prediction.\n\n        Returns: a vector of shape n containing the point scores outputed from sleap associated with pose predictions.\n        \"\"\"\n        return self._point_scores\n\n    @point_scores.setter\n    def point_scores(self, point_scores: ArrayLike) -&gt; None:\n        \"\"\"Set the point scores associated with the pose prediction.\n\n        Args:\n            point_scores: a vector of shape n containing the point scores\n            outputted from sleap associated with pose predictions.\n        \"\"\"\n        self._point_scores = point_scores\n\n    @property\n    def instance_score(self) -&gt; float:\n        \"\"\"Get the pose prediction score associated with the instance.\n\n        Returns: a float from 0-1 representing an instance_score.\n        \"\"\"\n        return self._instance_score\n\n    @instance_score.setter\n    def instance_score(self, instance_score: float) -&gt; None:\n        \"\"\"Set the pose prediction score associated with the instance.\n\n        Args:\n            instance_score: a float from 0-1 representing an instance_score.\n        \"\"\"\n        self._instance_score = instance_score\n\n    @property\n    def track_score(self) -&gt; float:\n        \"\"\"Get the track_score of the instance.\n\n        Returns: A float from 0-1 representing the output used in the tracker for assignment.\n        \"\"\"\n        return self._track_score\n\n    @track_score.setter\n    def track_score(self, track_score: float) -&gt; None:\n        \"\"\"Set the track_score of the instance.\n\n        Args:\n            track_score: A float from 0-1 representing the output used in the tracker for assignment.\n        \"\"\"\n        self._track_score = track_score\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.anchor","title":"<code>anchor: list[str]</code>  <code>property</code>","text":"<p>The anchor node name around which the crop was formed.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>the list of anchors around which each crop was formed the list of anchors around which each crop was formed</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.bbox","title":"<code>bbox: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>The bounding box coordinates of the instance in the original frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A (1,4) tensor containing the bounding box coordinates.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.centroid","title":"<code>centroid: dict[str, ArrayLike]</code>  <code>property</code> <code>writable</code>","text":"<p>The centroid around which the crop was formed.</p> <p>Returns:</p> Type Description <code>dict[str, ArrayLike]</code> <p>A dict containing the anchor name and the x, y bbox midpoint.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.crop","title":"<code>crop: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>The crop of the instance.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A (1, c, h , w) tensor containing the cropped image centered around the instance.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.device","title":"<code>device: str</code>  <code>property</code> <code>writable</code>","text":"<p>The device the instance is on.</p> <p>Returns:</p> Type Description <code>str</code> <p>The str representation of the device the gpu is on.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.features","title":"<code>features: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>Re-ID feature vector from backbone model to be used as input to transformer.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>a (1, d) tensor containing the reid feature vector.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.frame","title":"<code>frame: Frame</code>  <code>property</code> <code>writable</code>","text":"<p>Get the frame the instance belongs to.</p> <p>Returns:</p> Type Description <code>Frame</code> <p>The back reference to the <code>Frame</code> that this <code>Instance</code> belongs to.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.gt_track_id","title":"<code>gt_track_id: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>The ground truth track id of the instance.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the ground truth track id</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.instance_score","title":"<code>instance_score: float</code>  <code>property</code> <code>writable</code>","text":"<p>Get the pose prediction score associated with the instance.</p> <p>Returns: a float from 0-1 representing an instance_score.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.point_scores","title":"<code>point_scores: ArrayLike</code>  <code>property</code> <code>writable</code>","text":"<p>Get the point scores associated with the pose prediction.</p> <p>Returns: a vector of shape n containing the point scores outputed from sleap associated with pose predictions.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.pose","title":"<code>pose: dict[str, ArrayLike]</code>  <code>property</code> <code>writable</code>","text":"<p>Get the pose of the instance.</p> <p>Returns:</p> Type Description <code>dict[str, ArrayLike]</code> <p>A dictionary containing the node and corresponding x,y points</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.pred_track_id","title":"<code>pred_track_id: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>The track id predicted by the tracker using asso_output from model.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the predicted track id.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.shown_pose","title":"<code>shown_pose: dict[str, ArrayLike]</code>  <code>property</code>","text":"<p>Get the pose with shown nodes only.</p> <p>Returns: A dictionary filtered by nodes that are shown (points are not nan).</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.skeleton","title":"<code>skeleton: sio.Skeleton</code>  <code>property</code> <code>writable</code>","text":"<p>Get the skeleton associated with the instance.</p> <p>Returns: The sio.Skeleton associated with the instance.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.track_score","title":"<code>track_score: float</code>  <code>property</code> <code>writable</code>","text":"<p>Get the track_score of the instance.</p> <p>Returns: A float from 0-1 representing the output used in the tracker for assignment.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.__attrs_post_init__","title":"<code>__attrs_post_init__()</code>","text":"<p>Handle dimensionality and more intricate default initializations post-init.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def __attrs_post_init__(self) -&gt; None:\n    \"\"\"Handle dimensionality and more intricate default initializations post-init.\"\"\"\n    self.bbox = _expand_to_rank(self.bbox, 3)\n    self.crop = _expand_to_rank(self.crop, 4)\n    self.features = _expand_to_rank(self.features, 2)\n\n    if self.skeleton is None:\n        self.skeleton = sio.Skeleton([\"centroid\"])\n\n    if self.bbox.shape[-1] == 0:\n        self.bbox = torch.empty([1, 0, 4])\n\n    if self.crop.shape[-1] == 0 and self.bbox.shape[1] != 0:\n        y1, x1, y2, x2 = self.bbox.squeeze(dim=0).nanmean(dim=0)\n        self.centroid = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n    if len(self.pose) == 0 and self.bbox.shape[1]:\n        y1, x1, y2, x2 = self.bbox.squeeze(dim=0).mean(dim=0)\n        self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n    if self.point_scores is None and len(self.pose) != 0:\n        self._point_scores = np.zeros((len(self.pose), 2))\n\n    self.to(self.device)\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of the Instance.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation of the Instance.\"\"\"\n    return (\n        \"Instance(\"\n        f\"gt_track_id={self._gt_track_id.item()}, \"\n        f\"pred_track_id={self._pred_track_id.item()}, \"\n        f\"bbox={self._bbox}, \"\n        f\"centroid={self._centroid}, \"\n        f\"crop={self._crop.shape}, \"\n        f\"features={self._features.shape}, \"\n        f\"device={self._device}\"\n        \")\"\n    )\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.add_embedding","title":"<code>add_embedding(emb_type, embedding)</code>","text":"<p>Save embedding to instance embedding dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>Key/embedding type to be saved to dictionary</p> required <code>embedding</code> <code>Tensor</code> <p>The actual torch tensor embedding.</p> required Source code in <code>dreem/io/instance.py</code> <pre><code>def add_embedding(self, emb_type: str, embedding: torch.Tensor) -&gt; None:\n    \"\"\"Save embedding to instance embedding dictionary.\n\n    Args:\n        emb_type: Key/embedding type to be saved to dictionary\n        embedding: The actual torch tensor embedding.\n    \"\"\"\n    embedding = _expand_to_rank(embedding, 2)\n    self._embeddings[emb_type] = embedding\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.from_slp","title":"<code>from_slp(slp_instance, bbox_size=64, crop=None, device=None)</code>  <code>classmethod</code>","text":"<p>Convert a slp instance to a dreem instance.</p> <p>Parameters:</p> Name Type Description Default <code>slp_instance</code> <code>PredictedInstance | Instance</code> <p>A <code>sleap_io.Instance</code> object representing a detection</p> required <code>bbox_size</code> <code>int | tuple[int, int]</code> <p>size of the pose-centered bbox to form.</p> <code>64</code> <code>crop</code> <code>ArrayLike | None</code> <p>The corresponding crop of the bbox</p> <code>None</code> <code>device</code> <code>str | None</code> <p>which device to keep the instance on</p> <code>None</code> <p>Returns:     A dreem.Instance object with a pose-centered bbox and no crop.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>@classmethod\ndef from_slp(\n    cls,\n    slp_instance: sio.PredictedInstance | sio.Instance,\n    bbox_size: int | tuple[int, int] = 64,\n    crop: ArrayLike | None = None,\n    device: str | None = None,\n) -&gt; Self:\n    \"\"\"Convert a slp instance to a dreem instance.\n\n    Args:\n        slp_instance: A `sleap_io.Instance` object representing a detection\n        bbox_size: size of the pose-centered bbox to form.\n        crop: The corresponding crop of the bbox\n        device: which device to keep the instance on\n    Returns:\n        A dreem.Instance object with a pose-centered bbox and no crop.\n    \"\"\"\n    try:\n        track_id = int(slp_instance.track.name)\n    except ValueError:\n        track_id = int(\n            \"\".join([str(ord(c)) for c in slp_instance.track.name])\n        )  # better way to handle this?\n    if isinstance(bbox_size, int):\n        bbox_size = (bbox_size, bbox_size)\n\n    track_score = -1.0\n    point_scores = np.full(len(slp_instance.points), -1)\n    instance_score = -1\n    if isinstance(slp_instance, sio.PredictedInstance):\n        track_score = slp_instance.tracking_score\n        point_scores = slp_instance.numpy()[:, -1]\n        instance_score = slp_instance.score\n\n    centroid = np.nanmean(slp_instance.numpy(), axis=1)\n    bbox = [\n        centroid[1] - bbox_size[1],\n        centroid[0] - bbox_size[0],\n        centroid[1] + bbox_size[1],\n        centroid[0] + bbox_size[0],\n    ]\n    return cls(\n        gt_track_id=track_id,\n        bbox=bbox,\n        crop=crop,\n        centroid={\"centroid\": centroid},\n        track_score=track_score,\n        point_scores=point_scores,\n        instance_score=instance_score,\n        skeleton=slp_instance.skeleton,\n        pose={\n            node.name: point.numpy() for node, point in slp_instance.points.items()\n        },\n        device=device,\n    )\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.get_embedding","title":"<code>get_embedding(emb_type='all')</code>","text":"<p>Retrieve instance's spatial/temporal embedding.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>The string key of the embedding to retrieve. Should be \"pos\", \"temp\"</p> <code>'all'</code> <p>Returns:</p> Type Description <code>dict[str, Tensor] | Tensor | None</code> <ul> <li>A torch tensor representing the spatial/temporal location of the instance.</li> <li>None if the embedding is not stored</li> </ul> Source code in <code>dreem/io/instance.py</code> <pre><code>def get_embedding(\n    self, emb_type: str = \"all\"\n) -&gt; dict[str, torch.Tensor] | torch.Tensor | None:\n    \"\"\"Retrieve instance's spatial/temporal embedding.\n\n    Args:\n        emb_type: The string key of the embedding to retrieve. Should be \"pos\", \"temp\"\n\n    Returns:\n        * A torch tensor representing the spatial/temporal location of the instance.\n        * None if the embedding is not stored\n    \"\"\"\n    if emb_type.lower() == \"all\":\n        return self._embeddings\n    else:\n        try:\n            return self._embeddings[emb_type]\n        except KeyError:\n            logger.exception(\n                f\"{emb_type} not saved! Only {list(self._embeddings.keys())} are available\"\n            )\n    return None\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.has_bbox","title":"<code>has_bbox()</code>","text":"<p>Determine if the instance has a bbox.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has a bounding box, false otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_bbox(self) -&gt; bool:\n    \"\"\"Determine if the instance has a bbox.\n\n    Returns:\n        True if the instance has a bounding box, false otherwise.\n    \"\"\"\n    if self._bbox.shape[1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.has_crop","title":"<code>has_crop()</code>","text":"<p>Determine if the instance has a crop.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has an image otherwise False.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_crop(self) -&gt; bool:\n    \"\"\"Determine if the instance has a crop.\n\n    Returns:\n        True if the instance has an image otherwise False.\n    \"\"\"\n    if self._crop.shape[-1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.has_embedding","title":"<code>has_embedding(emb_type=None)</code>","text":"<p>Determine if the instance has embedding type requested.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str | None</code> <p>The key to check in the embedding dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if <code>emb_type</code> in embedding_dict else false</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_embedding(self, emb_type: str | None = None) -&gt; bool:\n    \"\"\"Determine if the instance has embedding type requested.\n\n    Args:\n        emb_type: The key to check in the embedding dictionary.\n\n    Returns:\n        True if `emb_type` in embedding_dict else false\n    \"\"\"\n    return emb_type in self._embeddings\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.has_features","title":"<code>has_features()</code>","text":"<p>Determine if the instance has computed reid features.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has reid features, False otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_features(self) -&gt; bool:\n    \"\"\"Determine if the instance has computed reid features.\n\n    Returns:\n        True if the instance has reid features, False otherwise.\n    \"\"\"\n    if self._features.shape[-1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.has_gt_track_id","title":"<code>has_gt_track_id()</code>","text":"<p>Determine if instance has a gt track assignment.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the gt track id is set, otherwise False.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_gt_track_id(self) -&gt; bool:\n    \"\"\"Determine if instance has a gt track assignment.\n\n    Returns:\n        True if the gt track id is set, otherwise False.\n    \"\"\"\n    if self._gt_track_id.shape[0] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.has_pose","title":"<code>has_pose()</code>","text":"<p>Check if the instance has a pose.</p> <p>Returns True if the instance has a pose.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_pose(self) -&gt; bool:\n    \"\"\"Check if the instance has a pose.\n\n    Returns True if the instance has a pose.\n    \"\"\"\n    if len(self.pose):\n        return True\n    return False\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.has_pred_track_id","title":"<code>has_pred_track_id()</code>","text":"<p>Determine whether instance has predicted track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if instance has a pred track id, False otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_pred_track_id(self) -&gt; bool:\n    \"\"\"Determine whether instance has predicted track id.\n\n    Returns:\n        True if instance has a pred track id, False otherwise.\n    \"\"\"\n    if self._pred_track_id.item() == -1 or self._pred_track_id.shape[0] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.to","title":"<code>to(map_location)</code>","text":"<p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <p>Parameters:</p> Name Type Description Default <code>map_location</code> <code>str | device</code> <p>Either the device or dtype for the instance to be moved.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>Self</code> <p>reference to the instance moved to correct device/dtype.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to(self, map_location: str | torch.device) -&gt; Self:\n    \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n    Args:\n        map_location: Either the device or dtype for the instance to be moved.\n\n    Returns:\n        self: reference to the instance moved to correct device/dtype.\n    \"\"\"\n    if map_location is not None and map_location != \"\":\n        self._gt_track_id = self._gt_track_id.to(map_location)\n        self._pred_track_id = self._pred_track_id.to(map_location)\n        self._bbox = self._bbox.to(map_location)\n        self._crop = self._crop.to(map_location)\n        self._features = self._features.to(map_location)\n        if isinstance(map_location, (str, torch.device)):\n            self.device = map_location\n\n    return self\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.to_h5","title":"<code>to_h5(frame_group=h5py.Group, label=None, **kwargs)</code>","text":"<p>Convert instance to an h5 group\".</p> By default we always save <ul> <li>the gt/pred track id</li> <li>bbox</li> <li>centroid</li> <li>pose</li> <li>instance/traj/points score</li> </ul> <p>Larger arrays (crops/features/embeddings) can be saved by passing as kwargs</p> <p>Parameters:</p> Name Type Description Default <code>frame_group</code> <p>the h5py group representing the frame the instance appears on</p> <code>Group</code> <code>label</code> <p>the name of the instance group that will be created</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>additional key:value pairs to be saved as datasets.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Group</code> <p>The h5 group representing this instance.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to_h5(self, frame_group=h5py.Group, label=None, **kwargs: dict) -&gt; h5py.Group:\n    \"\"\"Convert instance to an h5 group\".\n\n    By default we always save:\n        - the gt/pred track id\n        - bbox\n        - centroid\n        - pose\n        - instance/traj/points score\n    Larger arrays (crops/features/embeddings) can be saved by passing as kwargs\n\n    Args:\n        frame_group: the h5py group representing the frame the instance appears on\n        label: the name of the instance group that will be created\n        **kwargs: additional key:value pairs to be saved as datasets.\n\n    Returns:\n        The h5 group representing this instance.\n    \"\"\"\n    if label is None:\n        if pred_track_id != -1:\n            label = f\"instance_{self.pred_track_id.item()}\"\n        else:\n            label = f\"instance_{self.gt_track_id.item()}\"\n    instance_group = frame_group.create_group(label)\n    instance_group.attrs.create(\"gt_track_id\", self.gt_track_id.item())\n    instance_group.attrs.create(\"pred_track_id\", self.pred_track_id.item())\n    instance_group.attrs.create(\"track_score\", self.track_score)\n    instance_group.attrs.create(\"instance_score\", self.instance_score)\n\n    instance_group.create_dataset(\"bbox\", data=self.bbox.cpu().numpy())\n\n    pose_group = instance_group.create_group(\"pose\")\n    pose_group.create_dataset(\"points\", data=np.array(list(self.pose.values())))\n    pose_group.attrs.create(\"nodes\", list(self.pose.keys()))\n    pose_group.create_dataset(\"scores\", data=self.point_scores)\n\n    for key, value in kwargs.items():\n        if \"emb\" in key:\n            emb_group = instance_group.require_group(\"emb\")\n            emb_group.create_dataset(key, data=value)\n        else:\n            instance_group.create_dataset(key, data=value)\n\n    return instance_group\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.to_slp","title":"<code>to_slp(track_lookup={})</code>","text":"<p>Convert instance to sleap_io.PredictedInstance object.</p> <p>Parameters:</p> Name Type Description Default <code>track_lookup</code> <code>dict[int, Track]</code> <p>A track look up dictionary containing track_id:sio.Track.</p> <code>{}</code> <p>Returns: A sleap_io.PredictedInstance with necessary metadata     and a track_lookup dictionary to persist tracks.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to_slp(\n    self, track_lookup: dict[int, sio.Track] = {}\n) -&gt; tuple[sio.PredictedInstance, dict[int, sio.Track]]:\n    \"\"\"Convert instance to sleap_io.PredictedInstance object.\n\n    Args:\n        track_lookup: A track look up dictionary containing track_id:sio.Track.\n    Returns: A sleap_io.PredictedInstance with necessary metadata\n        and a track_lookup dictionary to persist tracks.\n    \"\"\"\n    try:\n        track_id = self.pred_track_id.item()\n        if track_id not in track_lookup:\n            track_lookup[track_id] = sio.Track(name=self.pred_track_id.item())\n\n        track = track_lookup[track_id]\n\n        return (\n            sio.PredictedInstance.from_numpy(\n                points=np.array(list(self.pose.values())),\n                skeleton=self.skeleton,\n                point_scores=self.point_scores,\n                instance_score=self.instance_score,\n                tracking_score=self.track_score,\n                track=track,\n            ),\n            track_lookup,\n        )\n    except Exception as e:\n        logger.exception(\n            f\"Pose: {np.array(list(self.pose.values())).shape}, Pose score shape {self.point_scores.shape}\"\n        )\n        raise RuntimeError(f\"Failed to convert to sio.PredictedInstance: {e}\")\n</code></pre>"},{"location":"reference/dreem/io/track/","title":"track","text":""},{"location":"reference/dreem/io/track/#dreem.io.track","title":"<code>dreem.io.track</code>","text":"<p>Module containing data structures for storing instances of the same Track.</p>"},{"location":"reference/dreem/io/track/#dreem.io.track.Track","title":"<code>Track</code>","text":"<p>Object for storing instances of the same track.</p> <p>Attributes:</p> Name Type Description <code>id</code> <p>the track label.</p> <code>instances</code> <code>list['Instances']</code> <p>A list of instances belonging to the track.</p> Source code in <code>dreem/io/track.py</code> <pre><code>@attrs.define(eq=False)\nclass Track:\n    \"\"\"Object for storing instances of the same track.\n\n    Attributes:\n        id: the track label.\n        instances: A list of instances belonging to the track.\n    \"\"\"\n\n    _id: int = attrs.field(alias=\"id\")\n    _instances: list[\"Instance\"] = attrs.field(alias=\"instances\", factory=list)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get the string representation of the track.\n\n        Returns:\n            the string representation of the Track.\n        \"\"\"\n        return f\"Track(id={self.id}, len={len(self)})\"\n\n    @property\n    def track_id(self) -&gt; int:\n        \"\"\"Get the id of the track.\n\n        Returns:\n            The integer id of the track.\n        \"\"\"\n        return self._id\n\n    @track_id.setter\n    def track_id(self, track_id: int) -&gt; None:\n        \"\"\"Set the id of the track.\n\n        Args:\n            track_id: the int id of the track.\n        \"\"\"\n        self._id = track_id\n\n    @property\n    def instances(self) -&gt; list[\"Instances\"]:\n        \"\"\"Get the instances belonging to this track.\n\n        Returns:\n            A list of instances with this track id.\n        \"\"\"\n        return self._instances\n\n    @instances.setter\n    def instances(self, instances) -&gt; None:\n        \"\"\"Set the instances belonging to this track.\n\n        Args:\n            instances: A list of instances that belong to the same track.\n        \"\"\"\n        self._instances = instances\n\n    @property\n    def frames(self) -&gt; set[\"Frame\"]:\n        \"\"\"Get the frames where this track appears.\n\n        Returns:\n            A set of `Frame` objects where this track appears.\n        \"\"\"\n        return set([instance.frame for instance in self.instances])\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the length of the track.\n\n        Returns:\n            The number of instances/frames in the track.\n        \"\"\"\n        return len(self.instances)\n\n    def __getitem__(self, ind: int | list[int]) -&gt; \"Instance\" | list[\"Instance\"]:\n        \"\"\"Get an instance from the track.\n\n        Args:\n            ind: Either a single int or list of int indices.\n\n        Returns:\n            the instance at that index of the track.instances.\n        \"\"\"\n        if isinstance(ind, int):\n            return self.instances[ind]\n        elif isinstance(ind, list):\n            return [self.instances[i] for i in ind]\n        else:\n            raise ValueError(f\"Ind must be an int or list of ints, found {type(ind)}\")\n</code></pre>"},{"location":"reference/dreem/io/track/#dreem.io.track.Track.frames","title":"<code>frames: set['Frame']</code>  <code>property</code>","text":"<p>Get the frames where this track appears.</p> <p>Returns:</p> Type Description <code>set['Frame']</code> <p>A set of <code>Frame</code> objects where this track appears.</p>"},{"location":"reference/dreem/io/track/#dreem.io.track.Track.instances","title":"<code>instances: list['Instances']</code>  <code>property</code> <code>writable</code>","text":"<p>Get the instances belonging to this track.</p> <p>Returns:</p> Type Description <code>list['Instances']</code> <p>A list of instances with this track id.</p>"},{"location":"reference/dreem/io/track/#dreem.io.track.Track.track_id","title":"<code>track_id: int</code>  <code>property</code> <code>writable</code>","text":"<p>Get the id of the track.</p> <p>Returns:</p> Type Description <code>int</code> <p>The integer id of the track.</p>"},{"location":"reference/dreem/io/track/#dreem.io.track.Track.__getitem__","title":"<code>__getitem__(ind)</code>","text":"<p>Get an instance from the track.</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> <code>int | list[int]</code> <p>Either a single int or list of int indices.</p> required <p>Returns:</p> Type Description <code>'Instance' | list['Instance']</code> <p>the instance at that index of the track.instances.</p> Source code in <code>dreem/io/track.py</code> <pre><code>def __getitem__(self, ind: int | list[int]) -&gt; \"Instance\" | list[\"Instance\"]:\n    \"\"\"Get an instance from the track.\n\n    Args:\n        ind: Either a single int or list of int indices.\n\n    Returns:\n        the instance at that index of the track.instances.\n    \"\"\"\n    if isinstance(ind, int):\n        return self.instances[ind]\n    elif isinstance(ind, list):\n        return [self.instances[i] for i in ind]\n    else:\n        raise ValueError(f\"Ind must be an int or list of ints, found {type(ind)}\")\n</code></pre>"},{"location":"reference/dreem/io/track/#dreem.io.track.Track.__len__","title":"<code>__len__()</code>","text":"<p>Get the length of the track.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of instances/frames in the track.</p> Source code in <code>dreem/io/track.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the length of the track.\n\n    Returns:\n        The number of instances/frames in the track.\n    \"\"\"\n    return len(self.instances)\n</code></pre>"},{"location":"reference/dreem/io/track/#dreem.io.track.Track.__repr__","title":"<code>__repr__()</code>","text":"<p>Get the string representation of the track.</p> <p>Returns:</p> Type Description <code>str</code> <p>the string representation of the Track.</p> Source code in <code>dreem/io/track.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get the string representation of the track.\n\n    Returns:\n        the string representation of the Track.\n    \"\"\"\n    return f\"Track(id={self.id}, len={len(self)})\"\n</code></pre>"},{"location":"reference/dreem/io/visualize/","title":"visualize","text":""},{"location":"reference/dreem/io/visualize/#dreem.io.visualize","title":"<code>dreem.io.visualize</code>","text":"<p>Helper functions for visualizing tracking.</p>"},{"location":"reference/dreem/io/visualize/#dreem.io.visualize.annotate_video","title":"<code>annotate_video(video, labels, key, color_palette=palette, trails=2, boxes=(64, 64), names=True, track_scores=0.5, centroids=4, poses=False, save_path='debug_animal.mp4', fps=30, alpha=0.2)</code>","text":"<p>Annotate video frames with labels.</p> <p>Labels video with bboxes, centroids, trajectory trails, and/or poses.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>Reader</code> <p>The video to be annotated in an ndarray</p> required <code>labels</code> <code>DataFrame</code> <p>The pandas dataframe containing the centroid and/or pose locations of the instances</p> required <code>key</code> <code>str</code> <p>The key where labels are stored in the dataframe - mostly used for choosing whether to annotate based on pred or gt labels</p> required <code>color_palette</code> <code>list | str</code> <p>The matplotlib colorpalette to use for annotating the video. Defaults to <code>tab10</code></p> <code>palette</code> <code>trails</code> <code>int</code> <p>The size of the trajectory trail. If trails size &lt;= 0 or None then it is not added</p> <code>2</code> <code>boxes</code> <code>int</code> <p>The size of the bbox. If bbox size &lt;= 0 or None then it is not added</p> <code>(64, 64)</code> <code>names</code> <code>bool</code> <p>Whether or not to annotate with name</p> <code>True</code> <code>centroids</code> <code>int</code> <p>The size of the centroid. If centroid size &lt;= 0 or None then it is not added</p> <code>4</code> <code>poses</code> <code>bool</code> <p>Whether or not to annotate with poses</p> <code>False</code> <code>fps</code> <code>int</code> <p>The frame rate of the generated video</p> <code>30</code> <code>alpha</code> <code>float</code> <p>The opacity of the annotations.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of annotated video frames</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def annotate_video(\n    video: \"imageio.core.format.Reader\",\n    labels: pd.DataFrame,\n    key: str,\n    color_palette: list | str = palette,\n    trails: int = 2,\n    boxes: int = (64, 64),\n    names: bool = True,\n    track_scores=0.5,\n    centroids: int = 4,\n    poses: bool = False,\n    save_path: str = \"debug_animal.mp4\",\n    fps: int = 30,\n    alpha: float = 0.2,\n) -&gt; list:\n    \"\"\"Annotate video frames with labels.\n\n    Labels video with bboxes, centroids, trajectory trails, and/or poses.\n\n    Args:\n        video: The video to be annotated in an ndarray\n        labels: The pandas dataframe containing the centroid and/or pose locations of the instances\n        key: The key where labels are stored in the dataframe - mostly used for choosing whether to annotate based on pred or gt labels\n        color_palette: The matplotlib colorpalette to use for annotating the video. Defaults to `tab10`\n        trails: The size of the trajectory trail. If trails size &lt;= 0 or None then it is not added\n        boxes: The size of the bbox. If bbox size &lt;= 0 or None then it is not added\n        names: Whether or not to annotate with name\n        centroids: The size of the centroid. If centroid size &lt;= 0 or None then it is not added\n        poses: Whether or not to annotate with poses\n        fps: The frame rate of the generated video\n        alpha: The opacity of the annotations.\n\n    Returns:\n        A list of annotated video frames\n    \"\"\"\n    writer = imageio.get_writer(save_path, fps=fps)\n    color_palette = (\n        sns.color_palette(color_palette)\n        if isinstance(color_palette, str)\n        else deepcopy(color_palette)\n    )\n\n    if trails:\n        track_trails = {}\n    try:\n        for i in tqdm(sorted(labels[\"Frame\"].unique()), desc=\"Frame\", unit=\"Frame\"):\n            frame = video.get_data(i)\n            if frame.shape[0] == 1 or frame.shape[-1] == 1:\n                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n            # else:\n            #     frame = frame.copy()\n\n            lf = labels[labels[\"Frame\"] == i]\n            for idx, instance in lf.iterrows():\n                if not trails:\n                    track_trails = {}\n\n                if poses:\n                    # TODO figure out best way to store poses (maybe pass a slp labels file too?)\n                    trails = False\n                    centroids = False\n                    for idx, (pose, edge) in enumerate(\n                        zip(instance[\"poses\"], instance[\"edges\"])\n                    ):\n                        pose = fill_missing(pose.numpy())\n\n                        pred_track_id = instance[key][idx].numpy().tolist()\n\n                        # Add midpt to track trail.\n                        if pred_track_id not in list(track_trails.keys()):\n                            track_trails[pred_track_id] = []\n\n                        # Select a color based on track_id.\n                        track_color_idx = pred_track_id % len(color_palette)\n                        track_color = (\n                            (np.array(color_palette[track_color_idx]) * 255)\n                            .astype(np.uint8)\n                            .tolist()[::-1]\n                        )\n\n                        for p in pose:\n                            # try:\n                            #    p = tuple([int(i) for i in p.numpy()][::-1])\n                            # except:\n                            #    continue\n\n                            p = tuple(int(i) for i in p)[::-1]\n\n                            track_trails[pred_track_id].append(p)\n\n                            frame = cv2.circle(\n                                frame, p, radius=2, color=track_color, thickness=-1\n                            )\n\n                        for e in edge:\n                            source = tuple(int(i) for i in pose[int(e[0])])[::-1]\n                            target = tuple(int(i) for i in pose[int(e[1])])[::-1]\n\n                            frame = cv2.line(frame, source, target, track_color, 1)\n\n                if (boxes) or centroids:\n                    # Get coordinates for detected objects in the current frame.\n                    if isinstance(boxes, int):\n                        boxes = (boxes, boxes)\n\n                    box_w, box_h = boxes\n                    x = instance[\"X\"]\n                    y = instance[\"Y\"]\n                    min_x, min_y, max_x, max_y = (\n                        int(x - box_w / 2),\n                        int(y - box_h / 2),\n                        int(x + box_w / 2),\n                        int(y + box_h / 2),\n                    )\n                    midpt = (int(x), int(y))\n\n                    pred_track_id = instance[key]\n\n                    if \"Track_score\" in instance.index:\n                        track_score = instance[\"Track_score\"]\n                    else:\n                        track_scores = 0\n\n                    # Add midpt to track trail.\n                    if pred_track_id not in list(track_trails.keys()):\n                        track_trails[pred_track_id] = []\n                    track_trails[pred_track_id].append(midpt)\n\n                    # Select a color based on track_id.\n                    track_color_idx = int(pred_track_id) % len(color_palette)\n                    track_color = (\n                        (np.array(color_palette[track_color_idx]) * 255)\n                        .astype(np.uint8)\n                        .tolist()[::-1]\n                    )\n\n                    # Bbox.\n                    if boxes is not None:\n                        frame = cv2.rectangle(\n                            frame,\n                            (min_x, min_y),\n                            (max_x, max_y),\n                            color=track_color,\n                            thickness=2,\n                        )\n\n                    # Track trail.\n                    if centroids:\n                        frame = cv2.circle(\n                            frame,\n                            midpt,\n                            radius=centroids,\n                            color=track_color,\n                            thickness=-1,\n                        )\n                        for i in range(0, len(track_trails[pred_track_id]) - 1):\n                            frame = cv2.addWeighted(\n                                cv2.circle(\n                                    frame,  # .copy(),\n                                    track_trails[pred_track_id][i],\n                                    radius=4,\n                                    color=track_color,\n                                    thickness=-1,\n                                ),\n                                alpha,\n                                frame,\n                                1 - alpha,\n                                0,\n                            )\n                            if trails:\n                                frame = cv2.line(\n                                    frame,\n                                    track_trails[pred_track_id][i],\n                                    track_trails[pred_track_id][i + 1],\n                                    color=track_color,\n                                    thickness=trails,\n                                )\n\n                # Track name.\n                name_str = \"\"\n\n                if names:\n                    name_str += f\"track_{pred_track_id}\"\n                if names and track_scores:\n                    name_str += \" | \"\n                if track_scores:\n                    name_str += f\"score: {track_score:0.3f}\"\n\n                if len(name_str) &gt; 0:\n                    frame = cv2.putText(\n                        frame,\n                        # f\"idx:{idx} | track_{pred_track_id}\",\n                        name_str,\n                        org=(int(min_x), max(0, int(min_y) - 10)),\n                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n                        fontScale=0.9,\n                        color=track_color,\n                        thickness=2,\n                    )\n            writer.append_data(frame)\n            # if i % fps == 0:\n            #     gc.collect()\n\n    except Exception as e:\n        writer.close()\n        logger.exception(e)\n        return False\n\n    writer.close()\n    return True\n</code></pre>"},{"location":"reference/dreem/io/visualize/#dreem.io.visualize.bold","title":"<code>bold(val, thresh=0.01)</code>","text":"<p>Bold value if it is over a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>float</code> <p>The value to bold or not</p> required <code>thresh</code> <code>float</code> <p>The threshold the value has to exceed to be bolded</p> <code>0.01</code> <p>Returns:</p> Type Description <code>str</code> <p>A string indicating how to bold the item.</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def bold(val: float, thresh: float = 0.01) -&gt; str:\n    \"\"\"Bold value if it is over a threshold.\n\n    Args:\n        val: The value to bold or not\n        thresh: The threshold the value has to exceed to be bolded\n\n    Returns:\n        A string indicating how to bold the item.\n    \"\"\"\n    bold = \"bold\" if float(val) &gt; thresh else \"\"\n    return f\"font-weight: {bold}\"\n</code></pre>"},{"location":"reference/dreem/io/visualize/#dreem.io.visualize.color","title":"<code>color(val, thresh=0.01)</code>","text":"<p>Highlight value in dataframe if it is over a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>float</code> <p>The value to color</p> required <code>thresh</code> <code>float</code> <p>The threshold for which to color</p> <code>0.01</code> <p>Returns:</p> Type Description <code>str</code> <p>A string containing how to highlight the value</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def color(val: float, thresh: float = 0.01) -&gt; str:\n    \"\"\"Highlight value in dataframe if it is over a threshold.\n\n    Args:\n        val: The value to color\n        thresh: The threshold for which to color\n\n    Returns:\n        A string containing how to highlight the value\n    \"\"\"\n    color = \"lightblue\" if float(val) &gt; thresh else \"\"\n    return f\"background-color: {color}\"\n</code></pre>"},{"location":"reference/dreem/io/visualize/#dreem.io.visualize.fill_missing","title":"<code>fill_missing(data, kind='linear')</code>","text":"<p>Fill missing values independently along each dimension after the first.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>the array for which to fill missing value</p> required <code>kind</code> <code>str</code> <p>How to interpolate missing values using <code>scipy.interpoloate.interp1d</code></p> <code>'linear'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The array with missing values filled in</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def fill_missing(data: np.ndarray, kind: str = \"linear\") -&gt; np.ndarray:\n    \"\"\"Fill missing values independently along each dimension after the first.\n\n    Args:\n        data: the array for which to fill missing value\n        kind: How to interpolate missing values using `scipy.interpoloate.interp1d`\n\n    Returns:\n        The array with missing values filled in\n    \"\"\"\n    # Store initial shape.\n    initial_shape = data.shape\n\n    # Flatten after first dim.\n    data = data.reshape((initial_shape[0], -1))\n\n    # Interpolate along each slice.\n    for i in range(data.shape[-1]):\n        y = data[:, i]\n\n        # Build interpolant.\n        x = np.flatnonzero(~np.isnan(y))\n        f = interp1d(x, y[x], kind=kind, fill_value=np.nan, bounds_error=False)\n\n        # Fill missing\n        xq = np.flatnonzero(np.isnan(y))\n        y[xq] = f(xq)\n\n        # Fill leading or trailing NaNs with the nearest non-NaN values\n        mask = np.isnan(y)\n        y[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), y[~mask])\n\n        # Save slice\n        data[:, i] = y\n\n    # Restore to initial shape.\n    data = data.reshape(initial_shape)\n\n    return data\n</code></pre>"},{"location":"reference/dreem/io/visualize/#dreem.io.visualize.main","title":"<code>main(cfg)</code>","text":"<p>Take in a path to a video + labels file, annotates a video and saves it to the specified path.</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>@hydra.main(config_path=None, config_name=None, version_base=None)\ndef main(cfg: DictConfig):\n    \"\"\"Take in a path to a video + labels file, annotates a video and saves it to the specified path.\"\"\"\n    labels = pd.read_csv(cfg.labels_path)\n    video = imageio.get_reader(cfg.vid_path, \"ffmpeg\")\n    frames_annotated = annotate_video(\n        video, labels, save_path=cfg.save_path, **cfg.annotate\n    )\n\n    if frames_annotated:\n        logger.info(\"Video saved to {cfg.save_path}!\")\n    else:\n        logger.error(\"Failed to annotate video!\")\n</code></pre>"},{"location":"reference/dreem/io/visualize/#dreem.io.visualize.save_vid","title":"<code>save_vid(annotated_frames, save_path='debug_animal', fps=30)</code>","text":"<p>Save video to file.</p> <p>Parameters:</p> Name Type Description Default <code>annotated_frames</code> <code>list</code> <p>a list of frames annotated by <code>annotate_frames</code></p> required <code>save_path</code> <code>str</code> <p>The path of the annotated file.</p> <code>'debug_animal'</code> <code>fps</code> <code>int</code> <p>The frame rate in frames per second of the annotated video</p> <code>30</code> Source code in <code>dreem/io/visualize.py</code> <pre><code>def save_vid(\n    annotated_frames: list,\n    save_path: str = \"debug_animal\",\n    fps: int = 30,\n):\n    \"\"\"Save video to file.\n\n    Args:\n        annotated_frames: a list of frames annotated by `annotate_frames`\n        save_path: The path of the annotated file.\n        fps: The frame rate in frames per second of the annotated video\n    \"\"\"\n    for idx, (ds_name, data) in enumerate([(save_path, annotated_frames)]):\n        imageio.mimwrite(f\"{ds_name}.mp4\", data, fps=fps, macro_block_size=1)\n</code></pre>"},{"location":"reference/dreem/models/","title":"models","text":""},{"location":"reference/dreem/models/#dreem.models","title":"<code>dreem.models</code>","text":"<p>Model architectures and layers.</p>"},{"location":"reference/dreem/models/attention_head/","title":"attention_head","text":""},{"location":"reference/dreem/models/attention_head/#dreem.models.attention_head","title":"<code>dreem.models.attention_head</code>","text":"<p>Module containing different components of multi-head attention heads.</p>"},{"location":"reference/dreem/models/attention_head/#dreem.models.attention_head.ATTWeightHead","title":"<code>ATTWeightHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Single attention head.</p> Source code in <code>dreem/models/attention_head.py</code> <pre><code>class ATTWeightHead(torch.nn.Module):\n    \"\"\"Single attention head.\"\"\"\n\n    def __init__(\n        self,\n        feature_dim: int,\n        num_layers: int,\n        dropout: float,\n    ):\n        \"\"\"Initialize an instance of ATTWeightHead.\n\n        Args:\n            feature_dim: The dimensionality of input features.\n            num_layers: The number of hidden layers in the MLP.\n            dropout: Dropout probability.\n        \"\"\"\n        super().__init__()\n\n        self.q_proj = MLP(feature_dim, feature_dim, feature_dim, num_layers, dropout)\n        self.k_proj = MLP(feature_dim, feature_dim, feature_dim, num_layers, dropout)\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the attention weights of a query tensor using the key tensor.\n\n        Args:\n            query: Input tensor of shape (batch_size, num_frame_instances, feature_dim).\n            key: Input tensor of shape (batch_size, num_window_instances, feature_dim).\n\n        Returns:\n            Output tensor of shape (batch_size, num_frame_instances, num_window_instances).\n        \"\"\"\n        k = self.k_proj(key)\n        q = self.q_proj(query)\n        attn_weights = torch.bmm(q, k.transpose(1, 2))\n\n        return attn_weights  # (B, N_t, N)\n</code></pre>"},{"location":"reference/dreem/models/attention_head/#dreem.models.attention_head.ATTWeightHead.__init__","title":"<code>__init__(feature_dim, num_layers, dropout)</code>","text":"<p>Initialize an instance of ATTWeightHead.</p> <p>Parameters:</p> Name Type Description Default <code>feature_dim</code> <code>int</code> <p>The dimensionality of input features.</p> required <code>num_layers</code> <code>int</code> <p>The number of hidden layers in the MLP.</p> required <code>dropout</code> <code>float</code> <p>Dropout probability.</p> required Source code in <code>dreem/models/attention_head.py</code> <pre><code>def __init__(\n    self,\n    feature_dim: int,\n    num_layers: int,\n    dropout: float,\n):\n    \"\"\"Initialize an instance of ATTWeightHead.\n\n    Args:\n        feature_dim: The dimensionality of input features.\n        num_layers: The number of hidden layers in the MLP.\n        dropout: Dropout probability.\n    \"\"\"\n    super().__init__()\n\n    self.q_proj = MLP(feature_dim, feature_dim, feature_dim, num_layers, dropout)\n    self.k_proj = MLP(feature_dim, feature_dim, feature_dim, num_layers, dropout)\n</code></pre>"},{"location":"reference/dreem/models/attention_head/#dreem.models.attention_head.ATTWeightHead.forward","title":"<code>forward(query, key)</code>","text":"<p>Compute the attention weights of a query tensor using the key tensor.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, num_frame_instances, feature_dim).</p> required <code>key</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, num_window_instances, feature_dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, num_frame_instances, num_window_instances).</p> Source code in <code>dreem/models/attention_head.py</code> <pre><code>def forward(\n    self,\n    query: torch.Tensor,\n    key: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the attention weights of a query tensor using the key tensor.\n\n    Args:\n        query: Input tensor of shape (batch_size, num_frame_instances, feature_dim).\n        key: Input tensor of shape (batch_size, num_window_instances, feature_dim).\n\n    Returns:\n        Output tensor of shape (batch_size, num_frame_instances, num_window_instances).\n    \"\"\"\n    k = self.k_proj(key)\n    q = self.q_proj(query)\n    attn_weights = torch.bmm(q, k.transpose(1, 2))\n\n    return attn_weights  # (B, N_t, N)\n</code></pre>"},{"location":"reference/dreem/models/embedding/","title":"embedding","text":""},{"location":"reference/dreem/models/embedding/#dreem.models.embedding","title":"<code>dreem.models.embedding</code>","text":"<p>Module containing different position and temporal embeddings.</p>"},{"location":"reference/dreem/models/embedding/#dreem.models.embedding.Embedding","title":"<code>Embedding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Class that wraps around different embedding types.</p> <p>Used for both learned and fixed embeddings.</p> Source code in <code>dreem/models/embedding.py</code> <pre><code>class Embedding(torch.nn.Module):\n    \"\"\"Class that wraps around different embedding types.\n\n    Used for both learned and fixed embeddings.\n    \"\"\"\n\n    EMB_TYPES = {\n        \"temp\": {},\n        \"pos\": {\"over_boxes\"},\n        \"off\": {},\n        None: {},\n    }  # dict of valid args:keyword params\n    EMB_MODES = {\n        \"fixed\": {\"temperature\", \"scale\", \"normalize\"},\n        \"learned\": {\"emb_num\"},\n        \"off\": {},\n    }  # dict of valid args:keyword params\n\n    def __init__(\n        self,\n        emb_type: str,\n        mode: str,\n        features: int,\n        n_points: int = 1,\n        emb_num: int = 16,\n        over_boxes: bool = True,\n        temperature: int = 10000,\n        normalize: bool = False,\n        scale: float | None = None,\n        mlp_cfg: dict | None = None,\n    ):\n        \"\"\"Initialize embeddings.\n\n        Args:\n            emb_type: The type of embedding to compute. Must be one of `{\"temp\", \"pos\", \"off\"}`\n            mode: The mode or function used to map positions to vector embeddings.\n                  Must be one of `{\"fixed\", \"learned\", \"off\"}`\n            features: The embedding dimensions. Must match the dimension of the\n                      input vectors for the transformer model.\n            n_points: the number of points that will be embedded.\n            emb_num: the number of embeddings in the `self.lookup` table (Only used in learned embeddings).\n            over_boxes: Whether to compute the position embedding for each bbox coordinate (y1x1y2x2) or the centroid + bbox size (yxwh).\n            temperature: the temperature constant to be used when computing the sinusoidal position embedding\n            normalize: whether or not to normalize the positions (Only used in fixed embeddings).\n            scale: factor by which to scale the positions after normalizing (Only used in fixed embeddings).\n            mlp_cfg: A dictionary of mlp hyperparameters for projecting embedding to correct space.\n                    Example: {\"hidden_dims\": 256, \"num_layers\":3, \"dropout\": 0.3}\n        \"\"\"\n        self._check_init_args(emb_type, mode)\n\n        super().__init__()\n\n        self.emb_type = emb_type\n        self.mode = mode\n        self.features = features\n        self.emb_num = emb_num\n        self.over_boxes = over_boxes\n        self.temperature = temperature\n        self.normalize = normalize\n        self.scale = scale\n        self.n_points = n_points\n\n        if self.normalize and self.scale is None:\n            self.scale = 2 * math.pi\n\n        if self.emb_type == \"pos\" and mlp_cfg is not None and mlp_cfg[\"num_layers\"] &gt; 0:\n            if self.mode == \"fixed\":\n                self.mlp = MLP(\n                    input_dim=n_points * self.features,\n                    output_dim=self.features,\n                    **mlp_cfg,\n                )\n            else:\n                in_dim = (self.features // (4 * n_points)) * (4 * n_points)\n                self.mlp = MLP(\n                    input_dim=in_dim,\n                    output_dim=self.features,\n                    **mlp_cfg,\n                )\n        else:\n            self.mlp = torch.nn.Identity()\n\n        self._emb_func = lambda tensor: torch.zeros(\n            (tensor.shape[0], self.features), dtype=tensor.dtype, device=tensor.device\n        )  # turn off embedding by returning zeros\n\n        self.lookup = None\n\n        if self.mode == \"learned\":\n            if self.emb_type == \"pos\":\n                self.lookup = torch.nn.Embedding(\n                    self.emb_num * 4 * self.n_points, self.features // (4 * n_points)\n                )\n                self._emb_func = self._learned_pos_embedding\n            elif self.emb_type == \"temp\":\n                self.lookup = torch.nn.Embedding(self.emb_num, self.features)\n                self._emb_func = self._learned_temp_embedding\n\n        elif self.mode == \"fixed\":\n            if self.emb_type == \"pos\":\n                self._emb_func = self._sine_box_embedding\n            elif self.emb_type == \"temp\":\n                self._emb_func = self._sine_temp_embedding\n\n    def _check_init_args(self, emb_type: str, mode: str):\n        \"\"\"Check whether the correct arguments were passed to initialization.\n\n        Args:\n            emb_type: The type of embedding to compute. Must be one of `{\"temp\", \"pos\", \"\"}`\n            mode: The mode or function used to map positions to vector embeddings.\n                Must be one of `{\"fixed\", \"learned\"}`\n\n        Raises:\n            ValueError:\n              * if the incorrect `emb_type` or `mode` string are passed\n            NotImplementedError: if `emb_type` is `temp` and `mode` is `fixed`.\n        \"\"\"\n        if emb_type.lower() not in self.EMB_TYPES:\n            raise ValueError(\n                f\"Embedding `emb_type` must be one of {self.EMB_TYPES} not {emb_type}\"\n            )\n\n        if mode.lower() not in self.EMB_MODES:\n            raise ValueError(\n                f\"Embedding `mode` must be one of {self.EMB_MODES} not {mode}\"\n            )\n\n    def forward(self, seq_positions: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Get the sequence positional embeddings.\n\n        Args:\n            seq_positions:\n                * An (`N`, 1) tensor where seq_positions[i] represents the temporal position of instance_i in the sequence.\n                * An (`N`, n_anchors x 4) tensor where seq_positions[i, j, :] represents the [y1, x1, y2, x2] spatial locations of jth point of instance_i in the sequence.\n\n        Returns:\n            An `N` x `self.features` tensor representing the corresponding spatial or temporal embedding.\n        \"\"\"\n        emb = self._emb_func(seq_positions)\n\n        if emb.shape[-1] != self.features:\n            raise RuntimeError(\n                (\n                    f\"Output embedding dimension is {emb.shape[-1]} but requested {self.features} dimensions! \\n\"\n                    f\"hint: Try turning the MLP on by passing `mlp_cfg` to the constructor to project to the correct embedding dimensions.\"\n                )\n            )\n        return emb\n\n    def _torch_int_div(\n        self, tensor1: torch.Tensor, tensor2: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Perform integer division of two tensors.\n\n        Args:\n            tensor1: dividend tensor.\n            tensor2: divisor tensor.\n\n        Returns:\n            torch.Tensor, resulting tensor.\n        \"\"\"\n        return torch.div(tensor1, tensor2, rounding_mode=\"floor\")\n\n    def _sine_box_embedding(self, boxes: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute sine positional embeddings for boxes using given parameters.\n\n         Args:\n             boxes: the input boxes of shape N, n_anchors, 4 or B, N, n_anchors, 4\n                    where the last dimension is the bbox coords in [y1, x1, y2, x2].\n                    (Note currently `B=batch_size=1`).\n\n        Returns:\n             torch.Tensor, the sine positional embeddings\n             (embedding[:, 4i] = sin(x)\n              embedding[:, 4i+1] = cos(x)\n              embedding[:, 4i+2] = sin(y)\n              embedding[:, 4i+3] = cos(y)\n              )\n        \"\"\"\n        if self.scale is not None and self.normalize is False:\n            raise ValueError(\"normalize should be True if scale is passed\")\n\n        if len(boxes.size()) == 3:\n            boxes = boxes.unsqueeze(0)\n\n        if self.normalize:\n            boxes = boxes / (boxes[:, :, -1:] + 1e-6) * self.scale\n\n        dim_t = torch.arange(self.features // 4, dtype=torch.float32)\n\n        dim_t = self.temperature ** (\n            2 * self._torch_int_div(dim_t, 2) / (self.features // 4)\n        )\n\n        # (b, n_t, n_anchors, 4, D//4)\n        pos_emb = boxes[:, :, :, :, None] / dim_t.to(boxes.device)\n\n        pos_emb = torch.stack(\n            (pos_emb[:, :, :, :, 0::2].sin(), pos_emb[:, :, :, :, 1::2].cos()), dim=4\n        )\n        pos_emb = pos_emb.flatten(2).squeeze(0)  # (N_t, n_anchors * D)\n\n        pos_emb = self.mlp(pos_emb)\n\n        pos_emb = pos_emb.view(boxes.shape[1], self.features)\n\n        return pos_emb\n\n    def _sine_temp_embedding(self, times: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute fixed sine temporal embeddings.\n\n        Args:\n            times: the input times of shape (N,) or (N,1) where N = (sum(instances_per_frame))\n            which is the frame index of the instance relative\n            to the batch size\n            (e.g. `torch.tensor([0, 0, ..., 0, 1, 1, ..., 1, 2, 2, ..., 2,..., B, B, ...B])`).\n\n        Returns:\n            an n_instances x D embedding representing the temporal embedding.\n        \"\"\"\n        T = times.int().max().item() + 1\n        d = self.features\n        n = self.temperature\n\n        positions = torch.arange(0, T).unsqueeze(1)\n        temp_lookup = torch.zeros(T, d, device=times.device)\n\n        denominators = torch.pow(\n            n, 2 * torch.arange(0, d // 2) / d\n        )  # 10000^(2i/d_model), i is the index of embedding\n        temp_lookup[:, 0::2] = torch.sin(\n            positions / denominators\n        )  # sin(pos/10000^(2i/d_model))\n        temp_lookup[:, 1::2] = torch.cos(\n            positions / denominators\n        )  # cos(pos/10000^(2i/d_model))\n\n        temp_emb = temp_lookup[times.int()]\n        return temp_emb  # .view(len(times), self.features)\n\n    def _learned_pos_embedding(self, boxes: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute learned positional embeddings for boxes using given parameters.\n\n        Args:\n            boxes: the input boxes of shape N x 4 or B x N x 4\n                   where the last dimension is the bbox coords in [y1, x1, y2, x2].\n                   (Note currently `B=batch_size=1`).\n\n        Returns:\n            torch.Tensor, the learned positional embeddings.\n        \"\"\"\n        pos_lookup = self.lookup\n\n        N, n_anchors, _ = boxes.shape\n        boxes = boxes.view(N, n_anchors, 4)\n\n        if self.over_boxes:\n            xywh = boxes\n        else:\n            xywh = torch.cat(\n                [\n                    (boxes[:, :, 2:] + boxes[:, :, :2]) / 2,\n                    (boxes[:, :, 2:] - boxes[:, :, :2]),\n                ],\n                dim=1,\n            )\n\n        left_ind, right_ind, left_weight, right_weight = self._compute_weights(xywh)\n        f = pos_lookup.weight.shape[1]  # self.features // 4\n\n        try:\n            pos_emb_table = pos_lookup.weight.view(\n                self.emb_num, n_anchors, 4, f\n            )  # T x 4 x (D * 4)\n        except RuntimeError as e:\n            logger.exception(\n                f\"Hint: `n_points` ({self.n_points}) may be set incorrectly!\"\n            )\n            logger.exception(e)\n            raise (e)\n\n        left_emb = pos_emb_table.gather(\n            0,\n            left_ind[:, :, :, None].to(pos_emb_table.device).expand(N, n_anchors, 4, f),\n        )  # N x 4 x d\n        right_emb = pos_emb_table.gather(\n            0,\n            right_ind[:, :, :, None]\n            .to(pos_emb_table.device)\n            .expand(N, n_anchors, 4, f),\n        )  # N x 4 x d\n        pos_emb = left_weight[:, :, :, None] * right_emb.to(\n            left_weight.device\n        ) + right_weight[:, :, :, None] * left_emb.to(right_weight.device)\n\n        pos_emb = pos_emb.flatten(1)\n        pos_emb = self.mlp(pos_emb)\n\n        return pos_emb.view(N, self.features)\n\n    def _learned_temp_embedding(self, times: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute learned temporal embeddings for times using given parameters.\n\n        Args:\n            times: the input times of shape (N,) or (N,1) where N = (sum(instances_per_frame))\n            which is the frame index of the instance relative\n            to the batch size\n            (e.g. `torch.tensor([0, 0, ..., 0, 1, 1, ..., 1, 2, 2, ..., 2,..., B, B, ...B])`).\n\n        Returns:\n            torch.Tensor, the learned temporal embeddings.\n        \"\"\"\n        temp_lookup = self.lookup\n        N = times.shape[0]\n\n        left_ind, right_ind, left_weight, right_weight = self._compute_weights(times)\n\n        left_emb = temp_lookup.weight[\n            left_ind.to(temp_lookup.weight.device)\n        ]  # T x D --&gt; N x D\n        right_emb = temp_lookup.weight[right_ind.to(temp_lookup.weight.device)]\n\n        temp_emb = left_weight[:, None] * right_emb.to(\n            left_weight.device\n        ) + right_weight[:, None] * left_emb.to(right_weight.device)\n\n        return temp_emb.view(N, self.features)\n\n    def _compute_weights(self, data: torch.Tensor) -&gt; tuple[torch.Tensor, ...]:\n        \"\"\"Compute left and right learned embedding weights.\n\n        Args:\n            data: the input data (e.g boxes or times).\n\n        Returns:\n            A torch.Tensor for each of the left/right indices and weights, respectively\n        \"\"\"\n        data = data * self.emb_num\n\n        left_ind = data.clamp(min=0, max=self.emb_num - 1).long()  # N x 4\n        right_ind = (left_ind + 1).clamp(min=0, max=self.emb_num - 1).long()  # N x 4\n\n        left_weight = data - left_ind.float()  # N x 4\n\n        right_weight = 1.0 - left_weight\n\n        return left_ind, right_ind, left_weight, right_weight\n</code></pre>"},{"location":"reference/dreem/models/embedding/#dreem.models.embedding.Embedding.__init__","title":"<code>__init__(emb_type, mode, features, n_points=1, emb_num=16, over_boxes=True, temperature=10000, normalize=False, scale=None, mlp_cfg=None)</code>","text":"<p>Initialize embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>The type of embedding to compute. Must be one of <code>{\"temp\", \"pos\", \"off\"}</code></p> required <code>mode</code> <code>str</code> <p>The mode or function used to map positions to vector embeddings.   Must be one of <code>{\"fixed\", \"learned\", \"off\"}</code></p> required <code>features</code> <code>int</code> <p>The embedding dimensions. Must match the dimension of the       input vectors for the transformer model.</p> required <code>n_points</code> <code>int</code> <p>the number of points that will be embedded.</p> <code>1</code> <code>emb_num</code> <code>int</code> <p>the number of embeddings in the <code>self.lookup</code> table (Only used in learned embeddings).</p> <code>16</code> <code>over_boxes</code> <code>bool</code> <p>Whether to compute the position embedding for each bbox coordinate (y1x1y2x2) or the centroid + bbox size (yxwh).</p> <code>True</code> <code>temperature</code> <code>int</code> <p>the temperature constant to be used when computing the sinusoidal position embedding</p> <code>10000</code> <code>normalize</code> <code>bool</code> <p>whether or not to normalize the positions (Only used in fixed embeddings).</p> <code>False</code> <code>scale</code> <code>float | None</code> <p>factor by which to scale the positions after normalizing (Only used in fixed embeddings).</p> <code>None</code> <code>mlp_cfg</code> <code>dict | None</code> <p>A dictionary of mlp hyperparameters for projecting embedding to correct space.     Example: {\"hidden_dims\": 256, \"num_layers\":3, \"dropout\": 0.3}</p> <code>None</code> Source code in <code>dreem/models/embedding.py</code> <pre><code>def __init__(\n    self,\n    emb_type: str,\n    mode: str,\n    features: int,\n    n_points: int = 1,\n    emb_num: int = 16,\n    over_boxes: bool = True,\n    temperature: int = 10000,\n    normalize: bool = False,\n    scale: float | None = None,\n    mlp_cfg: dict | None = None,\n):\n    \"\"\"Initialize embeddings.\n\n    Args:\n        emb_type: The type of embedding to compute. Must be one of `{\"temp\", \"pos\", \"off\"}`\n        mode: The mode or function used to map positions to vector embeddings.\n              Must be one of `{\"fixed\", \"learned\", \"off\"}`\n        features: The embedding dimensions. Must match the dimension of the\n                  input vectors for the transformer model.\n        n_points: the number of points that will be embedded.\n        emb_num: the number of embeddings in the `self.lookup` table (Only used in learned embeddings).\n        over_boxes: Whether to compute the position embedding for each bbox coordinate (y1x1y2x2) or the centroid + bbox size (yxwh).\n        temperature: the temperature constant to be used when computing the sinusoidal position embedding\n        normalize: whether or not to normalize the positions (Only used in fixed embeddings).\n        scale: factor by which to scale the positions after normalizing (Only used in fixed embeddings).\n        mlp_cfg: A dictionary of mlp hyperparameters for projecting embedding to correct space.\n                Example: {\"hidden_dims\": 256, \"num_layers\":3, \"dropout\": 0.3}\n    \"\"\"\n    self._check_init_args(emb_type, mode)\n\n    super().__init__()\n\n    self.emb_type = emb_type\n    self.mode = mode\n    self.features = features\n    self.emb_num = emb_num\n    self.over_boxes = over_boxes\n    self.temperature = temperature\n    self.normalize = normalize\n    self.scale = scale\n    self.n_points = n_points\n\n    if self.normalize and self.scale is None:\n        self.scale = 2 * math.pi\n\n    if self.emb_type == \"pos\" and mlp_cfg is not None and mlp_cfg[\"num_layers\"] &gt; 0:\n        if self.mode == \"fixed\":\n            self.mlp = MLP(\n                input_dim=n_points * self.features,\n                output_dim=self.features,\n                **mlp_cfg,\n            )\n        else:\n            in_dim = (self.features // (4 * n_points)) * (4 * n_points)\n            self.mlp = MLP(\n                input_dim=in_dim,\n                output_dim=self.features,\n                **mlp_cfg,\n            )\n    else:\n        self.mlp = torch.nn.Identity()\n\n    self._emb_func = lambda tensor: torch.zeros(\n        (tensor.shape[0], self.features), dtype=tensor.dtype, device=tensor.device\n    )  # turn off embedding by returning zeros\n\n    self.lookup = None\n\n    if self.mode == \"learned\":\n        if self.emb_type == \"pos\":\n            self.lookup = torch.nn.Embedding(\n                self.emb_num * 4 * self.n_points, self.features // (4 * n_points)\n            )\n            self._emb_func = self._learned_pos_embedding\n        elif self.emb_type == \"temp\":\n            self.lookup = torch.nn.Embedding(self.emb_num, self.features)\n            self._emb_func = self._learned_temp_embedding\n\n    elif self.mode == \"fixed\":\n        if self.emb_type == \"pos\":\n            self._emb_func = self._sine_box_embedding\n        elif self.emb_type == \"temp\":\n            self._emb_func = self._sine_temp_embedding\n</code></pre>"},{"location":"reference/dreem/models/embedding/#dreem.models.embedding.Embedding.forward","title":"<code>forward(seq_positions)</code>","text":"<p>Get the sequence positional embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>seq_positions</code> <code>Tensor</code> <ul> <li>An (<code>N</code>, 1) tensor where seq_positions[i] represents the temporal position of instance_i in the sequence.</li> <li>An (<code>N</code>, n_anchors x 4) tensor where seq_positions[i, j, :] represents the [y1, x1, y2, x2] spatial locations of jth point of instance_i in the sequence.</li> </ul> required <p>Returns:</p> Type Description <code>Tensor</code> <p>An <code>N</code> x <code>self.features</code> tensor representing the corresponding spatial or temporal embedding.</p> Source code in <code>dreem/models/embedding.py</code> <pre><code>def forward(self, seq_positions: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Get the sequence positional embeddings.\n\n    Args:\n        seq_positions:\n            * An (`N`, 1) tensor where seq_positions[i] represents the temporal position of instance_i in the sequence.\n            * An (`N`, n_anchors x 4) tensor where seq_positions[i, j, :] represents the [y1, x1, y2, x2] spatial locations of jth point of instance_i in the sequence.\n\n    Returns:\n        An `N` x `self.features` tensor representing the corresponding spatial or temporal embedding.\n    \"\"\"\n    emb = self._emb_func(seq_positions)\n\n    if emb.shape[-1] != self.features:\n        raise RuntimeError(\n            (\n                f\"Output embedding dimension is {emb.shape[-1]} but requested {self.features} dimensions! \\n\"\n                f\"hint: Try turning the MLP on by passing `mlp_cfg` to the constructor to project to the correct embedding dimensions.\"\n            )\n        )\n    return emb\n</code></pre>"},{"location":"reference/dreem/models/global_tracking_transformer/","title":"global_tracking_transformer","text":""},{"location":"reference/dreem/models/global_tracking_transformer/#dreem.models.global_tracking_transformer","title":"<code>dreem.models.global_tracking_transformer</code>","text":"<p>Module containing GTR model used for training.</p>"},{"location":"reference/dreem/models/global_tracking_transformer/#dreem.models.global_tracking_transformer.GlobalTrackingTransformer","title":"<code>GlobalTrackingTransformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Modular GTR model composed of visual encoder + transformer used for tracking.</p> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>class GlobalTrackingTransformer(torch.nn.Module):\n    \"\"\"Modular GTR model composed of visual encoder + transformer used for tracking.\"\"\"\n\n    def __init__(\n        self,\n        encoder_cfg: dict | None = None,\n        d_model: int = 1024,\n        nhead: int = 8,\n        num_encoder_layers: int = 6,\n        num_decoder_layers: int = 6,\n        dropout: int = 0.1,\n        activation: str = \"relu\",\n        return_intermediate_dec: bool = False,\n        norm: bool = False,\n        num_layers_attn_head: int = 2,\n        dropout_attn_head: int = 0.1,\n        embedding_meta: dict | None = None,\n        return_embedding: bool = False,\n        decoder_self_attn: bool = False,\n    ):\n        \"\"\"Initialize GTR.\n\n        Args:\n            encoder_cfg: Dictionary of arguments to pass to the CNN constructor,\n                e.g: `cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}`\n            d_model: The number of features in the encoder/decoder inputs.\n            nhead: The number of heads in the transfomer encoder/decoder.\n            num_encoder_layers: The number of encoder-layers in the encoder.\n            num_decoder_layers: The number of decoder-layers in the decoder.\n            dropout: Dropout value applied to the output of transformer layers.\n            activation: Activation function to use.\n            return_intermediate_dec: Return intermediate layers from decoder.\n            norm: If True, normalize output of encoder and decoder.\n            num_layers_attn_head: The number of layers in the attention head.\n            dropout_attn_head: Dropout value for the attention_head.\n            embedding_meta: Metadata for positional embeddings. See below.\n            return_embedding: Whether to return the positional embeddings\n            decoder_self_attn: If True, use decoder self attention.\n\n                More details on `embedding_meta`:\n                    By default this will be an empty dict and indicate\n                    that no positional embeddings should be used. To use the positional embeddings\n                    pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                    `{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},\n                    \"temp\": {'mode': 'learned', 'emb_num': 16}}`. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                    and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n        \"\"\"\n        super().__init__()\n\n        if encoder_cfg is not None:\n            self.visual_encoder = VisualEncoder(d_model=d_model, **encoder_cfg)\n        else:\n            self.visual_encoder = VisualEncoder(d_model=d_model)\n\n        self.transformer = Transformer(\n            d_model=d_model,\n            nhead=nhead,\n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers,\n            dropout=dropout,\n            activation=activation,\n            return_intermediate_dec=return_intermediate_dec,\n            norm=norm,\n            num_layers_attn_head=num_layers_attn_head,\n            dropout_attn_head=dropout_attn_head,\n            embedding_meta=embedding_meta,\n            return_embedding=return_embedding,\n            decoder_self_attn=decoder_self_attn,\n        )\n\n    def forward(\n        self, ref_instances: list[\"Instance\"], query_instances: list[\"Instance\"] = None\n    ) -&gt; list[\"AssociationMatrix\"]:\n        \"\"\"Execute forward pass of GTR Model to get asso matrix.\n\n        Args:\n            ref_instances: List of instances from chunk containing crops of objects + gt label info\n            query_instances: list of instances used as query in decoder.\n\n        Returns:\n            An N_T x N association matrix\n        \"\"\"\n        # Extract feature representations with pre-trained encoder.\n        self.extract_features(ref_instances)\n\n        if query_instances:\n            self.extract_features(query_instances)\n\n        asso_preds = self.transformer(ref_instances, query_instances)\n\n        return asso_preds\n\n    def extract_features(\n        self, instances: list[\"Instance\"], force_recompute: bool = False\n    ) -&gt; None:\n        \"\"\"Extract features from instances using visual encoder backbone.\n\n        Args:\n            instances: A list of instances to compute features for\n            force_recompute: indicate whether to compute features for all instances regardless of if they have instances\n        \"\"\"\n        if not force_recompute:\n            instances_to_compute = [\n                instance\n                for instance in instances\n                if instance.has_crop() and not instance.has_features()\n            ]\n        else:\n            instances_to_compute = instances\n\n        if len(instances_to_compute) == 0:\n            return\n        elif len(instances_to_compute) == 1:  # handle batch norm error when B=1\n            instances_to_compute = instances\n\n        crops = torch.concatenate([instance.crop for instance in instances_to_compute])\n\n        features = self.visual_encoder(crops)\n\n        for i, z_i in enumerate(features):\n            instances_to_compute[i].features = z_i\n</code></pre>"},{"location":"reference/dreem/models/global_tracking_transformer/#dreem.models.global_tracking_transformer.GlobalTrackingTransformer.__init__","title":"<code>__init__(encoder_cfg=None, d_model=1024, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dropout=0.1, activation='relu', return_intermediate_dec=False, norm=False, num_layers_attn_head=2, dropout_attn_head=0.1, embedding_meta=None, return_embedding=False, decoder_self_attn=False)</code>","text":"<p>Initialize GTR.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_cfg</code> <code>dict | None</code> <p>Dictionary of arguments to pass to the CNN constructor, e.g: <code>cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}</code></p> <code>None</code> <code>d_model</code> <code>int</code> <p>The number of features in the encoder/decoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads in the transfomer encoder/decoder.</p> <code>8</code> <code>num_encoder_layers</code> <code>int</code> <p>The number of encoder-layers in the encoder.</p> <code>6</code> <code>num_decoder_layers</code> <code>int</code> <p>The number of decoder-layers in the decoder.</p> <code>6</code> <code>dropout</code> <code>int</code> <p>Dropout value applied to the output of transformer layers.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>return_intermediate_dec</code> <code>bool</code> <p>Return intermediate layers from decoder.</p> <code>False</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of encoder and decoder.</p> <code>False</code> <code>num_layers_attn_head</code> <code>int</code> <p>The number of layers in the attention head.</p> <code>2</code> <code>dropout_attn_head</code> <code>int</code> <p>Dropout value for the attention_head.</p> <code>0.1</code> <code>embedding_meta</code> <code>dict | None</code> <p>Metadata for positional embeddings. See below.</p> <code>None</code> <code>return_embedding</code> <code>bool</code> <p>Whether to return the positional embeddings</p> <code>False</code> <code>decoder_self_attn</code> <code>bool</code> <p>If True, use decoder self attention.</p> <p>More details on <code>embedding_meta</code>:     By default this will be an empty dict and indicate     that no positional embeddings should be used. To use the positional embeddings     pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:     <code>{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},     \"temp\": {'mode': 'learned', 'emb_num': 16}}</code>. (see <code>dreem.models.embeddings.Embedding.EMB_TYPES</code>     and <code>dreem.models.embeddings.Embedding.EMB_MODES</code> for embedding parameters).</p> <code>False</code> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def __init__(\n    self,\n    encoder_cfg: dict | None = None,\n    d_model: int = 1024,\n    nhead: int = 8,\n    num_encoder_layers: int = 6,\n    num_decoder_layers: int = 6,\n    dropout: int = 0.1,\n    activation: str = \"relu\",\n    return_intermediate_dec: bool = False,\n    norm: bool = False,\n    num_layers_attn_head: int = 2,\n    dropout_attn_head: int = 0.1,\n    embedding_meta: dict | None = None,\n    return_embedding: bool = False,\n    decoder_self_attn: bool = False,\n):\n    \"\"\"Initialize GTR.\n\n    Args:\n        encoder_cfg: Dictionary of arguments to pass to the CNN constructor,\n            e.g: `cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}`\n        d_model: The number of features in the encoder/decoder inputs.\n        nhead: The number of heads in the transfomer encoder/decoder.\n        num_encoder_layers: The number of encoder-layers in the encoder.\n        num_decoder_layers: The number of decoder-layers in the decoder.\n        dropout: Dropout value applied to the output of transformer layers.\n        activation: Activation function to use.\n        return_intermediate_dec: Return intermediate layers from decoder.\n        norm: If True, normalize output of encoder and decoder.\n        num_layers_attn_head: The number of layers in the attention head.\n        dropout_attn_head: Dropout value for the attention_head.\n        embedding_meta: Metadata for positional embeddings. See below.\n        return_embedding: Whether to return the positional embeddings\n        decoder_self_attn: If True, use decoder self attention.\n\n            More details on `embedding_meta`:\n                By default this will be an empty dict and indicate\n                that no positional embeddings should be used. To use the positional embeddings\n                pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                `{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},\n                \"temp\": {'mode': 'learned', 'emb_num': 16}}`. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n    \"\"\"\n    super().__init__()\n\n    if encoder_cfg is not None:\n        self.visual_encoder = VisualEncoder(d_model=d_model, **encoder_cfg)\n    else:\n        self.visual_encoder = VisualEncoder(d_model=d_model)\n\n    self.transformer = Transformer(\n        d_model=d_model,\n        nhead=nhead,\n        num_encoder_layers=num_encoder_layers,\n        num_decoder_layers=num_decoder_layers,\n        dropout=dropout,\n        activation=activation,\n        return_intermediate_dec=return_intermediate_dec,\n        norm=norm,\n        num_layers_attn_head=num_layers_attn_head,\n        dropout_attn_head=dropout_attn_head,\n        embedding_meta=embedding_meta,\n        return_embedding=return_embedding,\n        decoder_self_attn=decoder_self_attn,\n    )\n</code></pre>"},{"location":"reference/dreem/models/global_tracking_transformer/#dreem.models.global_tracking_transformer.GlobalTrackingTransformer.extract_features","title":"<code>extract_features(instances, force_recompute=False)</code>","text":"<p>Extract features from instances using visual encoder backbone.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[Instance]</code> <p>A list of instances to compute features for</p> required <code>force_recompute</code> <code>bool</code> <p>indicate whether to compute features for all instances regardless of if they have instances</p> <code>False</code> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def extract_features(\n    self, instances: list[\"Instance\"], force_recompute: bool = False\n) -&gt; None:\n    \"\"\"Extract features from instances using visual encoder backbone.\n\n    Args:\n        instances: A list of instances to compute features for\n        force_recompute: indicate whether to compute features for all instances regardless of if they have instances\n    \"\"\"\n    if not force_recompute:\n        instances_to_compute = [\n            instance\n            for instance in instances\n            if instance.has_crop() and not instance.has_features()\n        ]\n    else:\n        instances_to_compute = instances\n\n    if len(instances_to_compute) == 0:\n        return\n    elif len(instances_to_compute) == 1:  # handle batch norm error when B=1\n        instances_to_compute = instances\n\n    crops = torch.concatenate([instance.crop for instance in instances_to_compute])\n\n    features = self.visual_encoder(crops)\n\n    for i, z_i in enumerate(features):\n        instances_to_compute[i].features = z_i\n</code></pre>"},{"location":"reference/dreem/models/global_tracking_transformer/#dreem.models.global_tracking_transformer.GlobalTrackingTransformer.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute forward pass of GTR Model to get asso matrix.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>List of instances from chunk containing crops of objects + gt label info</p> required <code>query_instances</code> <code>list[Instance]</code> <p>list of instances used as query in decoder.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[AssociationMatrix]</code> <p>An N_T x N association matrix</p> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def forward(\n    self, ref_instances: list[\"Instance\"], query_instances: list[\"Instance\"] = None\n) -&gt; list[\"AssociationMatrix\"]:\n    \"\"\"Execute forward pass of GTR Model to get asso matrix.\n\n    Args:\n        ref_instances: List of instances from chunk containing crops of objects + gt label info\n        query_instances: list of instances used as query in decoder.\n\n    Returns:\n        An N_T x N association matrix\n    \"\"\"\n    # Extract feature representations with pre-trained encoder.\n    self.extract_features(ref_instances)\n\n    if query_instances:\n        self.extract_features(query_instances)\n\n    asso_preds = self.transformer(ref_instances, query_instances)\n\n    return asso_preds\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/","title":"gtr_runner","text":""},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner","title":"<code>dreem.models.gtr_runner</code>","text":"<p>Module containing training, validation and inference logic.</p>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner","title":"<code>GTRRunner</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A lightning wrapper around GTR model.</p> <p>Used for training, validation and inference.</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>class GTRRunner(LightningModule):\n    \"\"\"A lightning wrapper around GTR model.\n\n    Used for training, validation and inference.\n    \"\"\"\n\n    DEFAULT_METRICS = {\n        \"train\": [],\n        \"val\": [\"num_switches\"],\n        \"test\": [\"num_switches\"],\n    }\n    DEFAULT_TRACKING = {\n        \"train\": False,\n        \"val\": True,\n        \"test\": True,\n    }\n    DEFAULT_SAVE = {\"train\": False, \"val\": False, \"test\": False}\n\n    def __init__(\n        self,\n        model_cfg: dict | None = None,\n        tracker_cfg: dict | None = None,\n        loss_cfg: dict | None = None,\n        optimizer_cfg: dict | None = None,\n        scheduler_cfg: dict | None = None,\n        metrics: dict[str, list[str]] | None = None,\n        persistent_tracking: dict[str, bool] | None = None,\n        test_save_path=\"./test_results.h5\",\n    ):\n        \"\"\"Initialize a lightning module for GTR.\n\n        Args:\n            model_cfg: hyperparameters for GlobalTrackingTransformer\n            tracker_cfg: The parameters used for the tracker post-processing\n            loss_cfg: hyperparameters for AssoLoss\n            optimizer_cfg: hyper parameters used for optimizer.\n                       Only used to overwrite `configure_optimizer`\n            scheduler_cfg: hyperparameters for lr_scheduler used to overwrite `configure_optimizer\n            metrics: a dict containing the metrics to be computed during train, val, and test.\n            persistent_tracking: a dict containing whether to use persistent tracking during train, val and test inference.\n            test_save_path: path to an .h5 file to save the test results to\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.model_cfg = model_cfg if model_cfg else {}\n        self.loss_cfg = loss_cfg if loss_cfg else {}\n        self.tracker_cfg = tracker_cfg if tracker_cfg else {}\n\n        _ = self.model_cfg.pop(\"ckpt_path\", None)\n        self.model = GlobalTrackingTransformer(**self.model_cfg)\n        self.loss = AssoLoss(**self.loss_cfg)\n        self.tracker = Tracker(**self.tracker_cfg)\n\n        self.optimizer_cfg = optimizer_cfg\n        self.scheduler_cfg = scheduler_cfg\n\n        self.metrics = metrics if metrics is not None else self.DEFAULT_METRICS\n        self.persistent_tracking = (\n            persistent_tracking\n            if persistent_tracking is not None\n            else self.DEFAULT_TRACKING\n        )\n        self.test_results = {\"metrics\": [], \"preds\": [], \"save_path\": test_save_path}\n\n    def forward(\n        self,\n        ref_instances: list[\"dreem.io.Instance\"],\n        query_instances: list[\"dreem.io.Instance\"] | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute forward pass of the lightning module.\n\n        Args:\n            ref_instances: a list of `Instance` objects containing crops and other data needed for transformer model\n            query_instances: a list of `Instance` objects used as queries in the decoder. Mostly used for inference.\n\n        Returns:\n            An association matrix between objects\n        \"\"\"\n        asso_preds = self.model(ref_instances, query_instances)\n        return asso_preds\n\n    def training_step(\n        self, train_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single training step for model.\n\n        Args:\n            train_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the train loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(train_batch[0], mode=\"train\")\n        self.log_metrics(result, len(train_batch[0]), \"train\")\n\n        return result\n\n    def validation_step(\n        self, val_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single val step for model.\n\n        Args:\n            val_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the val loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(val_batch[0], mode=\"val\")\n        self.log_metrics(result, len(val_batch[0]), \"val\")\n\n        return result\n\n    def test_step(\n        self, test_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single test step for model.\n\n        Args:\n            test_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the val loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(test_batch[0], mode=\"test\")\n        self.log_metrics(result, len(test_batch[0]), \"test\")\n\n        return result\n\n    def predict_step(\n        self, batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; list[\"dreem.io.Frame\"]:\n        \"\"\"Run inference for model.\n\n        Computes association + assignment.\n\n        Args:\n            batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A list of dicts where each dict is a frame containing the predicted track ids\n        \"\"\"\n        frames_pred = self.tracker(self.model, batch[0])\n        return frames_pred\n\n    def _shared_eval_step(\n        self, frames: list[\"dreem.io.Frame\"], mode: str\n    ) -&gt; dict[str, float]:\n        \"\"\"Run evaluation used by train, test, and val steps.\n\n        Args:\n            frames: A list of dicts where each dict is a frame containing gt data\n            mode: which metrics to compute and whether to use persistent tracking or not\n\n        Returns:\n            a dict containing the loss and any other metrics specified by `eval_metrics`\n        \"\"\"\n        try:\n            instances = [instance for frame in frames for instance in frame.instances]\n\n            if len(instances) == 0:\n                return None\n\n            eval_metrics = self.metrics[mode]\n            persistent_tracking = self.persistent_tracking[mode]\n\n            logits = self(instances)\n            logits = [asso.matrix for asso in logits]\n            loss = self.loss(logits, frames)\n\n            return_metrics = {\"loss\": loss}\n            if eval_metrics is not None and len(eval_metrics) &gt; 0:\n                self.tracker.persistent_tracking = persistent_tracking\n\n                frames_pred = self.tracker(self.model, frames)\n\n                frames_mm = metrics.to_track_eval(frames_pred)\n                clearmot = metrics.get_pymotmetrics(frames_mm, eval_metrics)\n\n                return_metrics.update(clearmot.to_dict())\n\n                if mode == \"test\":\n                    self.test_results[\"preds\"].append(\n                        [frame.to(\"cpu\") for frame in frames_pred]\n                    )\n                    self.test_results[\"metrics\"].append(return_metrics)\n            return_metrics[\"batch_size\"] = len(frames)\n        except Exception as e:\n            logger.exception(\n                f\"Failed on frame {frames[0].frame_id} of video {frames[0].video_id}\"\n            )\n            logger.exception(e)\n            raise (e)\n\n        return return_metrics\n\n    def configure_optimizers(self) -&gt; dict:\n        \"\"\"Get optimizers and schedulers for training.\n\n        Is overridden by config but defaults to Adam + ReduceLROnPlateau.\n\n        Returns:\n            an optimizer config dict containing the optimizer, scheduler, and scheduler params\n        \"\"\"\n        # todo: init from config\n        if self.optimizer_cfg is None:\n            optimizer = torch.optim.Adam(self.parameters(), lr=1e-4, betas=(0.9, 0.999))\n        else:\n            optimizer = init_optimizer(self.parameters(), self.optimizer_cfg)\n\n        if self.scheduler_cfg is None:\n            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer, \"min\", 0.5, 10\n            )\n        else:\n            scheduler = init_scheduler(optimizer, self.scheduler_cfg)\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n                \"interval\": \"epoch\",\n                \"frequency\": 10,\n            },\n        }\n\n    def log_metrics(self, result: dict, batch_size: int, mode: str) -&gt; None:\n        \"\"\"Log metrics computed during evaluation.\n\n        Args:\n            result: A dict containing metrics to be logged.\n            batch_size: the size of the batch used to compute the metrics\n            mode: One of {'train', 'test' or 'val'}. Used as prefix while logging.\n        \"\"\"\n        if result:\n            batch_size = result.pop(\"batch_size\")\n            for metric, val in result.items():\n                if isinstance(val, torch.Tensor):\n                    val = val.item()\n                self.log(f\"{mode}_{metric}\", val, batch_size=batch_size)\n\n    def on_validation_epoch_end(self):\n        \"\"\"Execute hook for validation end.\n\n        Currently, we simply clear the gpu cache and do garbage collection.\n        \"\"\"\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def on_test_epoch_end(self):\n        \"\"\"Execute hook for test end.\n\n        Currently, we save results to an h5py file. and clear the predictions\n        \"\"\"\n        fname = self.test_results[\"save_path\"]\n        test_results = {\n            key: val for key, val in self.test_results.items() if key != \"save_path\"\n        }\n        metrics_dict = [\n            {\n                key: (\n                    val.detach().cpu().numpy() if isinstance(val, torch.Tensor) else val\n                )\n                for key, val in metrics.items()\n            }\n            for metrics in test_results[\"metrics\"]\n        ]\n        results_df = pd.DataFrame(metrics_dict)\n        preds = test_results[\"preds\"]\n\n        with h5py.File(fname, \"a\") as results_file:\n            for key in results_df.columns:\n                avg_result = results_df[key].mean()\n                results_file.attrs.create(key, avg_result)\n            for i, (metrics, frames) in enumerate(zip(metrics_dict, preds)):\n                vid_name = frames[0].vid_name.split(\"/\")[-1].split(\".\")[0]\n                vid_group = results_file.require_group(vid_name)\n                clip_group = vid_group.require_group(f\"clip_{i}\")\n                for key, val in metrics.items():\n                    clip_group.attrs.create(key, val)\n                for frame in frames:\n                    if metrics.get(\"num_switches\", 0) &gt; 0:\n                        _ = frame.to_h5(\n                            clip_group,\n                            frame.get_gt_track_ids().cpu().numpy(),\n                            save={\"crop\": True, \"features\": True, \"embeddings\": True},\n                        )\n                    else:\n                        _ = frame.to_h5(\n                            clip_group, frame.get_gt_track_ids().cpu().numpy()\n                        )\n        self.test_results = {\"metrics\": [], \"preds\": [], \"save_path\": fname}\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.__init__","title":"<code>__init__(model_cfg=None, tracker_cfg=None, loss_cfg=None, optimizer_cfg=None, scheduler_cfg=None, metrics=None, persistent_tracking=None, test_save_path='./test_results.h5')</code>","text":"<p>Initialize a lightning module for GTR.</p> <p>Parameters:</p> Name Type Description Default <code>model_cfg</code> <code>dict | None</code> <p>hyperparameters for GlobalTrackingTransformer</p> <code>None</code> <code>tracker_cfg</code> <code>dict | None</code> <p>The parameters used for the tracker post-processing</p> <code>None</code> <code>loss_cfg</code> <code>dict | None</code> <p>hyperparameters for AssoLoss</p> <code>None</code> <code>optimizer_cfg</code> <code>dict | None</code> <p>hyper parameters used for optimizer.        Only used to overwrite <code>configure_optimizer</code></p> <code>None</code> <code>scheduler_cfg</code> <code>dict | None</code> <p>hyperparameters for lr_scheduler used to overwrite `configure_optimizer</p> <code>None</code> <code>metrics</code> <code>dict[str, list[str]] | None</code> <p>a dict containing the metrics to be computed during train, val, and test.</p> <code>None</code> <code>persistent_tracking</code> <code>dict[str, bool] | None</code> <p>a dict containing whether to use persistent tracking during train, val and test inference.</p> <code>None</code> <code>test_save_path</code> <p>path to an .h5 file to save the test results to</p> <code>'./test_results.h5'</code> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def __init__(\n    self,\n    model_cfg: dict | None = None,\n    tracker_cfg: dict | None = None,\n    loss_cfg: dict | None = None,\n    optimizer_cfg: dict | None = None,\n    scheduler_cfg: dict | None = None,\n    metrics: dict[str, list[str]] | None = None,\n    persistent_tracking: dict[str, bool] | None = None,\n    test_save_path=\"./test_results.h5\",\n):\n    \"\"\"Initialize a lightning module for GTR.\n\n    Args:\n        model_cfg: hyperparameters for GlobalTrackingTransformer\n        tracker_cfg: The parameters used for the tracker post-processing\n        loss_cfg: hyperparameters for AssoLoss\n        optimizer_cfg: hyper parameters used for optimizer.\n                   Only used to overwrite `configure_optimizer`\n        scheduler_cfg: hyperparameters for lr_scheduler used to overwrite `configure_optimizer\n        metrics: a dict containing the metrics to be computed during train, val, and test.\n        persistent_tracking: a dict containing whether to use persistent tracking during train, val and test inference.\n        test_save_path: path to an .h5 file to save the test results to\n    \"\"\"\n    super().__init__()\n    self.save_hyperparameters()\n\n    self.model_cfg = model_cfg if model_cfg else {}\n    self.loss_cfg = loss_cfg if loss_cfg else {}\n    self.tracker_cfg = tracker_cfg if tracker_cfg else {}\n\n    _ = self.model_cfg.pop(\"ckpt_path\", None)\n    self.model = GlobalTrackingTransformer(**self.model_cfg)\n    self.loss = AssoLoss(**self.loss_cfg)\n    self.tracker = Tracker(**self.tracker_cfg)\n\n    self.optimizer_cfg = optimizer_cfg\n    self.scheduler_cfg = scheduler_cfg\n\n    self.metrics = metrics if metrics is not None else self.DEFAULT_METRICS\n    self.persistent_tracking = (\n        persistent_tracking\n        if persistent_tracking is not None\n        else self.DEFAULT_TRACKING\n    )\n    self.test_results = {\"metrics\": [], \"preds\": [], \"save_path\": test_save_path}\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Get optimizers and schedulers for training.</p> <p>Is overridden by config but defaults to Adam + ReduceLROnPlateau.</p> <p>Returns:</p> Type Description <code>dict</code> <p>an optimizer config dict containing the optimizer, scheduler, and scheduler params</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def configure_optimizers(self) -&gt; dict:\n    \"\"\"Get optimizers and schedulers for training.\n\n    Is overridden by config but defaults to Adam + ReduceLROnPlateau.\n\n    Returns:\n        an optimizer config dict containing the optimizer, scheduler, and scheduler params\n    \"\"\"\n    # todo: init from config\n    if self.optimizer_cfg is None:\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4, betas=(0.9, 0.999))\n    else:\n        optimizer = init_optimizer(self.parameters(), self.optimizer_cfg)\n\n    if self.scheduler_cfg is None:\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, \"min\", 0.5, 10\n        )\n    else:\n        scheduler = init_scheduler(optimizer, self.scheduler_cfg)\n\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"monitor\": \"val_loss\",\n            \"interval\": \"epoch\",\n            \"frequency\": 10,\n        },\n    }\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute forward pass of the lightning module.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>a list of <code>Instance</code> objects containing crops and other data needed for transformer model</p> required <code>query_instances</code> <code>list[Instance] | None</code> <p>a list of <code>Instance</code> objects used as queries in the decoder. Mostly used for inference.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>An association matrix between objects</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def forward(\n    self,\n    ref_instances: list[\"dreem.io.Instance\"],\n    query_instances: list[\"dreem.io.Instance\"] | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Execute forward pass of the lightning module.\n\n    Args:\n        ref_instances: a list of `Instance` objects containing crops and other data needed for transformer model\n        query_instances: a list of `Instance` objects used as queries in the decoder. Mostly used for inference.\n\n    Returns:\n        An association matrix between objects\n    \"\"\"\n    asso_preds = self.model(ref_instances, query_instances)\n    return asso_preds\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.log_metrics","title":"<code>log_metrics(result, batch_size, mode)</code>","text":"<p>Log metrics computed during evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>dict</code> <p>A dict containing metrics to be logged.</p> required <code>batch_size</code> <code>int</code> <p>the size of the batch used to compute the metrics</p> required <code>mode</code> <code>str</code> <p>One of {'train', 'test' or 'val'}. Used as prefix while logging.</p> required Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def log_metrics(self, result: dict, batch_size: int, mode: str) -&gt; None:\n    \"\"\"Log metrics computed during evaluation.\n\n    Args:\n        result: A dict containing metrics to be logged.\n        batch_size: the size of the batch used to compute the metrics\n        mode: One of {'train', 'test' or 'val'}. Used as prefix while logging.\n    \"\"\"\n    if result:\n        batch_size = result.pop(\"batch_size\")\n        for metric, val in result.items():\n            if isinstance(val, torch.Tensor):\n                val = val.item()\n            self.log(f\"{mode}_{metric}\", val, batch_size=batch_size)\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.on_test_epoch_end","title":"<code>on_test_epoch_end()</code>","text":"<p>Execute hook for test end.</p> <p>Currently, we save results to an h5py file. and clear the predictions</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def on_test_epoch_end(self):\n    \"\"\"Execute hook for test end.\n\n    Currently, we save results to an h5py file. and clear the predictions\n    \"\"\"\n    fname = self.test_results[\"save_path\"]\n    test_results = {\n        key: val for key, val in self.test_results.items() if key != \"save_path\"\n    }\n    metrics_dict = [\n        {\n            key: (\n                val.detach().cpu().numpy() if isinstance(val, torch.Tensor) else val\n            )\n            for key, val in metrics.items()\n        }\n        for metrics in test_results[\"metrics\"]\n    ]\n    results_df = pd.DataFrame(metrics_dict)\n    preds = test_results[\"preds\"]\n\n    with h5py.File(fname, \"a\") as results_file:\n        for key in results_df.columns:\n            avg_result = results_df[key].mean()\n            results_file.attrs.create(key, avg_result)\n        for i, (metrics, frames) in enumerate(zip(metrics_dict, preds)):\n            vid_name = frames[0].vid_name.split(\"/\")[-1].split(\".\")[0]\n            vid_group = results_file.require_group(vid_name)\n            clip_group = vid_group.require_group(f\"clip_{i}\")\n            for key, val in metrics.items():\n                clip_group.attrs.create(key, val)\n            for frame in frames:\n                if metrics.get(\"num_switches\", 0) &gt; 0:\n                    _ = frame.to_h5(\n                        clip_group,\n                        frame.get_gt_track_ids().cpu().numpy(),\n                        save={\"crop\": True, \"features\": True, \"embeddings\": True},\n                    )\n                else:\n                    _ = frame.to_h5(\n                        clip_group, frame.get_gt_track_ids().cpu().numpy()\n                    )\n    self.test_results = {\"metrics\": [], \"preds\": [], \"save_path\": fname}\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.on_validation_epoch_end","title":"<code>on_validation_epoch_end()</code>","text":"<p>Execute hook for validation end.</p> <p>Currently, we simply clear the gpu cache and do garbage collection.</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Execute hook for validation end.\n\n    Currently, we simply clear the gpu cache and do garbage collection.\n    \"\"\"\n    gc.collect()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.predict_step","title":"<code>predict_step(batch, batch_idx)</code>","text":"<p>Run inference for model.</p> <p>Computes association + assignment.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects     with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A list of dicts where each dict is a frame containing the predicted track ids</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def predict_step(\n    self, batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; list[\"dreem.io.Frame\"]:\n    \"\"\"Run inference for model.\n\n    Computes association + assignment.\n\n    Args:\n        batch: A single batch from the dataset which is a list of `Frame` objects\n                with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A list of dicts where each dict is a frame containing the predicted track ids\n    \"\"\"\n    frames_pred = self.tracker(self.model, batch[0])\n    return frames_pred\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.test_step","title":"<code>test_step(test_batch, batch_idx)</code>","text":"<p>Execute single test step for model.</p> <p>Parameters:</p> Name Type Description Default <code>test_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the val loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def test_step(\n    self, test_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single test step for model.\n\n    Args:\n        test_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the val loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(test_batch[0], mode=\"test\")\n    self.log_metrics(result, len(test_batch[0]), \"test\")\n\n    return result\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.training_step","title":"<code>training_step(train_batch, batch_idx)</code>","text":"<p>Execute single training step for model.</p> <p>Parameters:</p> Name Type Description Default <code>train_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the train loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def training_step(\n    self, train_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single training step for model.\n\n    Args:\n        train_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the train loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(train_batch[0], mode=\"train\")\n    self.log_metrics(result, len(train_batch[0]), \"train\")\n\n    return result\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.validation_step","title":"<code>validation_step(val_batch, batch_idx)</code>","text":"<p>Execute single val step for model.</p> <p>Parameters:</p> Name Type Description Default <code>val_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the val loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def validation_step(\n    self, val_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single val step for model.\n\n    Args:\n        val_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the val loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(val_batch[0], mode=\"val\")\n    self.log_metrics(result, len(val_batch[0]), \"val\")\n\n    return result\n</code></pre>"},{"location":"reference/dreem/models/mlp/","title":"mlp","text":""},{"location":"reference/dreem/models/mlp/#dreem.models.mlp","title":"<code>dreem.models.mlp</code>","text":"<p>Multi-Layer Perceptron (MLP) module.</p>"},{"location":"reference/dreem/models/mlp/#dreem.models.mlp.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>Multi-Layer Perceptron (MLP) module.</p> Source code in <code>dreem/models/mlp.py</code> <pre><code>class MLP(torch.nn.Module):\n    \"\"\"Multi-Layer Perceptron (MLP) module.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        output_dim: int,\n        num_layers: int,\n        dropout: float = 0.0,\n    ):\n        \"\"\"Initialize MLP.\n\n        Args:\n            input_dim: Dimensionality of the input features.\n            hidden_dim: Number of units in the hidden layers.\n            output_dim: Dimensionality of the output features.\n            num_layers: Number of hidden layers.\n            dropout: Dropout probability.\n        \"\"\"\n        super().__init__()\n\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        if self.num_layers &gt; 0:\n            h = [hidden_dim] * (num_layers - 1)\n            self.layers = torch.nn.ModuleList(\n                [\n                    torch.nn.Linear(n, k)\n                    for n, k in zip([input_dim] + h, h + [output_dim])\n                ]\n            )\n            if self.dropout &gt; 0.0:\n                self.dropouts = torch.nn.ModuleList(\n                    [torch.nn.Dropout(dropout) for _ in range(self.num_layers - 1)]\n                )\n        else:\n            self.layers = []\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of the MLP.\n\n        Args:\n            x: Input tensor of shape (batch_size, num_instances, input_dim).\n\n        Returns:\n            Output tensor of shape (batch_size, num_instances, output_dim).\n        \"\"\"\n        for i, layer in enumerate(self.layers):\n            x = F.relu(layer(x)) if i &lt; self.num_layers - 1 else layer(x)\n            if i &lt; self.num_layers - 1 and self.dropout &gt; 0.0:\n                x = self.dropouts[i](x)\n\n        return x\n</code></pre>"},{"location":"reference/dreem/models/mlp/#dreem.models.mlp.MLP.__init__","title":"<code>__init__(input_dim, hidden_dim, output_dim, num_layers, dropout=0.0)</code>","text":"<p>Initialize MLP.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of the input features.</p> required <code>hidden_dim</code> <code>int</code> <p>Number of units in the hidden layers.</p> required <code>output_dim</code> <code>int</code> <p>Dimensionality of the output features.</p> required <code>num_layers</code> <code>int</code> <p>Number of hidden layers.</p> required <code>dropout</code> <code>float</code> <p>Dropout probability.</p> <code>0.0</code> Source code in <code>dreem/models/mlp.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    hidden_dim: int,\n    output_dim: int,\n    num_layers: int,\n    dropout: float = 0.0,\n):\n    \"\"\"Initialize MLP.\n\n    Args:\n        input_dim: Dimensionality of the input features.\n        hidden_dim: Number of units in the hidden layers.\n        output_dim: Dimensionality of the output features.\n        num_layers: Number of hidden layers.\n        dropout: Dropout probability.\n    \"\"\"\n    super().__init__()\n\n    self.num_layers = num_layers\n    self.dropout = dropout\n\n    if self.num_layers &gt; 0:\n        h = [hidden_dim] * (num_layers - 1)\n        self.layers = torch.nn.ModuleList(\n            [\n                torch.nn.Linear(n, k)\n                for n, k in zip([input_dim] + h, h + [output_dim])\n            ]\n        )\n        if self.dropout &gt; 0.0:\n            self.dropouts = torch.nn.ModuleList(\n                [torch.nn.Dropout(dropout) for _ in range(self.num_layers - 1)]\n            )\n    else:\n        self.layers = []\n</code></pre>"},{"location":"reference/dreem/models/mlp/#dreem.models.mlp.MLP.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the MLP.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, num_instances, input_dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, num_instances, output_dim).</p> Source code in <code>dreem/models/mlp.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the MLP.\n\n    Args:\n        x: Input tensor of shape (batch_size, num_instances, input_dim).\n\n    Returns:\n        Output tensor of shape (batch_size, num_instances, output_dim).\n    \"\"\"\n    for i, layer in enumerate(self.layers):\n        x = F.relu(layer(x)) if i &lt; self.num_layers - 1 else layer(x)\n        if i &lt; self.num_layers - 1 and self.dropout &gt; 0.0:\n            x = self.dropouts[i](x)\n\n    return x\n</code></pre>"},{"location":"reference/dreem/models/model_utils/","title":"model_utils","text":""},{"location":"reference/dreem/models/model_utils/#dreem.models.model_utils","title":"<code>dreem.models.model_utils</code>","text":"<p>Module containing model helper functions.</p>"},{"location":"reference/dreem/models/model_utils/#dreem.models.model_utils.get_boxes","title":"<code>get_boxes(instances)</code>","text":"<p>Extract the bounding boxes from the input list of instances.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[Instance]</code> <p>List of Instance objects.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>An (n_instances, n_points, 4) float tensor containing the bounding boxes normalized by the height and width of the image</p> Source code in <code>dreem/models/model_utils.py</code> <pre><code>def get_boxes(instances: list[\"dreem.io.Instance\"]) -&gt; torch.Tensor:\n    \"\"\"Extract the bounding boxes from the input list of instances.\n\n    Args:\n        instances: List of Instance objects.\n\n    Returns:\n        An (n_instances, n_points, 4) float tensor containing the bounding boxes\n        normalized by the height and width of the image\n    \"\"\"\n    boxes = []\n    for i, instance in enumerate(instances):\n        _, h, w = instance.frame.img_shape.flatten()\n        bbox = instance.bbox.clone()\n        bbox[:, :, [0, 2]] /= w\n        bbox[:, :, [1, 3]] /= h\n        boxes.append(bbox)\n\n    boxes = torch.cat(boxes, dim=0)  # N, n_anchors, 4\n\n    return boxes\n</code></pre>"},{"location":"reference/dreem/models/model_utils/#dreem.models.model_utils.get_device","title":"<code>get_device()</code>","text":"<p>Utility function to get available device.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The available device (one of 'cuda', 'mps', or 'cpu').</p> Source code in <code>dreem/models/model_utils.py</code> <pre><code>def get_device() -&gt; str:\n    \"\"\"Utility function to get available device.\n\n    Returns:\n        str: The available device (one of 'cuda', 'mps', or 'cpu').\n    \"\"\"\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif torch.backends.mps.is_available():\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    return device\n</code></pre>"},{"location":"reference/dreem/models/model_utils/#dreem.models.model_utils.get_times","title":"<code>get_times(ref_instances, query_instances=None)</code>","text":"<p>Extract the time indices of each instance relative to the window length.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>Set of instances to query against</p> required <code>query_instances</code> <code>list[Instance] | None</code> <p>Set of query instances to look up using decoder.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Tuple of Corresponding frame indices eg [0, 0, 1, 1, ..., T, T] for ref and query instances.</p> Source code in <code>dreem/models/model_utils.py</code> <pre><code>def get_times(\n    ref_instances: list[\"dreem.io.Instance\"],\n    query_instances: list[\"dreem.io.Instance\"] | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Extract the time indices of each instance relative to the window length.\n\n    Args:\n        ref_instances: Set of instances to query against\n        query_instances: Set of query instances to look up using decoder.\n\n    Returns:\n        Tuple of Corresponding frame indices eg [0, 0, 1, 1, ..., T, T] for ref and query instances.\n    \"\"\"\n    ref_inds = torch.tensor(\n        [instance.frame.frame_id.item() for instance in ref_instances],\n        device=ref_instances[0].device,\n    )\n\n    if query_instances is not None:\n        query_inds = torch.tensor(\n            [instance.frame.frame_id.item() for instance in query_instances],\n            device=ref_inds.device,\n        )\n    else:\n        query_inds = torch.tensor([], device=ref_inds.device)\n\n    frame_inds = torch.concat([ref_inds, query_inds])\n    window_length = len(frame_inds.unique())\n\n    frame_idx_mapping = {frame_inds.unique()[i].item(): i for i in range(window_length)}\n    ref_t = torch.tensor(\n        [frame_idx_mapping[ind.item()] for ind in ref_inds], device=ref_inds.device\n    )\n\n    query_t = torch.tensor(\n        [frame_idx_mapping[ind.item()] for ind in query_inds], device=ref_inds.device\n    )\n\n    return ref_t, query_t\n</code></pre>"},{"location":"reference/dreem/models/model_utils/#dreem.models.model_utils.init_logger","title":"<code>init_logger(logger_params, config=None)</code>","text":"<p>Initialize logger based on config parameters.</p> <p>Allows more flexibility in choosing which logger to use.</p> <p>Parameters:</p> Name Type Description Default <code>logger_params</code> <code>dict</code> <p>logger hyperparameters</p> required <code>config</code> <code>dict | None</code> <p>rest of hyperparameters to log (mostly used for WandB)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>logger</code> <code>Logger</code> <p>A logger with specified params (or None).</p> Source code in <code>dreem/models/model_utils.py</code> <pre><code>def init_logger(logger_params: dict, config: dict | None = None) -&gt; loggers.Logger:\n    \"\"\"Initialize logger based on config parameters.\n\n    Allows more flexibility in choosing which logger to use.\n\n    Args:\n        logger_params: logger hyperparameters\n        config: rest of hyperparameters to log (mostly used for WandB)\n\n    Returns:\n        logger: A logger with specified params (or None).\n    \"\"\"\n    logger_type = logger_params.pop(\"logger_type\", None)\n\n    valid_loggers = [\n        \"CSVLogger\",\n        \"TensorBoardLogger\",\n        \"WandbLogger\",\n    ]\n\n    if logger_type in valid_loggers:\n        logger_class = getattr(loggers, logger_type)\n        if logger_class == loggers.WandbLogger:\n            try:\n                return logger_class(config=config, **logger_params)\n            except Exception as e:\n                print(e, logger_type)\n        else:\n            try:\n                return logger_class(**logger_params)\n            except Exception as e:\n                print(e, logger_type)\n    else:\n        print(\n            f\"{logger_type} not one of {valid_loggers} or set to None, skipping logging\"\n        )\n        return None\n</code></pre>"},{"location":"reference/dreem/models/model_utils/#dreem.models.model_utils.init_optimizer","title":"<code>init_optimizer(params, config)</code>","text":"<p>Initialize optimizer based on config parameters.</p> <p>Allows more flexibility in which optimizer to use</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable</code> <p>model parameters to be optimized</p> required <code>config</code> <code>dict</code> <p>optimizer hyperparameters including optimizer name</p> required <p>Returns:</p> Name Type Description <code>optimizer</code> <code>Optimizer</code> <p>A torch.Optimizer with specified params</p> Source code in <code>dreem/models/model_utils.py</code> <pre><code>def init_optimizer(params: Iterable, config: dict) -&gt; torch.optim.Optimizer:\n    \"\"\"Initialize optimizer based on config parameters.\n\n    Allows more flexibility in which optimizer to use\n\n    Args:\n        params: model parameters to be optimized\n        config: optimizer hyperparameters including optimizer name\n\n    Returns:\n        optimizer: A torch.Optimizer with specified params\n    \"\"\"\n    optimizer = config[\"name\"]\n    optimizer_params = {\n        param: val for param, val in config.items() if param.lower() != \"name\"\n    }\n\n    try:\n        optimizer_class = getattr(torch.optim, optimizer)\n    except AttributeError:\n        if optimizer_class is None:\n            print(\n                f\"Couldn't instantiate {optimizer} as given. Trying with capitalization\"\n            )\n            optimizer_class = getattr(torch.optim, optimizer.lower().capitalize())\n        if optimizer_class is None:\n            print(\n                f\"Couldnt instantiate {optimizer} with capitalization, Final attempt with all caps\"\n            )\n            optimizer_class = getattr(torch.optim, optimizer.upper(), None)\n\n    if optimizer_class is None:\n        raise ValueError(f\"Unsupported optimizer type: {optimizer}\")\n\n    return optimizer_class(params, **optimizer_params)\n</code></pre>"},{"location":"reference/dreem/models/model_utils/#dreem.models.model_utils.init_scheduler","title":"<code>init_scheduler(optimizer, config)</code>","text":"<p>Initialize scheduler based on config parameters.</p> <p>Allows more flexibility in choosing which scheduler to use.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>optimizer for which to adjust lr</p> required <code>config</code> <code>dict</code> <p>lr scheduler hyperparameters including scheduler name</p> required <p>Returns:</p> Name Type Description <code>scheduler</code> <code>LRScheduler</code> <p>A scheduler with specified params</p> Source code in <code>dreem/models/model_utils.py</code> <pre><code>def init_scheduler(\n    optimizer: torch.optim.Optimizer, config: dict\n) -&gt; torch.optim.lr_scheduler.LRScheduler:\n    \"\"\"Initialize scheduler based on config parameters.\n\n    Allows more flexibility in choosing which scheduler to use.\n\n    Args:\n        optimizer: optimizer for which to adjust lr\n        config: lr scheduler hyperparameters including scheduler name\n\n    Returns:\n        scheduler: A scheduler with specified params\n    \"\"\"\n    scheduler = config[\"name\"]\n    scheduler_params = {\n        param: val for param, val in config.items() if param.lower() != \"name\"\n    }\n    try:\n        scheduler_class = getattr(torch.optim.lr_scheduler, scheduler)\n    except AttributeError:\n        if scheduler_class is None:\n            print(\n                f\"Couldn't instantiate {scheduler} as given. Trying with capitalization\"\n            )\n            scheduler_class = getattr(\n                torch.optim.lr_scheduler, scheduler.lower().capitalize()\n            )\n        if scheduler_class is None:\n            print(\n                f\"Couldnt instantiate {scheduler} with capitalization, Final attempt with all caps\"\n            )\n            scheduler_class = getattr(torch.optim.lr_scheduler, scheduler.upper(), None)\n\n    if scheduler_class is None:\n        raise ValueError(f\"Unsupported optimizer type: {scheduler}\")\n\n    return scheduler_class(optimizer, **scheduler_params)\n</code></pre>"},{"location":"reference/dreem/models/model_utils/#dreem.models.model_utils.softmax_asso","title":"<code>softmax_asso(asso_output)</code>","text":"<p>Apply the softmax activation function on asso_output.</p> <p>Parameters:</p> Name Type Description Default <code>asso_output</code> <code>list[Tensor]</code> <p>Raw logits output of the tracking transformer. A list of torch tensors of shape (T, N_t, N_i) where:     T: the length of the window     N_t: number of instances in current/query frame (rightmost frame         of the window).     N_i: number of detected instances in i-th frame of window.</p> required <p>Returns:</p> Name Type Description <code>asso_output</code> <code>list[Tensor]</code> <p>Probabilities following softmax function, with same shape     as input.</p> Source code in <code>dreem/models/model_utils.py</code> <pre><code>def softmax_asso(asso_output: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n    \"\"\"Apply the softmax activation function on asso_output.\n\n    Args:\n        asso_output: Raw logits output of the tracking transformer. A list of\n            torch tensors of shape (T, N_t, N_i) where:\n                T: the length of the window\n                N_t: number of instances in current/query frame (rightmost frame\n                    of the window).\n                N_i: number of detected instances in i-th frame of window.\n\n    Returns:\n        asso_output: Probabilities following softmax function, with same shape\n            as input.\n    \"\"\"\n    asso_active = []\n    for asso in asso_output:\n        asso = torch.cat([asso, asso.new_zeros((asso.shape[0], 1))], dim=1).softmax(\n            dim=1\n        )[:, :-1]\n        asso_active.append(asso)\n\n    return asso_active\n</code></pre>"},{"location":"reference/dreem/models/transformer/","title":"transformer","text":""},{"location":"reference/dreem/models/transformer/#dreem.models.transformer","title":"<code>dreem.models.transformer</code>","text":"<p>DETR Transformer class.</p> <p>Copyright \u00a9 Facebook, Inc. and its affiliates. All Rights Reserved</p> <ul> <li>Modified from https://github.com/facebookresearch/detr/blob/main/models/transformer.py</li> <li>Modified from https://github.com/xingyizhou/GTR/blob/master/gtr/modeling/roi_heads/transformer.py</li> <li>Modifications:<ul> <li>positional encodings are passed in MHattention</li> <li>extra LN at the end of encoder is removed</li> <li>decoder returns a stack of activations from all decoding layers</li> <li>added fixed embeddings over boxes</li> </ul> </li> </ul>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.Transformer","title":"<code>Transformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer class.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class Transformer(torch.nn.Module):\n    \"\"\"Transformer class.\"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 1024,\n        nhead: int = 8,\n        num_encoder_layers: int = 6,\n        num_decoder_layers: int = 6,\n        dropout: float = 0.1,\n        activation: str = \"relu\",\n        return_intermediate_dec: bool = False,\n        norm: bool = False,\n        num_layers_attn_head: int = 2,\n        dropout_attn_head: float = 0.1,\n        embedding_meta: dict | None = None,\n        return_embedding: bool = False,\n        decoder_self_attn: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize Transformer.\n\n        Args:\n            d_model: The number of features in the encoder/decoder inputs.\n            nhead: The number of heads in the transfomer encoder/decoder.\n            num_encoder_layers: The number of encoder-layers in the encoder.\n            num_decoder_layers: The number of decoder-layers in the decoder.\n            dropout: Dropout value applied to the output of transformer layers.\n            activation: Activation function to use.\n            return_intermediate_dec: Return intermediate layers from decoder.\n            norm: If True, normalize output of encoder and decoder.\n            num_layers_attn_head: The number of layers in the attention head.\n            dropout_attn_head: Dropout value for the attention_head.\n            embedding_meta: Metadata for positional embeddings. See below.\n            return_embedding: Whether to return the positional embeddings\n            decoder_self_attn: If True, use decoder self attention.\n\n                More details on `embedding_meta`:\n                    By default this will be an empty dict and indicate\n                    that no positional embeddings should be used. To use the positional embeddings\n                    pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                    {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},\n                    \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                    and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n        \"\"\"\n        super().__init__()\n\n        self.d_model = dim_feedforward = feature_dim_attn_head = d_model\n\n        self.embedding_meta = embedding_meta\n        self.return_embedding = return_embedding\n\n        self.pos_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n        self.temp_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n\n        if self.embedding_meta:\n            if \"pos\" in self.embedding_meta:\n                pos_emb_cfg = self.embedding_meta[\"pos\"]\n                if pos_emb_cfg:\n                    self.pos_emb = Embedding(\n                        emb_type=\"pos\", features=self.d_model, **pos_emb_cfg\n                    )\n            if \"temp\" in self.embedding_meta:\n                temp_emb_cfg = self.embedding_meta[\"temp\"]\n                if temp_emb_cfg:\n                    self.temp_emb = Embedding(\n                        emb_type=\"temp\", features=self.d_model, **temp_emb_cfg\n                    )\n\n        # Transformer Encoder\n        encoder_layer = TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, activation, norm\n        )\n\n        encoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n        self.encoder = TransformerEncoder(\n            encoder_layer, num_encoder_layers, encoder_norm\n        )\n\n        # Transformer Decoder\n        decoder_layer = TransformerDecoderLayer(\n            d_model,\n            nhead,\n            dim_feedforward,\n            dropout,\n            activation,\n            norm,\n            decoder_self_attn,\n        )\n\n        decoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n        self.decoder = TransformerDecoder(\n            decoder_layer, num_decoder_layers, return_intermediate_dec, decoder_norm\n        )\n\n        # Transformer attention head\n        self.attn_head = ATTWeightHead(\n            feature_dim=feature_dim_attn_head,\n            num_layers=num_layers_attn_head,\n            dropout=dropout_attn_head,\n        )\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        \"\"\"Initialize model weights from xavier distribution.\"\"\"\n        for p in self.parameters():\n            if not torch.nn.parameter.is_lazy(p) and p.dim() &gt; 1:\n                try:\n                    nn.init.xavier_uniform_(p)\n                except ValueError as e:\n                    print(f\"Failed Trying to initialize {p}\")\n                    raise (e)\n\n    def forward(\n        self,\n        ref_instances: list[\"dreem.io.Instance\"],\n        query_instances: list[\"dreem.io.Instance\"] | None = None,\n    ) -&gt; list[AssociationMatrix]:\n        \"\"\"Execute a forward pass through the transformer and attention head.\n\n        Args:\n            ref_instances: A list of instance objects (See `dreem.io.Instance` for more info.)\n            query_instances: An set of instances to be used as decoder queries.\n\n        Returns:\n            asso_output: A list of torch.Tensors of shape (L, n_query, total_instances) where:\n                L: number of decoder blocks\n                n_query: number of instances in current query/frame\n                total_instances: number of instances in window\n        \"\"\"\n        ref_features = torch.cat(\n            [instance.features for instance in ref_instances], dim=0\n        ).unsqueeze(0)\n\n        # window_length = len(frames)\n        # instances_per_frame = [frame.num_detected for frame in frames]\n        total_instances = len(ref_instances)\n        embed_dim = ref_features.shape[-1]\n        # print(f'T: {window_length}; N: {total_instances}; N_t: {instances_per_frame} n_reid: {reid_features.shape}')\n        ref_boxes = get_boxes(ref_instances)  # total_instances, 4\n        ref_boxes = torch.nan_to_num(ref_boxes, -1.0)\n        ref_times, query_times = get_times(ref_instances, query_instances)\n\n        window_length = len(ref_times.unique())\n\n        ref_temp_emb = self.temp_emb(ref_times / window_length)\n\n        ref_pos_emb = self.pos_emb(ref_boxes)\n\n        if self.return_embedding:\n            for i, instance in enumerate(ref_instances):\n                instance.add_embedding(\"pos\", ref_pos_emb[i])\n                instance.add_embedding(\"temp\", ref_temp_emb[i])\n\n        ref_emb = (ref_pos_emb + ref_temp_emb) / 2.0\n\n        ref_emb = ref_emb.view(1, total_instances, embed_dim)\n\n        ref_emb = ref_emb.permute(1, 0, 2)  # (total_instances, batch_size, embed_dim)\n\n        batch_size, total_instances, embed_dim = ref_features.shape\n\n        ref_features = ref_features.permute(\n            1, 0, 2\n        )  # (total_instances, batch_size, embed_dim)\n\n        encoder_queries = ref_features\n\n        encoder_features = self.encoder(\n            encoder_queries, pos_emb=ref_emb\n        )  # (total_instances, batch_size, embed_dim)\n\n        n_query = total_instances\n\n        query_features = ref_features\n        query_pos_emb = ref_pos_emb\n        query_temp_emb = ref_temp_emb\n        query_emb = ref_emb\n\n        if query_instances is not None:\n            n_query = len(query_instances)\n\n            query_features = torch.cat(\n                [instance.features for instance in query_instances], dim=0\n            ).unsqueeze(0)\n\n            query_features = query_features.permute(\n                1, 0, 2\n            )  # (n_query, batch_size, embed_dim)\n\n            query_boxes = get_boxes(query_instances)\n            query_boxes = torch.nan_to_num(query_boxes, -1.0)\n            query_temp_emb = self.temp_emb(query_times / window_length)\n\n            query_pos_emb = self.pos_emb(query_boxes)\n\n            query_emb = (query_pos_emb + query_temp_emb) / 2.0\n            query_emb = query_emb.view(1, n_query, embed_dim)\n            query_emb = query_emb.permute(1, 0, 2)  # (n_query, batch_size, embed_dim)\n\n        else:\n            query_instances = ref_instances\n\n        if self.return_embedding:\n            for i, instance in enumerate(query_instances):\n                instance.add_embedding(\"pos\", query_pos_emb[i])\n                instance.add_embedding(\"temp\", query_temp_emb[i])\n\n        decoder_features = self.decoder(\n            query_features,\n            encoder_features,\n            ref_pos_emb=ref_emb,\n            query_pos_emb=query_emb,\n        )  # (L, n_query, batch_size, embed_dim)\n\n        decoder_features = decoder_features.transpose(\n            1, 2\n        )  # # (L, batch_size, n_query, embed_dim)\n        encoder_features = encoder_features.permute(1, 0, 2).view(\n            batch_size, total_instances, embed_dim\n        )  # (batch_size, total_instances, embed_dim)\n\n        asso_output = []\n        for frame_features in decoder_features:\n            asso_matrix = self.attn_head(frame_features, encoder_features).view(\n                n_query, total_instances\n            )\n            asso_matrix = AssociationMatrix(asso_matrix, ref_instances, query_instances)\n\n            asso_output.append(asso_matrix)\n\n        # (L=1, n_query, total_instances)\n        return asso_output\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.Transformer.__init__","title":"<code>__init__(d_model=1024, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dropout=0.1, activation='relu', return_intermediate_dec=False, norm=False, num_layers_attn_head=2, dropout_attn_head=0.1, embedding_meta=None, return_embedding=False, decoder_self_attn=False)</code>","text":"<p>Initialize Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of features in the encoder/decoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads in the transfomer encoder/decoder.</p> <code>8</code> <code>num_encoder_layers</code> <code>int</code> <p>The number of encoder-layers in the encoder.</p> <code>6</code> <code>num_decoder_layers</code> <code>int</code> <p>The number of decoder-layers in the decoder.</p> <code>6</code> <code>dropout</code> <code>float</code> <p>Dropout value applied to the output of transformer layers.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>return_intermediate_dec</code> <code>bool</code> <p>Return intermediate layers from decoder.</p> <code>False</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of encoder and decoder.</p> <code>False</code> <code>num_layers_attn_head</code> <code>int</code> <p>The number of layers in the attention head.</p> <code>2</code> <code>dropout_attn_head</code> <code>float</code> <p>Dropout value for the attention_head.</p> <code>0.1</code> <code>embedding_meta</code> <code>dict | None</code> <p>Metadata for positional embeddings. See below.</p> <code>None</code> <code>return_embedding</code> <code>bool</code> <p>Whether to return the positional embeddings</p> <code>False</code> <code>decoder_self_attn</code> <code>bool</code> <p>If True, use decoder self attention.</p> <p>More details on <code>embedding_meta</code>:     By default this will be an empty dict and indicate     that no positional embeddings should be used. To use the positional embeddings     pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:     {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},     \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see <code>dreem.models.embeddings.Embedding.EMB_TYPES</code>     and <code>dreem.models.embeddings.Embedding.EMB_MODES</code> for embedding parameters).</p> <code>False</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    d_model: int = 1024,\n    nhead: int = 8,\n    num_encoder_layers: int = 6,\n    num_decoder_layers: int = 6,\n    dropout: float = 0.1,\n    activation: str = \"relu\",\n    return_intermediate_dec: bool = False,\n    norm: bool = False,\n    num_layers_attn_head: int = 2,\n    dropout_attn_head: float = 0.1,\n    embedding_meta: dict | None = None,\n    return_embedding: bool = False,\n    decoder_self_attn: bool = False,\n) -&gt; None:\n    \"\"\"Initialize Transformer.\n\n    Args:\n        d_model: The number of features in the encoder/decoder inputs.\n        nhead: The number of heads in the transfomer encoder/decoder.\n        num_encoder_layers: The number of encoder-layers in the encoder.\n        num_decoder_layers: The number of decoder-layers in the decoder.\n        dropout: Dropout value applied to the output of transformer layers.\n        activation: Activation function to use.\n        return_intermediate_dec: Return intermediate layers from decoder.\n        norm: If True, normalize output of encoder and decoder.\n        num_layers_attn_head: The number of layers in the attention head.\n        dropout_attn_head: Dropout value for the attention_head.\n        embedding_meta: Metadata for positional embeddings. See below.\n        return_embedding: Whether to return the positional embeddings\n        decoder_self_attn: If True, use decoder self attention.\n\n            More details on `embedding_meta`:\n                By default this will be an empty dict and indicate\n                that no positional embeddings should be used. To use the positional embeddings\n                pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},\n                \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n    \"\"\"\n    super().__init__()\n\n    self.d_model = dim_feedforward = feature_dim_attn_head = d_model\n\n    self.embedding_meta = embedding_meta\n    self.return_embedding = return_embedding\n\n    self.pos_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n    self.temp_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n\n    if self.embedding_meta:\n        if \"pos\" in self.embedding_meta:\n            pos_emb_cfg = self.embedding_meta[\"pos\"]\n            if pos_emb_cfg:\n                self.pos_emb = Embedding(\n                    emb_type=\"pos\", features=self.d_model, **pos_emb_cfg\n                )\n        if \"temp\" in self.embedding_meta:\n            temp_emb_cfg = self.embedding_meta[\"temp\"]\n            if temp_emb_cfg:\n                self.temp_emb = Embedding(\n                    emb_type=\"temp\", features=self.d_model, **temp_emb_cfg\n                )\n\n    # Transformer Encoder\n    encoder_layer = TransformerEncoderLayer(\n        d_model, nhead, dim_feedforward, dropout, activation, norm\n    )\n\n    encoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n    self.encoder = TransformerEncoder(\n        encoder_layer, num_encoder_layers, encoder_norm\n    )\n\n    # Transformer Decoder\n    decoder_layer = TransformerDecoderLayer(\n        d_model,\n        nhead,\n        dim_feedforward,\n        dropout,\n        activation,\n        norm,\n        decoder_self_attn,\n    )\n\n    decoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n    self.decoder = TransformerDecoder(\n        decoder_layer, num_decoder_layers, return_intermediate_dec, decoder_norm\n    )\n\n    # Transformer attention head\n    self.attn_head = ATTWeightHead(\n        feature_dim=feature_dim_attn_head,\n        num_layers=num_layers_attn_head,\n        dropout=dropout_attn_head,\n    )\n\n    self._reset_parameters()\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.Transformer.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute a forward pass through the transformer and attention head.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>A list of instance objects (See <code>dreem.io.Instance</code> for more info.)</p> required <code>query_instances</code> <code>list[Instance] | None</code> <p>An set of instances to be used as decoder queries.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>asso_output</code> <code>list[AssociationMatrix]</code> <p>A list of torch.Tensors of shape (L, n_query, total_instances) where:     L: number of decoder blocks     n_query: number of instances in current query/frame     total_instances: number of instances in window</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self,\n    ref_instances: list[\"dreem.io.Instance\"],\n    query_instances: list[\"dreem.io.Instance\"] | None = None,\n) -&gt; list[AssociationMatrix]:\n    \"\"\"Execute a forward pass through the transformer and attention head.\n\n    Args:\n        ref_instances: A list of instance objects (See `dreem.io.Instance` for more info.)\n        query_instances: An set of instances to be used as decoder queries.\n\n    Returns:\n        asso_output: A list of torch.Tensors of shape (L, n_query, total_instances) where:\n            L: number of decoder blocks\n            n_query: number of instances in current query/frame\n            total_instances: number of instances in window\n    \"\"\"\n    ref_features = torch.cat(\n        [instance.features for instance in ref_instances], dim=0\n    ).unsqueeze(0)\n\n    # window_length = len(frames)\n    # instances_per_frame = [frame.num_detected for frame in frames]\n    total_instances = len(ref_instances)\n    embed_dim = ref_features.shape[-1]\n    # print(f'T: {window_length}; N: {total_instances}; N_t: {instances_per_frame} n_reid: {reid_features.shape}')\n    ref_boxes = get_boxes(ref_instances)  # total_instances, 4\n    ref_boxes = torch.nan_to_num(ref_boxes, -1.0)\n    ref_times, query_times = get_times(ref_instances, query_instances)\n\n    window_length = len(ref_times.unique())\n\n    ref_temp_emb = self.temp_emb(ref_times / window_length)\n\n    ref_pos_emb = self.pos_emb(ref_boxes)\n\n    if self.return_embedding:\n        for i, instance in enumerate(ref_instances):\n            instance.add_embedding(\"pos\", ref_pos_emb[i])\n            instance.add_embedding(\"temp\", ref_temp_emb[i])\n\n    ref_emb = (ref_pos_emb + ref_temp_emb) / 2.0\n\n    ref_emb = ref_emb.view(1, total_instances, embed_dim)\n\n    ref_emb = ref_emb.permute(1, 0, 2)  # (total_instances, batch_size, embed_dim)\n\n    batch_size, total_instances, embed_dim = ref_features.shape\n\n    ref_features = ref_features.permute(\n        1, 0, 2\n    )  # (total_instances, batch_size, embed_dim)\n\n    encoder_queries = ref_features\n\n    encoder_features = self.encoder(\n        encoder_queries, pos_emb=ref_emb\n    )  # (total_instances, batch_size, embed_dim)\n\n    n_query = total_instances\n\n    query_features = ref_features\n    query_pos_emb = ref_pos_emb\n    query_temp_emb = ref_temp_emb\n    query_emb = ref_emb\n\n    if query_instances is not None:\n        n_query = len(query_instances)\n\n        query_features = torch.cat(\n            [instance.features for instance in query_instances], dim=0\n        ).unsqueeze(0)\n\n        query_features = query_features.permute(\n            1, 0, 2\n        )  # (n_query, batch_size, embed_dim)\n\n        query_boxes = get_boxes(query_instances)\n        query_boxes = torch.nan_to_num(query_boxes, -1.0)\n        query_temp_emb = self.temp_emb(query_times / window_length)\n\n        query_pos_emb = self.pos_emb(query_boxes)\n\n        query_emb = (query_pos_emb + query_temp_emb) / 2.0\n        query_emb = query_emb.view(1, n_query, embed_dim)\n        query_emb = query_emb.permute(1, 0, 2)  # (n_query, batch_size, embed_dim)\n\n    else:\n        query_instances = ref_instances\n\n    if self.return_embedding:\n        for i, instance in enumerate(query_instances):\n            instance.add_embedding(\"pos\", query_pos_emb[i])\n            instance.add_embedding(\"temp\", query_temp_emb[i])\n\n    decoder_features = self.decoder(\n        query_features,\n        encoder_features,\n        ref_pos_emb=ref_emb,\n        query_pos_emb=query_emb,\n    )  # (L, n_query, batch_size, embed_dim)\n\n    decoder_features = decoder_features.transpose(\n        1, 2\n    )  # # (L, batch_size, n_query, embed_dim)\n    encoder_features = encoder_features.permute(1, 0, 2).view(\n        batch_size, total_instances, embed_dim\n    )  # (batch_size, total_instances, embed_dim)\n\n    asso_output = []\n    for frame_features in decoder_features:\n        asso_matrix = self.attn_head(frame_features, encoder_features).view(\n            n_query, total_instances\n        )\n        asso_matrix = AssociationMatrix(asso_matrix, ref_instances, query_instances)\n\n        asso_output.append(asso_matrix)\n\n    # (L=1, n_query, total_instances)\n    return asso_output\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerDecoder","title":"<code>TransformerDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer Decoder Block composed of Transformer Decoder Layers.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class TransformerDecoder(nn.Module):\n    \"\"\"Transformer Decoder Block composed of Transformer Decoder Layers.\"\"\"\n\n    def __init__(\n        self,\n        decoder_layer: TransformerDecoderLayer,\n        num_layers: int,\n        return_intermediate: bool = False,\n        norm: nn.Module | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize transformer decoder block.\n\n        Args:\n            decoder_layer: An instance of TransformerDecoderLayer.\n            num_layers: The number of decoder layers to be stacked.\n            return_intermediate: Return intermediate layers from decoder.\n            norm: The normalization layer to be applied.\n        \"\"\"\n        super().__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.return_intermediate = return_intermediate\n        self.norm = norm if norm is not None else nn.Identity()\n\n    def forward(\n        self,\n        decoder_queries: torch.Tensor,\n        encoder_features: torch.Tensor,\n        ref_pos_emb: torch.Tensor | None = None,\n        query_pos_emb: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute a forward pass of the decoder block.\n\n        Args:\n            decoder_queries: Query sequence for decoder to generate (n_query, batch_size, embed_dim).\n            encoder_features: Output from encoder, that decoder uses to attend to relevant\n                parts of input sequence (total_instances, batch_size, embed_dim)\n            ref_pos_emb: The input positional embedding tensor of shape (total_instances, batch_size, embed_dim).\n            query_pos_emb: The query positional embedding of shape (n_query, batch_size, embed_dim)\n\n        Returns:\n            The output tensor of shape (L, n_query, batch_size, embed_dim).\n        \"\"\"\n        decoder_features = decoder_queries\n\n        intermediate = []\n\n        for layer in self.layers:\n            decoder_features = layer(\n                decoder_features,\n                encoder_features,\n                ref_pos_emb=ref_pos_emb,\n                query_pos_emb=query_pos_emb,\n            )\n            if self.return_intermediate:\n                intermediate.append(self.norm(decoder_features))\n\n        decoder_features = self.norm(decoder_features)\n        if self.return_intermediate:\n            intermediate.pop()\n            intermediate.append(decoder_features)\n\n            return torch.stack(intermediate)\n\n        return decoder_features.unsqueeze(0)\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerDecoder.__init__","title":"<code>__init__(decoder_layer, num_layers, return_intermediate=False, norm=None)</code>","text":"<p>Initialize transformer decoder block.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_layer</code> <code>TransformerDecoderLayer</code> <p>An instance of TransformerDecoderLayer.</p> required <code>num_layers</code> <code>int</code> <p>The number of decoder layers to be stacked.</p> required <code>return_intermediate</code> <code>bool</code> <p>Return intermediate layers from decoder.</p> <code>False</code> <code>norm</code> <code>Module | None</code> <p>The normalization layer to be applied.</p> <code>None</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    decoder_layer: TransformerDecoderLayer,\n    num_layers: int,\n    return_intermediate: bool = False,\n    norm: nn.Module | None = None,\n) -&gt; None:\n    \"\"\"Initialize transformer decoder block.\n\n    Args:\n        decoder_layer: An instance of TransformerDecoderLayer.\n        num_layers: The number of decoder layers to be stacked.\n        return_intermediate: Return intermediate layers from decoder.\n        norm: The normalization layer to be applied.\n    \"\"\"\n    super().__init__()\n    self.layers = _get_clones(decoder_layer, num_layers)\n    self.num_layers = num_layers\n    self.return_intermediate = return_intermediate\n    self.norm = norm if norm is not None else nn.Identity()\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerDecoder.forward","title":"<code>forward(decoder_queries, encoder_features, ref_pos_emb=None, query_pos_emb=None)</code>","text":"<p>Execute a forward pass of the decoder block.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_queries</code> <code>Tensor</code> <p>Query sequence for decoder to generate (n_query, batch_size, embed_dim).</p> required <code>encoder_features</code> <code>Tensor</code> <p>Output from encoder, that decoder uses to attend to relevant parts of input sequence (total_instances, batch_size, embed_dim)</p> required <code>ref_pos_emb</code> <code>Tensor | None</code> <p>The input positional embedding tensor of shape (total_instances, batch_size, embed_dim).</p> <code>None</code> <code>query_pos_emb</code> <code>Tensor | None</code> <p>The query positional embedding of shape (n_query, batch_size, embed_dim)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (L, n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self,\n    decoder_queries: torch.Tensor,\n    encoder_features: torch.Tensor,\n    ref_pos_emb: torch.Tensor | None = None,\n    query_pos_emb: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Execute a forward pass of the decoder block.\n\n    Args:\n        decoder_queries: Query sequence for decoder to generate (n_query, batch_size, embed_dim).\n        encoder_features: Output from encoder, that decoder uses to attend to relevant\n            parts of input sequence (total_instances, batch_size, embed_dim)\n        ref_pos_emb: The input positional embedding tensor of shape (total_instances, batch_size, embed_dim).\n        query_pos_emb: The query positional embedding of shape (n_query, batch_size, embed_dim)\n\n    Returns:\n        The output tensor of shape (L, n_query, batch_size, embed_dim).\n    \"\"\"\n    decoder_features = decoder_queries\n\n    intermediate = []\n\n    for layer in self.layers:\n        decoder_features = layer(\n            decoder_features,\n            encoder_features,\n            ref_pos_emb=ref_pos_emb,\n            query_pos_emb=query_pos_emb,\n        )\n        if self.return_intermediate:\n            intermediate.append(self.norm(decoder_features))\n\n    decoder_features = self.norm(decoder_features)\n    if self.return_intermediate:\n        intermediate.pop()\n        intermediate.append(decoder_features)\n\n        return torch.stack(intermediate)\n\n    return decoder_features.unsqueeze(0)\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerDecoderLayer","title":"<code>TransformerDecoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A single transformer decoder layer.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class TransformerDecoderLayer(nn.Module):\n    \"\"\"A single transformer decoder layer.\"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 1024,\n        nhead: int = 6,\n        dim_feedforward: int = 1024,\n        dropout: float = 0.1,\n        activation: str = \"relu\",\n        norm: bool = False,\n        decoder_self_attn: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize transformer decoder layer.\n\n        Args:\n            d_model: The number of features in the decoder inputs.\n            nhead: The number of heads for the decoder.\n            dim_feedforward: Dimension of the feedforward layers of decoder.\n            dropout: Dropout value applied to the output of decoder.\n            activation: Activation function to use.\n            norm: If True, normalize output of decoder.\n            decoder_self_attn: If True, use decoder self attention\n        \"\"\"\n        super().__init__()\n\n        self.decoder_self_attn = decoder_self_attn\n\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        if self.decoder_self_attn:\n            self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n\n        self.norm1 = nn.LayerNorm(d_model) if norm else nn.Identity()\n        self.norm2 = nn.LayerNorm(d_model) if norm else nn.Identity()\n        self.norm3 = nn.LayerNorm(d_model) if norm else nn.Identity()\n\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n\n    def forward(\n        self,\n        decoder_queries: torch.Tensor,\n        encoder_features: torch.Tensor,\n        ref_pos_emb: torch.Tensor | None = None,\n        query_pos_emb: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute forward pass of decoder layer.\n\n        Args:\n            decoder_queries: Target sequence for decoder to generate (n_query, batch_size, embed_dim).\n            encoder_features: Output from encoder, that decoder uses to attend to relevant\n                parts of input sequence (total_instances, batch_size, embed_dim)\n            ref_pos_emb: The input positional embedding tensor of shape (n_query, embed_dim).\n            query_pos_emb: The target positional embedding of shape (n_query, embed_dim)\n\n        Returns:\n            The output tensor of shape (n_query, batch_size, embed_dim).\n        \"\"\"\n        if query_pos_emb is None:\n            query_pos_emb = torch.zeros_like(decoder_queries)\n        if ref_pos_emb is None:\n            ref_pos_emb = torch.zeros_like(encoder_features)\n\n        decoder_queries = decoder_queries + query_pos_emb\n        encoder_features = encoder_features + ref_pos_emb\n\n        if self.decoder_self_attn:\n            self_attn_features = self.self_attn(\n                query=decoder_queries, key=decoder_queries, value=decoder_queries\n            )[0]\n            decoder_queries = decoder_queries + self.dropout1(self_attn_features)\n            decoder_queries = self.norm1(decoder_queries)\n\n        x_attn_features = self.multihead_attn(\n            query=decoder_queries,  # (n_query, batch_size, embed_dim)\n            key=encoder_features,  # (total_instances, batch_size, embed_dim)\n            value=encoder_features,  # (total_instances, batch_size, embed_dim)\n        )[\n            0\n        ]  # (n_query, batch_size, embed_dim)\n\n        decoder_queries = decoder_queries + self.dropout2(\n            x_attn_features\n        )  # (n_query, batch_size, embed_dim)\n        decoder_queries = self.norm2(\n            decoder_queries\n        )  # (n_query, batch_size, embed_dim)\n        projection = self.linear2(\n            self.dropout(self.activation(self.linear1(decoder_queries)))\n        )  # (n_query, batch_size, embed_dim)\n        decoder_queries = decoder_queries + self.dropout3(\n            projection\n        )  # (n_query, batch_size, embed_dim)\n        decoder_features = self.norm3(decoder_queries)\n\n        return decoder_features  # (n_query, batch_size, embed_dim)\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerDecoderLayer.__init__","title":"<code>__init__(d_model=1024, nhead=6, dim_feedforward=1024, dropout=0.1, activation='relu', norm=False, decoder_self_attn=False)</code>","text":"<p>Initialize transformer decoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of features in the decoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads for the decoder.</p> <code>6</code> <code>dim_feedforward</code> <code>int</code> <p>Dimension of the feedforward layers of decoder.</p> <code>1024</code> <code>dropout</code> <code>float</code> <p>Dropout value applied to the output of decoder.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of decoder.</p> <code>False</code> <code>decoder_self_attn</code> <code>bool</code> <p>If True, use decoder self attention</p> <code>False</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    d_model: int = 1024,\n    nhead: int = 6,\n    dim_feedforward: int = 1024,\n    dropout: float = 0.1,\n    activation: str = \"relu\",\n    norm: bool = False,\n    decoder_self_attn: bool = False,\n) -&gt; None:\n    \"\"\"Initialize transformer decoder layer.\n\n    Args:\n        d_model: The number of features in the decoder inputs.\n        nhead: The number of heads for the decoder.\n        dim_feedforward: Dimension of the feedforward layers of decoder.\n        dropout: Dropout value applied to the output of decoder.\n        activation: Activation function to use.\n        norm: If True, normalize output of decoder.\n        decoder_self_attn: If True, use decoder self attention\n    \"\"\"\n    super().__init__()\n\n    self.decoder_self_attn = decoder_self_attn\n\n    self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n    self.linear1 = nn.Linear(d_model, dim_feedforward)\n    self.dropout = nn.Dropout(dropout)\n    self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n    if self.decoder_self_attn:\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n\n    self.norm1 = nn.LayerNorm(d_model) if norm else nn.Identity()\n    self.norm2 = nn.LayerNorm(d_model) if norm else nn.Identity()\n    self.norm3 = nn.LayerNorm(d_model) if norm else nn.Identity()\n\n    self.dropout1 = nn.Dropout(dropout)\n    self.dropout2 = nn.Dropout(dropout)\n    self.dropout3 = nn.Dropout(dropout)\n\n    self.activation = _get_activation_fn(activation)\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerDecoderLayer.forward","title":"<code>forward(decoder_queries, encoder_features, ref_pos_emb=None, query_pos_emb=None)</code>","text":"<p>Execute forward pass of decoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_queries</code> <code>Tensor</code> <p>Target sequence for decoder to generate (n_query, batch_size, embed_dim).</p> required <code>encoder_features</code> <code>Tensor</code> <p>Output from encoder, that decoder uses to attend to relevant parts of input sequence (total_instances, batch_size, embed_dim)</p> required <code>ref_pos_emb</code> <code>Tensor | None</code> <p>The input positional embedding tensor of shape (n_query, embed_dim).</p> <code>None</code> <code>query_pos_emb</code> <code>Tensor | None</code> <p>The target positional embedding of shape (n_query, embed_dim)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self,\n    decoder_queries: torch.Tensor,\n    encoder_features: torch.Tensor,\n    ref_pos_emb: torch.Tensor | None = None,\n    query_pos_emb: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Execute forward pass of decoder layer.\n\n    Args:\n        decoder_queries: Target sequence for decoder to generate (n_query, batch_size, embed_dim).\n        encoder_features: Output from encoder, that decoder uses to attend to relevant\n            parts of input sequence (total_instances, batch_size, embed_dim)\n        ref_pos_emb: The input positional embedding tensor of shape (n_query, embed_dim).\n        query_pos_emb: The target positional embedding of shape (n_query, embed_dim)\n\n    Returns:\n        The output tensor of shape (n_query, batch_size, embed_dim).\n    \"\"\"\n    if query_pos_emb is None:\n        query_pos_emb = torch.zeros_like(decoder_queries)\n    if ref_pos_emb is None:\n        ref_pos_emb = torch.zeros_like(encoder_features)\n\n    decoder_queries = decoder_queries + query_pos_emb\n    encoder_features = encoder_features + ref_pos_emb\n\n    if self.decoder_self_attn:\n        self_attn_features = self.self_attn(\n            query=decoder_queries, key=decoder_queries, value=decoder_queries\n        )[0]\n        decoder_queries = decoder_queries + self.dropout1(self_attn_features)\n        decoder_queries = self.norm1(decoder_queries)\n\n    x_attn_features = self.multihead_attn(\n        query=decoder_queries,  # (n_query, batch_size, embed_dim)\n        key=encoder_features,  # (total_instances, batch_size, embed_dim)\n        value=encoder_features,  # (total_instances, batch_size, embed_dim)\n    )[\n        0\n    ]  # (n_query, batch_size, embed_dim)\n\n    decoder_queries = decoder_queries + self.dropout2(\n        x_attn_features\n    )  # (n_query, batch_size, embed_dim)\n    decoder_queries = self.norm2(\n        decoder_queries\n    )  # (n_query, batch_size, embed_dim)\n    projection = self.linear2(\n        self.dropout(self.activation(self.linear1(decoder_queries)))\n    )  # (n_query, batch_size, embed_dim)\n    decoder_queries = decoder_queries + self.dropout3(\n        projection\n    )  # (n_query, batch_size, embed_dim)\n    decoder_features = self.norm3(decoder_queries)\n\n    return decoder_features  # (n_query, batch_size, embed_dim)\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerEncoder","title":"<code>TransformerEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>A transformer encoder block composed of encoder layers.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class TransformerEncoder(nn.Module):\n    \"\"\"A transformer encoder block composed of encoder layers.\"\"\"\n\n    def __init__(\n        self,\n        encoder_layer: TransformerEncoderLayer,\n        num_layers: int,\n        norm: nn.Module | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize transformer encoder.\n\n        Args:\n            encoder_layer: An instance of the TransformerEncoderLayer.\n            num_layers: The number of encoder layers to be stacked.\n            norm: The normalization layer to be applied.\n        \"\"\"\n        super().__init__()\n\n        self.layers = _get_clones(encoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm if norm is not None else nn.Identity()\n\n    def forward(\n        self, queries: torch.Tensor, pos_emb: torch.Tensor = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute a forward pass of encoder layer.\n\n        Args:\n            queries: The input tensor of shape (n_query, batch_size, embed_dim).\n            pos_emb: The positional embedding tensor of shape (n_query, embed_dim).\n\n        Returns:\n            The output tensor of shape (n_query, batch_size, embed_dim).\n        \"\"\"\n        for layer in self.layers:\n            queries = layer(queries, pos_emb=pos_emb)\n\n        encoder_features = self.norm(queries)\n        return encoder_features\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerEncoder.__init__","title":"<code>__init__(encoder_layer, num_layers, norm=None)</code>","text":"<p>Initialize transformer encoder.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_layer</code> <code>TransformerEncoderLayer</code> <p>An instance of the TransformerEncoderLayer.</p> required <code>num_layers</code> <code>int</code> <p>The number of encoder layers to be stacked.</p> required <code>norm</code> <code>Module | None</code> <p>The normalization layer to be applied.</p> <code>None</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    encoder_layer: TransformerEncoderLayer,\n    num_layers: int,\n    norm: nn.Module | None = None,\n) -&gt; None:\n    \"\"\"Initialize transformer encoder.\n\n    Args:\n        encoder_layer: An instance of the TransformerEncoderLayer.\n        num_layers: The number of encoder layers to be stacked.\n        norm: The normalization layer to be applied.\n    \"\"\"\n    super().__init__()\n\n    self.layers = _get_clones(encoder_layer, num_layers)\n    self.num_layers = num_layers\n    self.norm = norm if norm is not None else nn.Identity()\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerEncoder.forward","title":"<code>forward(queries, pos_emb=None)</code>","text":"<p>Execute a forward pass of encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Tensor</code> <p>The input tensor of shape (n_query, batch_size, embed_dim).</p> required <code>pos_emb</code> <code>Tensor</code> <p>The positional embedding tensor of shape (n_query, embed_dim).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self, queries: torch.Tensor, pos_emb: torch.Tensor = None\n) -&gt; torch.Tensor:\n    \"\"\"Execute a forward pass of encoder layer.\n\n    Args:\n        queries: The input tensor of shape (n_query, batch_size, embed_dim).\n        pos_emb: The positional embedding tensor of shape (n_query, embed_dim).\n\n    Returns:\n        The output tensor of shape (n_query, batch_size, embed_dim).\n    \"\"\"\n    for layer in self.layers:\n        queries = layer(queries, pos_emb=pos_emb)\n\n    encoder_features = self.norm(queries)\n    return encoder_features\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerEncoderLayer","title":"<code>TransformerEncoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A single transformer encoder layer.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class TransformerEncoderLayer(nn.Module):\n    \"\"\"A single transformer encoder layer.\"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 1024,\n        nhead: int = 6,\n        dim_feedforward: int = 1024,\n        dropout: float = 0.1,\n        activation: str = \"relu\",\n        norm: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize a transformer encoder layer.\n\n        Args:\n            d_model: The number of features in the encoder inputs.\n            nhead: The number of heads for the encoder.\n            dim_feedforward: Dimension of the feedforward layers of encoder.\n            dropout: Dropout value applied to the output of encoder.\n            activation: Activation function to use.\n            norm: If True, normalize output of encoder.\n        \"\"\"\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model) if norm else nn.Identity()\n        self.norm2 = nn.LayerNorm(d_model) if norm else nn.Identity()\n\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n\n    def forward(\n        self, queries: torch.Tensor, pos_emb: torch.Tensor = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute a forward pass of the encoder layer.\n\n        Args:\n            queries: Input sequence for encoder (n_query, batch_size, embed_dim).\n            pos_emb: Position embedding, if provided is added to src\n\n        Returns:\n            The output tensor of shape (n_query, batch_size, embed_dim).\n        \"\"\"\n        if pos_emb is None:\n            pos_emb = torch.zeros_like(queries)\n\n        queries = queries + pos_emb\n\n        # q = k = src\n\n        attn_features = self.self_attn(\n            query=queries,\n            key=queries,\n            value=queries,\n        )[0]\n\n        queries = queries + self.dropout1(attn_features)\n        queries = self.norm1(queries)\n        projection = self.linear2(self.dropout(self.activation(self.linear1(queries))))\n        queries = queries + self.dropout2(projection)\n        encoder_features = self.norm2(queries)\n\n        return encoder_features\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerEncoderLayer.__init__","title":"<code>__init__(d_model=1024, nhead=6, dim_feedforward=1024, dropout=0.1, activation='relu', norm=False)</code>","text":"<p>Initialize a transformer encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of features in the encoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads for the encoder.</p> <code>6</code> <code>dim_feedforward</code> <code>int</code> <p>Dimension of the feedforward layers of encoder.</p> <code>1024</code> <code>dropout</code> <code>float</code> <p>Dropout value applied to the output of encoder.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of encoder.</p> <code>False</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    d_model: int = 1024,\n    nhead: int = 6,\n    dim_feedforward: int = 1024,\n    dropout: float = 0.1,\n    activation: str = \"relu\",\n    norm: bool = False,\n) -&gt; None:\n    \"\"\"Initialize a transformer encoder layer.\n\n    Args:\n        d_model: The number of features in the encoder inputs.\n        nhead: The number of heads for the encoder.\n        dim_feedforward: Dimension of the feedforward layers of encoder.\n        dropout: Dropout value applied to the output of encoder.\n        activation: Activation function to use.\n        norm: If True, normalize output of encoder.\n    \"\"\"\n    super().__init__()\n    self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n    self.linear1 = nn.Linear(d_model, dim_feedforward)\n    self.dropout = nn.Dropout(dropout)\n    self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n    self.norm1 = nn.LayerNorm(d_model) if norm else nn.Identity()\n    self.norm2 = nn.LayerNorm(d_model) if norm else nn.Identity()\n\n    self.dropout1 = nn.Dropout(dropout)\n    self.dropout2 = nn.Dropout(dropout)\n\n    self.activation = _get_activation_fn(activation)\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerEncoderLayer.forward","title":"<code>forward(queries, pos_emb=None)</code>","text":"<p>Execute a forward pass of the encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Tensor</code> <p>Input sequence for encoder (n_query, batch_size, embed_dim).</p> required <code>pos_emb</code> <code>Tensor</code> <p>Position embedding, if provided is added to src</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self, queries: torch.Tensor, pos_emb: torch.Tensor = None\n) -&gt; torch.Tensor:\n    \"\"\"Execute a forward pass of the encoder layer.\n\n    Args:\n        queries: Input sequence for encoder (n_query, batch_size, embed_dim).\n        pos_emb: Position embedding, if provided is added to src\n\n    Returns:\n        The output tensor of shape (n_query, batch_size, embed_dim).\n    \"\"\"\n    if pos_emb is None:\n        pos_emb = torch.zeros_like(queries)\n\n    queries = queries + pos_emb\n\n    # q = k = src\n\n    attn_features = self.self_attn(\n        query=queries,\n        key=queries,\n        value=queries,\n    )[0]\n\n    queries = queries + self.dropout1(attn_features)\n    queries = self.norm1(queries)\n    projection = self.linear2(self.dropout(self.activation(self.linear1(queries))))\n    queries = queries + self.dropout2(projection)\n    encoder_features = self.norm2(queries)\n\n    return encoder_features\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/","title":"visual_encoder","text":""},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder","title":"<code>dreem.models.visual_encoder</code>","text":"<p>Module for different visual feature extractors.</p>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.VisualEncoder","title":"<code>VisualEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Class wrapping around a visual feature extractor backbone.</p> <p>Currently CNN only.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>class VisualEncoder(torch.nn.Module):\n    \"\"\"Class wrapping around a visual feature extractor backbone.\n\n    Currently CNN only.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"resnet18\",\n        d_model: int = 512,\n        in_chans: int = 3,\n        backend: int = \"timm\",\n        **kwargs: Any | None,\n    ):\n        \"\"\"Initialize Visual Encoder.\n\n        Args:\n            model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n            d_model (int): Output embedding dimension.\n            in_chans: the number of input channels of the image.\n            backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n            kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n        \"\"\"\n        super().__init__()\n\n        self.model_name = model_name.lower()\n        self.d_model = d_model\n        self.backend = backend\n        if in_chans == 1:\n            self.in_chans = 3\n        else:\n            self.in_chans = in_chans\n\n        self.feature_extractor = self.select_feature_extractor(\n            model_name=self.model_name,\n            in_chans=self.in_chans,\n            backend=self.backend,\n            **kwargs,\n        )\n\n        self.out_layer = torch.nn.Linear(\n            self.encoder_dim(self.feature_extractor), self.d_model\n        )\n\n    def select_feature_extractor(\n        self, model_name: str, in_chans: int, backend: str, **kwargs: Any\n    ) -&gt; torch.nn.Module:\n        \"\"\"Select the appropriate feature extractor based on config.\n\n        Args:\n            model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n            in_chans: the number of input channels of the image.\n            backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n            kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n\n        Returns:\n            a CNN encoder based on the config and backend selected.\n        \"\"\"\n        if \"timm\" in backend.lower():\n            feature_extractor = timm.create_model(\n                model_name=self.model_name,\n                in_chans=self.in_chans,\n                num_classes=0,\n                **kwargs,\n            )\n        elif \"torch\" in backend.lower():\n            if model_name.lower() == \"resnet18\":\n                feature_extractor = torchvision.models.resnet18(**kwargs)\n\n            elif model_name.lower() == \"resnet50\":\n                feature_extractor = torchvision.models.resnet50(**kwargs)\n\n            else:\n                raise ValueError(\n                    f\"Only `[resnet18, resnet50]` are available when backend is {backend}. Found {model_name}\"\n                )\n            feature_extractor = torch.nn.Sequential(\n                *list(feature_extractor.children())[:-1]\n            )\n            input_layer = feature_extractor[0]\n            if in_chans != 3:\n                feature_extractor[0] = torch.nn.Conv2d(\n                    in_channels=in_chans,\n                    out_channels=input_layer.out_channels,\n                    kernel_size=input_layer.kernel_size,\n                    stride=input_layer.stride,\n                    padding=input_layer.padding,\n                    dilation=input_layer.dilation,\n                    groups=input_layer.groups,\n                    bias=input_layer.bias,\n                    padding_mode=input_layer.padding_mode,\n                )\n\n        else:\n            raise ValueError(\n                f\"Only ['timm', 'torch'] backends are available! Found {backend}.\"\n            )\n        return feature_extractor\n\n    def encoder_dim(self, model: torch.nn.Module) -&gt; int:\n        \"\"\"Compute dummy forward pass of encoder model and get embedding dimension.\n\n        Args:\n            model: a vision encoder model.\n\n        Returns:\n            The embedding dimension size.\n        \"\"\"\n        _ = model.eval()\n        dummy_output = model(torch.randn(1, self.in_chans, 224, 224)).squeeze()\n        _ = model.train()  # to be safe\n        return dummy_output.shape[-1]\n\n    def forward(self, img: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of feature extractor to get feature vector.\n\n        Args:\n            img: Input image tensor of shape (B, C, H, W).\n\n        Returns:\n            feats: Normalized output tensor of shape (B, d_model).\n        \"\"\"\n        # If grayscale, tile the image to 3 channels.\n        if img.shape[1] == 1:\n            img = img.repeat([1, 3, 1, 1])  # (B, nc=3, H, W)\n        # Extract image features\n        feats = self.feature_extractor(\n            img\n        )  # (B, out_dim, 1, 1) if using resnet18 backbone.\n\n        # Reshape feature vectors\n        feats = feats.reshape([img.shape[0], -1])  # (B, out_dim)\n        # Map feature vectors to output dimension using linear layer.\n        feats = self.out_layer(feats)  # (B, d_model)\n        # Normalize output feature vectors.\n        feats = F.normalize(feats)  # (B, d_model)\n        return feats\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.VisualEncoder.__init__","title":"<code>__init__(model_name='resnet18', d_model=512, in_chans=3, backend='timm', **kwargs)</code>","text":"<p>Initialize Visual Encoder.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").</p> <code>'resnet18'</code> <code>d_model</code> <code>int</code> <p>Output embedding dimension.</p> <code>512</code> <code>in_chans</code> <code>int</code> <p>the number of input channels of the image.</p> <code>3</code> <code>backend</code> <code>int</code> <p>Which model backend to use. One of {\"timm\", \"torchvision\"}</p> <code>'timm'</code> <code>kwargs</code> <code>Any | None</code> <p>see <code>timm.create_model</code> and <code>torchvision.models.resnetX</code> for kwargs.</p> <code>{}</code> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"resnet18\",\n    d_model: int = 512,\n    in_chans: int = 3,\n    backend: int = \"timm\",\n    **kwargs: Any | None,\n):\n    \"\"\"Initialize Visual Encoder.\n\n    Args:\n        model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n        d_model (int): Output embedding dimension.\n        in_chans: the number of input channels of the image.\n        backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n        kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n    \"\"\"\n    super().__init__()\n\n    self.model_name = model_name.lower()\n    self.d_model = d_model\n    self.backend = backend\n    if in_chans == 1:\n        self.in_chans = 3\n    else:\n        self.in_chans = in_chans\n\n    self.feature_extractor = self.select_feature_extractor(\n        model_name=self.model_name,\n        in_chans=self.in_chans,\n        backend=self.backend,\n        **kwargs,\n    )\n\n    self.out_layer = torch.nn.Linear(\n        self.encoder_dim(self.feature_extractor), self.d_model\n    )\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.VisualEncoder.encoder_dim","title":"<code>encoder_dim(model)</code>","text":"<p>Compute dummy forward pass of encoder model and get embedding dimension.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>a vision encoder model.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The embedding dimension size.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def encoder_dim(self, model: torch.nn.Module) -&gt; int:\n    \"\"\"Compute dummy forward pass of encoder model and get embedding dimension.\n\n    Args:\n        model: a vision encoder model.\n\n    Returns:\n        The embedding dimension size.\n    \"\"\"\n    _ = model.eval()\n    dummy_output = model(torch.randn(1, self.in_chans, 224, 224)).squeeze()\n    _ = model.train()  # to be safe\n    return dummy_output.shape[-1]\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.VisualEncoder.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of feature extractor to get feature vector.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor</code> <p>Input image tensor of shape (B, C, H, W).</p> required <p>Returns:</p> Name Type Description <code>feats</code> <code>Tensor</code> <p>Normalized output tensor of shape (B, d_model).</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def forward(self, img: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of feature extractor to get feature vector.\n\n    Args:\n        img: Input image tensor of shape (B, C, H, W).\n\n    Returns:\n        feats: Normalized output tensor of shape (B, d_model).\n    \"\"\"\n    # If grayscale, tile the image to 3 channels.\n    if img.shape[1] == 1:\n        img = img.repeat([1, 3, 1, 1])  # (B, nc=3, H, W)\n    # Extract image features\n    feats = self.feature_extractor(\n        img\n    )  # (B, out_dim, 1, 1) if using resnet18 backbone.\n\n    # Reshape feature vectors\n    feats = feats.reshape([img.shape[0], -1])  # (B, out_dim)\n    # Map feature vectors to output dimension using linear layer.\n    feats = self.out_layer(feats)  # (B, d_model)\n    # Normalize output feature vectors.\n    feats = F.normalize(feats)  # (B, d_model)\n    return feats\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.VisualEncoder.select_feature_extractor","title":"<code>select_feature_extractor(model_name, in_chans, backend, **kwargs)</code>","text":"<p>Select the appropriate feature extractor based on config.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").</p> required <code>in_chans</code> <code>int</code> <p>the number of input channels of the image.</p> required <code>backend</code> <code>str</code> <p>Which model backend to use. One of {\"timm\", \"torchvision\"}</p> required <code>kwargs</code> <code>Any</code> <p>see <code>timm.create_model</code> and <code>torchvision.models.resnetX</code> for kwargs.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Module</code> <p>a CNN encoder based on the config and backend selected.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def select_feature_extractor(\n    self, model_name: str, in_chans: int, backend: str, **kwargs: Any\n) -&gt; torch.nn.Module:\n    \"\"\"Select the appropriate feature extractor based on config.\n\n    Args:\n        model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n        in_chans: the number of input channels of the image.\n        backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n        kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n\n    Returns:\n        a CNN encoder based on the config and backend selected.\n    \"\"\"\n    if \"timm\" in backend.lower():\n        feature_extractor = timm.create_model(\n            model_name=self.model_name,\n            in_chans=self.in_chans,\n            num_classes=0,\n            **kwargs,\n        )\n    elif \"torch\" in backend.lower():\n        if model_name.lower() == \"resnet18\":\n            feature_extractor = torchvision.models.resnet18(**kwargs)\n\n        elif model_name.lower() == \"resnet50\":\n            feature_extractor = torchvision.models.resnet50(**kwargs)\n\n        else:\n            raise ValueError(\n                f\"Only `[resnet18, resnet50]` are available when backend is {backend}. Found {model_name}\"\n            )\n        feature_extractor = torch.nn.Sequential(\n            *list(feature_extractor.children())[:-1]\n        )\n        input_layer = feature_extractor[0]\n        if in_chans != 3:\n            feature_extractor[0] = torch.nn.Conv2d(\n                in_channels=in_chans,\n                out_channels=input_layer.out_channels,\n                kernel_size=input_layer.kernel_size,\n                stride=input_layer.stride,\n                padding=input_layer.padding,\n                dilation=input_layer.dilation,\n                groups=input_layer.groups,\n                bias=input_layer.bias,\n                padding_mode=input_layer.padding_mode,\n            )\n\n    else:\n        raise ValueError(\n            f\"Only ['timm', 'torch'] backends are available! Found {backend}.\"\n        )\n    return feature_extractor\n</code></pre>"},{"location":"reference/dreem/training/","title":"training","text":""},{"location":"reference/dreem/training/#dreem.training","title":"<code>dreem.training</code>","text":"<p>Initialize training module.</p>"},{"location":"reference/dreem/training/losses/","title":"losses","text":""},{"location":"reference/dreem/training/losses/#dreem.training.losses","title":"<code>dreem.training.losses</code>","text":"<p>Module containing different loss functions to be optimized.</p>"},{"location":"reference/dreem/training/losses/#dreem.training.losses.AssoLoss","title":"<code>AssoLoss</code>","text":"<p>               Bases: <code>Module</code></p> <p>Default association loss used for training GTR model.</p> Source code in <code>dreem/training/losses.py</code> <pre><code>class AssoLoss(nn.Module):\n    \"\"\"Default association loss used for training GTR model.\"\"\"\n\n    def __init__(\n        self,\n        neg_unmatched: bool = False,\n        epsilon: float = 1e-4,\n        asso_weight: float = 1.0,\n    ):\n        \"\"\"Initialize Loss function.\n\n        Args:\n            neg_unmatched: Whether or not to set unmatched objects to background\n            epsilon: small number used for numeric precision to prevent dividing by zero\n            asso_weight: How much to weight the association loss by\n        \"\"\"\n        super().__init__()\n\n        self.neg_unmatched = neg_unmatched\n        self.epsilon = epsilon\n        self.asso_weight = asso_weight\n\n    def forward(\n        self, asso_preds: list[torch.Tensor], frames: list[\"Frame\"]\n    ) -&gt; torch.Tensor:\n        \"\"\"Calculate association loss.\n\n        Args:\n            asso_preds: a list containing the association matrix at each frame\n            frames: a list of Frames containing gt labels.\n\n        Returns:\n            the association loss between predicted association and actual\n        \"\"\"\n        # get number of detected objects and ground truth ids\n        n_t = [frame.num_detected for frame in frames]\n        target_inst_id = torch.cat(\n            [frame.get_gt_track_ids().to(asso_preds[-1].device) for frame in frames]\n        )\n        instances = [instance for frame in frames for instance in frame.instances]\n\n        # for now set equal since detections are fixed\n        pred_box = get_boxes(instances)\n        pred_time, _ = get_times(instances)\n        pred_box = torch.nanmean(pred_box, axis=1)\n        target_box, target_time = pred_box, pred_time\n\n        # todo: we should maybe reconsider how we label gt instances. The second\n        # criterion will return true on a single instance video, for example.\n        # For now we can ignore this since we train on dense labels.\n\n        \"\"\"\n            # Return a 0 loss if any of the 2 criteria are met\n            # 1. the video doesn\u2019t have gt bboxes\n            # 2. the maximum id is zero\n\n        sum_instance_lengths = sum(len(x) for x in instances)\n        max_instance_lengths = max(\n            x[\"gt_track_ids\"].max().item() for x in instances if len(x) &gt; 0\n        )\n\n        if sum_instance_lengths == 0 or max_instance_lengths == 0:\n            print(\"No bounding boxes detected, returning zero loss\")\n            print(f\"Sum instance lengths: {sum_instance_lengths}\")\n            print(f\"Max instance lengths: {max_instance_lengths}\")\n            loss = asso_preds[0].new_zeros((1,), dtype=torch.float32)[0]\n            return loss\n        \"\"\"\n\n        asso_gt, match_cues = self._get_asso_gt(\n            pred_box, pred_time, target_box, target_time, target_inst_id, n_t\n        )\n\n        loss = sum(\n            [\n                self.detr_asso_loss(asso_pred, asso_gt, match_cues, n_t)\n                for asso_pred in asso_preds\n            ]\n        )\n\n        loss *= self.asso_weight\n\n        return loss\n\n    def _get_asso_gt(\n        self,\n        pred_box: torch.Tensor,\n        pred_time: torch.Tensor,\n        target_box: torch.Tensor,\n        target_time: torch.Tensor,\n        target_inst_id: torch.Tensor,\n        n_t: torch.Tensor,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Compute the association ground truth for a batch.\n\n        Args:\n            pred_box: predicted bounding boxes (N x 4)\n            pred_time: predicted time intervals (N,)\n            target_box: target bounding boxes (N x 4)\n            target_time: target time intervals (N,)\n            target_inst_id: target instance IDs (N,)\n            n_t: number of ground truth instances (N,)\n\n        Returns:\n            A tuple containing:\n                asso_gt: Ground truth association matrix (K x N) denoting ground\n                    truth instances over time\n                match_cues: Tensor indicating which instance is assigned to each gt\n                    detection (K x 3) or (N,)\n        \"\"\"\n        # compute ious over bboxes, ignore pairs with different time stamps\n        ious = torchvision.ops.box_iou(pred_box, target_box)\n        ious[pred_time[:, None] != target_time[None, :]] = -1.0\n\n        # get unique instance ids\n        inst_ids = torch.unique(target_inst_id[target_inst_id &gt; -1])\n\n        # initialize tensors\n        K, N = len(inst_ids), len(pred_box)\n        match_cues = pred_box.new_full((N,), -1, dtype=torch.long)\n        T = len(n_t)\n        asso_gt = pred_box.new_zeros((K, T), dtype=torch.long)\n\n        # split ious by frames\n        ious_per_frame = ious.split(n_t, dim=0)\n\n        for k, inst_id in enumerate(inst_ids):\n            # get ground truth indices, init index\n            target_inds = target_inst_id == inst_id\n            base_ind = 0\n\n            for t in range(T):\n                # get relevant ious\n                iou_t = ious_per_frame[t][:, target_inds]\n\n                # if there are no detections, asso_gt = # gt instances at time step\n                if iou_t.numel() == 0:\n                    asso_gt[k, t] = n_t[t]\n                else:\n                    # get max iou and index, select positive ious\n                    val, inds = iou_t.max(dim=0)\n                    ind = inds[val &gt; 0.0]\n\n                    # make sure there is at most one detection\n                    assert len(ind) &lt;= 1, f\"{target_inst_id} {n_t}\"\n\n                    # if there is one detection with pos IOU, select it\n                    if len(ind) == 1:\n                        obj_ind = ind[0].item()\n                        asso_gt[k, t] = obj_ind\n                        match_cues[base_ind + obj_ind] = k\n\n                    # otherwise asso_gt = # gt instances at time step\n                    else:\n                        asso_gt[k, t] = n_t[t]\n\n                base_ind += n_t[t]\n\n        return asso_gt, match_cues\n\n    def detr_asso_loss(\n        self,\n        asso_pred: torch.Tensor,\n        asso_gt: torch.Tensor,\n        match_cues: torch.Tensor,\n        n_t: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Calculate association loss between predicted and gt boxes.\n\n        Args:\n            asso_pred: Association matrix output from the transformer forward\n                pass denoting predicted instances over time (M x N)\n            asso_gt: Ground truth association matrix (K x N) denoting ground\n                truth instances over time\n            match_cues: Tensor indicating which instance is assigned to each gt\n                detection (K x 3) or (N,)\n            n_t: number of ground truth instances (N,)\n\n        Returns:\n            loss: association loss normalized by number of objects\n        \"\"\"\n        # get matches between preds and gt\n        src_inds, target_inds = self._match(asso_pred, asso_gt, match_cues, n_t)\n\n        loss = 0\n        num_objs = 0\n\n        zero = asso_pred.new_zeros((asso_pred.shape[0], 1))  # M x 1\n        asso_pred_image = asso_pred.split(n_t, dim=1)  # T x [M x n_t]\n\n        for t in range(len(n_t)):\n            # add background class\n            asso_pred_with_bg = torch.cat(\n                [asso_pred_image[t], zero], dim=1\n            )  # M x (n_t + 1)\n\n            if self.neg_unmatched:\n                # set unmatched preds to background\n                asso_gt_t = asso_gt.new_full((asso_pred.shape[0],), float(n_t[t]))  # M\n                asso_gt_t[src_inds] = asso_gt[target_inds, t]  # M\n            else:\n                # keep only unmatched preds\n                asso_pred_with_bg = asso_pred_with_bg[src_inds]  # K x (n_t + 1)\n                asso_gt_t = asso_gt[target_inds, t]  # K\n\n            num_objs += (asso_gt_t != n_t[t]).float().sum()\n\n            loss += F.cross_entropy(asso_pred_with_bg, asso_gt_t, reduction=\"none\")\n\n        return loss.sum() / (num_objs + self.epsilon)\n\n    @torch.no_grad()\n    def _match(\n        self,\n        asso_pred: torch.Tensor,\n        asso_gt: torch.Tensor,\n        match_cues: torch.Tensor,\n        n_t: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Match predicted scores to gt scores using match cues.\n\n        Args:\n            asso_pred: Association matrix output from the transformer forward\n                pass denoting predicted instances over time (M x N)\n            asso_gt: Ground truth association matrix (K x N) denoting ground\n                truth instances over time\n            match_cues: Tensor indicating which instance is assigned to each gt\n                detection (K x 3) or (N,)\n            n_t: number of ground truth instances (N,)\n\n        Returns:\n            src_inds: Matched source indices (N,)\n            target_inds: Matched target indices (N,)\n        \"\"\"\n        src_inds = torch.where(match_cues &gt;= 0)[0]\n        target_inds = match_cues[src_inds]\n\n        return (src_inds, target_inds)\n</code></pre>"},{"location":"reference/dreem/training/losses/#dreem.training.losses.AssoLoss.__init__","title":"<code>__init__(neg_unmatched=False, epsilon=0.0001, asso_weight=1.0)</code>","text":"<p>Initialize Loss function.</p> <p>Parameters:</p> Name Type Description Default <code>neg_unmatched</code> <code>bool</code> <p>Whether or not to set unmatched objects to background</p> <code>False</code> <code>epsilon</code> <code>float</code> <p>small number used for numeric precision to prevent dividing by zero</p> <code>0.0001</code> <code>asso_weight</code> <code>float</code> <p>How much to weight the association loss by</p> <code>1.0</code> Source code in <code>dreem/training/losses.py</code> <pre><code>def __init__(\n    self,\n    neg_unmatched: bool = False,\n    epsilon: float = 1e-4,\n    asso_weight: float = 1.0,\n):\n    \"\"\"Initialize Loss function.\n\n    Args:\n        neg_unmatched: Whether or not to set unmatched objects to background\n        epsilon: small number used for numeric precision to prevent dividing by zero\n        asso_weight: How much to weight the association loss by\n    \"\"\"\n    super().__init__()\n\n    self.neg_unmatched = neg_unmatched\n    self.epsilon = epsilon\n    self.asso_weight = asso_weight\n</code></pre>"},{"location":"reference/dreem/training/losses/#dreem.training.losses.AssoLoss.detr_asso_loss","title":"<code>detr_asso_loss(asso_pred, asso_gt, match_cues, n_t)</code>","text":"<p>Calculate association loss between predicted and gt boxes.</p> <p>Parameters:</p> Name Type Description Default <code>asso_pred</code> <code>Tensor</code> <p>Association matrix output from the transformer forward pass denoting predicted instances over time (M x N)</p> required <code>asso_gt</code> <code>Tensor</code> <p>Ground truth association matrix (K x N) denoting ground truth instances over time</p> required <code>match_cues</code> <code>Tensor</code> <p>Tensor indicating which instance is assigned to each gt detection (K x 3) or (N,)</p> required <code>n_t</code> <code>Tensor</code> <p>number of ground truth instances (N,)</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>Tensor</code> <p>association loss normalized by number of objects</p> Source code in <code>dreem/training/losses.py</code> <pre><code>def detr_asso_loss(\n    self,\n    asso_pred: torch.Tensor,\n    asso_gt: torch.Tensor,\n    match_cues: torch.Tensor,\n    n_t: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Calculate association loss between predicted and gt boxes.\n\n    Args:\n        asso_pred: Association matrix output from the transformer forward\n            pass denoting predicted instances over time (M x N)\n        asso_gt: Ground truth association matrix (K x N) denoting ground\n            truth instances over time\n        match_cues: Tensor indicating which instance is assigned to each gt\n            detection (K x 3) or (N,)\n        n_t: number of ground truth instances (N,)\n\n    Returns:\n        loss: association loss normalized by number of objects\n    \"\"\"\n    # get matches between preds and gt\n    src_inds, target_inds = self._match(asso_pred, asso_gt, match_cues, n_t)\n\n    loss = 0\n    num_objs = 0\n\n    zero = asso_pred.new_zeros((asso_pred.shape[0], 1))  # M x 1\n    asso_pred_image = asso_pred.split(n_t, dim=1)  # T x [M x n_t]\n\n    for t in range(len(n_t)):\n        # add background class\n        asso_pred_with_bg = torch.cat(\n            [asso_pred_image[t], zero], dim=1\n        )  # M x (n_t + 1)\n\n        if self.neg_unmatched:\n            # set unmatched preds to background\n            asso_gt_t = asso_gt.new_full((asso_pred.shape[0],), float(n_t[t]))  # M\n            asso_gt_t[src_inds] = asso_gt[target_inds, t]  # M\n        else:\n            # keep only unmatched preds\n            asso_pred_with_bg = asso_pred_with_bg[src_inds]  # K x (n_t + 1)\n            asso_gt_t = asso_gt[target_inds, t]  # K\n\n        num_objs += (asso_gt_t != n_t[t]).float().sum()\n\n        loss += F.cross_entropy(asso_pred_with_bg, asso_gt_t, reduction=\"none\")\n\n    return loss.sum() / (num_objs + self.epsilon)\n</code></pre>"},{"location":"reference/dreem/training/losses/#dreem.training.losses.AssoLoss.forward","title":"<code>forward(asso_preds, frames)</code>","text":"<p>Calculate association loss.</p> <p>Parameters:</p> Name Type Description Default <code>asso_preds</code> <code>list[Tensor]</code> <p>a list containing the association matrix at each frame</p> required <code>frames</code> <code>list[Frame]</code> <p>a list of Frames containing gt labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>the association loss between predicted association and actual</p> Source code in <code>dreem/training/losses.py</code> <pre><code>def forward(\n    self, asso_preds: list[torch.Tensor], frames: list[\"Frame\"]\n) -&gt; torch.Tensor:\n    \"\"\"Calculate association loss.\n\n    Args:\n        asso_preds: a list containing the association matrix at each frame\n        frames: a list of Frames containing gt labels.\n\n    Returns:\n        the association loss between predicted association and actual\n    \"\"\"\n    # get number of detected objects and ground truth ids\n    n_t = [frame.num_detected for frame in frames]\n    target_inst_id = torch.cat(\n        [frame.get_gt_track_ids().to(asso_preds[-1].device) for frame in frames]\n    )\n    instances = [instance for frame in frames for instance in frame.instances]\n\n    # for now set equal since detections are fixed\n    pred_box = get_boxes(instances)\n    pred_time, _ = get_times(instances)\n    pred_box = torch.nanmean(pred_box, axis=1)\n    target_box, target_time = pred_box, pred_time\n\n    # todo: we should maybe reconsider how we label gt instances. The second\n    # criterion will return true on a single instance video, for example.\n    # For now we can ignore this since we train on dense labels.\n\n    \"\"\"\n        # Return a 0 loss if any of the 2 criteria are met\n        # 1. the video doesn\u2019t have gt bboxes\n        # 2. the maximum id is zero\n\n    sum_instance_lengths = sum(len(x) for x in instances)\n    max_instance_lengths = max(\n        x[\"gt_track_ids\"].max().item() for x in instances if len(x) &gt; 0\n    )\n\n    if sum_instance_lengths == 0 or max_instance_lengths == 0:\n        print(\"No bounding boxes detected, returning zero loss\")\n        print(f\"Sum instance lengths: {sum_instance_lengths}\")\n        print(f\"Max instance lengths: {max_instance_lengths}\")\n        loss = asso_preds[0].new_zeros((1,), dtype=torch.float32)[0]\n        return loss\n    \"\"\"\n\n    asso_gt, match_cues = self._get_asso_gt(\n        pred_box, pred_time, target_box, target_time, target_inst_id, n_t\n    )\n\n    loss = sum(\n        [\n            self.detr_asso_loss(asso_pred, asso_gt, match_cues, n_t)\n            for asso_pred in asso_preds\n        ]\n    )\n\n    loss *= self.asso_weight\n\n    return loss\n</code></pre>"},{"location":"reference/dreem/training/train/","title":"train","text":""},{"location":"reference/dreem/training/train/#dreem.training.train","title":"<code>dreem.training.train</code>","text":"<p>Training script for training model.</p> <p>Used for training a single model or deploying a batch train job on RUNAI CLI</p>"},{"location":"reference/dreem/training/train/#dreem.training.train.run","title":"<code>run(cfg)</code>","text":"<p>Train model based on config.</p> <p>Handles all config parsing and initialization then calls <code>trainer.train()</code>. If <code>batch_config</code> is included then run will be assumed to be a batch job.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>The config dict parsed by <code>hydra</code></p> required Source code in <code>dreem/training/train.py</code> <pre><code>@hydra.main(config_path=None, config_name=None, version_base=None)\ndef run(cfg: DictConfig):\n    \"\"\"Train model based on config.\n\n    Handles all config parsing and initialization then calls `trainer.train()`.\n    If `batch_config` is included then run will be assumed to be a batch job.\n\n    Args:\n        cfg: The config dict parsed by `hydra`\n    \"\"\"\n    torch.set_float32_matmul_precision(\"medium\")\n    train_cfg = Config(cfg)\n\n    # update with parameters for batch train job\n    if \"batch_config\" in cfg.keys():\n        try:\n            index = int(os.environ[\"POD_INDEX\"])\n        except KeyError as e:\n            index = int(\n                input(f\"{e}. Assuming single run!\\nPlease input task index to run:\")\n            )\n\n        hparams_df = pd.read_csv(cfg.batch_config)\n        hparams = hparams_df.iloc[index].to_dict()\n\n        if train_cfg.set_hparams(hparams):\n            logger.debug(\"Updated the following hparams to the following values\")\n            logger.debug(hparams)\n    else:\n        hparams = {}\n    logger.info(f\"Final train config: {train_cfg}\")\n\n    model = train_cfg.get_model()\n    train_dataset = train_cfg.get_dataset(mode=\"train\")\n    train_dataloader = train_cfg.get_dataloader(train_dataset, mode=\"train\")\n\n    val_dataset = train_cfg.get_dataset(mode=\"val\")\n    val_dataloader = train_cfg.get_dataloader(val_dataset, mode=\"val\")\n\n    test_dataset = train_cfg.get_dataset(mode=\"test\")\n    test_dataloader = train_cfg.get_dataloader(test_dataset, mode=\"test\")\n\n    dataset = TrackingDataset(\n        train_dl=train_dataloader, val_dl=val_dataloader, test_dl=test_dataloader\n    )\n\n    if cfg.view_batch.enable:\n        instances = next(iter(train_dataset))\n        view_training_batch(instances, num_frames=cfg.view_batch.num_frames)\n\n        if cfg.view_batch.no_train:\n            return\n\n    model = train_cfg.get_gtr_runner()  # TODO see if we can use torch.compile()\n\n    run_logger = train_cfg.get_logger()\n\n    callbacks = []\n    _ = callbacks.extend(train_cfg.get_checkpointing())\n    _ = callbacks.append(pl.callbacks.LearningRateMonitor())\n    _ = callbacks.append(train_cfg.get_early_stopping())\n\n    accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n    devices = torch.cuda.device_count() if torch.cuda.is_available() else cpu_count()\n\n    trainer = train_cfg.get_trainer(\n        callbacks,\n        run_logger,\n        accelerator=accelerator,\n        devices=devices,\n    )\n\n    trainer.fit(model, dataset)\n</code></pre>"}]}