model:
  ckpt_path: null
  # simple encoder config for resnet18 encoder
  # encoder_cfg: 
  #   model_name: "resnet18"
  #   backend: "timm"
  #   in_chans: 3
  # below is a sample of the new encoder_cfg structure for different encoder types
  encoder_cfg: 
    # based on encoder_type, the relevant settings are selected
    encoder_type: "descriptor"
    ndim: 5 # dim of the visual feature vector; 5 for descriptor, 12 for descriptor + hu moments
    resnet:
      model_name: "resnet18"
      in_chans: 3
      backend: "timm"
      pretrained: false
    descriptor:
      use_hu_moments: false
  d_model: 1024
  nhead: 8
  num_encoder_layers: 1
  num_decoder_layers: 1
  dropout: 0.1
  activation: "relu"
  return_intermediate_dec: False
  norm: False
  num_layers_attn_head: 2
  dropout_attn_head: 0.1
  embedding_meta: 
    pos:
        mode: "fixed"
        normalize: true
    temp:
        mode: "fixed"
  return_embedding: False
  decoder_self_attn: False

loss:
  neg_unmatched: false
  epsilon: 1e-4
  asso_weight: 1.0

#currently assumes adam. TODO adapt logic for other optimizers like sgd
optimizer:
  name: "Adam"
  lr: 0.001
  betas: [0.9, 0.999]
  eps: 1e-8
  weight_decay: 0.01

#currently assumes reduce lr on plateau
scheduler:
  name: "ReduceLROnPlateau"
  mode: "min"
  factor: 0.5
  patience: 10
  threshold: 1e-4
  threshold_mode: "rel"

  # scheduler: # providing a list of schedulers will apply each scheduler sequentially
  # name: ["LinearLR", "LinearLR"]
  # "0": 
  #   start_factor: 0.1
  #   end_factor: 1
  #   total_iters: 3
  # "1":
  #   start_factor: 1
  #   end_factor: 0.1
  #   total_iters: 30
  # milestones: [10]

tracker:
  window_size: 8
  use_vis_feats: true
  overlap_thresh: 0.01
  mult_thresh: true
  decay_time: null
  iou: null
  max_center_dist: null

runner:
  metrics:
      train: ['num_switches']
      val: ['num_switches']
      test: ['num_switches']
  persistent_tracking:
      train: false
      val: true
      test: true
      
dataset:
  train_dataset:
    slp_files: ["../../tests/data/sleap/two_flies.slp"]
    video_files: ["../../tests/data/sleap/two_flies.mp4"]
    padding: 5
    crop_size: 128
    chunk: true
    clip_length: 32

  val_dataset:
    slp_files: ["../../tests/data/sleap/two_flies.slp"]
    video_files: ["../../tests/data/sleap/two_flies.mp4"]
    padding: 5
    crop_size: 128 
    chunk: True
    clip_length: 32

  test_dataset:
    slp_files: ["../../tests/data/sleap/two_flies.slp"]
    video_files: ["../../tests/data/sleap/two_flies.mp4"]
    padding: 5
    crop_size: 128 
    chunk: True
    clip_length: 32

dataloader:
  train_dataloader:
    shuffle: true
    num_workers: 0
  val_dataloader:
    shuffle: false
    num_workers: 0
  test_dataloader: 
    shuffle: false
    num_workers: 0

logging:
  logger_type: null
  name: "example_train"
  entity: null
  job_type: "train"
  notes: "Example train job"
  dir: "./logs"
  group: "example"
  save_dir: './logs'
  project: "GTR"
  log_model: "all"

early_stopping:
  monitor: "val_loss"
  min_delta: 0.1
  patience: 10
  mode: "min"
  check_finite: true
  stopping_threshold: 1e-8
  divergence_threshold: 30

checkpointing:
  monitor: ["val_loss","val_num_switches"]
  verbose: true
  save_last: true
  dirpath: null
  auto_insert_metric_name: true
  every_n_epochs: 10

trainer:
  check_val_every_n_epoch: 1
  enable_checkpointing: true
  gradient_clip_val: null
  limit_train_batches: 1.0
  limit_test_batches: 1.0
  limit_val_batches: 1.0
  log_every_n_steps: 1
  max_epochs: 100
  min_epochs: 10

view_batch:
  enable: False
  num_frames: 0
  no_train: False
