# to implement - config class that handles getters/setters
"""Data structures for handling config parsing."""
import hydra
import sys
import ast
import torch
import pytorch_lightning as pl
from biogtr.models.model_utils import init_optimizer, init_scheduler
from biogtr.models.gtr_runner import GTRRunner
from biogtr.models.global_tracking_transformer import GlobalTrackingTransformer
from biogtr.training.losses import AssoLoss
from biogtr.datasets.sleap_dataset import SleapDataset
from biogtr.datasets.microscopy_dataset import MicroscopyDataset
from omegaconf import DictConfig, OmegaConf
from typing import Union, Iterable
from pprint import pprint


class Config:
    """Class handling loading components based on config params."""

    def __init__(self, cfg: DictConfig):
        """Initialize the class with config from hydra/omega conf.

        First uses `base_param` file then overwrites with specific `params_config`.

        Args:
            cfg: The `DictConfig` containing all the hyperparameters needed for training/evaluation
        """
        base_cfg = cfg
        print(f"Base Config: {cfg}")

        if "params_config" in cfg:
            # merge configs
            params_config = OmegaConf.load(cfg.params_config)
            pprint(f"Overwriting base config with {params_config}")
            self.cfg = OmegaConf.merge(base_cfg, params_config)
        else:
            # just use base config
            self.cfg = base_cfg

    def __repr__(self):
        """Object representation of config class."""
        return f"Config({self.cfg})"

    def __str__(self):
        """String representation of config class."""
        return f"Config({self.cfg})"

    def set_hparams(self, hparams: dict) -> bool:
        """Setter function for overwriting specific hparams.

        Useful for changing 1 or 2 hyperparameters such as dataset.

        Args:
            hparams: A dict containing the hyperparameter to be overwritten and the value to be changed t

        Returns:
            `True` if config is successfully updated, `False` otherwise
        """
        if hparams == {} or hparams is None:
            print("Nothing to update!")
            return False
        for hparam, val in hparams.items():
            try:
                OmegaConf.update(self.cfg, hparam, val)
            except Exception as e:
                print(f"Failed to update {hparam} to {val} due to {e}")
                return False
        return True

    def get_model(self) -> GlobalTrackingTransformer:
        """Getter for gtr model.

        Returns:
            A global tracking transformer with parameters indicated by cfg
        """
        model_params = self.cfg.model
        return GlobalTrackingTransformer(**model_params)

    def get_tracker_cfg(self) -> dict:
        """Getter for tracker config params.

        Returns:
            A dict containing the init params for `Tracker`.
        """
        tracker_params = self.cfg.tracker
        tracker_cfg = {}
        for key, val in tracker_params.items():
            tracker_cfg[key] = val
        return tracker_cfg

    def get_gtr_runner(self):
        """Get lightning module for training, validation, and inference."""
        model_params = self.cfg.model
        tracker_params = self.cfg.tracker
        optimizer_params = self.cfg.optimizer
        scheduler_params = self.cfg.scheduler
        loss_params = self.cfg.loss
        gtr_runner_params = self.cfg.runner
        return GTRRunner(
            model_params,
            tracker_params,
            loss_params,
            optimizer_params,
            scheduler_params,
            **gtr_runner_params,
        )

    def get_dataset(
        self, type: str, mode: str
    ) -> Union[SleapDataset, MicroscopyDataset]:
        """Getter for datasets.

        Args:
            type: Either "sleap" or "microscopy". Whether to return a `SleapDataset` or `MicroscopyDataset`
            mode: [None, "train", "test", "val"]. Indicates whether to use train, val, or test params for dataset
        Returns:
            Either a `SleapDataset` or `MicroscopyDataset` with params indicated by cfg
        """
        if mode.lower() == "train":
            dataset_params = self.cfg.dataset.train_dataset
        elif mode.lower() == "val":
            dataset_params = self.cfg.dataset.val_dataset
        elif mode.lower() == "test":
            dataset_params = self.cfg.dataset.test_dataset
        else:
            raise ValueError(
                "`mode` must be one of ['train', 'val','test'], not '{mode}'"
            )
        if type.lower() == "sleap":
            return SleapDataset(**dataset_params)
        elif type.lower() == "microscopy":
            return MicroscopyDataset(**dataset_params)
        else:
            raise ValueError(
                f"`type` must be one of ['sleap', 'microscopy'] not '{type}'!"
            )

    def get_dataloader(
        self, dataset: Union[SleapDataset, MicroscopyDataset], mode: str
    ) -> torch.utils.data.DataLoader:
        """Getter for dataloader.

        Args:
            dataset: the Sleap or Microscopy Dataset used to initialize the dataloader
            mode: either ["train", "val", or "test"] indicates which dataset config to use
        Returns:
            A torch dataloader for `dataset` with parameters configured as specified
        """
        if mode.lower() == "train":
            dataloader_params = self.cfg.dataloader.train_dataloader
        elif mode.lower() == "val":
            dataloader_params = self.cfg.dataloader.val_dataloader
        elif mode.lower() == "test":
            dataloader_params = self.cfg.dataloader.test_dataloader
        else:
            raise ValueError(
                "`mode` must be one of ['train', 'val','test'], not '{mode}'"
            )
        if dataloader_params.num_workers > 0:
            # prevent too many open files error
            pin_memory = True
            torch.multiprocessing.set_sharing_strategy("file_system")
        else:
            pin_memory = False

        generator = (
            torch.Generator(device="cuda") if torch.cuda.is_available() else None
        )
        return torch.utils.data.DataLoader(
            dataset=dataset,
            batch_size=1,
            pin_memory=pin_memory,
            generator=generator,
            collate_fn=dataset.no_batching_fn,
            **dataloader_params,
        )

    def get_optimizer(self, params: Iterable) -> torch.optim.Optimizer:
        """Getter for optimizer.

        Args:
            params: iterable of model parameters to optimize or dicts defining parameter groups
        Returns:
            A torch Optimizer with specified params
        """
        optimizer_params = self.cfg.optimizer
        return init_optimizer(params, optimizer_params)

    def get_scheduler(
        self, optimizer: torch.optim.Optimizer
    ) -> torch.optim.lr_scheduler.LRScheduler:
        """Getter for lr scheduler.

        Args:
            optimizer: The optimizer to wrap the scheduler around
        Returns:
            A torch learning rate scheduler with specified params
        """
        lr_scheduler_params = self.cfg.scheduler
        return init_scheduler(optimizer, lr_scheduler_params)

    def get_loss(self) -> AssoLoss:
        """Getter for loss functions.

        Returns:
            An AssoLoss with specified params
        """
        loss_params = self.cfg.loss
        return AssoLoss(**loss_params)

    def get_logger(self) -> pl.loggers.WandbLogger:
        """Getter for lightning logging callback.

        Returns:
            A lightning Logger with specified params
        """
        logger_params = self.cfg.logging
        return pl.loggers.WandbLogger(config=self.cfg, **logger_params)

    def get_early_stopping(self) -> pl.callbacks.EarlyStopping:
        """Getter for lightning early stopping callback.

        Returns:
            A lightning early stopping callback with specified params
        """
        early_stopping_params = self.cfg.early_stopping
        return pl.callbacks.EarlyStopping(**early_stopping_params)

    def get_checkpointing(self) -> pl.callbacks.ModelCheckpoint:
        """Getter for lightning checkpointing callback.

        Returns:
            A lightning checkpointing callback with specified params
        """
        # convert to dict to enable extracting/removing params
        checkpoint_params = OmegaConf.to_container(self.cfg.checkpointing, resolve=True)
        logging_params = self.cfg.logging
        if "dirpath" not in checkpoint_params or checkpoint_params["dirpath"] is None:
            dirpath = f"./models/{logging_params.group}/{logging_params.name}"

        else:
            dirpath = checkpoint_params["dirpath"]
        _ = checkpoint_params.pop("dirpath")
        checkpointers = []
        monitor = checkpoint_params.pop("monitor")
        for metric in monitor:
            checkpointer = pl.callbacks.ModelCheckpoint(
                monitor=metric, dirpath=dirpath, **checkpoint_params
            )
            checkpointer.CHECKPOINT_NAME_LAST = f"{{epoch}}-best-{{{metric}}}"
            checkpointers.append(checkpointer)
        return checkpointers

    def get_trainer(
        self,
        callbacks: list[pl.callbacks.Callback],
        logger: pl.loggers.WandbLogger,
        accelerator: str,
        devices: int,
    ) -> pl.Trainer:
        """Getter for the lightning trainer.

        Args:
            callbacks: a list of lightning callbacks preconfigured to be used for training
            logger: the Wandb logger used for logging during training
            accelerator: either "gpu" or "cpu" specifies which device to use
            devices: The number of gpus to be used. 0 means cpu
        Returns:
            A lightning Trainer with specified params
        """
        trainer_params = self.cfg.trainer
        return pl.Trainer(
            callbacks=callbacks,
            logger=logger,
            accelerator=accelerator,
            devices=devices,
            **trainer_params,
        )

    def get_ckpt_path(self):
        """Get model ckpt path for loading."""
        return self.cfg.model.ckpt_path
