{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DREEM Relates Every Entity's Motion","text":"<p>Welcome to the documentation for DREEM \u2013 an open-source tool for multiple object tracking. DREEM is a framework that enables you to train your own models, run inference on new data, and evaluate your results. DREEM supports a variety of detection types, including keypoints, bounding boxes, and segmentation masks. You can use any detection model you want, convert the output to a format DREEM can use, and train a model or run inference using a pretrained model.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Command-Line &amp; API Access: Use DREEM via a simple CLI or integrate into your own Python scripts.</li> <li>Configurable Workflows: Easily customize training and inference using YAML configuration files.</li> <li>Pretrained Models: Get started quickly with models trained specially for microscopy and animal domains.</li> <li>Visualization: Tracking outputs are directly compatible with SLEAP's GUI.</li> <li>Examples: Step-by-step notebooks and guides for common workflows.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Head over to the Installation Guide to get started.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Ready to try DREEM? Follow the Quickstart Guide to:</p> <ol> <li>Download example datasets and pretrained models</li> <li>Run tracking on sample videos</li> <li>Visualize your results</li> </ol>"},{"location":"#example-workflows","title":"Example Workflows","text":"<p>Explore the Examples section for notebooks that walk you through the DREEM pipeline. We have an end-to-end demo that includes model training, as well as a microscopy example that shows how to use DREEM with an off-the-shelf detection model.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Installation</li> <li>Quickstart</li> <li>Usage Guide</li> <li>Examples</li> <li>API Reference</li> </ul>"},{"location":"#get-help","title":"Get Help","text":"<ul> <li>Questions? Open an issue on GitHub.</li> <li>Contributions: We welcome contributions! See our Contributing Guide for details (link to be added).</li> </ul>"},{"location":"cli/","title":"Command-line Interface","text":"<p>DREEM provides several types of functionality accessible through a command prompt.</p>"},{"location":"cli/#training","title":"Training","text":""},{"location":"cli/#dreem-train","title":"<code>dreem-train</code>","text":"<p><code>dreem-train</code> is the command-line interface for training. Use this for training on a remote machine/cluster/colab notebook instead of using the API directly.</p>"},{"location":"cli/#usage","title":"Usage","text":"<pre><code>usage: dreem-train [-h] [--hydra-help] [--config-dir] [--config-name] [+params_config] [+batch_config]\n\npositional arguments:\n    --config-dir    Path to configs dir\n    --config-name   Name of the .yaml config file stored in config-dir without the .yaml extension\n\noptional arguments:\n    -h, --help      Shows the application's help and exit.\n    --hydra-help    Shows Hydra specific flags (recommended over -h) \n    +params_config  Path to .yaml file containing subset of params to override\n    +batch_config   Path to .csv file where each row indicates params to override for a single task in a batch job\n</code></pre> <p>See the usage guide for a more in-depth explanation on how to use <code>dreem-train</code> and see the training config walkthrough for all available parameters.</p>"},{"location":"cli/#eval","title":"Eval","text":""},{"location":"cli/#dreem-eval","title":"<code>dreem-eval</code>","text":"<p><code>dreem-eval</code> is the command-line interface for inference. Use this for evaluating a trained model using pymotmetrics on a remote machine/cluster/notebook instead of using the API directly.</p>"},{"location":"cli/#usage_1","title":"Usage","text":"<p><pre><code>usage: dreem-eval [-h] [--hydra-help] [--config-dir] [--config-name] [+params_config] [+batch_config]\n\npositional arguments:\n    --config-dir    Path to configs dir\n    --config-name   Name of the .yaml config file stored in config-dir without the .yaml extension\n\noptional arguments:\n    -h, --help      Shows the application's help and exit.\n    --hydra-help    Shows Hydra specific flags (recommended over -h) \n    +params_config  Path to .yaml file containing subset of params to override\n    +batch_config   Path to .csv file where each row indicates params to override for a single task in a batch job\n</code></pre> See the usage guide for a more in-depth explanation on how to use <code>dreem-track</code> and see the inference config walkthrough for all available parameters.</p>"},{"location":"cli/#inference","title":"Inference","text":""},{"location":"cli/#dreem-track","title":"<code>dreem-track</code>","text":"<p><code>dreem-track</code> is the command-line interface for inference. Use this for tracking using a pretrained model on a remote machine/cluster/notebook instead of using the API directly.</p>"},{"location":"cli/#usage_2","title":"Usage","text":"<p><pre><code>usage: dreem-track [-h] [--hydra-help] [--config-dir] [--config-name] [+params_config] [+batch_config]\n\npositional arguments:\n    --config-dir    Path to configs dir\n    --config-name   Name of the .yaml config file stored in config-dir without the .yaml extension\n\noptional arguments:\n    -h, --help      Shows the application's help and exit.\n    --hydra-help    Shows Hydra specific flags (recommended over -h) \n    +params_config  Path to .yaml file containing subset of params to override\n    +batch_config   Path to .csv file where each row indicates params to override for a single task in a batch job\n</code></pre> See the usage guide for a more in-depth explanation on how to use <code>dreem-track</code> and see the inference config walkthrough for all available parameters.</p>"},{"location":"cli/#dreem-visualize","title":"<code>dreem-visualize</code>","text":"<p><code>dreem-visualize</code> is the command-line utility for generating annotated videos based on DREEM output</p>"},{"location":"cli/#usage_3","title":"Usage","text":"<pre><code>usage: dreem-visualize [-h] [--hydra-help] [+labels_path] [+vid_path] [+save_path] [+annotate.key] [--config-path] [+annotate.color_palette=\"tab20\"] [+annotate.trails=2] [+annotate.boxes=64] [+annotate.names=True] [+annotate.track_scores=0.5] [+annotate.centroids=4] [+annotate.poses=False] [+annotate.fps=30] [+annotate.alpha=0.2]\n\npositional arguments:\n    +labels_path                The path to the dreem tracks as a `.csv` file containing the following keys:\n                                    - \"X\": the X position of the instance in pixels\n                                    - \"Y\": the Y position of the instance in pixels\n                                    - \"Frame_id\": The frame of the video in which the instance occurs\n                                    - \"Pred_track_id\": The track id output from DREEM\n                                    - \"Track_score\": The trajectory score output from DREEM\n                                    - (Optional) \"Gt_track_id\": the gt track id for the instance (mostly used for debugging)\n                                where each row is an instance. See `dreem.inference.track.export_trajectories` \n    +vid_path                   The path to the video file in an `imageio`-accepted format (See: https://imageio.readthedocs.io/en/v2.4.1/formats.html)\n    +save_path                  The path to where you want to store the annotated video (e.g \"./tracked_videos/dreem_output.mp4\")\n    +annotate.key               The key where labels are stored in the dataframe - mostly used for choosing whether to annotate based on pred or gt labels\n\noptional arguments\n    -h, --help                  Shows the application's help and exit.\n    --hydra-help                Shows Hydra specific flags (recommended over -h) \n    --config-path               The path to the config .yaml file containing params for annotating. We recommend using this file in lieu of any \"annotate.*\" arguments.\n    +annotate.color_palette     The matplotlib colorpalette to use for annotating the video. Defaults to `tab10`\n    +annotate.trails            The size of the track trails. If the trail size &lt;=0 or none then it is not used.\n    +annotate.boxes             The size of the bbox. If bbox size &lt;= 0 or None then it is not added\n    +annotate.names             Whether or not to annotate with name\n    +annotate.centroids         The size of the centroid. If centroid size &lt;=0 or None then it is not added.\n    +annotate.poses             Whether or not to annotate with poses\n    +annotate.fps               The frames-per-second of the annotated video\n    +annotate.alpha             The opacity of the centroid annotations.\n</code></pre> <p>As noted above, instead of specifying each individual <code>+annotate.*</code> param we recommend setting up a visualize config (e.g <code>./configs/visualize.yaml</code>) which looks like:</p> <pre><code>annotate:\n    color_palette: \"tab10\"\n    trails: 5\n    boxes: (64, 64)\n    names: true\n    centroids: 10\n    poses: false\n    fps: 30\n    alpha: 0.2\n</code></pre> <p>and then running:</p> <pre><code>dreem-visualize +labels_path=\"/path/to/labels.csv\" +vid_path=\"/path/to/vid.{VID EXTENSION} +annotate.key=\"Pred_track_id\" --config-path=\"/path/to/configs/visualize.yaml\"\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#clone-the-repository","title":"Clone the repository:","text":"<pre><code>git clone https://github.com/talmolab/dreem &amp;&amp; cd dreem\n</code></pre>"},{"location":"installation/#set-up-in-a-new-conda-environment","title":"Set up in a new conda environment:","text":""},{"location":"installation/#linuxwindows","title":"Linux/Windows:","text":""},{"location":"installation/#gpu-accelerated-requires-cudanvidia-gpu","title":"GPU-accelerated (requires CUDA/nvidia gpu)","text":"<pre><code>conda env create -f environment.yml &amp;&amp; conda activate dreem\n</code></pre>"},{"location":"installation/#cpu","title":"CPU:","text":"<pre><code>conda env create -f environment_cpu.yml &amp;&amp; conda activate dreem\n</code></pre>"},{"location":"installation/#osx-apple-silicon","title":"OSX (Apple Silicon)","text":"<pre><code>conda env create -f environment_osx-arm64.yml &amp;&amp; conda activate dreem\n</code></pre>"},{"location":"installation/#uninstall","title":"Uninstall","text":"<pre><code>conda env remove -n dreem\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":"<p>DREEM operations can be performed either through the command-line interface or through the API (for more advanced users). The CLI provides all the commands for the main functionalities from training a model to running eval and inference to generating visualizations. All operations can be customized via configs that are managed by Hydra. For a more in-depth walkthrough of DREEM, see the Examples section. For a complete reference of all commands and options, see the API Reference. </p> <p>To quickly test your installation and familiarize yourself with DREEM, you can follow the quickstart guide below.</p>"},{"location":"quickstart/#fly-tracking","title":"Fly tracking","text":"<p>In this example we will track a social interaction between two flies from the SLEAP fly32 dataset using a pretrained model. This example assumes that you have a conda environment installed with the dreem package. Please see the installation guide if you haven't installed it yet.</p>"},{"location":"quickstart/#download-the-data","title":"Download the data","text":"<p>First, we need at least one video and a set of corresponding detections to work with. For this example, we provide a video and a <code>.slp</code> file with pose keypoints for the video. This dataset can be downloaded from Hugging Face. The dataset includes configuration files needed for running inference.</p> <p>First, make sure you have <code>huggingface-hub</code> installed (it should already be installed in your environment if you installed using the commands on the installation page):</p> <pre><code>pip install huggingface_hub\n</code></pre> <p>Download the dataset. The <code>--local-dir</code> flag allows you to specify the download location on your computer.</p> <pre><code>huggingface-cli download talmolab/sample-flies --repo-type dataset --local-dir ./data\n</code></pre>"},{"location":"quickstart/#download-a-model","title":"Download a model","text":"<p>Now we'll pull a pretrained model trained on various animal data in order to run inference.</p> <pre><code>huggingface-cli download talmolab/animals-pretrained animals-pretrained.ckpt --local-dir=./models\n</code></pre> <p>To confirm this downloaded properly you can run <pre><code>ls animals-pretrained.ckpt\n</code></pre></p> <p>Your directory structure should look like this: <pre><code>./data\n    /test\n        190719_090330_wt_18159206_rig1.2@15000-17560.mp4\n        GT_190719_090330_wt_18159206_rig1.2@15000-17560.slp\n    /train\n        190612_110405_wt_18159111_rig2.2@4427.mp4\n        GT_190612_110405_wt_18159111_rig2.2@4427.slp\n    /val\n        two_flies.mp4\n        GT_two_flies.slp\n    /inference\n        190719_090330_wt_18159206_rig1.2@15000-17560.mp4\n        190719_090330_wt_18159206_rig1.2@15000-17560.slp\n    /configs\n        inference.yaml\n        base.yaml\n        eval.yaml\n./models\n    animals-pretrained.ckpt\n</code></pre></p>"},{"location":"quickstart/#run-tracking","title":"Run Tracking","text":"<p>Tracking is easy to run using the CLI. Simply specify the path to the directory containing <code>inference.yaml</code> and the path to the model checkpoint.</p> <pre><code>dreem-track --config-dir=./data/sample-flies/configs --config-name=inference ckpt_path=./models/animals-pretrained.ckpt\n</code></pre> <p>Once completed, it should output a file in a new <code>results</code> folder called <code>GT_190719_090330_wt_18159206_rig1.2@15000-17560.&lt;timestamp&gt;dreem_inference.slp</code></p>"},{"location":"quickstart/#visualize-results","title":"Visualize Results","text":"<p>First, we recommend visualizing the outputs of the tracks you just made. You can do so by first installing sleap via its installation guide and then running</p> <pre><code>sleap-label eval/GT_190719_090330_wt_18159206_rig1.2@15000-17560.dreem_inference.slp\n</code></pre>"},{"location":"quickstart/#check-out-the-example-notebooks","title":"Check out the example notebooks","text":"<p>Once you're ready to dive deeper, head over to the Examples section and check out the notebooks there. Go through the end-to-end demo  of the DREEM pipeline, from obtaining data to training a model, evaluating on a held-out dataset, and visualizing the results, all in the notebook. We also have microscopy examples in which we use an off-the-shelf detection model to show how DREEM can be used with existing tools.</p>"},{"location":"quickstart/#run-through-full-workflow","title":"Run through full workflow","text":"<p>For more detail on the CLI, configuration files, and more, see the Usage Guide</p>"},{"location":"quickstart/#get-some-sleap-and-sweet-dreems","title":"Get some SLEAP and sweet DREEMs!","text":"<p>The animal checkpoint we used above was trained on mice, flies and zebrafish. You can generate detections on raw videos via SLEAP and then use our pretrained model as we just did to run tracking.</p>"},{"location":"usage/","title":"Usage","text":"<p>This page gives detailed instructions for using DREEM. We also have a quickstart guide and notebooks in the Examples section to help you get started.</p>"},{"location":"usage/#installation","title":"Installation","text":"<p>Head over to the installation guide to get started.</p>"},{"location":"usage/#training","title":"Training","text":"<p>DREEM enables you to train your own model based on your own annotated data. This can be useful when the pretrained models, or traditional approaches to tracking don't work well for your data.</p>"},{"location":"usage/#generate-ground-truth-data","title":"Generate Ground Truth Data","text":"<p>To train a model, you need:</p> <ol> <li>A video<ul> <li>For animal data see the <code>imageio</code> docs for supported file types. Common ones include mp4, avi, etc.</li> <li>For microscopy data we currently support <code>.tif</code> stacks.</li> </ul> </li> <li>A ground truth labels file in SLEAP or CellTrackingChallenge format. This labels file must contain:<ol> <li>Detections (i.e. locations of the instances in each frame). This can come in the form of centroids or pose keypoints for SLEAP format data, or segmentation masks for Cell Tracking Challenge format data.</li> <li>Ground truth identities (also called tracks). These are temporally consistent labels that link detections across time.</li> </ol> </li> </ol>"},{"location":"usage/#get-initial-labels","title":"Get Initial labels","text":"<p>To generate your initial labels we recommend a couple methods:</p> <ul> <li>For animal tracking, we recommend using SLEAP. SLEAP provides a graphical user interface that makes it easy to annotate data from scratch, and output the labels file in the SLEAP format.</li> <li>For microscopy tracking, check out CellPose or Ilastik. These methods output segmentation masks, but do not provide tracks. Fiji offers several end-to-end segmentation and tracking options. Recall that your labels file must contain tracks.</li> </ul>"},{"location":"usage/#proofreading","title":"Proofreading","text":"<p>Once you have your labels file containing initial detections and tracks, you'll want to proofread your labels. Obtaining good results relies on having accurate ground truth tracks. The annotated data should follow these guidelines:</p> <ol> <li> <p>No identity switches. This is important for training a model that maintains temporally consistent identities.</p> </li> <li> <p>Good detections. Since the input to the model is a crop centered around each detection, we want to make sure the coordinates we crop around are as accurate as possible.</p> </li> </ol> <p>We recommend using the <code>sleap-label</code> GUI for proofreading. SLEAP provides tools that make it easy to correct errors in tracking.</p>"},{"location":"usage/#converting-data-to-a-sleap-compatible-format","title":"Converting data to a SLEAP compatible format","text":"<p>In order to use the SLEAP GUI you'll need to have your labels and videos in a SLEAP compatible format. Check out the sleap-io docs for available formats. The easiest way to ensure your labels are compatible with sleap is to convert them to a <code>.slp</code> file. Otherwise if you used a different system (e.g DeepLabCut) check out <code>sleap.io.convert</code> for available converters. With microscopy, we highly recommend starting out with TrackMate and then proofread in SLEAP's gui. Here is a converter from trackmate's output to a <code>.slp</code> file. In general, you can use <code>sleap-io</code> to write a custom converter to <code>.slp</code> if you'd like to use the sleap-gui for proofreading.</p>"},{"location":"usage/#organize-data","title":"Organize data","text":"<p>For animal tracking, you'll need video/slp pairs in a directory that you can specify in the configuration files when training. For instance, you can have separate train/val/test directories, each with slp/video pairs. The naming convention is not important, as the .slp labels file has a reference to its associated video file when its created.</p> <p>For microscopy tracking, you'll need to organize your data in the Cell Tracking Challenge format. We've provided a sample directory structure below. Check out this guide for more details.</p> <p>Using the SLEAP format: <pre><code>dataset_name/\n    train/\n        vid_1.{VID_EXTENSION}\n        vid_1.slp\n            .\n            .\n            .\n        vid_n.{VID_EXTENSION}\n        vid_n.slp\n    val/\n        vid_1.{VID_EXTENSION}\n        vid_1.slp\n            .\n            .\n            .\n        vid_n.{VID_EXTENSION}\n        vid_n.slp\n    test/ # optional; test sets are not automatically evaluated as part of training\n        vid_1.{VID_EXTENSION}\n        vid_1.slp\n            .\n            .\n            .\n        vid_n.slp\n        vid_n.slp\n</code></pre> The CellTrackingChallenge format requires a directory with raw tifs, and a matching directory with labelled segmentation masks for the track labels. The directory structure is as follows: <pre><code>dataset_name/\n    train/\n        subdir_0/\n            frame0.tif # these are raw images\n            ...\n            frameN.tif\n        subdir_0_GT/TRA # these are labelled segmentation masks\n            frame0.tif\n            ...\n            frameN.tif\n        subdir_1/\n            frame0.tif\n            ...\n            frameN.tif\n        subdir_1_GT/TRA\n            frame0.tif\n            ...\n            frameN.tif\n        ...\n    val/\n        subdir_0/\n        subdir_0_GT/TRA\n        subdir_1/\n        subdir_1_GT/TRA\n        ...\n    test/ # optional; test sets are not automatically evaluated as part of training\n        subdir_0/\n        subdir_0_GT/TRA\n        subdir_1/\n        subdir_1_GT/TRA\n        ...\n</code></pre></p>"},{"location":"usage/#training_1","title":"Training","text":"<p>Now that you have your dataset set up, let's start training a model! We provide a CLI that allows you to train with a simple command and a single yaml configuration file.</p>"},{"location":"usage/#setup-config","title":"Setup Config","text":"<p>The input into our training script is a <code>.yaml</code> file that contains all the parameters needed for training. Please see here for a detailed description of all the parameters and how to set up the config. We provide up-to-date sample configs on HuggingFace Hub in the talmolab/microscopy-pretrained and talmolab/animals-pretrained. In general, the best practice is to keep a single <code>base.yaml</code> file which has all the default arguments you'd like to use. Then you can have a second <code>.yaml</code> file which will override a specific set of parameters when training.</p>"},{"location":"usage/#train-model","title":"Train Model","text":"<p>Once you have your config file and dataset set up, training is as easy as running</p> <p><pre><code>dreem-train --config-base=[CONFIG_DIR] --config-name=[CONFIG_STEM]\n</code></pre> where <code>CONFIG_DIR</code> is the directory that <code>hydra</code> should search for the <code>config.yaml</code> and <code>CONFIG_STEM</code> is the name of the config without the <code>.yaml</code> extension.</p> <p>e.g. If you have a config file called <code>base.yaml</code> inside your <code>/home/user/dreem_configs</code> directory you can call <pre><code>dreem-train --config-base=/home/user/dreem_configs --config-name=base\n</code></pre></p> <p>Note: you can use relative paths as well but may be a bit riskier so we recommend absolute paths whenever possible.</p> <p>If you've been through the example notebooks, you'll notice that training was done using the API rather than the CLI. You can use whichever you prefer.</p>"},{"location":"usage/#overriding-arguments","title":"Overriding Arguments","text":"<p>Instead of changing the <code>base.yaml</code> file every time you want to train a model using different configurations, <code>hydra</code> enables us to either</p> <ol> <li>provide another <code>.yaml</code> file with a subset of the parameters to override</li> <li>provide the args to the cli directly</li> </ol> <p>We recommend using the file-based override for logging and reproducibility.</p> <p>For overriding specific params with an override file, you can run:</p> <pre><code>dreem-train --config-base=[CONFIG_DIR] --config-name=[CONFIG_STEM] ++params_config=\"/path/to/override_params.yaml\"\n</code></pre> <p>e.g. If you have a <code>override_params.yaml</code> file inside your <code>/home/user/dreem_configs</code> directory that contains a only a small selection of parameters that you'd like to override, you can run:</p> <pre><code>dreem-train --config-base=/home/user/dreem_configs --config-name=base ++params_config=/home/user/dreem_configs/override_params.yaml\n</code></pre>"},{"location":"usage/#output","title":"Output","text":"<p>The output of the train script will be at least 1 <code>ckpt</code> file, assuming you've configured the <code>checkpointing</code> section of the config correctly.</p>"},{"location":"usage/#eval-with-ground-truth-labels","title":"Eval (with ground truth labels)","text":"<p>To test the performance of your model, you can use the <code>dreem-eval</code> CLI. It computes multi-object tracking metrics on your test data and outputs it in h5 format.</p>"},{"location":"usage/#setup-data","title":"Setup data","text":"<p>Note that your data should have ground truth labels. You can arrange it in a /test directory as shown above.</p>"},{"location":"usage/#setup-config_1","title":"Setup config","text":"<p>Samples are available at talmolab/microscopy-pretrained and talmolab/animals-pretrained, while a detailed walkthrough is available here</p>"},{"location":"usage/#run-evaluation","title":"Run evaluation","text":"<p>We provide a CLI that allows you to evaluate your model with a simple command and a single yaml configuration file.</p> <pre><code>dreem-eval --config-base=[CONFIG_DIR] --config-name=[CONFIG_STEM]\n</code></pre>"},{"location":"usage/#output_1","title":"Output","text":"<p>Tracking results will be saved as .slp in the directory specified by the <code>outdir</code> argument. If you don't enter an <code>outdir</code> in the config, it will save to <code>./results</code>.</p> <p>You should now be ready to use DREEM to train and track your own data!</p>"},{"location":"Examples/dreem-demo/","title":"End-to-end demo","text":"<p>This notebook will walk you through the DREEM pipeline end to end, from obtaining data to training a model, evaluating on a held-out dataset, and visualizing the results. Here, we'll use the API, but we also provide a CLI interface for convenience.</p> <p>To run this demo, we have provided sample data and configurations. The data used in this demo is small enough to be run on a single machine, though a GPU is recommended. </p>"},{"location":"Examples/dreem-demo/#directory-structure-after-downloading-data","title":"Directory structure after downloading data:","text":"<pre><code>./data\n    /test\n        190719_090330_wt_18159206_rig1.2@15000-17560.mp4\n        GT_190719_090330_wt_18159206_rig1.2@15000-17560.slp\n    /train\n        190612_110405_wt_18159111_rig2.2@4427.mp4\n        GT_190612_110405_wt_18159111_rig2.2@4427.slp\n    /val\n        two_flies.mp4\n        GT_two_flies.slp\n    /inference\n        190719_090330_wt_18159206_rig1.2@15000-17560.mp4\n        190719_090330_wt_18159206_rig1.2@15000-17560.slp\n    /configs\n        inference.yaml\n        base.yaml\n        eval.yaml\n</code></pre> <pre><code>import torch\nimport pandas as pd\nimport numpy as np\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\nimport pytorch_lightning as pl\nfrom omegaconf import OmegaConf\nfrom dreem.io import Config\nfrom dreem.datasets import TrackingDataset\nfrom dreem.models import GTRRunner\nfrom dreem.inference import Tracker\nimport sleap_io as sio\nimport matplotlib.pyplot as plt\nimport h5py\n</code></pre> <p>Check if a GPU is available. For Apple silicon users, you can run on MPS, but ensure your version of PyTorch is compatible with MPS, and that you have installed the correct version of DREEM. You can also run without a GPU. The demo has been tested on an M3 Macbook Air running only on a CPU.</p> <pre><code>cuda_available = torch.cuda.is_available()\nprint(f\"CUDA available: {cuda_available}\")\nif cuda_available:\n    accelerator = \"cuda\"\nelif torch.backends.mps.is_available():\n    accelerator = \"mps\"\n    devices = 1\nelse:\n    accelerator = \"cpu\"\nprint(\"Using device: \", accelerator)\n\ntorch.set_float32_matmul_precision(\"medium\")\n</code></pre>"},{"location":"Examples/dreem-demo/#download-data-and-configs","title":"Download data and configs","text":"<pre><code>!huggingface-cli download talmolab/sample-flies --repo-type dataset --local-dir ./data\n</code></pre>"},{"location":"Examples/dreem-demo/#training","title":"Training","text":""},{"location":"Examples/dreem-demo/#setup-configs","title":"Setup configs","text":"<p>The configs provided are good defaults. You can change them as you see fit.</p> <pre><code>config_path = \"./data/configs/base.yaml\"\n# use OmegaConf to load the config\ncfg = OmegaConf.load(config_path)\ntrain_cfg = Config(cfg)\n</code></pre>"},{"location":"Examples/dreem-demo/#create-a-model","title":"Create a model","text":"<p>The model is a Lightning wrapper around our model. Lightning simplifies training, validation, logging, and checkpointing.</p> <pre><code>model = train_cfg.get_gtr_runner()\n</code></pre>"},{"location":"Examples/dreem-demo/#prepare-torch-datasets-and-dataloader","title":"Prepare torch datasets and dataloader","text":"<p>Note: We use a batch size of 1 - we handle the batching ourselves since we are dealing with video data and associated labels in .slp format. The default clip length is set to 32 frames.</p> <pre><code>train_dataset = train_cfg.get_dataset(mode=\"train\")\ntrain_dataloader = train_cfg.get_dataloader(train_dataset, mode=\"train\")\n\nval_dataset = train_cfg.get_dataset(mode=\"val\")\nval_dataloader = train_cfg.get_dataloader(val_dataset, mode=\"val\")\n\n# wrap the dataloaders\ndataset = TrackingDataset(train_dl=train_dataloader, val_dl=val_dataloader)\n</code></pre>"},{"location":"Examples/dreem-demo/#visualize-the-input-data","title":"Visualize the input data","text":"<p>The input data to the model is a set of crops taken around a particular keypoint on the instance. For animals, this can be any keypoint, and for microscopy, this is often the centroid. Augmentations are also applied. Since we shuffle the data, the frame ids you get may not be the first frames of the video.</p> <pre><code># load a batch of data in\nviewer = iter(train_dataloader)\nbatch = next(viewer)\n# save the crops for all frames in the batch\ncrops = {}\nfor frame in batch[0]:\n    crops[frame.frame_id.item()] = []\n    for instance in frame.instances:\n        crops[frame.frame_id.item()].append(instance.crop.squeeze().permute(1,2,0).numpy())\n</code></pre> <p>Plot the crops for all frames in the batch</p> <pre><code>total_crops = sum(len(crops) for crops in crops.items())\n\n# Determine a grid size\nn_cols = int(np.ceil(np.sqrt(total_crops)))\nn_rows = int(np.ceil(total_crops / n_cols))\n\n# Create figure and axes\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(25,25))\nfig.suptitle(f\"Video: {frame.video}, Frames: {min(crops.keys())} to {max(crops.keys())}\", fontsize=16)\n\n# Ensure axes is always a 2D array\nif n_rows == 1 and n_cols == 1: axes = np.array([[axes]])\nelif n_rows == 1: axes = axes.reshape(1, -1)\nelif n_cols == 1: axes = axes.reshape(-1, 1)\n\n# Flatten for easier indexing\naxes_flat = axes.flatten()\n\n# Plot each crop\nax_idx = 0\nfor frame_id, vid_crops in sorted(crops.items()):\n    for i, crop in enumerate(vid_crops):\n        if ax_idx &lt; len(axes_flat):\n            ax = axes_flat[ax_idx]\n\n            # Handle both RGB and grayscale images\n            if crop.ndim == 3:\n                # Normalize if needed\n                if crop.max() &gt; 1.0:\n                    crop = crop / 255.0\n                ax.imshow(crop)\n            else:\n                ax.imshow(crop, cmap='gray')\n\n            ax.set_title(f\"Frame {frame_id}, Inst {i}\")\n            ax.axis('off')\n            ax_idx += 1\n\n# Hide unused subplots\nfor i in range(ax_idx, len(axes_flat)):\n    axes_flat[i].axis('off')\n\n# Adjust layout to minimize whitespace\nplt.tight_layout()\nplt.subplots_adjust(top=0.95) \n</code></pre>"},{"location":"Examples/dreem-demo/#train-the-model","title":"Train the model","text":"<p>First setup various training features such as loss curve plotting, early stopping and more, through Lightning's callbacks, then setup the Trainer and train the model.</p> <pre><code># to plot loss curves \nclass NotebookPlotCallback(pl.Callback):\n    def __init__(self):\n        super().__init__()\n        self.train_losses = []\n        self.val_losses = []\n        self.epochs = []\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        train_loss = trainer.callback_metrics.get('train_loss')\n        self.train_losses.append(train_loss.item())\n        self.epochs.append(trainer.current_epoch)\n\n    def on_validation_epoch_end(self, trainer, pl_module):\n        val_loss = trainer.callback_metrics.get('val_loss')\n        self.val_losses.append(val_loss.item())\n\nnotebook_plot_callback = NotebookPlotCallback()\n</code></pre> <pre><code>callbacks = []\n_ = callbacks.extend(train_cfg.get_checkpointing())\n_ = callbacks.append(pl.callbacks.LearningRateMonitor())\nearly_stopping = train_cfg.get_early_stopping()\nif early_stopping is not None:\n    callbacks.append(early_stopping)\ncallbacks.append(notebook_plot_callback)\n</code></pre> <p>The default maximum epochs is set to 4 in the provided config. You can change this in the trainer section of the config.</p> <pre><code># setup Lightning Trainer\ntrainer = train_cfg.get_trainer(\n    callbacks,\n    accelerator=accelerator,\n    devices=1\n)\n# train the model\ntrainer.fit(model, dataset)\n</code></pre>"},{"location":"Examples/dreem-demo/#visualize-the-train-and-validation-loss-curves","title":"Visualize the train and validation loss curves","text":"<pre><code>plt.figure(figsize=(6,4))\nplt.plot(notebook_plot_callback.epochs, notebook_plot_callback.train_losses, label='Train Loss', marker='o')\nplt.plot(notebook_plot_callback.epochs, notebook_plot_callback.val_losses[1:], label='Validation Loss', marker='x')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Examples/dreem-demo/#inference","title":"Inference","text":""},{"location":"Examples/dreem-demo/#here-we-run-inference-on-a-video-with-no-ground-truth-labels","title":"Here we run inference on a video with no ground truth labels","text":"<pre><code># get the model from the directory it saves to \n# (see logging.name in the config)\nckpt_dir = \"./models/example_train\"\nckpts = os.listdir(ckpt_dir)\nfor ckpt in ckpts:\n    if \"final\" in ckpt: # assumes the final checkpoint is the best one\n        best_checkpoint_path = os.path.join(ckpt_dir, ckpt)\n        break\nmodel = GTRRunner.load_from_checkpoint(best_checkpoint_path)\n</code></pre>"},{"location":"Examples/dreem-demo/#setup-inference-configs","title":"Setup inference configs","text":"<pre><code>pred_cfg_path = \"./data/configs/inference.yaml\"\n# use OmegaConf to load the config\npred_cfg = OmegaConf.load(pred_cfg_path)\npred_cfg = Config(pred_cfg)\n</code></pre> <p>Get the tracker settings from the config and initialize the tracker</p> <pre><code>tracker_cfg = pred_cfg.get_tracker_cfg()\nmodel.tracker_cfg = tracker_cfg\nmodel.tracker = Tracker(**model.tracker_cfg)\ntrainer = pred_cfg.get_trainer()\n# inference results will be saved here\noutdir = \"./results\"\nos.makedirs(outdir, exist_ok=True)\n</code></pre>"},{"location":"Examples/dreem-demo/#prepare-data-and-run-inference","title":"Prepare data and run inference","text":"<pre><code>labels_files, vid_files = pred_cfg.get_data_paths(mode=\"test\", data_cfg=pred_cfg.cfg.dataset.test_dataset)\n\nfor label_file, vid_file in zip(labels_files, vid_files):\n    dataset = pred_cfg.get_dataset(\n        label_files=[label_file], vid_files=[vid_file], mode=\"test\"\n    )\n    dataloader = pred_cfg.get_dataloader(dataset, mode=\"test\")\n\n    # the actual inference is done here\n    preds = trainer.predict(model, dataloader)\n\n    # convert the predictions to sleap format\n    pred_slp = []\n    tracks = {}\n    for batch in preds:\n        for frame in batch:\n            if frame.frame_id.item() == 0:\n                video = (\n                    sio.Video(frame.video)\n                    if isinstance(frame.video, str)\n                    else sio.Video\n                )\n            lf, tracks = frame.to_slp(tracks, video=video)\n            pred_slp.append(lf)\n    pred_slp = sio.Labels(pred_slp)\n    # save the predictions to disk (requires sleap-io)\n    outpath = os.path.join(\n        outdir, f\"{Path(label_file).stem}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.slp\"\n    )\n    pred_slp.save(outpath)\n</code></pre>"},{"location":"Examples/dreem-demo/#visualize-the-results","title":"Visualize the results","text":"<pre><code>import cv2\nimport matplotlib.colors as mcolors\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib.patches import Circle\nimport matplotlib.cm as cm\nfrom IPython.display import HTML, display\nimport io\nimport base64\nfrom IPython.display import Video\n\ndef create_animal_tracking_animation_notebook(video_path, metadata_df, \n                                             fps=30, text_size=8, marker_size=20,\n                                             max_frames=None, display_width=800):\n    \"\"\"\n    Create and display an animal tracking animation directly in the notebook.\n\n    Parameters:\n    -----------\n    video_path : str\n        Path to the input MP4 video file\n    metadata_df : pandas.DataFrame\n        DataFrame with columns: frame_id, track_id, centroid\n    fps : int\n        Frames per second for the animation\n    text_size : int\n        Size of the ID text\n    marker_size : int\n        Size of the marker circle\n    max_frames : int, optional\n        Maximum number of frames to process (useful for previewing)\n    display_width : int\n        Width of the displayed animation in the notebook\n    \"\"\"\n    # Open the video file\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        raise ValueError(f\"Could not open video file: {video_path}\")\n\n    # Get video properties\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Create a colormap for track IDs\n    unique_ids = metadata_df['track_id'].unique()\n    cmap = cm.get_cmap('tab10', len(unique_ids))  # Using 'hsv' for bright, distinct colors\n    id_to_color = {id_val: cmap(i) for i, id_val in enumerate(unique_ids)}\n\n    # Set up the figure and axis with the correct aspect ratio\n    fig_width = display_width / 100  # Convert to inches (assuming 100 dpi)\n    fig_height = fig_width * (height / width)\n    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n\n    # Initialize the plot elements\n    frame_img = ax.imshow(np.zeros((height, width, 3), dtype=np.uint8))\n    markers = []\n    texts = []\n\n    # Get the list of frame IDs from the metadata\n    frame_ids = sorted(metadata_df['frame_id'].unique())\n\n    # Limit the number of frames if specified\n    if max_frames is not None and max_frames &lt; len(frame_ids):\n        frame_ids = frame_ids[:max_frames]\n        print(f\"Limiting preview to {max_frames} frames\")\n\n    # Function to update the animation for each frame\n    def update(frame_num):\n        # Read the frame from the video\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n        ret, frame = cap.read()\n        if not ret:\n            print(f\"Failed to read frame {frame_num}\")\n            return []\n\n        # Convert BGR to RGB (OpenCV uses BGR, matplotlib uses RGB)\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frame_img.set_array(frame_rgb)\n\n        # Clear previous markers and texts\n        for marker in markers:\n            marker.remove()\n        markers.clear()\n\n        for text in texts:\n            text.remove()\n        texts.clear()\n\n        # Get data for the current frame\n        frame_data = metadata_df[metadata_df['frame_id'] == frame_num]\n\n        # Add markers and IDs for each animal in the current frame\n        for _, row in frame_data.iterrows():\n            track_id = row['track_id']\n            x, y = row['centroid']\n            color = id_to_color[track_id]\n\n            # Add circle marker\n            circle = Circle((x, y), marker_size, color=color, alpha=0.7)\n            markers.append(ax.add_patch(circle))\n\n            # Add ID text\n            text = ax.text(x, y, str(track_id), color='white', \n                          fontsize=text_size, ha='center', va='center', \n                          fontweight='bold')\n            texts.append(text)\n\n        # Add frame number for reference\n        frame_text = ax.text(10, 20, f\"Frame: {frame_num}\", color='white', \n                            fontsize=text_size, backgroundcolor='black')\n        texts.append(frame_text)\n\n        return [frame_img] + markers + texts\n\n    # Set up the axis\n    ax.set_xlim(0, width)\n    ax.set_ylim(height, 0)  # Invert y-axis to match image coordinates\n    ax.axis('off')\n\n    # Create the animation\n    print(f\"Creating animation with {len(frame_ids)} frames...\")\n    anim = FuncAnimation(fig, update, frames=frame_ids, blit=True)\n\n    # Display the animation in the notebook\n    plt.close(fig)  # Prevent duplicate display\n\n    # Display as HTML5 video\n    html_video = HTML(anim.to_html5_video())\n    display(html_video)\n\n    return anim\n\n# Option to save the animation to a file for later viewing\ndef save_animation(anim, output_path, fps=10, dpi=100):\n    \"\"\"Save the animation to a file\"\"\"\n    anim.save(output_path, writer='ffmpeg', fps=fps, dpi=dpi)\n    print(f\"Animation saved to {output_path}\")\n\n    # Display the saved video in the notebook\n    return Video(output_path, embed=True, width=800)\n</code></pre> <p>Load the predictions into a dataframe to make an animation</p> <pre><code>list_frames = []\nfor lf in pred_slp:\n    for instance in lf.instances:\n        centroid = np.nanmean(instance.numpy(), axis=0)\n        track_id = int(instance.track.name)\n        list_frames.append({'frame_id': lf.frame_idx, 'track_id': track_id, 'centroid': centroid})\ndf = pd.DataFrame(list_frames)\n</code></pre> <p>Create and display the animation in the notebook</p> <pre><code>for file in os.listdir(pred_cfg.cfg.dataset.test_dataset['dir']['path']):\n    if file.endswith('.mp4'):\n        video_path = os.path.join(pred_cfg.cfg.dataset.test_dataset['dir']['path'], file)\n\nanim = create_animal_tracking_animation_notebook(\n    video_path=video_path,\n    metadata_df=df,\n    fps=15,\n    text_size=8,\n    marker_size=20,\n    max_frames=300\n)\n\n# save the animation\nvideo = save_animation(anim, f\"./animal_tracking_vis-{video_path.split('/')[-1]}\")\n</code></pre>"},{"location":"Examples/dreem-demo/#evaluate-the-tracking-results","title":"Evaluate the tracking results","text":""},{"location":"Examples/dreem-demo/#here-we-run-inference-on-a-video-with-ground-truth-labels-then-we-will-compute-metrics-for-our-tracking-results","title":"Here we run inference on a video with ground truth labels. Then we will compute metrics for our tracking results.","text":"<p>Note that we are only using separate configs for inference and evaluation so you can verify that the test file has no ground truth in it for inference, and that it does for evaluation. For eval, the slp file should have a \"GT_\" prefix to indicate that it is a ground truth file. For eval, you can also specify the metrics you want to compute. We offer a CLI interface for both evaluation and inference.</p> <pre><code>pred_cfg_path = \"./data/configs/eval.yaml\"\n# use OmegaConf to load the config\neval_cfg = OmegaConf.load(pred_cfg_path)\neval_cfg = Config(eval_cfg)\n</code></pre> <pre><code>model.metrics[\"test\"] = eval_cfg.get(\"metrics\", {}).get(\"test\", \"all\")\nmodel.test_results[\"save_path\"] = eval_cfg.get(\"outdir\", \"./eval\")\nos.makedirs(model.test_results[\"save_path\"], exist_ok=True)\n</code></pre> <p>Run evaluation pipeline. Note how we use trainer.test() to run evaluation whereas earlier, we used trainer.predict() to run inference</p> <pre><code>labels_files, vid_files = eval_cfg.get_data_paths(mode=\"test\", data_cfg=eval_cfg.cfg.dataset.test_dataset)\ntrainer = eval_cfg.get_trainer()\nfor label_file, vid_file in zip(labels_files, vid_files):\n    dataset = eval_cfg.get_dataset(\n        label_files=[label_file], vid_files=[vid_file], mode=\"test\"\n    )\n    dataloader = eval_cfg.get_dataloader(dataset, mode=\"test\")\n    metrics = trainer.test(model, dataloader)\n</code></pre>"},{"location":"Examples/dreem-demo/#extract-the-results-and-view-key-metrics","title":"Extract the results and view key metrics","text":"<p>The results get saved to an HDF5 file in the directory specified in the config</p> <pre><code>for file in os.listdir(model.test_results[\"save_path\"]):\n    if file.endswith(\".h5\"):\n        h5_path = os.path.join(model.test_results[\"save_path\"], file)\n</code></pre> <pre><code>dict_vid_motmetrics = {}\ndict_vid_gta = {}\ndict_vid_switch_frame_crops = {}\n\nwith h5py.File(h5_path, \"r\") as results_file:\n    # Iterate through all video groups\n    for vid_name in results_file.keys():\n        print(\"Extracting metrics and crops for video: \", vid_name)\n        vid_group = results_file[vid_name]\n        # Load MOT summary\n        if \"mot_summary\" in vid_group:\n            mot_summary_keys = list(vid_group[\"mot_summary\"].attrs)\n            mot_summary_values = [vid_group[\"mot_summary\"].attrs[key] for key in mot_summary_keys]\n            df_motmetrics = pd.DataFrame(list(zip(mot_summary_keys, mot_summary_values)), columns=[\"metric\", \"value\"])\n            dict_vid_motmetrics[vid_name] = df_motmetrics\n        # Load global tracking accuracy if available\n        if \"global_tracking_accuracy\" in vid_group:\n            gta_keys = list(vid_group[\"global_tracking_accuracy\"].attrs)\n            gta_values = [vid_group[\"global_tracking_accuracy\"].attrs[key] for key in gta_keys]\n            df_gta = pd.DataFrame(list(zip(gta_keys, gta_values)), columns=[\"metric\", \"value\"])\n            dict_vid_gta[vid_name] = df_gta\n        # Find all frames with switches and save the crops\n        frame_crop_dict = {}\n        for key in vid_group.keys():\n            if key.startswith(\"frame_\"):\n                frame = vid_group[key]\n                frame_id = frame.attrs[\"frame_id\"]\n                for key in frame.keys():\n                    if key.startswith(\"instance_\"):\n                        instance = frame[key]\n                        if \"crop\" in instance.keys():\n                            frame_crop_dict[frame_id] = instance[\"crop\"][:].squeeze().transpose(1,2,0)\n        dict_vid_switch_frame_crops[vid_name] = frame_crop_dict\n</code></pre> <p>Check the switch count (and other mot metrics) for the whole video. You should see 0 switches. This means that the tracker consistently maintained identities across the video.</p> <pre><code>motmetrics = list(dict_vid_motmetrics.values())[0]\n# motmetrics.loc[motmetrics['metric'] == 'num_switches']\nmotmetrics\n</code></pre> <p>Check global tracking accuracy. This represents the percentage of frames where the tracker correctly maintained identities for each instance. In this case, since there were no switches, the global tracking accuracy is 100% for all instances.</p> <pre><code>gta = list(dict_vid_gta.values())[0]\ngta\n</code></pre>"},{"location":"Examples/microscopy-demo-full-api/","title":"Microscopy - in-depth API usage","text":""},{"location":"Examples/microscopy-demo-full-api/#from-raw-tiff-stacks-to-tracked-identities","title":"From raw tiff stacks to tracked identities","text":"<p>This notebook will walk you through the typical workflow for microscopy identity tracking. We start with an off-the-shelf detection model, and feed those results into DREEM.  Here, we'll use the API, but we also provide a CLI interface for convenience.</p> <p>To run this demo, we have provided sample data, model checkpoints, and configurations. The data used in this demo is small enough to be run on a single machine, though a GPU is recommended. </p>"},{"location":"Examples/microscopy-demo-full-api/#directory-structure-data-models-and-configs-will-be-downloaded","title":"Directory structure (data, models and configs will be downloaded)","text":"<pre><code>./data\n    /dynamicnuclearnet\n        /test_1\n        /mp4-for-visualization\n    /lysosomes\n        /7-2\n        /7-2_GT\n        /mp4-for-visualization\n./configs\n    sample-eval-microscopy.yaml\n./models\n    pretrained_microscopy.ckpt\n microscopy-demo-full-api.ipynb\n</code></pre>"},{"location":"Examples/microscopy-demo-full-api/#install-huggingface-hub-to-access-models-and-data","title":"Install huggingface hub to access models and data","text":"<pre><code>!pip install huggingface_hub\n</code></pre>"},{"location":"Examples/microscopy-demo-full-api/#import-necessary-packages","title":"Import necessary packages","text":"<pre><code>import torch\nimport pandas as pd\nimport numpy as np\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\nimport pytorch_lightning as pl\nfrom omegaconf import OmegaConf\nfrom dreem.io import Config\nfrom dreem.datasets import TrackingDataset\nfrom dreem.models import GTRRunner\nfrom dreem.inference import Tracker\nimport sleap_io as sio\nimport matplotlib.pyplot as plt\nimport h5py\nfrom huggingface_hub import hf_hub_download\n</code></pre>"},{"location":"Examples/microscopy-demo-full-api/#download-a-pretrained-model-configs-and-some-data","title":"Download a pretrained model, configs and some data","text":"<pre><code>model_save_dir = \"./models\"\nconfig_save_dir = \"./configs\"\ndata_save_dir = \"./data\"\nos.makedirs(config_save_dir, exist_ok=True)\nos.makedirs(data_save_dir, exist_ok=True)\nos.makedirs(model_save_dir, exist_ok=True)\n</code></pre> <pre><code>model_path = hf_hub_download(repo_id=\"talmolab/microscopy-pretrained\", filename=\"pretrained-microscopy.ckpt\",\nlocal_dir=model_save_dir)\n\nconfig_path = hf_hub_download(repo_id=\"talmolab/microscopy-pretrained\", filename=\"sample-eval-microscopy.yaml\",\nlocal_dir=config_save_dir)\n</code></pre> <pre><code>!huggingface-cli download talmolab/microscopy-demo --repo-type dataset --local-dir ./data\n</code></pre>"},{"location":"Examples/microscopy-demo-full-api/#verify-that-the-model-loads-properly","title":"Verify that the model loads properly","text":"<pre><code>m = GTRRunner.load_from_checkpoint(model_path, strict=False)\n</code></pre> <p>Check if a GPU is available. For Apple silicon users, you can run on MPS, but ensure your version of PyTorch is compatible with MPS, and that you have installed the correct version of DREEM. You can also run without a GPU. The demo has been tested on an M3 Macbook Air running only on a CPU.</p> <pre><code>cuda_available = torch.cuda.is_available()\nprint(f\"CUDA available: {cuda_available}\")\nif cuda_available:\n    accelerator = \"cuda\"\nelif torch.backends.mps.is_available():\n    accelerator = \"mps\"\n    devices = 1\nelse:\n    accelerator = \"cpu\"\nprint(\"Using device: \", accelerator)\n\ntorch.set_float32_matmul_precision(\"medium\")\n</code></pre>"},{"location":"Examples/microscopy-demo-full-api/#detection","title":"Detection","text":"<p>Here we use CellPose to create segmentation masks for our instances. If you want to skip this stage, we have provided segmentation masks for the lysosomes dataset located at ./data/lysosomes. You can enter this path in the configuration file provided, under dataset.test_dataset.dir.path, and then skip straight ahead to the section labelled DREEM Inference below</p>"},{"location":"Examples/microscopy-demo-full-api/#install-cellpose","title":"Install CellPose","text":"<pre><code>!pip install git+https://www.github.com/mouseland/cellpose.git\n</code></pre> <pre><code>import tifffile\nfrom cellpose import models\n</code></pre> <pre><code>data_path = \"./data/dynamicnuclearnet/test_1\"\nsegmented_path = \"./data/dynamicnuclearnet/test_1_GT/TRA\"\nos.makedirs(segmented_path, exist_ok=True)\n</code></pre> <p>Set the approximate diameter (in pixels) of the instances you want to segment</p> <pre><code>diam_px = 25\n</code></pre>"},{"location":"Examples/microscopy-demo-full-api/#run-detection-model","title":"Run detection model","text":"<pre><code>tiff_files = [f for f in os.listdir(data_path) if f.endswith('.tif') or f.endswith('.tiff')]\nstack = np.stack([tifffile.imread(os.path.join(data_path, f)) for f in tiff_files])\nframes, Y, X = stack.shape\n\nchannels = [0, 0]\n# use builtin latest model\nmodel = models.CellposeModel(gpu=True)\nall_masks = np.zeros_like(stack)\nfor i, img in enumerate(stack):\n    masks, flows, styles = model.eval(\n        img,\n        diameter=diam_px,\n        cellprob_threshold=0.0,\n        channels=channels,\n        z_axis=None,\n    )\n    all_masks[i] = masks\n</code></pre>"},{"location":"Examples/microscopy-demo-full-api/#save-the-segmentation-masks","title":"Save the segmentation masks","text":"<pre><code>os.makedirs(segmented_path, exist_ok=True)\nfor i, (mask, filename) in enumerate(zip(all_masks, tiff_files)):\n    new_tiff_path = os.path.join(segmented_path, f\"{os.path.splitext(filename)[0]}.tif\")\n    print(f\"exporting frame {i} to tiff at {new_tiff_path}\")\n    tifffile.imwrite(new_tiff_path, mask)\n</code></pre>"},{"location":"Examples/microscopy-demo-full-api/#view-the-segmentation-result-and-original-image","title":"View the segmentation result and original image","text":"<pre><code>fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\nax1.imshow(all_masks[0])\nax1.set_title('Segmentation Mask')\nax2.imshow(stack[0])\nax2.set_title('Original Image')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Examples/microscopy-demo-full-api/#dreem-inference","title":"DREEM Inference","text":"<p>In this section, we demonstrate the standard DREEM inference pipeline using the API</p> <pre><code>model = GTRRunner.load_from_checkpoint(model_path, strict=False)\n</code></pre>"},{"location":"Examples/microscopy-demo-full-api/#setup-inference-configs","title":"Setup inference configs","text":""},{"location":"Examples/microscopy-demo-full-api/#note-we-can-only-specify-1-directory-at-a-time-when-running-inference-to-test-a-different-dataset-just-enter-the-path-to-the-directory-containing-the-dataset-see-the-config-for-an-example","title":"NOTE: We can only specify 1 directory at a time when running inference. To test a different dataset, just enter the path to the directory containing the dataset. See the config for an example","text":"<pre><code>pred_cfg_path = \"./configs/sample-eval-microscopy.yaml\"\n# use OmegaConf to load the config\npred_cfg = OmegaConf.load(pred_cfg_path)\npred_cfg = Config(pred_cfg)\n</code></pre> <p>Get the tracker settings from the config and initialize the tracker</p> <pre><code>tracker_cfg = pred_cfg.get_tracker_cfg()\nmodel.tracker_cfg = tracker_cfg\nmodel.tracker = Tracker(**model.tracker_cfg)\ntrainer = pred_cfg.get_trainer()\n# inference results will be saved here\noutdir = \"./results\"\nos.makedirs(outdir, exist_ok=True)\n</code></pre>"},{"location":"Examples/microscopy-demo-full-api/#prepare-data-and-run-inference","title":"Prepare data and run inference","text":"<pre><code>labels_files, vid_files = pred_cfg.get_data_paths(mode=\"test\", data_cfg=pred_cfg.cfg.dataset.test_dataset)\n\nfor label_file, vid_file in zip(labels_files, vid_files):\n    dataset = pred_cfg.get_dataset(\n        label_files=[label_file], vid_files=[vid_file], mode=\"test\"\n    )\n    dataloader = pred_cfg.get_dataloader(dataset, mode=\"test\")\n\n    # the actual inference is done here\n    preds = trainer.predict(model, dataloader)\n\n    # convert the predictions to sleap format\n    pred_slp = []\n    tracks = {}\n    for batch in preds:\n        for frame in batch:\n            if frame.frame_id.item() == 0:\n                video = (\n                    sio.Video(frame.video)\n                    if isinstance(frame.video, str)\n                    else sio.Video\n                )\n            lf, tracks = frame.to_slp(tracks, video=video)\n            pred_slp.append(lf)\n    pred_slp = sio.Labels(pred_slp)\n    # save the predictions to disk (requires sleap-io)\n    if isinstance(vid_file, list):\n        save_file_name = vid_file[0].split(\"/\")[-2]\n    else:\n        save_file_name = vid_file\n    outpath = os.path.join(\n        outdir,\n        f\"{Path(save_file_name).stem}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.slp\",\n    )\n    pred_slp.save(outpath)\n</code></pre>"},{"location":"Examples/microscopy-demo-full-api/#visualize-the-results","title":"Visualize the results","text":"<pre><code>import cv2\nimport matplotlib.colors as mcolors\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib.patches import Circle\nimport matplotlib.cm as cm\nfrom IPython.display import HTML, display\nimport io\nimport base64\nfrom IPython.display import Video\n\ndef create_tracking_animation(video_path, metadata_df, \n                                             fps=30, text_size=8, marker_size=20,\n                                             max_frames=None, display_width=800):\n    \"\"\"\n    Create and display an animal tracking animation directly in the notebook.\n\n    Parameters:\n    -----------\n    video_path : str\n        Path to the input MP4 video file\n    metadata_df : pandas.DataFrame\n        DataFrame with columns: frame_id, track_id, centroid\n    fps : int\n        Frames per second for the animation\n    text_size : int\n        Size of the ID text\n    marker_size : int\n        Size of the marker circle\n    max_frames : int, optional\n        Maximum number of frames to process (useful for previewing)\n    display_width : int\n        Width of the displayed animation in the notebook\n    \"\"\"\n    # Open the video file\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        raise ValueError(f\"Could not open video file: {video_path}\")\n\n    # Get video properties\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Create a colormap for track IDs\n    unique_ids = metadata_df['track_id'].unique()\n    cmap = cm.get_cmap('viridis', len(unique_ids))  # Using 'hsv' for bright, distinct colors\n    id_to_color = {id_val: cmap(i) for i, id_val in enumerate(unique_ids)}\n\n    # Set up the figure and axis with the correct aspect ratio\n    fig_width = display_width / 100  # Convert to inches (assuming 100 dpi)\n    fig_height = fig_width * (height / width)\n    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n\n    # Initialize the plot elements\n    frame_img = ax.imshow(np.zeros((height, width, 3), dtype=np.uint8))\n    markers = []\n    texts = []\n\n    # Get the list of frame IDs from the metadata\n    frame_ids = sorted(metadata_df['frame_id'].unique())\n\n    # Limit the number of frames if specified\n    if max_frames is not None and max_frames &lt; len(frame_ids):\n        frame_ids = frame_ids[:max_frames]\n        print(f\"Limiting preview to {max_frames} frames\")\n\n    # Function to update the animation for each frame\n    def update(frame_num):\n        # Read the frame from the video\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n        ret, frame = cap.read()\n        if not ret:\n            print(f\"Failed to read frame {frame_num}\")\n            return []\n\n        # Convert BGR to RGB (OpenCV uses BGR, matplotlib uses RGB)\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frame_img.set_array(frame_rgb)\n\n        # Clear previous markers and texts\n        for marker in markers:\n            marker.remove()\n        markers.clear()\n\n        for text in texts:\n            text.remove()\n        texts.clear()\n\n        # Get data for the current frame\n        frame_data = metadata_df[metadata_df['frame_id'] == frame_num]\n\n        # Add markers and IDs for each animal in the current frame\n        for _, row in frame_data.iterrows():\n            track_id = row['track_id']\n            x, y = row['centroid']\n            color = id_to_color[track_id]\n\n            # Add circle marker\n            circle = Circle((x, y), marker_size, color=color, alpha=0.3)\n            markers.append(ax.add_patch(circle))\n\n            # Add ID text\n            text = ax.text(x, y, str(track_id), color='white', \n                          fontsize=text_size, ha='center', va='center', \n                          fontweight='bold')\n            texts.append(text)\n\n        # Add frame number for reference\n        frame_text = ax.text(10, 20, f\"Frame: {frame_num}\", color='white', \n                            fontsize=text_size, backgroundcolor='black')\n        texts.append(frame_text)\n\n        return [frame_img] + markers + texts\n\n    # Set up the axis\n    ax.set_xlim(0, width)\n    ax.set_ylim(height, 0)  # Invert y-axis to match image coordinates\n    ax.axis('off')\n\n    # Create the animation\n    print(f\"Creating animation with {len(frame_ids)} frames...\")\n    anim = FuncAnimation(fig, update, frames=frame_ids, blit=True)\n\n    # Display the animation in the notebook\n    plt.close(fig)  # Prevent duplicate display\n\n    # Display as HTML5 video\n    html_video = HTML(anim.to_html5_video())\n    display(html_video)\n\n    return anim\n\n# Option to save the animation to a file for later viewing\ndef save_animation(anim, output_path, fps=10, dpi=100):\n    \"\"\"Save the animation to a file\"\"\"\n    anim.save(output_path, writer='ffmpeg', fps=fps, dpi=dpi)\n    print(f\"Animation saved to {output_path}\")\n\n    # Display the saved video in the notebook\n    return Video(output_path, embed=True, width=800)\n</code></pre> <p>Load the predictions into a dataframe to make an animation</p> <pre><code>list_frames = []\nfor lf in pred_slp:\n    for instance in lf.instances:\n        centroid = np.nanmean(instance.numpy(), axis=0)\n        track_id = int(instance.track.name)\n        list_frames.append({'frame_id': lf.frame_idx, 'track_id': track_id, 'centroid': centroid})\ndf = pd.DataFrame(list_frames)\n</code></pre> <p>Create and display the animation in the notebook</p> <pre><code>for file in os.listdir(os.path.join(pred_cfg.cfg.dataset.test_dataset['dir']['path'], 'mp4-for-visualization')):\n    if file.endswith('.mp4'):\n        video_path = os.path.join(pred_cfg.cfg.dataset.test_dataset['dir']['path'], 'mp4-for-visualization', file)\n\nanim = create_tracking_animation(\n    video_path=video_path,\n    metadata_df=df,\n    fps=15,\n    text_size=5,\n    marker_size=8,\n    max_frames=300\n)\n\n# save the animation\nvideo = save_animation(anim, f\"./tracking_vis-{video_path.split('/')[-1]}\")\n</code></pre>"},{"location":"Examples/microscopy-demo-full-api/#evaluate-the-tracking-results","title":"Evaluate the tracking results","text":"<p>In this section, we evaluate metrics on a ground truth labelled test set. Note that in this example, the test set we used to demonstrate the inference pipeline is the same as the one we use here. To verify that we do not in fact use any ground truth information during tracking, go to our full dreem-demo notebook, where we use de-labelled data to verify this</p> <pre><code>pred_cfg_path = \"./configs/eval.yaml\"\n# use OmegaConf to load the config\neval_cfg = OmegaConf.load(pred_cfg_path)\neval_cfg = Config(eval_cfg)\n</code></pre> <pre><code>model.metrics[\"test\"] = eval_cfg.get(\"metrics\", {}).get(\"test\", \"all\")\nmodel.test_results[\"save_path\"] = eval_cfg.get(\"outdir\", \"./eval\")\nos.makedirs(model.test_results[\"save_path\"], exist_ok=True)\n</code></pre> <p>Run evaluation pipeline. Note how we use trainer.test() to run evaluation whereas earlier, we used trainer.predict() to run inference</p> <pre><code>labels_files, vid_files = eval_cfg.get_data_paths(mode=\"test\", data_cfg=eval_cfg.cfg.dataset.test_dataset)\ntrainer = eval_cfg.get_trainer()\nfor label_file, vid_file in zip(labels_files, vid_files):\n    dataset = eval_cfg.get_dataset(\n        label_files=[label_file], vid_files=[vid_file], mode=\"test\"\n    )\n    dataloader = eval_cfg.get_dataloader(dataset, mode=\"test\")\n    metrics = trainer.test(model, dataloader)\n</code></pre>"},{"location":"Examples/microscopy-demo-full-api/#extract-the-results-and-view-key-metrics","title":"Extract the results and view key metrics","text":"<p>The results get saved to an HDF5 file in the directory specified in the config</p> <pre><code>for file in os.listdir(model.test_results[\"save_path\"]):\n    if file.endswith(\".h5\"):\n        h5_path = os.path.join(model.test_results[\"save_path\"], file)\n</code></pre> <pre><code>dict_vid_motmetrics = {}\ndict_vid_gta = {}\ndict_vid_switch_frame_crops = {}\n\nwith h5py.File(h5_path, \"r\") as results_file:\n    # Iterate through all video groups\n    for vid_name in results_file.keys():\n        print(\"Extracting metrics and crops for video: \", vid_name)\n        vid_group = results_file[vid_name]\n        # Load MOT summary\n        if \"mot_summary\" in vid_group:\n            mot_summary_keys = list(vid_group[\"mot_summary\"].attrs)\n            mot_summary_values = [vid_group[\"mot_summary\"].attrs[key] for key in mot_summary_keys]\n            df_motmetrics = pd.DataFrame(list(zip(mot_summary_keys, mot_summary_values)), columns=[\"metric\", \"value\"])\n            dict_vid_motmetrics[vid_name] = df_motmetrics\n        # Load global tracking accuracy if available\n        if \"global_tracking_accuracy\" in vid_group:\n            gta_keys = list(vid_group[\"global_tracking_accuracy\"].attrs)\n            gta_values = [vid_group[\"global_tracking_accuracy\"].attrs[key] for key in gta_keys]\n            df_gta = pd.DataFrame(list(zip(gta_keys, gta_values)), columns=[\"metric\", \"value\"])\n            dict_vid_gta[vid_name] = df_gta\n        # Find all frames with switches and save the crops\n        frame_crop_dict = {}\n        for key in vid_group.keys():\n            if key.startswith(\"frame_\"):\n                frame = vid_group[key]\n                frame_id = frame.attrs[\"frame_id\"]\n                for key in frame.keys():\n                    if key.startswith(\"instance_\"):\n                        instance = frame[key]\n                        if \"crop\" in instance.keys():\n                            frame_crop_dict[frame_id] = instance[\"crop\"][:].squeeze().transpose(1,2,0)\n        dict_vid_switch_frame_crops[vid_name] = frame_crop_dict\n</code></pre> <p>Check the switch count (and other mot metrics) for the whole video</p> <pre><code>motmetrics = list(dict_vid_motmetrics.values())[0]\n# motmetrics.loc[motmetrics['metric'] == 'num_switches']\nmotmetrics\n</code></pre> <p>Check global tracking accuracy. This represents the percentage of frames where the tracker correctly maintained identities for each instance.</p> <pre><code>gta = list(dict_vid_gta.values())[0]\ngta\n</code></pre>"},{"location":"Examples/microscopy-demo-simple/","title":"Microscopy","text":""},{"location":"Examples/microscopy-demo-simple/#from-raw-tiff-stacks-to-tracked-identities","title":"From raw tiff stacks to tracked identities","text":"<p>This notebook will walk you through the typical workflow for microscopy identity tracking. We start with a raw tiff stack, pass it through an off-the-shelf detection model, and feed those detections into DREEM. </p> <p>This notebook uses a simple entrypoint into the tracking code. You only need to specify a configuration file, and a few lines of code!</p> <p>To run this demo, we have provided sample data, model checkpoints, and configurations. The data used in this demo is small enough to be run on a CPU</p>"},{"location":"Examples/microscopy-demo-simple/#directory-structure-data-models-and-configs-will-be-downloaded","title":"Directory structure: (data, models and configs will be downloaded)","text":"<pre><code>./data\n    /dynamicnuclearnet\n        /test_1\n        /mp4-for-visualization\n    /lysosomes\n        /7-2\n        /7-2_GT\n        /mp4-for-visualization\n./configs\n    eval.yaml\n./models\n    pretrained_microscopy.ckpt\n microscopy-demo-simple.ipynb\n</code></pre>"},{"location":"Examples/microscopy-demo-simple/#install-huggingface-hub-to-access-models-and-data","title":"Install huggingface hub to access models and data","text":"<pre><code>!pip install huggingface_hub\n</code></pre>"},{"location":"Examples/microscopy-demo-simple/#import-necessary-packages","title":"Import necessary packages","text":"<pre><code>import os\nimport torch\nimport numpy as np\nfrom omegaconf import OmegaConf\nfrom dreem.inference import track\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport pandas as pd\nfrom huggingface_hub import hf_hub_download\n</code></pre>"},{"location":"Examples/microscopy-demo-simple/#download-a-pretrained-model-configs-and-some-data","title":"Download a pretrained model, configs and some data","text":"<pre><code>model_save_dir = \"./models\"\nconfig_save_dir = \"./configs\"\ndata_save_dir = \"./data\"\nos.makedirs(config_save_dir, exist_ok=True)\nos.makedirs(data_save_dir, exist_ok=True)\nos.makedirs(model_save_dir, exist_ok=True)\n</code></pre> <pre><code>model_path = hf_hub_download(repo_id=\"talmolab/microscopy-pretrained\", filename=\"pretrained-microscopy.ckpt\",\nlocal_dir=model_save_dir)\n\nconfig_path = hf_hub_download(repo_id=\"talmolab/microscopy-pretrained\", filename=\"sample-eval-microscopy.yaml\",\nlocal_dir=config_save_dir)\n</code></pre> <pre><code>!huggingface-cli download talmolab/microscopy-demo --repo-type dataset --local-dir ./data\n</code></pre>"},{"location":"Examples/microscopy-demo-simple/#verify-that-the-model-loads-properly","title":"Verify that the model loads properly","text":"<pre><code>m = GTRRunner.load_from_checkpoint(model_path, strict=False)\n</code></pre> <p>Check if a GPU is available. For Apple silicon users, you can run on MPS, but ensure your version of PyTorch is compatible with MPS, and that you have installed the correct version of DREEM. You can also run without a GPU. The demo has been tested on an M3 Macbook Air running only on a CPU.</p> <pre><code>cuda_available = torch.cuda.is_available()\nprint(f\"CUDA available: {cuda_available}\")\nif cuda_available:\n    accelerator = \"cuda\"\nelif torch.backends.mps.is_available():\n    accelerator = \"mps\"\n    devices = 1\nelse:\n    accelerator = \"cpu\"\nprint(\"Using device: \", accelerator)\n\ntorch.set_float32_matmul_precision(\"medium\")\n</code></pre>"},{"location":"Examples/microscopy-demo-simple/#detection","title":"Detection","text":"<p>Here we use CellPose to create segmentation masks for our instances. If you want to skip this stage, we have provided segmentation masks for the lysosomes dataset located at ./data/lysosomes. You can enter this path in the configuration file provided, under dataset.test_dataset.dir.path, and then skip straight ahead to the section labelled DREEM Inference below</p>"},{"location":"Examples/microscopy-demo-simple/#install-cellpose","title":"Install CellPose","text":"<pre><code>!pip install git+https://www.github.com/mouseland/cellpose.git\n</code></pre> <pre><code>import tifffile\nfrom cellpose import models\n</code></pre> <pre><code>data_path = \"./data/dynamicnuclearnet/test_1\"\nsegmented_path = \"./data/dynamicnuclearnet/test_1_GT/TRA\"\nos.makedirs(segmented_path, exist_ok=True)\n</code></pre> <p>Set the approximate diameter (in pixels) of the instances you want to segment</p> <pre><code>diam_px = 25\n</code></pre>"},{"location":"Examples/microscopy-demo-simple/#run-detection-model","title":"Run detection model","text":"<pre><code>tiff_files = [f for f in os.listdir(data_path) if f.endswith('.tif') or f.endswith('.tiff')]\nstack = np.stack([tifffile.imread(os.path.join(data_path, f)) for f in tiff_files])\nframes, Y, X = stack.shape\n\nchannels = [0, 0]\n# use builtin latest model\nmodel = models.CellposeModel(gpu=True)\nall_masks = np.zeros_like(stack)\nfor i, img in enumerate(stack):\n    masks, flows, styles = model.eval(\n        img,\n        diameter=diam_px,\n        cellprob_threshold=0.0,\n        channels=channels,\n        z_axis=None,\n    )\n    all_masks[i] = masks\n</code></pre>"},{"location":"Examples/microscopy-demo-simple/#save-the-segmentation-masks","title":"Save the segmentation masks","text":"<pre><code>os.makedirs(segmented_path, exist_ok=True)\nfor i, (mask, filename) in enumerate(zip(all_masks, tiff_files)):\n    new_tiff_path = os.path.join(segmented_path, f\"{os.path.splitext(filename)[0]}.tif\")\n    print(f\"exporting frame {i} to tiff at {new_tiff_path}\")\n    tifffile.imwrite(new_tiff_path, mask)\n</code></pre>"},{"location":"Examples/microscopy-demo-simple/#view-the-segmentation-result-and-original-image","title":"View the segmentation result and original image","text":"<pre><code>fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\nax1.imshow(all_masks[0])\nax1.set_title('Segmentation Mask')\nax2.imshow(stack[0])\nax2.set_title('Original Image')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Examples/microscopy-demo-simple/#dreem-inference","title":"DREEM Inference","text":"<pre><code>pred_cfg_path = \"./configs/sample-eval-microscopy.yaml\"\npred_cfg = OmegaConf.load(pred_cfg_path)\n</code></pre> <pre><code>preds = track.run(pred_cfg)\n</code></pre>"},{"location":"Examples/microscopy-demo-simple/#visualize-the-results","title":"Visualize the results","text":"<pre><code>import cv2\nimport matplotlib.colors as mcolors\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib.patches import Circle\nimport matplotlib.cm as cm\nfrom IPython.display import HTML, display\nimport io\nimport base64\nfrom IPython.display import Video\n\ndef create_tracking_animation(video_path, metadata_df, \n                                             fps=30, text_size=8, marker_size=20,\n                                             max_frames=None, display_width=800):\n    \"\"\"\n    Create and display an animal tracking animation directly in the notebook.\n\n    Parameters:\n    -----------\n    video_path : str\n        Path to the input MP4 video file\n    metadata_df : pandas.DataFrame\n        DataFrame with columns: frame_id, track_id, centroid\n    fps : int\n        Frames per second for the animation\n    text_size : int\n        Size of the ID text\n    marker_size : int\n        Size of the marker circle\n    max_frames : int, optional\n        Maximum number of frames to process (useful for previewing)\n    display_width : int\n        Width of the displayed animation in the notebook\n    \"\"\"\n    # Open the video file\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        raise ValueError(f\"Could not open video file: {video_path}\")\n\n    # Get video properties\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Create a colormap for track IDs\n    unique_ids = metadata_df['track_id'].unique()\n    cmap = cm.get_cmap('viridis', len(unique_ids))  # Using 'hsv' for bright, distinct colors\n    id_to_color = {id_val: cmap(i) for i, id_val in enumerate(unique_ids)}\n\n    # Set up the figure and axis with the correct aspect ratio\n    fig_width = display_width / 100  # Convert to inches (assuming 100 dpi)\n    fig_height = fig_width * (height / width)\n    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n\n    # Initialize the plot elements\n    frame_img = ax.imshow(np.zeros((height, width, 3), dtype=np.uint8))\n    markers = []\n    texts = []\n\n    # Get the list of frame IDs from the metadata\n    frame_ids = sorted(metadata_df['frame_id'].unique())\n\n    # Limit the number of frames if specified\n    if max_frames is not None and max_frames &lt; len(frame_ids):\n        frame_ids = frame_ids[:max_frames]\n        print(f\"Limiting preview to {max_frames} frames\")\n\n    # Function to update the animation for each frame\n    def update(frame_num):\n        # Read the frame from the video\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n        ret, frame = cap.read()\n        if not ret:\n            print(f\"Failed to read frame {frame_num}\")\n            return []\n\n        # Convert BGR to RGB (OpenCV uses BGR, matplotlib uses RGB)\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frame_img.set_array(frame_rgb)\n\n        # Clear previous markers and texts\n        for marker in markers:\n            marker.remove()\n        markers.clear()\n\n        for text in texts:\n            text.remove()\n        texts.clear()\n\n        # Get data for the current frame\n        frame_data = metadata_df[metadata_df['frame_id'] == frame_num]\n\n        # Add markers and IDs for each animal in the current frame\n        for _, row in frame_data.iterrows():\n            track_id = row['track_id']\n            x, y = row['centroid']\n            color = id_to_color[track_id]\n\n            # Add circle marker\n            circle = Circle((x, y), marker_size, color=color, alpha=0.3)\n            markers.append(ax.add_patch(circle))\n\n            # Add ID text\n            text = ax.text(x, y, str(track_id), color='white', \n                          fontsize=text_size, ha='center', va='center', \n                          fontweight='bold')\n            texts.append(text)\n\n        # Add frame number for reference\n        frame_text = ax.text(10, 20, f\"Frame: {frame_num}\", color='white', \n                            fontsize=text_size, backgroundcolor='black')\n        texts.append(frame_text)\n\n        return [frame_img] + markers + texts\n\n    # Set up the axis\n    ax.set_xlim(0, width)\n    ax.set_ylim(height, 0)  # Invert y-axis to match image coordinates\n    ax.axis('off')\n\n    # Create the animation\n    print(f\"Creating animation with {len(frame_ids)} frames...\")\n    anim = FuncAnimation(fig, update, frames=frame_ids, blit=True)\n\n    # Display the animation in the notebook\n    plt.close(fig)  # Prevent duplicate display\n\n    # Display as HTML5 video\n    html_video = HTML(anim.to_html5_video())\n    display(html_video)\n\n    return anim\n\n# Option to save the animation to a file for later viewing\ndef save_animation(anim, output_path, fps=10, dpi=100):\n    \"\"\"Save the animation to a file\"\"\"\n    anim.save(output_path, writer='ffmpeg', fps=fps, dpi=dpi)\n    print(f\"Animation saved to {output_path}\")\n\n    # Display the saved video in the notebook\n    return Video(output_path, embed=True, width=800)\n</code></pre> <p>Load the predictions into a dataframe to make an animation</p> <pre><code>list_frames = []\nfor lf in preds:\n    for instance in lf.instances:\n        centroid = np.nanmean(instance.numpy(), axis=0)\n        track_id = int(instance.track.name)\n        list_frames.append({'frame_id': lf.frame_idx, 'track_id': track_id, 'centroid': centroid})\ndf = pd.DataFrame(list_frames)\n</code></pre> <p>Create and display the animation in the notebook</p> <pre><code>for file in os.listdir(os.path.join(pred_cfg.dataset.test_dataset['dir']['path'], \"mp4-for-visualization\")):\n    if file.endswith('.mp4'):\n        video_path = os.path.join(pred_cfg.dataset.test_dataset['dir']['path'], \"mp4-for-visualization\", file)\n\nanim = create_tracking_animation(\n    video_path=video_path,\n    metadata_df=df,\n    fps=15,\n    text_size=5,\n    marker_size=8,\n    max_frames=200\n)\n\n# save the animation\nvideo = save_animation(anim, f\"./tracking_vis-{video_path.split('/')[-1]}\")\n</code></pre>"},{"location":"configs/","title":"DREEM Config API","text":"<p>We utilize <code>.yaml</code> based configs with <code>hydra</code> and <code>omegaconf</code> for config parsing.</p>"},{"location":"configs/config/","title":"<code>Config</code> Parser","text":""},{"location":"configs/config/#dreem.io.Config","title":"<code>dreem.io.Config</code>","text":"<p>Class handling loading components based on config params.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class with config from hydra/omega conf.</p> <code>__repr__</code> <p>Object representation of config class.</p> <code>__str__</code> <p>Return a string representation of config class.</p> <code>from_yaml</code> <p>Load config directly from yaml.</p> <code>get</code> <p>Get config item.</p> <code>get_checkpointing</code> <p>Getter for lightning checkpointing callback.</p> <code>get_ctc_paths</code> <p>Get file paths from directory. Only for CTC datasets.</p> <code>get_data_paths</code> <p>Get file paths from directory. Only for SLEAP datasets.</p> <code>get_dataloader</code> <p>Getter for dataloader.</p> <code>get_dataset</code> <p>Getter for datasets.</p> <code>get_early_stopping</code> <p>Getter for lightning early stopping callback.</p> <code>get_gtr_runner</code> <p>Get lightning module for training, validation, and inference.</p> <code>get_logger</code> <p>Getter for logging callback.</p> <code>get_loss</code> <p>Getter for loss functions.</p> <code>get_model</code> <p>Getter for gtr model.</p> <code>get_optimizer</code> <p>Getter for optimizer.</p> <code>get_scheduler</code> <p>Getter for lr scheduler.</p> <code>get_tracker_cfg</code> <p>Getter for tracker config params.</p> <code>get_trainer</code> <p>Getter for the lightning trainer.</p> <code>set_hparams</code> <p>Setter function for overwriting specific hparams.</p> <p>Attributes:</p> Name Type Description <code>data_paths</code> <p>Get data paths.</p> Source code in <code>dreem/io/config.py</code> <pre><code>class Config:\n    \"\"\"Class handling loading components based on config params.\"\"\"\n\n    def __init__(self, cfg: DictConfig, params_cfg: DictConfig | None = None):\n        \"\"\"Initialize the class with config from hydra/omega conf.\n\n        First uses `base_param` file then overwrites with specific `params_config`.\n\n        Args:\n            cfg: The `DictConfig` containing all the hyperparameters needed for\n                training/evaluation.\n            params_cfg: The `DictConfig` containing subset of hyperparameters to override.\n                training/evaluation\n        \"\"\"\n        base_cfg = cfg\n        logger.info(f\"Base Config: {cfg}\")\n\n        if \"params_config\" in cfg:\n            params_cfg = OmegaConf.load(cfg.params_config)\n\n        if params_cfg:\n            logger.info(f\"Overwriting base config with {params_cfg}\")\n            with open_dict(base_cfg):\n                self.cfg = OmegaConf.merge(base_cfg, params_cfg)  # merge configs\n        else:\n            self.cfg = cfg\n\n        OmegaConf.set_struct(self.cfg, False)\n\n        self._vid_files = {}\n\n    def __repr__(self):\n        \"\"\"Object representation of config class.\"\"\"\n        return f\"Config({self.cfg})\"\n\n    def __str__(self):\n        \"\"\"Return a string representation of config class.\"\"\"\n        return f\"Config({self.cfg})\"\n\n    @classmethod\n    def from_yaml(cls, base_cfg_path: str, params_cfg_path: str | None = None) -&gt; None:\n        \"\"\"Load config directly from yaml.\n\n        Args:\n            base_cfg_path: path to base config file.\n            params_cfg_path: path to override params.\n        \"\"\"\n        base_cfg = OmegaConf.load(base_cfg_path)\n        params_cfg = OmegaConf.load(params_cfg_path) if params_cfg_path else None\n        return cls(base_cfg, params_cfg)\n\n    def set_hparams(self, hparams: dict) -&gt; bool:\n        \"\"\"Setter function for overwriting specific hparams.\n\n        Useful for changing 1 or 2 hyperparameters such as dataset.\n\n        Args:\n            hparams: A dict containing the hyperparameter to be overwritten and\n                the value to be changed\n\n        Returns:\n            `True` if config is successfully updated, `False` otherwise\n        \"\"\"\n        if hparams == {} or hparams is None:\n            logger.warning(\"Nothing to update!\")\n            return False\n        for hparam, val in hparams.items():\n            try:\n                OmegaConf.update(self.cfg, hparam, val)\n            except Exception as e:\n                logger.exception(f\"Failed to update {hparam} to {val} due to {e}\")\n                return False\n        return True\n\n    def get(self, key: str, default=None, cfg: dict = None):\n        \"\"\"Get config item.\n\n        Args:\n            key: key of item to return\n            default: default value to return if key is missing.\n            cfg: the config dict from which to retrieve an item\n        \"\"\"\n        if cfg is None:\n            cfg = self.cfg\n\n        param = cfg.get(key, default)\n\n        if isinstance(param, DictConfig):\n            param = OmegaConf.to_container(param, resolve=True)\n\n        return param\n\n    def get_model(self) -&gt; \"GlobalTrackingTransformer\":\n        \"\"\"Getter for gtr model.\n\n        Returns:\n            A global tracking transformer with parameters indicated by cfg\n        \"\"\"\n        from dreem.models import GlobalTrackingTransformer, GTRRunner\n\n        model_params = self.get(\"model\", {})\n\n        ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n        if ckpt_path is not None and len(ckpt_path) &gt; 0:\n            return GTRRunner.load_from_checkpoint(ckpt_path).model\n\n        return GlobalTrackingTransformer(**model_params)\n\n    def get_tracker_cfg(self) -&gt; dict:\n        \"\"\"Getter for tracker config params.\n\n        Returns:\n            A dict containing the init params for `Tracker`.\n        \"\"\"\n        return self.get(\"tracker\", {})\n\n    def get_gtr_runner(self, ckpt_path: str | None = None) -&gt; \"GTRRunner\":\n        \"\"\"Get lightning module for training, validation, and inference.\n\n        Args:\n            ckpt_path: path to checkpoint for override\n\n        Returns:\n            a gtr runner model\n        \"\"\"\n        from dreem.models import GTRRunner\n\n        keys = [\"tracker\", \"optimizer\", \"scheduler\", \"loss\", \"runner\", \"model\"]\n        args = [key + \"_cfg\" if key != \"runner\" else key for key in keys]\n\n        params = {}\n        for key, arg in zip(keys, args):\n            sub_params = self.get(key, {})\n\n            if len(sub_params) == 0:\n                logger.warning(\n                    f\"`{key}` not found in config or is empty. Using defaults for {arg}!\"\n                )\n\n            if key == \"runner\":\n                runner_params = sub_params\n                for k, v in runner_params.items():\n                    params[k] = v\n            else:\n                params[arg] = sub_params\n\n        ckpt_path = params[\"model_cfg\"].pop(\"ckpt_path\", None)\n\n        if ckpt_path is not None and ckpt_path != \"\":\n            model = GTRRunner.load_from_checkpoint(\n                ckpt_path, tracker_cfg=params[\"tracker_cfg\"], **runner_params\n            )\n\n        else:\n            model = GTRRunner(**params)\n\n        return model\n\n    def get_ctc_paths(\n        self, list_dir_path: list[str]\n    ) -&gt; tuple[list[str], list[str], list[str]]:\n        \"\"\"Get file paths from directory. Only for CTC datasets.\n\n        Args:\n            list_dir_path: list of directories to search for labels and videos\n\n        Returns:\n            lists of labels file paths and video file paths\n        \"\"\"\n        gt_list = []\n        raw_img_list = []\n        ctc_track_meta = []\n        # user can specify a list of directories, each of which can contain several subdirectories that come in pairs of (dset_name, dset_name_GT/TRA)\n        for dir_path in list_dir_path:\n            for subdir in os.listdir(dir_path):\n                if subdir.endswith(\"_GT\"):\n                    gt_path = os.path.join(dir_path, subdir, \"TRA\")\n                    raw_img_path = os.path.join(dir_path, subdir.replace(\"_GT\", \"\"))\n                    # get filepaths for all tif files in gt_path\n                    gt_list.append(glob.glob(os.path.join(gt_path, \"*.tif\")))\n                    # get filepaths for all tif files in raw_img_path\n                    raw_img_list.append(glob.glob(os.path.join(raw_img_path, \"*.tif\")))\n                    man_track_file = glob.glob(os.path.join(gt_path, \"man_track.txt\"))\n                    if len(man_track_file) &gt; 0:\n                        ctc_track_meta.append(man_track_file[0])\n                    else:\n                        logger.debug(\n                            f\"No man_track.txt file found in {gt_path}. Continuing...\"\n                        )\n                else:\n                    continue\n\n        return gt_list, raw_img_list, ctc_track_meta\n\n    def get_data_paths(self, mode: str, data_cfg: dict) -&gt; tuple[list[str], list[str]]:\n        \"\"\"Get file paths from directory. Only for SLEAP datasets.\n\n        Args:\n            mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n                train, val, or test params for dataset\n            data_cfg: Config for the dataset containing \"dir\" key.\n\n        Returns:\n            lists of labels file paths and video file paths respectively\n        \"\"\"\n        # hack to get around the fact that for test mode, get_data_paths is called before get_dataset.\n        # also, for train/val mode, data_cfg has had the dir key popped through self.get() called in get_dataset()\n        if mode == \"test\":\n            list_dir_path = data_cfg.get(\"dir\", {}).get(\"path\", None)\n            if list_dir_path is None:\n                raise ValueError(\n                    \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n                )\n            self.labels_suffix = data_cfg.get(\"dir\", {}).get(\"labels_suffix\")\n            self.vid_suffix = data_cfg.get(\"dir\", {}).get(\"vid_suffix\")\n        else:\n            list_dir_path = self.data_dirs\n        if not isinstance(list_dir_path, list):\n            list_dir_path = [list_dir_path]\n\n        if self.labels_suffix == \".slp\":\n            label_files = []\n            vid_files = []\n            for dir_path in list_dir_path:\n                logger.debug(f\"Searching `{dir_path}` directory\")\n                labels_path = f\"{dir_path}/*{self.labels_suffix}\"\n                vid_path = f\"{dir_path}/*{self.vid_suffix}\"\n                logger.debug(f\"Searching for labels matching {labels_path}\")\n                label_files.extend(glob.glob(labels_path))\n                logger.debug(f\"Searching for videos matching {vid_path}\")\n                vid_files.extend(glob.glob(vid_path))\n\n        elif self.labels_suffix == \".tif\":\n            label_files, vid_files, ctc_track_meta = self.get_ctc_paths(list_dir_path)\n\n        logger.debug(f\"Found {len(label_files)} labels and {len(vid_files)} videos\")\n\n        # backdoor to set label files directly in the configs (i.e. bypass dir.path)\n        if data_cfg.get(\"slp_files\", None):\n            logger.debug(\"Overriding label files with user provided list\")\n            slp_files = data_cfg.get(\"slp_files\")\n            if len(slp_files) &gt; 0:\n                label_files = slp_files\n        if data_cfg.get(\"video_files\", None):\n            individual_video_files = data_cfg.get(\"video_files\")\n            if len(individual_video_files) &gt; 0:\n                vid_files = individual_video_files\n        return label_files, vid_files\n\n    def get_dataset(\n        self,\n        mode: str,\n        label_files: list[str] | None = None,\n        vid_files: list[str | list[str]] = None,\n    ) -&gt; \"SleapDataset\" | \"CellTrackingDataset\":\n        \"\"\"Getter for datasets.\n\n        Args:\n            mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n                train, val, or test params for dataset\n            label_files: path to label_files for override\n            vid_files: path to vid_files for override\n\n        Returns:\n            Either a `SleapDataset` or `CellTrackingDataset` with params indicated by cfg\n        \"\"\"\n        from dreem.datasets import CellTrackingDataset, SleapDataset\n\n        dataset_params = self.get(\"dataset\")\n        if dataset_params is None:\n            raise KeyError(\"`dataset` key is missing from cfg!\")\n\n        if mode.lower() == \"train\":\n            dataset_params = self.get(\"train_dataset\", {}, dataset_params)\n        elif mode.lower() == \"val\":\n            dataset_params = self.get(\"val_dataset\", {}, dataset_params)\n        elif mode.lower() == \"test\":\n            dataset_params = self.get(\"test_dataset\", {}, dataset_params)\n        else:\n            raise ValueError(\n                \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n            )\n\n        # input validation\n        self.data_dirs = dataset_params.get(\"dir\", {}).get(\"path\", None)\n        self.labels_suffix = dataset_params.get(\"dir\", {}).get(\"labels_suffix\")\n        self.vid_suffix = dataset_params.get(\"dir\", {}).get(\"vid_suffix\")\n        if self.data_dirs is None:\n            raise ValueError(\n                \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n            )\n        if self.labels_suffix is None or self.vid_suffix is None:\n            raise KeyError(\n                f\"Must provide a labels suffix and vid suffix to search for but found {self.labels_suffix} and {self.vid_suffix}\"\n            )\n\n        # infer dataset type from the user provided suffix\n        if self.labels_suffix == \".slp\":\n            # during training, multiple files can be used at once, so label_files is not passed in\n            # during inference, a single label_files string can be passed in as get_data_paths is\n            # called before get_dataset, hence the check\n            if label_files is None or vid_files is None:\n                label_files, vid_files = self.get_data_paths(mode, dataset_params)\n            dataset_params[\"slp_files\"] = label_files\n            dataset_params[\"video_files\"] = vid_files\n            dataset_params[\"data_dirs\"] = self.data_dirs\n            self.data_paths = (mode, vid_files)\n\n            return SleapDataset(**dataset_params)\n\n        elif self.labels_suffix == \".tif\":\n            # for CTC datasets, pass in a list of gt and raw image directories, eaech of which contain tifs\n            ctc_track_meta = None\n            list_dir_path = self.data_dirs  # don't modify self.data_dirs\n            if not isinstance(list_dir_path, list):\n                list_dir_path = [list_dir_path]\n            if label_files is None or vid_files is None:\n                label_files, vid_files, ctc_track_meta = self.get_ctc_paths(\n                    list_dir_path\n                )\n            dataset_params[\"data_dirs\"] = self.data_dirs\n            # extract filepaths of all raw images and gt images (i.e. labelled masks)\n            dataset_params[\"gt_list\"] = label_files\n            dataset_params[\"raw_img_list\"] = vid_files\n            dataset_params[\"ctc_track_meta\"] = ctc_track_meta\n\n            return CellTrackingDataset(**dataset_params)\n\n        else:\n            raise ValueError(\n                \"Could not resolve dataset type from Config! Only .slp (SLEAP) and .tif (Cell Tracking Challenge) data formats are supported.\"\n            )\n\n    @property\n    def data_paths(self):\n        \"\"\"Get data paths.\"\"\"\n        return self._vid_files\n\n    @data_paths.setter\n    def data_paths(self, paths: tuple[str, list[str]]):\n        \"\"\"Set data paths.\n\n        Args:\n            paths: A tuple containing (mode, vid_files)\n        \"\"\"\n        mode, vid_files = paths\n        self._vid_files[mode] = vid_files\n\n    def get_dataloader(\n        self,\n        dataset: \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\",\n        mode: str,\n    ) -&gt; torch.utils.data.DataLoader:\n        \"\"\"Getter for dataloader.\n\n        Args:\n            dataset: the Sleap or Microscopy Dataset used to initialize the dataloader\n            mode: either [\"train\", \"val\", or \"test\"] indicates which dataset\n                config to use\n\n        Returns:\n            A torch dataloader for `dataset` with parameters configured as specified\n        \"\"\"\n        dataloader_params = self.get(\"dataloader\", {})\n        if mode.lower() == \"train\":\n            dataloader_params = self.get(\"train_dataloader\", {}, dataloader_params)\n        elif mode.lower() == \"val\":\n            dataloader_params = self.get(\"val_dataloader\", {}, dataloader_params)\n        elif mode.lower() == \"test\":\n            dataloader_params = self.get(\"test_dataloader\", {}, dataloader_params)\n        else:\n            raise ValueError(\n                \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n            )\n        if dataloader_params.get(\"num_workers\", 0) &gt; 0:\n            # prevent too many open files error\n            pin_memory = True\n            torch.multiprocessing.set_sharing_strategy(\"file_system\")\n        else:\n            pin_memory = False\n\n        return torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size=1,\n            pin_memory=pin_memory,\n            collate_fn=dataset.no_batching_fn,\n            **dataloader_params,\n        )\n\n    def get_optimizer(self, params: Iterable) -&gt; torch.optim.Optimizer:\n        \"\"\"Getter for optimizer.\n\n        Args:\n            params: iterable of model parameters to optimize or dicts defining\n                parameter groups\n\n        Returns:\n            A torch Optimizer with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_optimizer\n\n        optimizer_params = self.get(\"optimizer\")\n\n        return init_optimizer(params, optimizer_params)\n\n    def get_scheduler(\n        self, optimizer: torch.optim.Optimizer\n    ) -&gt; torch.optim.lr_scheduler.LRScheduler | None:\n        \"\"\"Getter for lr scheduler.\n\n        Args:\n            optimizer: The optimizer to wrap the scheduler around\n\n        Returns:\n            A torch learning rate scheduler with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_scheduler\n\n        lr_scheduler_params = self.get(\"scheduler\")\n\n        if lr_scheduler_params is None:\n            logger.warning(\n                \"`scheduler` key not found in cfg or is empty. No scheduler will be returned!\"\n            )\n            return None\n        return init_scheduler(optimizer, lr_scheduler_params)\n\n    def get_loss(self) -&gt; \"dreem.training.losses.AssoLoss\":\n        \"\"\"Getter for loss functions.\n\n        Returns:\n            An AssoLoss with specified params\n        \"\"\"\n        from dreem.training.losses import AssoLoss\n\n        loss_params = self.get(\"loss\", {})\n\n        if len(loss_params) == 0:\n            logger.warning(\n                \"`loss` key not found in cfg. Using default params for `AssoLoss`\"\n            )\n\n        return AssoLoss(**loss_params)\n\n    def get_logger(self) -&gt; pl.loggers.Logger:\n        \"\"\"Getter for logging callback.\n\n        Returns:\n            A Logger with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_logger\n\n        logger_params = self.get(\"logging\", {})\n        if len(logger_params) == 0:\n            logger.warning(\n                \"`logging` key not found in cfg. No logger will be configured!\"\n            )\n\n        return init_logger(\n            logger_params, OmegaConf.to_container(self.cfg, resolve=True)\n        )\n\n    def get_early_stopping(self) -&gt; pl.callbacks.EarlyStopping:\n        \"\"\"Getter for lightning early stopping callback.\n\n        Returns:\n            A lightning early stopping callback with specified params\n        \"\"\"\n        early_stopping_params = self.get(\"early_stopping\", None)\n\n        if early_stopping_params is None:\n            logger.warning(\n                \"`early_stopping` was not found in cfg or was `null`. Early stopping will not be used!\"\n            )\n            return None\n        elif len(early_stopping_params) == 0:\n            logger.warning(\"`early_stopping` cfg is empty! Using defaults\")\n        return pl.callbacks.EarlyStopping(**early_stopping_params)\n\n    def get_checkpointing(self) -&gt; pl.callbacks.ModelCheckpoint:\n        \"\"\"Getter for lightning checkpointing callback.\n\n        Returns:\n            A lightning checkpointing callback with specified params\n        \"\"\"\n        # convert to dict to enable extracting/removing params\n        checkpoint_params = self.get(\"checkpointing\", {})\n        logging_params = self.get(\"logging\", {})\n\n        dirpath = checkpoint_params.pop(\"dirpath\", None)\n\n        if dirpath is None:\n            dirpath = f\"./models/{self.get('group', '', logging_params)}/{self.get('name', '', logging_params)}\"\n\n        dirpath = Path(dirpath).resolve()\n        if not Path(dirpath).exists():\n            try:\n                Path(dirpath).mkdir(parents=True, exist_ok=True)\n            except OSError as e:\n                logger.exception(\n                    f\"Cannot create a new folder!. Check the permissions to {dirpath}. \\n {e}\"\n                )\n\n        _ = checkpoint_params.pop(\"dirpath\", None)\n        monitor = checkpoint_params.pop(\"monitor\", [\"val_loss\"])\n        checkpointers = []\n\n        logger.info(\n            f\"Saving checkpoints to `{dirpath}` based on the following metrics: {monitor}\"\n        )\n        if len(checkpoint_params) == 0:\n            logger.warning(\n                \"\"\"`checkpointing` key was not found in cfg or was empty!\n                Configuring checkpointing to use default params!\"\"\"\n            )\n\n        for metric in monitor:\n            checkpointer = pl.callbacks.ModelCheckpoint(\n                monitor=metric,\n                dirpath=dirpath,\n                filename=f\"{{epoch}}-{{{metric}}}\",\n                **checkpoint_params,\n            )\n            checkpointer.CHECKPOINT_NAME_LAST = f\"{{epoch}}-final-{{{metric}}}\"\n            checkpointers.append(checkpointer)\n        return checkpointers\n\n    def get_trainer(\n        self,\n        callbacks: list[pl.callbacks.Callback] | None = None,\n        logger: pl.loggers.WandbLogger | None = None,\n        devices: int = 1,\n        accelerator: str = \"auto\",\n    ) -&gt; pl.Trainer:\n        \"\"\"Getter for the lightning trainer.\n\n        Args:\n            callbacks: a list of lightning callbacks preconfigured to be used\n                for training\n            logger: the Wandb logger used for logging during training\n            devices: The number of gpus to be used. 0 means cpu\n            accelerator: either \"gpu\" or \"cpu\" specifies which device to use\n\n        Returns:\n            A lightning Trainer with specified params\n        \"\"\"\n        trainer_params = self.get(\"trainer\", {})\n        profiler = trainer_params.pop(\"profiler\", None)\n        if len(trainer_params) == 0:\n            print(\n                \"`trainer` key was not found in cfg or was empty. Using defaults for `pl.Trainer`!\"\n            )\n\n        if \"accelerator\" not in trainer_params:\n            trainer_params[\"accelerator\"] = accelerator\n        if \"devices\" not in trainer_params:\n            trainer_params[\"devices\"] = devices\n\n        map_profiler = {\n            \"advanced\": pl.profilers.AdvancedProfiler,\n            \"simple\": pl.profilers.SimpleProfiler,\n            \"pytorch\": pl.profilers.PyTorchProfiler,\n            \"passthrough\": pl.profilers.PassThroughProfiler,\n            \"xla\": pl.profilers.XLAProfiler,\n        }\n\n        if profiler:\n            if profiler in map_profiler:\n                profiler = map_profiler[profiler](filename=\"profile\")\n            else:\n                raise ValueError(\n                    f\"Profiler {profiler} not supported! Please use one of {list(map_profiler.keys())}\"\n                )\n\n        return pl.Trainer(\n            callbacks=callbacks,\n            logger=logger,\n            profiler=profiler,\n            **trainer_params,\n        )\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.data_paths","title":"<code>data_paths</code>  <code>property</code> <code>writable</code>","text":"<p>Get data paths.</p>"},{"location":"configs/config/#dreem.io.Config.__init__","title":"<code>__init__(cfg, params_cfg=None)</code>","text":"<p>Initialize the class with config from hydra/omega conf.</p> <p>First uses <code>base_param</code> file then overwrites with specific <code>params_config</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>The <code>DictConfig</code> containing all the hyperparameters needed for training/evaluation.</p> required <code>params_cfg</code> <code>DictConfig | None</code> <p>The <code>DictConfig</code> containing subset of hyperparameters to override. training/evaluation</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>def __init__(self, cfg: DictConfig, params_cfg: DictConfig | None = None):\n    \"\"\"Initialize the class with config from hydra/omega conf.\n\n    First uses `base_param` file then overwrites with specific `params_config`.\n\n    Args:\n        cfg: The `DictConfig` containing all the hyperparameters needed for\n            training/evaluation.\n        params_cfg: The `DictConfig` containing subset of hyperparameters to override.\n            training/evaluation\n    \"\"\"\n    base_cfg = cfg\n    logger.info(f\"Base Config: {cfg}\")\n\n    if \"params_config\" in cfg:\n        params_cfg = OmegaConf.load(cfg.params_config)\n\n    if params_cfg:\n        logger.info(f\"Overwriting base config with {params_cfg}\")\n        with open_dict(base_cfg):\n            self.cfg = OmegaConf.merge(base_cfg, params_cfg)  # merge configs\n    else:\n        self.cfg = cfg\n\n    OmegaConf.set_struct(self.cfg, False)\n\n    self._vid_files = {}\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.__repr__","title":"<code>__repr__()</code>","text":"<p>Object representation of config class.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def __repr__(self):\n    \"\"\"Object representation of config class.\"\"\"\n    return f\"Config({self.cfg})\"\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of config class.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def __str__(self):\n    \"\"\"Return a string representation of config class.\"\"\"\n    return f\"Config({self.cfg})\"\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.from_yaml","title":"<code>from_yaml(base_cfg_path, params_cfg_path=None)</code>  <code>classmethod</code>","text":"<p>Load config directly from yaml.</p> <p>Parameters:</p> Name Type Description Default <code>base_cfg_path</code> <code>str</code> <p>path to base config file.</p> required <code>params_cfg_path</code> <code>str | None</code> <p>path to override params.</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>@classmethod\ndef from_yaml(cls, base_cfg_path: str, params_cfg_path: str | None = None) -&gt; None:\n    \"\"\"Load config directly from yaml.\n\n    Args:\n        base_cfg_path: path to base config file.\n        params_cfg_path: path to override params.\n    \"\"\"\n    base_cfg = OmegaConf.load(base_cfg_path)\n    params_cfg = OmegaConf.load(params_cfg_path) if params_cfg_path else None\n    return cls(base_cfg, params_cfg)\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get","title":"<code>get(key, default=None, cfg=None)</code>","text":"<p>Get config item.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key of item to return</p> required <code>default</code> <p>default value to return if key is missing.</p> <code>None</code> <code>cfg</code> <code>dict</code> <p>the config dict from which to retrieve an item</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>def get(self, key: str, default=None, cfg: dict = None):\n    \"\"\"Get config item.\n\n    Args:\n        key: key of item to return\n        default: default value to return if key is missing.\n        cfg: the config dict from which to retrieve an item\n    \"\"\"\n    if cfg is None:\n        cfg = self.cfg\n\n    param = cfg.get(key, default)\n\n    if isinstance(param, DictConfig):\n        param = OmegaConf.to_container(param, resolve=True)\n\n    return param\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_checkpointing","title":"<code>get_checkpointing()</code>","text":"<p>Getter for lightning checkpointing callback.</p> <p>Returns:</p> Type Description <code>ModelCheckpoint</code> <p>A lightning checkpointing callback with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_checkpointing(self) -&gt; pl.callbacks.ModelCheckpoint:\n    \"\"\"Getter for lightning checkpointing callback.\n\n    Returns:\n        A lightning checkpointing callback with specified params\n    \"\"\"\n    # convert to dict to enable extracting/removing params\n    checkpoint_params = self.get(\"checkpointing\", {})\n    logging_params = self.get(\"logging\", {})\n\n    dirpath = checkpoint_params.pop(\"dirpath\", None)\n\n    if dirpath is None:\n        dirpath = f\"./models/{self.get('group', '', logging_params)}/{self.get('name', '', logging_params)}\"\n\n    dirpath = Path(dirpath).resolve()\n    if not Path(dirpath).exists():\n        try:\n            Path(dirpath).mkdir(parents=True, exist_ok=True)\n        except OSError as e:\n            logger.exception(\n                f\"Cannot create a new folder!. Check the permissions to {dirpath}. \\n {e}\"\n            )\n\n    _ = checkpoint_params.pop(\"dirpath\", None)\n    monitor = checkpoint_params.pop(\"monitor\", [\"val_loss\"])\n    checkpointers = []\n\n    logger.info(\n        f\"Saving checkpoints to `{dirpath}` based on the following metrics: {monitor}\"\n    )\n    if len(checkpoint_params) == 0:\n        logger.warning(\n            \"\"\"`checkpointing` key was not found in cfg or was empty!\n            Configuring checkpointing to use default params!\"\"\"\n        )\n\n    for metric in monitor:\n        checkpointer = pl.callbacks.ModelCheckpoint(\n            monitor=metric,\n            dirpath=dirpath,\n            filename=f\"{{epoch}}-{{{metric}}}\",\n            **checkpoint_params,\n        )\n        checkpointer.CHECKPOINT_NAME_LAST = f\"{{epoch}}-final-{{{metric}}}\"\n        checkpointers.append(checkpointer)\n    return checkpointers\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_ctc_paths","title":"<code>get_ctc_paths(list_dir_path)</code>","text":"<p>Get file paths from directory. Only for CTC datasets.</p> <p>Parameters:</p> Name Type Description Default <code>list_dir_path</code> <code>list[str]</code> <p>list of directories to search for labels and videos</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str], list[str]]</code> <p>lists of labels file paths and video file paths</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_ctc_paths(\n    self, list_dir_path: list[str]\n) -&gt; tuple[list[str], list[str], list[str]]:\n    \"\"\"Get file paths from directory. Only for CTC datasets.\n\n    Args:\n        list_dir_path: list of directories to search for labels and videos\n\n    Returns:\n        lists of labels file paths and video file paths\n    \"\"\"\n    gt_list = []\n    raw_img_list = []\n    ctc_track_meta = []\n    # user can specify a list of directories, each of which can contain several subdirectories that come in pairs of (dset_name, dset_name_GT/TRA)\n    for dir_path in list_dir_path:\n        for subdir in os.listdir(dir_path):\n            if subdir.endswith(\"_GT\"):\n                gt_path = os.path.join(dir_path, subdir, \"TRA\")\n                raw_img_path = os.path.join(dir_path, subdir.replace(\"_GT\", \"\"))\n                # get filepaths for all tif files in gt_path\n                gt_list.append(glob.glob(os.path.join(gt_path, \"*.tif\")))\n                # get filepaths for all tif files in raw_img_path\n                raw_img_list.append(glob.glob(os.path.join(raw_img_path, \"*.tif\")))\n                man_track_file = glob.glob(os.path.join(gt_path, \"man_track.txt\"))\n                if len(man_track_file) &gt; 0:\n                    ctc_track_meta.append(man_track_file[0])\n                else:\n                    logger.debug(\n                        f\"No man_track.txt file found in {gt_path}. Continuing...\"\n                    )\n            else:\n                continue\n\n    return gt_list, raw_img_list, ctc_track_meta\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_data_paths","title":"<code>get_data_paths(mode, data_cfg)</code>","text":"<p>Get file paths from directory. Only for SLEAP datasets.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>[None, \"train\", \"test\", \"val\"]. Indicates whether to use train, val, or test params for dataset</p> required <code>data_cfg</code> <code>dict</code> <p>Config for the dataset containing \"dir\" key.</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str]]</code> <p>lists of labels file paths and video file paths respectively</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_data_paths(self, mode: str, data_cfg: dict) -&gt; tuple[list[str], list[str]]:\n    \"\"\"Get file paths from directory. Only for SLEAP datasets.\n\n    Args:\n        mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n            train, val, or test params for dataset\n        data_cfg: Config for the dataset containing \"dir\" key.\n\n    Returns:\n        lists of labels file paths and video file paths respectively\n    \"\"\"\n    # hack to get around the fact that for test mode, get_data_paths is called before get_dataset.\n    # also, for train/val mode, data_cfg has had the dir key popped through self.get() called in get_dataset()\n    if mode == \"test\":\n        list_dir_path = data_cfg.get(\"dir\", {}).get(\"path\", None)\n        if list_dir_path is None:\n            raise ValueError(\n                \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n            )\n        self.labels_suffix = data_cfg.get(\"dir\", {}).get(\"labels_suffix\")\n        self.vid_suffix = data_cfg.get(\"dir\", {}).get(\"vid_suffix\")\n    else:\n        list_dir_path = self.data_dirs\n    if not isinstance(list_dir_path, list):\n        list_dir_path = [list_dir_path]\n\n    if self.labels_suffix == \".slp\":\n        label_files = []\n        vid_files = []\n        for dir_path in list_dir_path:\n            logger.debug(f\"Searching `{dir_path}` directory\")\n            labels_path = f\"{dir_path}/*{self.labels_suffix}\"\n            vid_path = f\"{dir_path}/*{self.vid_suffix}\"\n            logger.debug(f\"Searching for labels matching {labels_path}\")\n            label_files.extend(glob.glob(labels_path))\n            logger.debug(f\"Searching for videos matching {vid_path}\")\n            vid_files.extend(glob.glob(vid_path))\n\n    elif self.labels_suffix == \".tif\":\n        label_files, vid_files, ctc_track_meta = self.get_ctc_paths(list_dir_path)\n\n    logger.debug(f\"Found {len(label_files)} labels and {len(vid_files)} videos\")\n\n    # backdoor to set label files directly in the configs (i.e. bypass dir.path)\n    if data_cfg.get(\"slp_files\", None):\n        logger.debug(\"Overriding label files with user provided list\")\n        slp_files = data_cfg.get(\"slp_files\")\n        if len(slp_files) &gt; 0:\n            label_files = slp_files\n    if data_cfg.get(\"video_files\", None):\n        individual_video_files = data_cfg.get(\"video_files\")\n        if len(individual_video_files) &gt; 0:\n            vid_files = individual_video_files\n    return label_files, vid_files\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_dataloader","title":"<code>get_dataloader(dataset, mode)</code>","text":"<p>Getter for dataloader.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>'SleapDataset' | 'MicroscopyDataset' | 'CellTrackingDataset'</code> <p>the Sleap or Microscopy Dataset used to initialize the dataloader</p> required <code>mode</code> <code>str</code> <p>either [\"train\", \"val\", or \"test\"] indicates which dataset config to use</p> required <p>Returns:</p> Type Description <code>DataLoader</code> <p>A torch dataloader for <code>dataset</code> with parameters configured as specified</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_dataloader(\n    self,\n    dataset: \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\",\n    mode: str,\n) -&gt; torch.utils.data.DataLoader:\n    \"\"\"Getter for dataloader.\n\n    Args:\n        dataset: the Sleap or Microscopy Dataset used to initialize the dataloader\n        mode: either [\"train\", \"val\", or \"test\"] indicates which dataset\n            config to use\n\n    Returns:\n        A torch dataloader for `dataset` with parameters configured as specified\n    \"\"\"\n    dataloader_params = self.get(\"dataloader\", {})\n    if mode.lower() == \"train\":\n        dataloader_params = self.get(\"train_dataloader\", {}, dataloader_params)\n    elif mode.lower() == \"val\":\n        dataloader_params = self.get(\"val_dataloader\", {}, dataloader_params)\n    elif mode.lower() == \"test\":\n        dataloader_params = self.get(\"test_dataloader\", {}, dataloader_params)\n    else:\n        raise ValueError(\n            \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n        )\n    if dataloader_params.get(\"num_workers\", 0) &gt; 0:\n        # prevent too many open files error\n        pin_memory = True\n        torch.multiprocessing.set_sharing_strategy(\"file_system\")\n    else:\n        pin_memory = False\n\n    return torch.utils.data.DataLoader(\n        dataset=dataset,\n        batch_size=1,\n        pin_memory=pin_memory,\n        collate_fn=dataset.no_batching_fn,\n        **dataloader_params,\n    )\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_dataset","title":"<code>get_dataset(mode, label_files=None, vid_files=None)</code>","text":"<p>Getter for datasets.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>[None, \"train\", \"test\", \"val\"]. Indicates whether to use train, val, or test params for dataset</p> required <code>label_files</code> <code>list[str] | None</code> <p>path to label_files for override</p> <code>None</code> <code>vid_files</code> <code>list[str | list[str]]</code> <p>path to vid_files for override</p> <code>None</code> <p>Returns:</p> Type Description <code>'SleapDataset' | 'CellTrackingDataset'</code> <p>Either a <code>SleapDataset</code> or <code>CellTrackingDataset</code> with params indicated by cfg</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_dataset(\n    self,\n    mode: str,\n    label_files: list[str] | None = None,\n    vid_files: list[str | list[str]] = None,\n) -&gt; \"SleapDataset\" | \"CellTrackingDataset\":\n    \"\"\"Getter for datasets.\n\n    Args:\n        mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n            train, val, or test params for dataset\n        label_files: path to label_files for override\n        vid_files: path to vid_files for override\n\n    Returns:\n        Either a `SleapDataset` or `CellTrackingDataset` with params indicated by cfg\n    \"\"\"\n    from dreem.datasets import CellTrackingDataset, SleapDataset\n\n    dataset_params = self.get(\"dataset\")\n    if dataset_params is None:\n        raise KeyError(\"`dataset` key is missing from cfg!\")\n\n    if mode.lower() == \"train\":\n        dataset_params = self.get(\"train_dataset\", {}, dataset_params)\n    elif mode.lower() == \"val\":\n        dataset_params = self.get(\"val_dataset\", {}, dataset_params)\n    elif mode.lower() == \"test\":\n        dataset_params = self.get(\"test_dataset\", {}, dataset_params)\n    else:\n        raise ValueError(\n            \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n        )\n\n    # input validation\n    self.data_dirs = dataset_params.get(\"dir\", {}).get(\"path\", None)\n    self.labels_suffix = dataset_params.get(\"dir\", {}).get(\"labels_suffix\")\n    self.vid_suffix = dataset_params.get(\"dir\", {}).get(\"vid_suffix\")\n    if self.data_dirs is None:\n        raise ValueError(\n            \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n        )\n    if self.labels_suffix is None or self.vid_suffix is None:\n        raise KeyError(\n            f\"Must provide a labels suffix and vid suffix to search for but found {self.labels_suffix} and {self.vid_suffix}\"\n        )\n\n    # infer dataset type from the user provided suffix\n    if self.labels_suffix == \".slp\":\n        # during training, multiple files can be used at once, so label_files is not passed in\n        # during inference, a single label_files string can be passed in as get_data_paths is\n        # called before get_dataset, hence the check\n        if label_files is None or vid_files is None:\n            label_files, vid_files = self.get_data_paths(mode, dataset_params)\n        dataset_params[\"slp_files\"] = label_files\n        dataset_params[\"video_files\"] = vid_files\n        dataset_params[\"data_dirs\"] = self.data_dirs\n        self.data_paths = (mode, vid_files)\n\n        return SleapDataset(**dataset_params)\n\n    elif self.labels_suffix == \".tif\":\n        # for CTC datasets, pass in a list of gt and raw image directories, eaech of which contain tifs\n        ctc_track_meta = None\n        list_dir_path = self.data_dirs  # don't modify self.data_dirs\n        if not isinstance(list_dir_path, list):\n            list_dir_path = [list_dir_path]\n        if label_files is None or vid_files is None:\n            label_files, vid_files, ctc_track_meta = self.get_ctc_paths(\n                list_dir_path\n            )\n        dataset_params[\"data_dirs\"] = self.data_dirs\n        # extract filepaths of all raw images and gt images (i.e. labelled masks)\n        dataset_params[\"gt_list\"] = label_files\n        dataset_params[\"raw_img_list\"] = vid_files\n        dataset_params[\"ctc_track_meta\"] = ctc_track_meta\n\n        return CellTrackingDataset(**dataset_params)\n\n    else:\n        raise ValueError(\n            \"Could not resolve dataset type from Config! Only .slp (SLEAP) and .tif (Cell Tracking Challenge) data formats are supported.\"\n        )\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_early_stopping","title":"<code>get_early_stopping()</code>","text":"<p>Getter for lightning early stopping callback.</p> <p>Returns:</p> Type Description <code>EarlyStopping</code> <p>A lightning early stopping callback with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_early_stopping(self) -&gt; pl.callbacks.EarlyStopping:\n    \"\"\"Getter for lightning early stopping callback.\n\n    Returns:\n        A lightning early stopping callback with specified params\n    \"\"\"\n    early_stopping_params = self.get(\"early_stopping\", None)\n\n    if early_stopping_params is None:\n        logger.warning(\n            \"`early_stopping` was not found in cfg or was `null`. Early stopping will not be used!\"\n        )\n        return None\n    elif len(early_stopping_params) == 0:\n        logger.warning(\"`early_stopping` cfg is empty! Using defaults\")\n    return pl.callbacks.EarlyStopping(**early_stopping_params)\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_gtr_runner","title":"<code>get_gtr_runner(ckpt_path=None)</code>","text":"<p>Get lightning module for training, validation, and inference.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_path</code> <code>str | None</code> <p>path to checkpoint for override</p> <code>None</code> <p>Returns:</p> Type Description <code>'GTRRunner'</code> <p>a gtr runner model</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_gtr_runner(self, ckpt_path: str | None = None) -&gt; \"GTRRunner\":\n    \"\"\"Get lightning module for training, validation, and inference.\n\n    Args:\n        ckpt_path: path to checkpoint for override\n\n    Returns:\n        a gtr runner model\n    \"\"\"\n    from dreem.models import GTRRunner\n\n    keys = [\"tracker\", \"optimizer\", \"scheduler\", \"loss\", \"runner\", \"model\"]\n    args = [key + \"_cfg\" if key != \"runner\" else key for key in keys]\n\n    params = {}\n    for key, arg in zip(keys, args):\n        sub_params = self.get(key, {})\n\n        if len(sub_params) == 0:\n            logger.warning(\n                f\"`{key}` not found in config or is empty. Using defaults for {arg}!\"\n            )\n\n        if key == \"runner\":\n            runner_params = sub_params\n            for k, v in runner_params.items():\n                params[k] = v\n        else:\n            params[arg] = sub_params\n\n    ckpt_path = params[\"model_cfg\"].pop(\"ckpt_path\", None)\n\n    if ckpt_path is not None and ckpt_path != \"\":\n        model = GTRRunner.load_from_checkpoint(\n            ckpt_path, tracker_cfg=params[\"tracker_cfg\"], **runner_params\n        )\n\n    else:\n        model = GTRRunner(**params)\n\n    return model\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_logger","title":"<code>get_logger()</code>","text":"<p>Getter for logging callback.</p> <p>Returns:</p> Type Description <code>Logger</code> <p>A Logger with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_logger(self) -&gt; pl.loggers.Logger:\n    \"\"\"Getter for logging callback.\n\n    Returns:\n        A Logger with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_logger\n\n    logger_params = self.get(\"logging\", {})\n    if len(logger_params) == 0:\n        logger.warning(\n            \"`logging` key not found in cfg. No logger will be configured!\"\n        )\n\n    return init_logger(\n        logger_params, OmegaConf.to_container(self.cfg, resolve=True)\n    )\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_loss","title":"<code>get_loss()</code>","text":"<p>Getter for loss functions.</p> <p>Returns:</p> Type Description <code>'dreem.training.losses.AssoLoss'</code> <p>An AssoLoss with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_loss(self) -&gt; \"dreem.training.losses.AssoLoss\":\n    \"\"\"Getter for loss functions.\n\n    Returns:\n        An AssoLoss with specified params\n    \"\"\"\n    from dreem.training.losses import AssoLoss\n\n    loss_params = self.get(\"loss\", {})\n\n    if len(loss_params) == 0:\n        logger.warning(\n            \"`loss` key not found in cfg. Using default params for `AssoLoss`\"\n        )\n\n    return AssoLoss(**loss_params)\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_model","title":"<code>get_model()</code>","text":"<p>Getter for gtr model.</p> <p>Returns:</p> Type Description <code>'GlobalTrackingTransformer'</code> <p>A global tracking transformer with parameters indicated by cfg</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_model(self) -&gt; \"GlobalTrackingTransformer\":\n    \"\"\"Getter for gtr model.\n\n    Returns:\n        A global tracking transformer with parameters indicated by cfg\n    \"\"\"\n    from dreem.models import GlobalTrackingTransformer, GTRRunner\n\n    model_params = self.get(\"model\", {})\n\n    ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n    if ckpt_path is not None and len(ckpt_path) &gt; 0:\n        return GTRRunner.load_from_checkpoint(ckpt_path).model\n\n    return GlobalTrackingTransformer(**model_params)\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_optimizer","title":"<code>get_optimizer(params)</code>","text":"<p>Getter for optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable</code> <p>iterable of model parameters to optimize or dicts defining parameter groups</p> required <p>Returns:</p> Type Description <code>Optimizer</code> <p>A torch Optimizer with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_optimizer(self, params: Iterable) -&gt; torch.optim.Optimizer:\n    \"\"\"Getter for optimizer.\n\n    Args:\n        params: iterable of model parameters to optimize or dicts defining\n            parameter groups\n\n    Returns:\n        A torch Optimizer with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_optimizer\n\n    optimizer_params = self.get(\"optimizer\")\n\n    return init_optimizer(params, optimizer_params)\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_scheduler","title":"<code>get_scheduler(optimizer)</code>","text":"<p>Getter for lr scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to wrap the scheduler around</p> required <p>Returns:</p> Type Description <code>LRScheduler | None</code> <p>A torch learning rate scheduler with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_scheduler(\n    self, optimizer: torch.optim.Optimizer\n) -&gt; torch.optim.lr_scheduler.LRScheduler | None:\n    \"\"\"Getter for lr scheduler.\n\n    Args:\n        optimizer: The optimizer to wrap the scheduler around\n\n    Returns:\n        A torch learning rate scheduler with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_scheduler\n\n    lr_scheduler_params = self.get(\"scheduler\")\n\n    if lr_scheduler_params is None:\n        logger.warning(\n            \"`scheduler` key not found in cfg or is empty. No scheduler will be returned!\"\n        )\n        return None\n    return init_scheduler(optimizer, lr_scheduler_params)\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_tracker_cfg","title":"<code>get_tracker_cfg()</code>","text":"<p>Getter for tracker config params.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict containing the init params for <code>Tracker</code>.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_tracker_cfg(self) -&gt; dict:\n    \"\"\"Getter for tracker config params.\n\n    Returns:\n        A dict containing the init params for `Tracker`.\n    \"\"\"\n    return self.get(\"tracker\", {})\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.get_trainer","title":"<code>get_trainer(callbacks=None, logger=None, devices=1, accelerator='auto')</code>","text":"<p>Getter for the lightning trainer.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[Callback] | None</code> <p>a list of lightning callbacks preconfigured to be used for training</p> <code>None</code> <code>logger</code> <code>WandbLogger | None</code> <p>the Wandb logger used for logging during training</p> <code>None</code> <code>devices</code> <code>int</code> <p>The number of gpus to be used. 0 means cpu</p> <code>1</code> <code>accelerator</code> <code>str</code> <p>either \"gpu\" or \"cpu\" specifies which device to use</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>Trainer</code> <p>A lightning Trainer with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_trainer(\n    self,\n    callbacks: list[pl.callbacks.Callback] | None = None,\n    logger: pl.loggers.WandbLogger | None = None,\n    devices: int = 1,\n    accelerator: str = \"auto\",\n) -&gt; pl.Trainer:\n    \"\"\"Getter for the lightning trainer.\n\n    Args:\n        callbacks: a list of lightning callbacks preconfigured to be used\n            for training\n        logger: the Wandb logger used for logging during training\n        devices: The number of gpus to be used. 0 means cpu\n        accelerator: either \"gpu\" or \"cpu\" specifies which device to use\n\n    Returns:\n        A lightning Trainer with specified params\n    \"\"\"\n    trainer_params = self.get(\"trainer\", {})\n    profiler = trainer_params.pop(\"profiler\", None)\n    if len(trainer_params) == 0:\n        print(\n            \"`trainer` key was not found in cfg or was empty. Using defaults for `pl.Trainer`!\"\n        )\n\n    if \"accelerator\" not in trainer_params:\n        trainer_params[\"accelerator\"] = accelerator\n    if \"devices\" not in trainer_params:\n        trainer_params[\"devices\"] = devices\n\n    map_profiler = {\n        \"advanced\": pl.profilers.AdvancedProfiler,\n        \"simple\": pl.profilers.SimpleProfiler,\n        \"pytorch\": pl.profilers.PyTorchProfiler,\n        \"passthrough\": pl.profilers.PassThroughProfiler,\n        \"xla\": pl.profilers.XLAProfiler,\n    }\n\n    if profiler:\n        if profiler in map_profiler:\n            profiler = map_profiler[profiler](filename=\"profile\")\n        else:\n            raise ValueError(\n                f\"Profiler {profiler} not supported! Please use one of {list(map_profiler.keys())}\"\n            )\n\n    return pl.Trainer(\n        callbacks=callbacks,\n        logger=logger,\n        profiler=profiler,\n        **trainer_params,\n    )\n</code></pre>"},{"location":"configs/config/#dreem.io.Config.set_hparams","title":"<code>set_hparams(hparams)</code>","text":"<p>Setter function for overwriting specific hparams.</p> <p>Useful for changing 1 or 2 hyperparameters such as dataset.</p> <p>Parameters:</p> Name Type Description Default <code>hparams</code> <code>dict</code> <p>A dict containing the hyperparameter to be overwritten and the value to be changed</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if config is successfully updated, <code>False</code> otherwise</p> Source code in <code>dreem/io/config.py</code> <pre><code>def set_hparams(self, hparams: dict) -&gt; bool:\n    \"\"\"Setter function for overwriting specific hparams.\n\n    Useful for changing 1 or 2 hyperparameters such as dataset.\n\n    Args:\n        hparams: A dict containing the hyperparameter to be overwritten and\n            the value to be changed\n\n    Returns:\n        `True` if config is successfully updated, `False` otherwise\n    \"\"\"\n    if hparams == {} or hparams is None:\n        logger.warning(\"Nothing to update!\")\n        return False\n    for hparam, val in hparams.items():\n        try:\n            OmegaConf.update(self.cfg, hparam, val)\n        except Exception as e:\n            logger.exception(f\"Failed to update {hparam} to {val} due to {e}\")\n            return False\n    return True\n</code></pre>"},{"location":"configs/eval/","title":"Description of inference params","text":"<p>Here we describe the parameters used for inference. See here for an example inference config.</p> <ul> <li><code>ckpt_path</code>: (<code>str</code>) the path to the saved model checkpoint. Can optionally provide a list of models and this will trigger batch inference where each pod gets a model to run inference with. e.g: <pre><code>...\nckpt_path: \"/path/to/model.ckpt\"\n...\n</code></pre></li> </ul>"},{"location":"configs/eval/#tracker","title":"<code>tracker</code>","text":"<p>This section configures the tracker.</p> <ul> <li><code>window_size</code>: (<code>int</code>) the size of the window used during sliding inference.</li> <li><code>use_vis_feats</code>: (<code>bool</code>) Whether or not to use visual feature extractor.</li> <li><code>overlap_thresh</code>: (<code>float</code>) the trajectory overlap threshold to be used for assignment.</li> <li><code>mult_thresh</code>: (<code>bool</code>) Whether or not to use weight threshold.</li> <li><code>decay_time</code>: (<code>float</code>) weight for <code>decay_time</code> postprocessing.</li> <li><code>iou</code>: (<code>str</code> | <code>None</code>) Either <code>{None, '', \"mult\" or \"max\"}</code>. Whether to use multiplicative or max iou reweighting.</li> <li><code>max_center_dist</code>: (<code>float</code>) distance threshold for filtering trajectory score matrix.</li> <li><code>persistent_tracking</code>: (<code>bool</code>) whether to keep a buffer across chunks or not.</li> <li><code>max_gap</code>: (<code>int</code>) the max number of frames a trajectory can be missing before termination.</li> <li><code>max_tracks</code>: (<code>int</code>) the maximum number of tracks that can be created while tracking.     We force the tracker to assign instances to a track instead of creating a new track if <code>max_tracks</code>has been reached.</li> </ul>"},{"location":"configs/eval/#examples","title":"Examples:","text":"<pre><code>...\ntracker:\n    window_size: 8\n    overlap_thresh: 0.01\n    mult_thresh: false\n    decay_time: 0.9\n    iou: \"mult\"\n    max_center_dist: 0.1\n...\n</code></pre>"},{"location":"configs/eval/#dataset","title":"<code>dataset</code>","text":"<p>This section contains the params for initializing the datasets for training. Requires a <code>test_dataset</code> keys. </p>"},{"location":"configs/eval/#basedataset-args","title":"<code>BaseDataset</code> args","text":"<ul> <li><code>padding</code>: An <code>int</code> representing the amount of padding to be added to each side of the bounding box size</li> <li><code>crop_size</code>: (<code>int</code>|<code>tuple</code>) the size of the bounding box around which a crop will form.</li> <li><code>chunk</code>: Whether or not to chunk videos into smaller clips to feed to model</li> <li><code>clip_length</code>: the number of frames in each chunk</li> <li><code>mode</code>: <code>train</code> or <code>val</code>. Determines whether this dataset is used for training or validation.</li> <li><code>n_chunks</code>: Number of chunks to subsample from. Can either a fraction of the dataset (ie <code>(0,1.0]</code>) or number of chunks</li> <li><code>seed</code>: set a seed for reproducibility</li> <li><code>gt_list</code>: An optional path to .txt file containing ground truth for cell tracking challenge datasets.</li> </ul>"},{"location":"configs/eval/#dir","title":"<code>dir</code>:","text":"<p>This section allows you to pass a directory rather than paths to labels/videos individually</p> <ul> <li><code>path</code>: The path to the dir where the data is stored (recommend absolute path)</li> <li><code>labels_suffix</code>: (<code>str</code>) containing the file extension to search for labels files. e.g. <code>.slp</code>, <code>.csv</code>, or <code>.xml</code>.</li> <li><code>vid_suffix</code>: (<code>str</code>) containing the file extension to search for video files e.g <code>.mp4</code>, <code>.avi</code> or <code>.tif</code>.</li> </ul>"},{"location":"configs/eval/#examples_1","title":"Examples:","text":"<pre><code>...\ndataset:\n    ...\n    {MODE}_dataset:\n        dir:\n            path: \"/path/to/data/dir/mode\"\n            labels_suffix: \".slp\"\n            vid_suffix: \".mp4\"\n        ...\n    ...\n...\n</code></pre>"},{"location":"configs/eval/#augmentations","title":"<code>augmentations</code>:","text":"<p>This subsection contains params for albumentations. See <code>albumentations</code> for available visual augmentations. Other available augmentations include <code>NodeDropout</code> and <code>InstanceDropout</code>. Keys must match augmentation class name exactly and contain subsections with parameters for the augmentation</p>"},{"location":"configs/eval/#example","title":"Example","text":"<pre><code>augmentations: \n    Rotate:\n        limit: 45\n        p: 0.3\n    ...\n    MotionBlur:\n        blur_limit: [3,7]\n        p: 0.3\n</code></pre>"},{"location":"configs/eval/#sleapdataset-args","title":"<code>SleapDataset</code> Args:","text":"<ul> <li><code>slp_files</code>: (<code>str</code>) a list of .slp files storing tracking annotations</li> <li><code>video_files</code>: (<code>str</code>) a list of paths to video files</li> <li><code>anchors</code>: (<code>str</code> | <code>list</code> | <code>int</code>) One of:<ul> <li>a string indicating a single node to center crops around</li> <li>a list of skeleton node names to be used as the center of crops</li> <li>an int indicating the number of anchors to randomly select If unavailable then crop around the midpoint between all visible anchors.</li> </ul> </li> <li><code>handle_missing</code>: how to handle missing single nodes. one of [<code>\"drop\"</code>, <code>\"ignore\"</code>, <code>\"centroid\"</code>].<ul> <li>if <code>drop</code> then we dont include instances which are missing the <code>anchor</code>.</li> <li>if <code>ignore</code> then we use a mask instead of a crop and nan centroids/bboxes.</li> <li>if <code>centroid</code> then we default to the pose centroid as the node to crop around.</li> </ul> </li> </ul>"},{"location":"configs/eval/#microscopydataset","title":"<code>MicroscopyDataset</code>","text":"<ul> <li><code>videos</code>: (<code>list[str | list[str]]</code>) paths to raw microscopy videos</li> <li><code>tracks</code>: (<code>list[str]</code>) paths to trackmate gt labels (either <code>.xml</code> or <code>.csv</code>)</li> <li><code>source</code>: file format of gt labels based on label generator. Either <code>\"trackmate\"</code> or <code>\"isbi\"</code>.</li> </ul>"},{"location":"configs/eval/#celltrackingdataset","title":"<code>CellTrackingDataset</code>","text":"<ul> <li><code>raw_images</code>: (<code>list[list[str] | list[list[str]]]</code>) paths to raw microscopy images</li> <li><code>gt_images</code>: (<code>list[list[str] | list[list[str]]]</code>) paths to gt label images</li> <li><code>gt_list</code>: (<code>list[str]</code>) An optional path to .txt file containing gt ids stored in cell                 tracking challenge format: <code>\"track_id\", \"start_frame\",                 \"end_frame\", \"parent_id\"</code></li> </ul>"},{"location":"configs/eval/#dataset-examples","title":"<code>dataset</code> Examples","text":""},{"location":"configs/eval/#sleapdataset","title":"<code>SleapDataset</code>","text":"<pre><code>...\ndataset:\n    test_dataset:\n        slp_files: [\"/path/to/test/labels1.slp\", \"/path/to/test/labels2.slp\", ..., \"/path/to/test/labelsN.slp\"]\n        video_files: [\"/path/to/test/video1.mp4\", \"/path/to/test/video2.mp4\", ..., \"/path/to/test/videoN.mp4\"]\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        anchors: [\"node1\", \"node2\", ...\"node_n\"]\n        handle_missing: \"drop\"\n        ... # we don't include augmentations bc usually you shouldn't use augmentations during val/test\n...\n</code></pre>"},{"location":"configs/eval/#microscopydataset_1","title":"<code>MicroscopyDataset</code>","text":"<pre><code>dataset:\n    test_dataset:\n        tracks: [\"/path/to/test/labels1.csv\", \"/path/to/test/labels2.csv\", ..., \"/path/to/test/labelsN.csv\"]\n        videos: [\"/path/to/test/video1.tiff\", \"/path/to/test/video2.tiff\", ..., \"/path/to/test/videoN.tiff\"]\n        source: \"trackmate\"\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        ... # we don't include augmentations bc usually you shouldn't use augmentations during val/test\n</code></pre>"},{"location":"configs/eval/#dataloader","title":"dataloader","text":"<p>This section outlines the params needed for the dataloader. Should have a <code>test_dataloader</code> </p> <p>Below we list the args we found useful/necessary for the dataloaders. For more advanced users see <code>torch.utils.data.Dataloader</code> for more ways to initialize the dataloaders</p> <ul> <li><code>shuffle</code>: (<code>bool</code>) Set to <code>True</code> to have the data reshuffled at every epoch (during training, this should always be <code>True</code> and during val/test usually <code>False</code>) </li> <li><code>num_workers</code>: (<code>int</code>) How many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.</li> </ul>"},{"location":"configs/eval/#example_1","title":"Example","text":"<pre><code>...\ndataloader:\n    test_dataloader: # we leave out the `shuffle` field as default=`False` which is what we want\n        num_workers: 4\n...\n</code></pre>"},{"location":"configs/eval/#runner","title":"<code>runner</code>","text":"<p>This section outlines arguments to be overridden for the GTR Runner * <code>save_path</code>: Path to <code>*.hdf5</code> file where eval results will be saved * <code>metrics</code>: Contains a subkey called <code>test</code> with a list of pymotmetrics to be computed or <code>\"all\"</code> to compute all metrics</p>"},{"location":"configs/eval/#example-computing-all-metrics","title":"Example (Computing all metrics):","text":"<pre><code>runner:\n    save_path: \"./test_eval.hdf5\"\n    metrics:\n        test: \"all\"\n</code></pre>"},{"location":"configs/eval/#example-only-computing-num_switches","title":"Example (Only computing <code>num_switches</code>)","text":"<pre><code>runner:\n    save_path: \"./test_eval.hdf5\"\n    metrics:\n        test: [\"num_switches\"]\n</code></pre>"},{"location":"configs/eval/#example-config","title":"Example Config","text":"<pre><code>ckpt_path: \"../training/models/example/example_train/epoch=0-best-val_sw_cnt=31.06133270263672.ckpt\"\ntracker:\n  overlap_thresh: 0.01\n  decay_time: 0.9\n  iou: \"mult\"\n  max_center_dist: 1.0\n  persistent_tracking: True\n\ndataset:\n  test_dataset:\n    slp_files: [\"../training/190612_110405_wt_18159111_rig2.2@11730.slp\", \"../training/190612_110405_wt_18159111_rig2.2@11730.slp\"]\n    video_files: [\"../training/190612_110405_wt_18159111_rig2.2@11730.mp4\", \"../training/190612_110405_wt_18159111_rig2.2@11730.mp4\"]\n    chunk: True\n    clip_length: 32\n    anchor: \"centroid\"\n\ndataloader:\n  test_dataloader:\n    shuffle: False\n    num_workers: 0\n</code></pre>"},{"location":"configs/inference/","title":"Description of inference params","text":"<p>Here we describe the parameters used for inference. See here for an example inference config.</p> <ul> <li><code>ckpt_path</code>: (<code>str</code>) the path to the saved model checkpoint. Can optionally provide a list of models and this will trigger batch inference where each pod gets a model to run inference with. e.g: <pre><code>...\nckpt_path: \"/path/to/model.ckpt\"\n...\n</code></pre></li> <li><code>out_dir</code>: (<code>str</code>) a directory path where to store outputs. e.g: <pre><code>...\nout_dir: \"/path/to/results/dir\"\n...\n</code></pre></li> </ul>"},{"location":"configs/inference/#tracker","title":"<code>tracker</code>","text":"<p>This section configures the tracker.</p> <ul> <li><code>window_size</code>: (<code>int</code>) the size of the window used during sliding inference.</li> <li><code>use_vis_feats</code>: (<code>bool</code>) Whether or not to use visual feature extractor.</li> <li><code>overlap_thresh</code>: (<code>float</code>) the trajectory overlap threshold to be used for assignment.</li> <li><code>mult_thresh</code>: (<code>bool</code>) Whether or not to use weight threshold.</li> <li><code>decay_time</code>: (<code>float</code>) weight for <code>decay_time</code> postprocessing.</li> <li><code>iou</code>: (<code>str</code> | <code>None</code>) Either <code>{None, '', \"mult\" or \"max\"}</code>. Whether to use multiplicative or max iou reweighting.</li> <li><code>max_center_dist</code>: (<code>float</code>) distance threshold for filtering trajectory score matrix.</li> <li><code>persistent_tracking</code>: (<code>bool</code>) whether to keep a buffer across chunks or not.</li> <li><code>max_gap</code>: (<code>int</code>) the max number of frames a trajectory can be missing before termination.</li> <li><code>max_tracks</code>: (<code>int</code>) the maximum number of tracks that can be created while tracking.     We force the tracker to assign instances to a track instead of creating a new track if <code>max_tracks</code>has been reached.</li> </ul>"},{"location":"configs/inference/#examples","title":"Examples:","text":"<pre><code>...\ntracker:\n    window_size: 8\n    overlap_thresh: 0.01\n    mult_thresh: false\n    decay_time: 0.9\n    iou: \"mult\"\n    max_center_dist: 0.1\n...\n</code></pre>"},{"location":"configs/inference/#dataset","title":"<code>dataset</code>","text":"<p>This section contains the params for initializing the datasets for training. Requires a <code>test_dataset</code> keys. </p>"},{"location":"configs/inference/#basedataset-args","title":"<code>BaseDataset</code> args","text":"<ul> <li><code>padding</code>: An <code>int</code> representing the amount of padding to be added to each side of the bounding box size</li> <li><code>crop_size</code>: (<code>int</code>|<code>tuple</code>) the size of the bounding box around which a crop will form.</li> <li><code>chunk</code>: Whether or not to chunk videos into smaller clips to feed to model</li> <li><code>clip_length</code>: the number of frames in each chunk</li> <li><code>mode</code>: <code>train</code> or <code>val</code>. Determines whether this dataset is used for training or validation.</li> <li><code>n_chunks</code>: Number of chunks to subsample from. Can either a fraction of the dataset (ie <code>(0,1.0]</code>) or number of chunks</li> <li><code>seed</code>: set a seed for reproducibility</li> <li><code>gt_list</code>: An optional path to .txt file containing ground truth for cell tracking challenge datasets.</li> </ul>"},{"location":"configs/inference/#dir","title":"<code>dir</code>:","text":"<p>This section allows you to pass a directory rather than paths to labels/videos individually</p> <ul> <li><code>path</code>: The path to the dir where the data is stored (recommend absolute path)</li> <li><code>labels_suffix</code>: (<code>str</code>) containing the file extension to search for labels files. e.g. <code>.slp</code>, <code>.csv</code>, or <code>.xml</code>.</li> <li><code>vid_suffix</code>: (<code>str</code>) containing the file extension to search for video files e.g <code>.mp4</code>, <code>.avi</code> or <code>.tif</code>.</li> </ul>"},{"location":"configs/inference/#examples_1","title":"Examples:","text":"<pre><code>...\ndataset:\n    ...\n    {MODE}_dataset:\n        dir:\n            path: \"/path/to/data/dir/mode\"\n            labels_suffix: \".slp\"\n            vid_suffix: \".mp4\"\n        ...\n    ...\n...\n</code></pre>"},{"location":"configs/inference/#augmentations","title":"<code>augmentations</code>:","text":"<p>This subsection contains params for albumentations. See <code>albumentations</code> for available visual augmentations. Other available augmentations include <code>NodeDropout</code> and <code>InstanceDropout</code>. Keys must match augmentation class name exactly and contain subsections with parameters for the augmentation</p>"},{"location":"configs/inference/#example","title":"Example","text":"<pre><code>augmentations: \n    Rotate:\n        limit: 45\n        p: 0.3\n    ...\n    MotionBlur:\n        blur_limit: [3,7]\n        p: 0.3\n</code></pre>"},{"location":"configs/inference/#sleapdataset-args","title":"<code>SleapDataset</code> Args:","text":"<ul> <li><code>slp_files</code>: (<code>str</code>) a list of .slp files storing tracking annotations</li> <li><code>video_files</code>: (<code>str</code>) a list of paths to video files</li> <li><code>anchors</code>: (<code>str</code> | <code>list</code> | <code>int</code>) One of:<ul> <li>a string indicating a single node to center crops around</li> <li>a list of skeleton node names to be used as the center of crops</li> <li>an int indicating the number of anchors to randomly select If unavailable then crop around the midpoint between all visible anchors.</li> </ul> </li> <li><code>handle_missing</code>: how to handle missing single nodes. one of [<code>\"drop\"</code>, <code>\"ignore\"</code>, <code>\"centroid\"</code>].<ul> <li>if <code>drop</code> then we dont include instances which are missing the <code>anchor</code>.</li> <li>if <code>ignore</code> then we use a mask instead of a crop and nan centroids/bboxes.</li> <li>if <code>centroid</code> then we default to the pose centroid as the node to crop around.</li> </ul> </li> </ul>"},{"location":"configs/inference/#microscopydataset","title":"<code>MicroscopyDataset</code>","text":"<ul> <li><code>videos</code>: (<code>list[str | list[str]]</code>) paths to raw microscopy videos</li> <li><code>tracks</code>: (<code>list[str]</code>) paths to trackmate gt labels (either <code>.xml</code> or <code>.csv</code>)</li> <li><code>source</code>: file format of gt labels based on label generator. Either <code>\"trackmate\"</code> or <code>\"isbi\"</code>.</li> </ul>"},{"location":"configs/inference/#celltrackingdataset","title":"<code>CellTrackingDataset</code>","text":"<ul> <li><code>raw_images</code>: (<code>list[list[str] | list[list[str]]]</code>) paths to raw microscopy images</li> <li><code>gt_images</code>: (<code>list[list[str] | list[list[str]]]</code>) paths to gt label images</li> <li><code>gt_list</code>: (<code>list[str]</code>) An optional path to .txt file containing gt ids stored in cell                 tracking challenge format: <code>\"track_id\", \"start_frame\",                 \"end_frame\", \"parent_id\"</code></li> </ul>"},{"location":"configs/inference/#dataset-examples","title":"<code>dataset</code> Examples","text":""},{"location":"configs/inference/#sleapdataset","title":"<code>SleapDataset</code>","text":"<pre><code>...\ndataset:\n    test_dataset:\n        slp_files: [\"/path/to/test/labels1.slp\", \"/path/to/test/labels2.slp\", ..., \"/path/to/test/labelsN.slp\"]\n        video_files: [\"/path/to/test/video1.mp4\", \"/path/to/test/video2.mp4\", ..., \"/path/to/test/videoN.mp4\"]\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        anchors: [\"node1\", \"node2\", ...\"node_n\"]\n        handle_missing: \"drop\"\n        ... # we don't include augmentations bc usually you shouldn't use augmentations during val/test\n...\n</code></pre>"},{"location":"configs/inference/#microscopydataset_1","title":"<code>MicroscopyDataset</code>","text":"<pre><code>dataset:\n    test_dataset:\n        tracks: [\"/path/to/test/labels1.csv\", \"/path/to/test/labels2.csv\", ..., \"/path/to/test/labelsN.csv\"]\n        videos: [\"/path/to/test/video1.tiff\", \"/path/to/test/video2.tiff\", ..., \"/path/to/test/videoN.tiff\"]\n        source: \"trackmate\"\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        ... # we don't include augmentations bc usually you shouldn't use augmentations during val/test\n</code></pre>"},{"location":"configs/inference/#dataloader","title":"dataloader","text":"<p>This section outlines the params needed for the dataloader. Should have a <code>train_dataloader</code> and optionally <code>val_dataloader</code>/<code>test_dataloader</code> keys. </p> <p>Below we list the args we found useful/necessary for the dataloaders. For more advanced users see <code>torch.utils.data.Dataloader</code> for more ways to initialize the dataloaders</p> <ul> <li><code>shuffle</code>: (<code>bool</code>) Set to <code>True</code> to have the data reshuffled at every epoch (during training, this should always be <code>True</code> and during val/test usually <code>False</code>) </li> <li><code>num_workers</code>: (<code>int</code>) How many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.</li> </ul>"},{"location":"configs/inference/#example_1","title":"Example","text":"<pre><code>...\ndataloader:\n    test_dataloader: # we leave out the `shuffle` field as default=`False` which is what we want\n        num_workers: 4\n...\n</code></pre>"},{"location":"configs/inference/#example-config","title":"Example Config","text":"<pre><code>ckpt_path: \"../training/models/example/example_train/epoch=0-best-val_sw_cnt=31.06133270263672.ckpt\"\ntracker:\n  overlap_thresh: 0.01\n  decay_time: 0.9\n  iou: \"mult\"\n  max_center_dist: 1.0\n  persistent_tracking: True\n\ndataset:\n  test_dataset:\n    slp_files: [\"../training/190612_110405_wt_18159111_rig2.2@11730.slp\", \"../training/190612_110405_wt_18159111_rig2.2@11730.slp\"]\n    video_files: [\"../training/190612_110405_wt_18159111_rig2.2@11730.mp4\", \"../training/190612_110405_wt_18159111_rig2.2@11730.mp4\"]\n    chunk: True\n    clip_length: 32\n    anchor: \"centroid\"\n\ndataloader:\n  test_dataloader:\n    shuffle: False\n    num_workers: 0\n</code></pre>"},{"location":"configs/training/","title":"Description of training parameters","text":"<p>Here, we describe the hyperparameters used for setting up training. Please see here for an example training config.</p> <p>Note: for using defaults, simply leave the field blank or don't include the key. Using <code>null</code> will initialize the value to <code>None</code> which we use to represent turning off certain features such as logging, early stopping etc. e.g <pre><code>model:\n  d_model: null # defaults to 1024 \n  nhead: 8\n  ...\n</code></pre></p>"},{"location":"configs/training/#model","title":"<code>model</code>","text":"<p>This section contains all the parameters for initializing a <code>GTRRunner</code> object</p> <ul> <li><code>ckpt_path</code>: (<code>str</code>) the path to model <code>.ckpt</code> file. Used for resuming training.</li> <li><code>d_model</code>: (<code>int</code>) the size of the embedding dimensions used for input into the transformer</li> <li><code>nhead</code>: (<code>int</code>) the number of attention heads used in the transformer's encoder/decoder layers.</li> <li><code>num_encoder_layers</code>: (<code>int</code>) the number of layers in the transformer encoder block</li> <li><code>num_decoder_layers</code>: (<code>int</code>) the number of layers in the transformer decoder block</li> <li><code>dropout</code>: a <code>float</code> the dropout probability used in each transformer layer</li> <li><code>activation</code>: One of {<code>\"relu\"</code>, <code>\"gelu\"</code> <code>\"glu\"</code>}. Which activation function to use in the transformer.</li> <li><code>return_intermediate_dec</code>: (<code>bool</code>) whether or not to return the output from the intermediate decoder layers.</li> <li><code>norm</code>: (<code>bool</code>) whether or not to normalize output of encoder and decoder.</li> <li><code>num_layers_attn_head</code>: An <code>int</code> The number of layers in the <code>AttentionHead</code> block.</li> <li><code>dropout_attn_head</code>: (<code>float</code>)  the dropout probability for the <code>AttentionHead</code> block.</li> <li><code>return_embedding</code>: (<code>bool</code>) whether to return the spatiotemporal embeddings</li> <li><code>decoder_self_attn</code>: (<code>bool</code>) whether to use self attention in the decoder.</li> </ul>"},{"location":"configs/training/#embedding_meta","title":"<code>embedding_meta</code>:","text":"<p>This section contains parameters for initializing the <code>Embedding</code> Layer.</p>"},{"location":"configs/training/#pos","title":"<code>pos</code>","text":"<p>This subsection contains the parameters for initializing a Spatial <code>Embedding</code>.</p> <ul> <li><code>mode</code>: (<code>str</code>) One of {<code>\"fixed\"</code>, <code>\"learned\"</code>, <code>\"None\"</code>}. Indicates whether to use a fixed sinusoidal, learned, or no embedding.</li> <li><code>n_points</code>: (<code>int</code>) the number of points that will be embedded.</li> </ul>"},{"location":"configs/training/#fixed-sinusoidal-params","title":"Fixed Sinusoidal Params","text":"<ul> <li><code>temperature</code>: (<code>float</code>) the temperature constant to be used when computing the sinusoidal position embedding</li> <li><code>normalize</code>: (<code>bool</code>) whether or not to normalize the positions (Only used in fixed embeddings).</li> <li><code>scale</code>: (<code>float</code>) factor by which to scale the positions after normalizing (Only used in fixed embeddings).</li> </ul>"},{"location":"configs/training/#learned-params","title":"Learned Params:","text":"<ul> <li><code>emb_num</code>: (<code>int</code>) the number of embeddings in the <code>self.lookup</code> table (Only used in learned embeddings).</li> <li><code>over_boxes</code>: (<code>bool</code>) Whether to compute the position embedding for each bbox coordinate (<code>y1x1y2x2</code>) or the centroid + bbox size (<code>yxwh</code>).</li> </ul>"},{"location":"configs/training/#mlp_cfg","title":"<code>mlp_cfg</code>","text":"<p>This subsection contains <code>MLP</code> hyperparameters for projecting embedding to correct space. Required when <code>n_points &gt; 1</code>, optional otherwise.</p> <ul> <li><code>hidden_dims</code>: (<code>int</code>) The dimensionality of the MLP hidden layers.</li> <li><code>num_layers</code>: (<code>int</code>) Number of hidden layers.</li> <li><code>dropout</code>: (<code>float</code>) The dropout probability for each hidden layer.</li> </ul> <p>Example:  <pre><code>model:\n    ...\n    embedding_meta:\n        pos:\n            ...\n            n_points: 3 #could also be 1\n            ...\n            mlp_cfg: #cannot be null\n                hidden_dims: 256\n                num_layers: 3\n                dropout: 0.3\n</code></pre></p>"},{"location":"configs/training/#examples","title":"Examples:","text":""},{"location":"configs/training/#with-mlp","title":"With MLP:","text":"<pre><code>...\nmodel:\n    ...\n    embedding_meta:\n        pos:\n            mode: \"fixed\"\n            normalize: true\n            temperature: 10000\n            scale: null\n            n_points: 3 #could also be 1\n            mlp_cfg: \n                hidden_dims: 256\n                num_layers: 3\n                dropout: 0.3\n            ...\n        ...\n    ...\n...\n</code></pre>"},{"location":"configs/training/#with-no-mlp","title":"With no MLP","text":"<pre><code>model:\n    ...\n    embedding_meta:\n        pos:\n            mode: \"fixed\"\n            normalize: true\n            temperature: 10000\n            scale: null\n            n_points: 1 #must be 1\n            mlp_cfg: null\n        ...\n    ...\n...\n</code></pre>"},{"location":"configs/training/#temp","title":"<code>temp</code>","text":"<p>This subsection contains the parameters for initializing a Temporal <code>Embedding</code></p> <ul> <li><code>mode</code>: (<code>str</code>) One of {<code>\"fixed\"</code>, <code>\"learned\"</code>, <code>\"None\"</code>}. Indicates whether to use a fixed sinusoidal, learned, or no embedding.</li> </ul>"},{"location":"configs/training/#fixed-sinusoidal-params_1","title":"Fixed Sinusoidal Params","text":"<ul> <li><code>temperature</code>: (<code>float</code>) the temperature constant to be used when computing the sinusoidal position embedding</li> </ul>"},{"location":"configs/training/#learned-params_1","title":"Learned Params:","text":"<ul> <li><code>emb_num</code>: (<code>int</code>) the number of embeddings in the lookup table. Note: See <code>dreem.models.Embedding</code> for additional <code>kwargs</code> that can be passed</li> </ul>"},{"location":"configs/training/#examples_1","title":"Examples:","text":""},{"location":"configs/training/#fixed","title":"Fixed:","text":"<pre><code>model:\n    ...\n    embedding_meta:\n        temp:\n            mode: \"fixed\" # also accepts \"off\" or null\n            temperature: 10000\n        ...\n    ...\n...\n</code></pre>"},{"location":"configs/training/#embedding_meta-example","title":"<code>embedding_meta</code> Example:","text":"<p>Putting it all together, your <code>embedding_meta</code> section should look something like this</p> <pre><code>...\nmodel:\n    ...\n    embedding_meta:\n        pos:\n            mode: \"fixed\"\n            normalize: true\n            temperature: 10000\n            scale: null\n            n_points: 3 #could also be 1\n            mlp_cfg: \n                hidden_dims: 256\n                num_layers: 3\n                dropout: 0.3\n        temp:\n            mode: \"fixed\"\n            temperature: 10000\n    ...\n...\n</code></pre>"},{"location":"configs/training/#encoder_cfg","title":"<code>encoder_cfg</code>","text":"<p>This section contains all the parameters for initializing a <code>VisualEncoder</code> model.</p> <ul> <li><code>model_name</code>: (<code>str</code>) Thhe name of the visual encoder backbone to be used. When using <code>timm</code> as a backend, all models in <code>timm.list_model</code> are available. However, when using <code>torchvision</code> as a backend, only <code>resnet</code>s are available for now.</li> <li><code>backend</code>: (<code>str</code>) Either <code>\"timm\"</code> or <code>\"torchvision\"</code>. Indicates which deep learning library to use for initializing the visual encoder</li> <li><code>in_chans</code>: (<code>int</code>)  the number of input channels input images contain. Mostly used for multi-anchor crops</li> <li><code>pretrained</code>: (<code>bool</code>) Whether or not to use a pretrained backbone or initialize from random</li> </ul> <p>Note: For more advanced users, see <code>timm.create_model</code> or <code>torchvision.models.resnet</code> for additional <code>kwargs</code> that can be passed to the visual encoder.</p>"},{"location":"configs/training/#example","title":"Example:","text":""},{"location":"configs/training/#timm","title":"<code>timm</code>:","text":"<pre><code>...\nmodel:\n    ...\n    encoder_cfg:\n        model_name: \"resnet18\"\n        backend: \"timm\"\n        in_chans: 3\n        pretrained: false\n        ...\n    ...\n...\n</code></pre>"},{"location":"configs/training/#torchvision","title":"<code>torchvision</code>:","text":"<pre><code>...\nmodel:\n    ...\n    encoder_cfg:\n        model_name: \"resnet32\"\n        backend: \"torchvision\"\n        in_chans: 3\n        pretrained: false\n        ...\n    ...\n...\n</code></pre>"},{"location":"configs/training/#model-example","title":"<code>model</code> Example:","text":"<p>Putting it all together your <code>model</code> config section will look something like this <pre><code>...\nmodel:\n  ckpt_path: null\n  encoder_cfg: \n    model_name: \"resnet18\"\n    backend: \"timm\"\n    in_chans: 3\n  d_model: 1024\n  nhead: 8\n  num_encoder_layers: 1\n  num_decoder_layers: 1\n  dropout: 0.1\n  activation: \"relu\"\n  return_intermediate_dec: False\n  norm: False\n  num_layers_attn_head: 2\n  dropout_attn_head: 0.1\n  embedding_meta: \n    pos:\n        mode: \"fixed\"\n        normalize: true\n    temp:\n        mode: \"fixed\"\n  return_embedding: False\n  decoder_self_attn: False\n...\n</code></pre></p>"},{"location":"configs/training/#loss","title":"<code>loss</code>","text":"<p>This section contains parameters for the Association Loss function</p> <ul> <li><code>neg_unmatched</code> a bool whether to set unmatched objects to the background</li> <li><code>epsilon</code>: A small <code>float</code> used for numeric precision to prevent dividing by zero</li> <li><code>asso_weight</code>: (<code>float</code>) how much to weight the association loss by</li> </ul>"},{"location":"configs/training/#examples_2","title":"Examples:","text":"<pre><code>...\nloss:\n    neg_unmatched: false\n    epsilon: 1e-8\n    asso_weight: 1.0\n...\n</code></pre>"},{"location":"configs/training/#optimizer","title":"<code>optimizer</code>","text":"<p>This section contains the parameters for initializing the training optimizer</p> <ul> <li><code>name</code>: (<code>str</code>) representation of the optimizer.      &gt; See <code>torch.optim</code> for available optimizers.(<code>name</code> must match the optimizer name exactly (case-sensitive)).</li> </ul> <p>Below, we list the arguments we use for <code>Adam</code> which is the optimizer we use and is our default. For more advanced users please see the respective pytorch documentation page for the arguments expected in your requested optimizer.</p> <ul> <li><code>lr</code>: (<code>float</code>) learning rate</li> <li><code>betas</code>: (<code>tuple[float, float]</code>) coefficients used for computing running averages of gradient and its square</li> <li><code>eps</code>: (<code>float</code>): term added to the denominator to improve numerical stability</li> <li><code>weight_decay</code>: (<code>float</code>) weight decay (\\(L_2\\) penalty)</li> </ul>"},{"location":"configs/training/#examples_3","title":"Examples:","text":"<p>Here's an example for <code>Adam</code>: <pre><code>...\noptimizer:\n    name: \"Adam\"\n    lr: 0.001\n    betas: [0.9, 0.999]\n    eps: 1e-8\n    weight_decay: 0.01\n    ...\n...\n</code></pre></p>"},{"location":"configs/training/#scheduler","title":"<code>scheduler</code>","text":"<p>This section contains parameters for initializing the learning rate scheduler.</p> <ul> <li><code>name</code>: (<code>str</code>) Representation of the scheduler.      &gt; See <code>torch.optim.lr_scheduler</code> for available schedulers. <code>name</code> must match the scheduler name exactly (case-sensitive).</li> </ul> <p>Below, we list the arguments we use for <code>ReduceLROnPlateau</code> which is the scheduler we use and is our default. For more advanced users please see the respective pytorch documentation page for the arguments expected in your requested scheduler.</p> <ul> <li><code>mode</code>: (<code>str</code>) One of {<code>\"min\"</code>, <code>\"max\"</code>}. In <code>min</code> mode, lr will be reduced when the quantity monitored has stopped decreasing; in <code>max</code> mode it will be reduced when the quantity monitored has stopped increasing.</li> <li><code>factor</code>: (<code>float</code>) Factor by which the learning rate will be reduced. <code>new_lr = lr * factor</code></li> <li><code>patience</code>: (<code>int</code>) The number of allowed epochs with no improvement after which the learning rate will be reduced.</li> <li><code>threshold</code>: (<code>float</code>) Threshold for measuring the new optimum, to only focus on significant changes. </li> <li><code>threshold_mode</code>: (<code>str</code>)  One of {<code>\"rel\"</code>, \"<code>abs</code>\"}. In <code>rel</code> mode, <code>dynamic_threshold = best * ( 1 + threshold )</code> in <code>max</code> mode or <code>best * ( 1 - threshold )</code> in <code>min</code> mode. In <code>abs</code> mode, <code>dynamic_threshold = best + threshold</code> in <code>max</code> mode or <code>best - threshold</code> in <code>min</code> mode.</li> </ul>"},{"location":"configs/training/#examples_4","title":"Examples:","text":"<p>Here we give an example of configs for a Pytorch scheduler. For more detail, visit the PyTorch documentation page for the scheduler you are interested in.</p>"},{"location":"configs/training/#reduce-learning-rate-on-plateau","title":"<code>Reduce Learning Rate on Plateau</code>","text":"<pre><code>...\nscheduler:\n  name: \"ReduceLROnPlateau\" #must match torch.optim class name\n  mode: \"min\"\n  factor: 0.5\n  patience: 10\n  threshold: 1e-4\n  threshold_mode: \"rel\"\n  ...\n...\n</code></pre>"},{"location":"configs/training/#tracker","title":"<code>tracker</code>:","text":"<p>This section contains parameters for initializing the <code>Tracker</code></p> <ul> <li><code>window_size</code>: the size of the window used during sliding inference.</li> <li><code>use_vis_feats</code>: Whether or not to use visual feature extractor.</li> <li><code>overlap_thresh</code>: the trajectory overlap threshold to be used for assignment.</li> <li><code>mult_thresh</code>: Whether or not to use weight threshold.</li> <li><code>decay_time</code>: weight for <code>decay_time</code> postprocessing.</li> <li><code>iou</code>: Either <code>{None, '', \"mult\" or \"max\"}</code>. Whether to use multiplicative or max iou reweighting.</li> <li><code>max_center_dist</code>: distance threshold for filtering trajectory score matrix.</li> <li><code>persistent_tracking</code>: whether to keep a buffer across chunks or not.</li> <li><code>max_gap</code>: the max number of frames a trajectory can be missing before termination.</li> <li><code>max_tracks</code>: the maximum number of tracks that can be created while tracking.     We force the tracker to assign instances to a track instead of creating a new track if <code>max_tracks</code> has been reached.</li> </ul>"},{"location":"configs/training/#examples_5","title":"Examples:","text":"<pre><code>...\ntracker:\n    window_size: 8\n    overlap_thresh: 0.01\n    mult_thresh: false\n    decay_time: 0.9\n    iou: \"mult\"\n    max_center_dist: 0.1\n    ...\n...\n</code></pre>"},{"location":"configs/training/#runner","title":"<code>runner</code>","text":"<p>This section contains parameters for how to handle training/validation/testing</p>"},{"location":"configs/training/#metrics","title":"<code>metrics</code>","text":"<p>This section contains config for which metrics to compute during training/validation/testing. See <code>pymotmetrics.list_metrics</code> for available metrics.</p> <p>Should have a <code>train</code>, <code>val</code> and <code>test</code> key with corresponding list of metrics to compute during training.</p>"},{"location":"configs/training/#examples_6","title":"Examples:","text":""},{"location":"configs/training/#only-computing-the-loss","title":"Only computing the loss:","text":"<pre><code>...\nrunner:\n    ...\n    metrics:\n        train: []\n        val: []\n        test: []\n    ...\n...\n</code></pre>"},{"location":"configs/training/#computing-num_switches-during-validation","title":"Computing <code>num_switches</code> during validation:","text":"<pre><code>...\nrunner:\n    ...\n    metrics:\n        train: []\n        val: [\"num_switches\"]\n        test: []\n    ...\n...\n</code></pre>"},{"location":"configs/training/#computing-num_switches-and-mota-during-testing","title":"Computing <code>num_switches</code> and  <code>mota</code> during testing:","text":"<pre><code>...\nrunner:\n    ...\n    metrics:\n        train: []\n        val: [\"num_switches\"]\n        test: [\"num_switches\", \"mota\"]\n    ...\n...\n</code></pre>"},{"location":"configs/training/#persistent_tracking","title":"<code>persistent_tracking</code>","text":"<p>This section indicates whether or not to track across chunks during training/validation/testing</p> <p>Should have a <code>train</code>, <code>val</code> and <code>test</code> key with a corresponding <code>bool</code> whether to use persistent tracking. <code>persistent_tracking</code> should almost always be <code>False</code> during training. During validation and testing it may depend on whether you are testing on full videos or subsampled clips</p>"},{"location":"configs/training/#examples_7","title":"Examples:","text":"<pre><code>...\nrunner\n    ...\n    persistent_tracking:\n        train: false\n        val: false # assuming we validate on a subsample of clips\n        test: true # assuming we test on a contiguous video.\n</code></pre>"},{"location":"configs/training/#dataset","title":"<code>dataset</code>","text":"<p>This section contains the params for initializing the datasets for training. Requires a <code>train_dataset</code> and optionally <code>val_dataset</code>, <code>test_dataset</code> keys. </p>"},{"location":"configs/training/#basedataset-args","title":"<code>BaseDataset</code> args","text":"<ul> <li><code>padding</code>: An <code>int</code> representing the amount of padding to be added to each side of the bounding box size</li> <li><code>crop_size</code>: (<code>int</code>|<code>tuple</code>) the size of the bounding box around which a crop will form.</li> <li><code>chunk</code>: Whether or not to chunk videos into smaller clips to feed to model</li> <li><code>clip_length</code>: the number of frames in each chunk</li> <li><code>mode</code>: <code>train</code> or <code>val</code>. Determines whether this dataset is used for training or validation.</li> <li><code>n_chunks</code>: Number of chunks to subsample from. Can either a fraction of the dataset (ie <code>(0,1.0]</code>) or number of chunks</li> <li><code>seed</code>: set a seed for reproducibility</li> <li><code>gt_list</code>: An optional path to .txt file containing ground truth for cell tracking challenge datasets.</li> </ul>"},{"location":"configs/training/#dir","title":"<code>dir</code>:","text":"<p>This section allows you to pass a directory rather than paths to labels/videos individually</p> <ul> <li><code>path</code>: The path to the dir where the data is stored (recommend absolute path)</li> <li><code>labels_suffix</code>: (<code>str</code>) containing the file extension to search for labels files. e.g. <code>.slp</code>, <code>.csv</code>, or <code>.xml</code>.</li> <li><code>vid_suffix</code>: (<code>str</code>) containing the file extension to search for video files e.g <code>.mp4</code>, <code>.avi</code> or <code>.tif</code>.</li> </ul>"},{"location":"configs/training/#examples_8","title":"Examples:","text":"<pre><code>...\ndataset:\n    ...\n    {MODE}_dataset:\n        dir:\n            path: \"/path/to/data/dir/mode\"\n            labels_suffix: \".slp\"\n            vid_suffix: \".mp4\"\n        ...\n    ...\n...\n</code></pre>"},{"location":"configs/training/#augmentations","title":"<code>augmentations</code>:","text":"<p>This subsection contains params for albumentations. See <code>albumentations</code> for available visual augmentations. Other available augmentations include <code>NodeDropout</code> and <code>InstanceDropout</code>. Keys must match augmentation class name exactly and contain subsections with parameters for the augmentation</p>"},{"location":"configs/training/#example_1","title":"Example","text":"<pre><code>augmentations: \n    Rotate:\n        limit: 45\n        p: 0.3\n    ...\n    MotionBlur:\n        blur_limit: [3,7]\n        p: 0.3\n</code></pre>"},{"location":"configs/training/#sleapdataset-args","title":"<code>SleapDataset</code> Args:","text":"<ul> <li><code>slp_files</code>: (<code>str</code>) a list of .slp files storing tracking annotations</li> <li><code>video_files</code>: (<code>str</code>) a list of paths to video files</li> <li><code>anchors</code>: (<code>str</code> | <code>list</code> | <code>int</code>) One of:<ul> <li>a string indicating a single node to center crops around</li> <li>a list of skeleton node names to be used as the center of crops</li> <li>an int indicating the number of anchors to randomly select If unavailable then crop around the midpoint between all visible anchors.</li> </ul> </li> <li><code>handle_missing</code>: how to handle missing single nodes. one of [<code>\"drop\"</code>, <code>\"ignore\"</code>, <code>\"centroid\"</code>].<ul> <li>if <code>drop</code> then we dont include instances which are missing the <code>anchor</code>.</li> <li>if <code>ignore</code> then we use a mask instead of a crop and nan centroids/bboxes.</li> <li>if <code>centroid</code> then we default to the pose centroid as the node to crop around.</li> </ul> </li> </ul>"},{"location":"configs/training/#microscopydataset","title":"<code>MicroscopyDataset</code>","text":"<ul> <li><code>videos</code>: (<code>list[str | list[str]]</code>) paths to raw microscopy videos</li> <li><code>tracks</code>: (<code>list[str]</code>) paths to trackmate gt labels (either <code>.xml</code> or <code>.csv</code>)</li> <li><code>source</code>: file format of gt labels based on label generator. Either <code>\"trackmate\"</code> or <code>\"isbi\"</code>.</li> </ul>"},{"location":"configs/training/#celltrackingdataset","title":"<code>CellTrackingDataset</code>","text":"<ul> <li><code>raw_images</code>: (<code>list[list[str] | list[list[str]]]</code>) paths to raw microscopy images</li> <li><code>gt_images</code>: (<code>list[list[str] | list[list[str]]]</code>) paths to gt label images</li> <li><code>gt_list</code>: (<code>list[str]</code>) An optional path to .txt file containing gt ids stored in cell                 tracking challenge format: <code>\"track_id\", \"start_frame\",                 \"end_frame\", \"parent_id\"</code></li> </ul>"},{"location":"configs/training/#dataset-examples","title":"<code>dataset</code> Examples","text":""},{"location":"configs/training/#sleapdataset","title":"<code>SleapDataset</code>","text":"<pre><code>...\ndataset:\n    train_dataset:\n        slp_files: [\"/path/to/train/labels1.slp\", \"/path/to/train/labels2.slp\", ..., \"/path/to/train/labelsN.slp\"]\n        video_files: [\"/path/to/train/video1.mp4\", \"/path/to/train/video2.mp4\", ..., \"/path/to/train/videoN.mp4\"]\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        anchors: [\"node1\", \"node2\", ...\"node_n\"]\n        handle_missing: \"drop\"\n        augmentations: \n            Rotate:\n                limit: 45\n                p: 0.3\n            ...\n            MotionBlur:\n                blur_limit: [3,7]\n                p: 0.3\n        ...\n    val_dataset:\n        slp_files: [\"/path/to/val/labels1.slp\", \"/path/to/val/labels2.slp\", ..., \"/path/to/val/labelsN.slp\"]\n        video_files: [\"/path/to/val/video1.mp4\", \"/path/to/val/video2.mp4\", ..., \"/path/to/val/videoN.mp4\"]\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        anchors: [\"node1\", \"node2\", ...\"node_n\"]\n        handle_missing: \"drop\"\n        ... # we don't include augmentations bc usually you shouldn't use augmentations during val/test\n    test_dataset:\n        slp_files: [\"/path/to/test/labels1.slp\", \"/path/to/test/labels2.slp\", ..., \"/path/to/test/labelsN.slp\"]\n        video_files: [\"/path/to/test/video1.mp4\", \"/path/to/test/video2.mp4\", ..., \"/path/to/test/videoN.mp4\"]\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        anchors: [\"node1\", \"node2\", ...\"node_n\"]\n        handle_missing: \"drop\"\n        ... # we don't include augmentations bc usually you shouldn't use augmentations during val/test\n...\n</code></pre>"},{"location":"configs/training/#microscopydataset_1","title":"<code>MicroscopyDataset</code>","text":"<pre><code>dataset:\n    train_dataset:\n        tracks: [\"/path/to/train/labels1.csv\", \"/path/to/train/labels2.csv\", ..., \"/path/to/train/labelsN.csv\"]\n        videos: [\"/path/to/train/video1.tiff\", \"/path/to/train/video2.tiff\", ..., \"/path/to/train/videoN.tiff\"]\n        source: \"trackmate\"\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        augmentations: \n            Rotate:\n                limit: 45\n                p: 0.3\n            ...\n            MotionBlur:\n                blur_limit: [3,7]\n                p: 0.3\n        ...\n    val_dataset:\n        tracks: [\"/path/to/val/labels1.csv\", \"/path/to/val/labels2.csv\", ..., \"/path/to/val/labelsN.csv\"]\n        video: [\"/path/to/val/video1.tiff\", \"/path/to/val/video2.tiff\", ..., \"/path/to/val/videoN.tiff\"]\n        source: \"trackmate\"\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        ... # we don't include augmentations bc usually you shouldn't use augmentations during val/test\n    test_dataset:\n        tracks: [\"/path/to/test/labels1.csv\", \"/path/to/test/labels2.csv\", ..., \"/path/to/test/labelsN.csv\"]\n        videos: [\"/path/to/test/video1.tiff\", \"/path/to/test/video2.tiff\", ..., \"/path/to/test/videoN.tiff\"]\n        source: \"trackmate\"\n        padding: 5\n        crop_size: 128 \n        chunk: True\n        clip_length: 32\n        ... # we don't include augmentations bc usually you shouldn't use augmentations during val/test\n</code></pre>"},{"location":"configs/training/#dataloader","title":"<code>dataloader</code>","text":"<p>This section outlines the params needed for the dataloader. Should have a <code>train_dataloader</code> and optionally <code>val_dataloader</code>/<code>test_dataloader</code> keys. </p> <p>Below we list the args we found useful/necessary for the dataloaders. For more advanced users see <code>torch.utils.data.Dataloader</code> for more ways to initialize the dataloaders</p> <ul> <li><code>shuffle</code>: (<code>bool</code>) Set to <code>True</code> to have the data reshuffled at every epoch (during training, this should always be <code>True</code> and during val/test usually <code>False</code>) </li> <li><code>num_workers</code>: (<code>int</code>) How many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.</li> </ul>"},{"location":"configs/training/#example_2","title":"Example","text":"<pre><code>...\ndataloader:\n    train_dataloader:\n        shuffle: true\n        num_workers: 4\n    val_dataloader: # we leave out the `shuffle` field as default=`False` which is what we want\n        num_workers: 4\n    test_dataloader: # we leave out the `shuffle` field as default=`False` which is what we want\n        num_workers: 4\n</code></pre>"},{"location":"configs/training/#logging","title":"<code>logging</code>:","text":"<p>This section sets up logging for the training job. </p> <ul> <li><code>logger_type</code>: (<code>str</code>) Which logger to use. Available loggers are {<code>\"CSVLogger\"</code>, <code>\"TensorBoardLogger\"</code>,<code>\"WandbLogger\"</code>}</li> </ul> <p>Below we list the arguments we found useful for the <code>WandbLogger</code> as this is the logger we use and recommend. Please see the documentation for the corresponding logger at <code>lightning.loggers</code> for respective available parameters.</p> <ul> <li><code>name</code>: (<code>str</code>) A short display name for this run, which is how you'll identify this run in the UI.</li> <li><code>save_dir</code>: (<code>str</code>) An absolute path to a directory where metadata will be stored. </li> <li><code>version</code>: (<code>str</code>) A unique ID for this run, used for resuming. It must be unique in the project, and if you delete a run you can't reuse the ID.</li> <li><code>project</code>: (<code>str</code>)  The name of the project where you're sending the new run.</li> <li><code>log_model</code>: (<code>str</code>) Log checkpoints created by <code>ModelCheckpoint</code> as W&amp;B artifacts</li> <li><code>group</code>: (<code>str</code>) Specify a group to organize individual runs into a larger experiment</li> <li><code>entity</code>: (<code>str</code>) An entity is a username or team name where you're sending runs</li> <li><code>notes</code>: (<code>str</code>) A longer description of the run, like a <code>-m</code>commit message in git.</li> </ul> <p>See <code>wandb.init()</code> and <code>WandbLogger</code> for more fine-grained config args.</p>"},{"location":"configs/training/#examples_9","title":"Examples:","text":"<p>Here we provide a couple examples for different available loggers</p>"},{"location":"configs/training/#wandb","title":"<code>wandb</code>","text":"<pre><code>...\nlogging:\n  logger_type: \"WandbLogger\"\n  name: \"example_train\"\n  entity: \"example_user\"\n  job_type: \"train\"\n  notes: \"Example train job\"\n  dir: \"./logs\"\n  group: \"example\"\n  save_dir: './logs'\n  project: \"GTR\"\n  log_model: \"all\"\n  ...\n...\n</code></pre>"},{"location":"configs/training/#csv-logger","title":"<code>csv logger</code>:","text":"<pre><code>...\nlogging:\n    save_dir: \"./logs\"\n    name: \"example_train.csv\"\n    version: 1\n    flush_logs_every_n_steps: 1\n    ...\n...\n</code></pre>"},{"location":"configs/training/#early_stopping","title":"<code>early_stopping</code>","text":"<p>This section configures early stopping for training runs. </p> <p>Below we provide descriptions of the arguments we found useful for EarlyStopping. For advanced users, see `lightning.callbacks.EarlyStopping for available arguments for more fine grained control</p> <ul> <li><code>monitor</code> (<code>str</code>): quantity to be monitored.</li> <li><code>min_delta</code> (<code>float</code>): minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than or equal to min_delta, will count as no improvement.</li> <li><code>patience</code> (<code>int</code>): number of checks with no improvement after which training will be stopped. </li> <li><code>mode</code> (<code>str</code>): one of 'min', 'max'. In 'min' mode, training will stop when the quantity monitored has stopped decreasing and in 'max' mode it will stop when the quantity monitored has stopped increasing.</li> <li><code>check_finite</code> (<code>bool</code>): When set True, stops training when the monitor becomes NaN or infinite.</li> <li><code>stopping_threshold</code> (<code>float</code>): Stop training immediately once the monitored quantity reaches this threshold.</li> <li><code>divergence_threshold</code> (<code>float</code>): Stop training as soon as the monitored quantity becomes worse than this threshold.</li> </ul>"},{"location":"configs/training/#example_3","title":"Example:","text":"<pre><code>...\nearly_stopping:\n  monitor: \"val_loss\"\n  min_delta: 0.1\n  patience: 10\n  mode: \"min\"\n  check_finite: true\n  stopping_threshold: 1e-8\n  divergence_threshold: 30\n  ...\n...\n</code></pre>"},{"location":"configs/training/#checkpointing","title":"<code>checkpointing</code>","text":"<p>This section enables model checkpointing during training</p> <ul> <li><code>monitor</code>: A list of metrics to save best models for. Usually should be <code>\"val_{METRIC}\"</code> notation.     &gt; Note: We initialize a separate <code>ModelCheckpoint</code> for each metric to monitor.     &gt; This means that you'll save at least \\(|monitor|\\) checkpoints at the end of training.</li> </ul> <p>Below we describe the arguments we found useful for checkpointing. For more fine grained control see <code>lightning.callbacks.ModelCheckpoint</code> for available checkpointing params and generally more info on how <code>lightning</code> sets up checkpoints</p> <ul> <li><code>dirpath</code>: (<code>str</code>) Directory to save the models. If left empty then we first try to save to <code>./models/[GROUP]/[NAME]</code> or <code>./models/[NAME]</code> if logger is <code>wandb</code> otherwise we just save to <code>./models</code> </li> <li><code>save_last</code>: (<code>bool</code>): When <code>True</code>, saves a last.ckpt copy whenever a checkpoint file gets saved. Can be set to 'link' on a local filesystem to create a symbolic link. This allows accessing the latest checkpoint in a deterministic manner.</li> <li><code>save_top_k</code>: (<code>int</code>): if <code>save_top_k == k</code>, the best k models according to the quantity monitored will be saved. if <code>save_top_k == 0</code>, no models are saved. if <code>save_top_k == -1</code>, all models are saved. (Recommend -1)</li> <li><code>every_n_epochs</code>: (<code>int</code>) Number of epochs between checkpoints. This value must be <code>None</code> or non-negative. To disable saving top-k checkpoints, set <code>every_n_epochs = 0</code>. This argument does not impact the saving of <code>save_last=True</code> checkpoints.</li> </ul>"},{"location":"configs/training/#example_4","title":"Example:","text":"<pre><code>...\ncheckpointing:\n    monitor: [\"val_loss\", \"val_num_switches\"] #saves a model for best validation loss and a model for best validation switch count separately\n    dirpath: \"./models/example_run\"\n    save_last: true # will always save the best run\n    save_top_k: -1\n    every_n_epochs: 10 # saves the every 10th model regardless of if its the best.\n    ...\n...\n</code></pre>"},{"location":"configs/training/#trainer","title":"<code>trainer</code>","text":"<p>This section configures the <code>lightning.Trainer</code> object for training. </p> <p>Below we describe the arguments we found useful for the <code>Trainer</code>. If you're an advanced user, Please see <code>lightning.Trainer</code>(https://lightning.ai/docs/pytorch/stable/common/trainer.html) for more fine grained control and how the <code>trainer</code> works in general</p> <ul> <li><code>accelerator</code>: (<code>str</code>) Supports passing different accelerator types <code>(\u201ccpu\u201d, \u201cgpu\u201d, \u201ctpu\u201d, \u201cipu\u201d, \u201chpu\u201d, \u201cmps\u201d, \u201cauto\u201d)</code> as well as custom accelerator instances.</li> <li><code>strategy</code>: (<code>str</code>) Supports different training strategies with aliases as well custom strategies</li> <li><code>devices</code>: (<code>list[int]</code> | <code>str</code>| <code>int</code>)`The devices to use. Can be set to:<ul> <li>a positive number (<code>int</code> | <code>str</code>) </li> <li>a sequence of device indices (<code>list</code> | <code>str</code>), </li> <li>the value <code>-1</code> to indicate all available devices should be used</li> <li>\"auto\" for automatic selection based on the chosen accelerator </li> </ul> </li> <li><code>fast_dev_run</code>: (<code>int</code> | <code>bool</code>) Runs <code>n</code> (if set to <code>n</code> (<code>int</code>)) else <code>1</code> (if set to <code>True</code>) batch(es) of train, val and test to find any bugs (ie: a sort of unit test).</li> <li><code>check_val_every_n_epoch</code>: (<code>int</code>) Perform a validation loop every after every <code>N</code> training epochs</li> <li><code>enable_checkpointing</code>: (<code>bool</code>) If <code>True</code>, enable checkpointing. It will configure a default <code>ModelCheckpoint</code> callback if there is no user-defined <code>ModelCheckpoint</code> in callbacks.</li> <li><code>gradient_clip_val</code>:  (<code>float</code>) The value at which to clip gradients</li> <li><code>limit_train_batches</code>: (<code>int</code> | <code>float</code>) How much of training dataset to check (<code>float</code> = fraction, <code>int</code> = num_batches) (mostly for debugging)</li> <li><code>limit_test_batches</code>: (<code>int</code> | <code>float</code>) How much of test dataset to check (<code>float</code> = fraction, <code>int</code> = num_batches). (mostly for debugging)</li> <li><code>limit_val_batches</code>: (<code>int</code> | <code>float</code>) How much of validation dataset to check (<code>float</code> = fraction, <code>int</code> = num_batches) (mostly for debugging)</li> <li><code>limit_predict_batches</code>: (<code>int</code> | <code>float</code>) How much of prediction dataset to check (<code>float</code> = fraction, <code>int</code> = num_batches)</li> <li><code>log_every_n_steps</code>:  (<code>int</code>) How often to log within steps</li> <li><code>max_epochs</code>: (<code>int</code>) Stop training once this number of epochs is reached. To enable infinite training, set <code>max_epochs</code> = -1.</li> <li><code>min_epochs</code>: (<code>int</code>) Force training for at least these many epochs</li> </ul>"},{"location":"configs/training/#examples_10","title":"Examples:","text":"<pre><code>trainer:\n  check_val_every_n_epoch: 1\n  enable_checkpointing: true\n  gradient_clip_val: null\n  limit_train_batches: 1.0\n  limit_test_batches: 1.0\n  limit_val_batches: 1.0\n  log_every_n_steps: 1\n  max_epochs: 100\n  min_epochs: 10\n</code></pre>"},{"location":"io/","title":"DREEM <code>io</code> module.","text":"<p>The <code>io</code> module contains classes for storing, manipulating and generating model inputs and outputs.</p> <p>There are 6 main submodules:</p> <ol> <li><code>Instance</code> which represent detections. They are the main input into the model and contain <ul> <li>GT Track ID</li> <li>Predicted Track ID</li> <li>the crop of the image</li> <li>the location as a centroid <code>(x,y)</code>, bounding box <code>(y_1, x_1, y_2, x_2)</code> and (optionally) the pose <code>(1, n, 2)</code></li> <li>the features extracted by the visual encoder + spatiotemporal embedding</li> <li>association score</li> </ul> </li> <li><code>Frame</code> which stores the metadata for a single frame of a video. It contains:<ul> <li>The video index</li> <li>The video name</li> <li>The frame index</li> <li>The list of <code>Instance</code>s that appear in that frame</li> <li>The <code>AssociationMatrix</code> between the instances in that frame and all instances in the window</li> </ul> </li> <li><code>Config</code> which stores and parses the configs for training and/or</li> <li><code>AssociationMatrix</code> which stores the <code>GlobalTrackingTransformer</code> output. <code>AssociationMatrix[i,j]</code> represents the association score between the <code>i</code>th query instance and the <code>j</code>th instance in the window.</li> <li><code>Track</code> which stores references to all instances belonging to the same trajectory</li> <li><code>visualize</code> which contains util functions for generating videos annotated by the predicted tracks and scores.</li> </ol>"},{"location":"io/asso_matrix/","title":"<code>AssociationMatrix</code>","text":""},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix","title":"<code>dreem.io.AssociationMatrix</code>","text":"<p>Class representing the associations between detections.</p> <p>Attributes:</p> Name Type Description <code>matrix</code> <code>ndarray | Tensor</code> <p>the <code>n_query x n_ref</code> association matrix`</p> <code>ref_instances</code> <code>list[Instance]</code> <p>all instances used to associate against.</p> <code>query_instances</code> <code>list[Instance]</code> <p>query instances that were associated against ref instances.</p> <p>Methods:</p> Name Description <code>__getindices__</code> <p>Get the indices of the instance for lookup.</p> <code>__getitem__</code> <p>Get elements of the association matrix.</p> <code>__repr__</code> <p>Get the string representation of the Association Matrix.</p> <code>get_tracks</code> <p>Group instances by track.</p> <code>numpy</code> <p>Convert association matrix to a numpy array.</p> <code>reduce</code> <p>Aggregate the association matrix by specified dimensions and grouping.</p> <code>to</code> <p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <code>to_dataframe</code> <p>Convert the association matrix to a pandas DataFrame.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>@attrs.define\nclass AssociationMatrix:\n    \"\"\"Class representing the associations between detections.\n\n    Attributes:\n        matrix: the `n_query x n_ref` association matrix`\n        ref_instances: all instances used to associate against.\n        query_instances: query instances that were associated against ref instances.\n    \"\"\"\n\n    matrix: np.ndarray | torch.Tensor\n    ref_instances: list[Instance] = attrs.field()\n    query_instances: list[Instance] = attrs.field()\n\n    @ref_instances.validator\n    def _check_ref_instances(self, attribute, value):\n        \"\"\"Check to ensure that the number of association matrix columns and reference instances match.\n\n        Args:\n            attribute: The ref instances.\n            value: the list of ref instances.\n\n        Raises:\n            ValueError if the number of columns and reference instances don't match.\n        \"\"\"\n        if len(value) != self.matrix.shape[-1]:\n            raise ValueError(\n                (\n                    \"Ref instances must equal number of columns in Association matrix\"\n                    f\"Found {len(value)} ref instances but {self.matrix.shape[-1]} columns.\"\n                )\n            )\n\n    @query_instances.validator\n    def _check_query_instances(self, attribute, value):\n        \"\"\"Check to ensure that the number of association matrix rows and query instances match.\n\n        Args:\n            attribute: The query instances.\n            value: the list of query instances.\n\n        Raises:\n            ValueError if the number of rows and query instances don't match.\n        \"\"\"\n        if len(value) != self.matrix.shape[0]:\n            raise ValueError(\n                (\n                    \"Query instances must equal number of rows in Association matrix\"\n                    f\"Found {len(value)} query instances but {self.matrix.shape[0]} rows.\"\n                )\n            )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get the string representation of the Association Matrix.\n\n        Returns:\n            the string representation of the association matrix.\n        \"\"\"\n        return (\n            f\"AssociationMatrix({self.matrix},\"\n            f\"query_instances={len(self.query_instances)},\"\n            f\"ref_instances={len(self.ref_instances)})\"\n        )\n\n    def numpy(self) -&gt; np.ndarray:\n        \"\"\"Convert association matrix to a numpy array.\n\n        Returns:\n            The association matrix as a numpy array.\n        \"\"\"\n        if isinstance(self.matrix, torch.Tensor):\n            return self.matrix.detach().cpu().numpy()\n        return self.matrix\n\n    def to_dataframe(\n        self, row_labels: str = \"gt\", col_labels: str = \"gt\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Convert the association matrix to a pandas DataFrame.\n\n        Args:\n            row_labels: How to label the rows(queries).\n                If list, then must match # of rows/queries\n                If `\"gt\"` then label by gt track id.\n                If `\"pred\"` then label by pred track id.\n                Otherwise label by the query_instance indices\n            col_labels: How to label the columns(references).\n                If list, then must match # of columns/refs\n                If `\"gt\"` then label by gt track id.\n                If `\"pred\"` then label by pred track id.\n                Otherwise label by the ref_instance indices\n\n        Returns:\n            The association matrix as a pandas dataframe.\n        \"\"\"\n        matrix = self.numpy()\n\n        if not isinstance(row_labels, str):\n            if len(row_labels) == len(self.query_instances):\n                row_inds = row_labels\n\n            else:\n                raise ValueError(\n                    (\n                        \"Mismatched # of rows and labels!\",\n                        f\"Found {len(row_labels)} with {len(self.query_instances)} rows\",\n                    )\n                )\n\n        else:\n            if row_labels == \"gt\":\n                row_inds = [\n                    instance.gt_track_id.item() for instance in self.query_instances\n                ]\n\n            elif row_labels == \"pred\":\n                row_inds = [\n                    instance.pred_track_id.item() for instance in self.query_instances\n                ]\n\n            else:\n                row_inds = np.arange(len(self.query_instances))\n\n        if not isinstance(col_labels, str):\n            if len(col_labels) == len(self.ref_instances):\n                col_inds = col_labels\n\n            else:\n                raise ValueError(\n                    (\n                        \"Mismatched # of columns and labels!\",\n                        f\"Found {len(col_labels)} with {len(self.ref_instances)} columns\",\n                    )\n                )\n\n        else:\n            if col_labels == \"gt\":\n                col_inds = [\n                    instance.gt_track_id.item() for instance in self.ref_instances\n                ]\n\n            elif col_labels == \"pred\":\n                col_inds = [\n                    instance.pred_track_id.item() for instance in self.ref_instances\n                ]\n\n            else:\n                col_inds = np.arange(len(self.ref_instances))\n\n        asso_df = pd.DataFrame(matrix, index=row_inds, columns=col_inds)\n\n        return asso_df\n\n    def reduce(\n        self,\n        row_dims: str = \"instance\",\n        col_dims: str = \"track\",\n        row_grouping: str | None = None,\n        col_grouping: str = \"pred\",\n        reduce_method: callable = np.sum,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Aggregate the association matrix by specified dimensions and grouping.\n\n        Args:\n           row_dims: A str indicating how to what dimensions to reduce rows to.\n                Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).\n           col_dims: A str indicating how to dimensions to reduce rows to.\n                Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)\n           row_grouping: A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".\n           col_grouping: A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".\n           reduce_method: A callable function that operates on numpy matrices and can take an `axis` arg for reducing.\n\n        Returns:\n            The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.\n        \"\"\"\n        n_rows = len(self.query_instances)\n        n_cols = len(self.ref_instances)\n\n        col_tracks = {-1: self.ref_instances}\n        row_tracks = {-1: self.query_instances}\n\n        col_inds = [i for i in range(len(self.ref_instances))]\n        row_inds = [i for i in range(len(self.query_instances))]\n\n        if col_dims == \"track\":\n            col_tracks = self.get_tracks(self.ref_instances, col_grouping)\n            col_inds = list(col_tracks.keys())\n            n_cols = len(col_inds)\n\n        if row_dims == \"track\":\n            row_tracks = self.get_tracks(self.query_instances, row_grouping)\n            row_inds = list(row_tracks.keys())\n            n_rows = len(row_inds)\n\n        reduced_matrix = []\n        for row_track, row_instances in row_tracks.items():\n            for col_track, col_instances in col_tracks.items():\n                asso_matrix = self[row_instances, col_instances]\n\n                if col_dims == \"track\":\n                    asso_matrix = reduce_method(asso_matrix, axis=1)\n\n                if row_dims == \"track\":\n                    asso_matrix = reduce_method(asso_matrix, axis=0)\n\n                reduced_matrix.append(asso_matrix)\n\n        reduced_matrix = np.array(reduced_matrix).reshape(n_cols, n_rows).T\n\n        return pd.DataFrame(reduced_matrix, index=row_inds, columns=col_inds)\n\n    def __getitem__(\n        self, inds: tuple[int | Instance | list[int | Instance]]\n    ) -&gt; np.ndarray:\n        \"\"\"Get elements of the association matrix.\n\n        Args:\n            inds: A tuple of query indices and reference indices.\n                Indices can be either:\n                    A single instance or integer.\n                    A list of instances or integers.\n\n        Returns:\n            An np.ndarray containing the elements requested.\n        \"\"\"\n        query_inst, ref_inst = inds\n\n        query_ind = self.__getindices__(query_inst, self.query_instances)\n        ref_ind = self.__getindices__(ref_inst, self.ref_instances)\n\n        try:\n            return self.numpy()[query_ind[:, None], ref_ind].squeeze()\n        except IndexError as e:\n            logger.exception(f\"Query_insts: {type(query_inst)}\")\n            logger.exception(f\"Query_inds: {query_ind}\")\n            logger.exception(f\"Ref_insts: {type(ref_inst)}\")\n            logger.exception(f\"Ref_ind: {ref_ind}\")\n            logger.exception(e)\n            raise (e)\n\n    def __getindices__(\n        self,\n        instance: Instance | int | np.typing.ArrayLike,\n        instance_lookup: list[Instance],\n    ) -&gt; np.ndarray:\n        \"\"\"Get the indices of the instance for lookup.\n\n        Args:\n            instance: The instance(s) to be retrieved\n                Can either be a single int/instance or a list of int/instances\n            instance_lookup: A list of Instances to be used to retrieve indices\n\n        Returns:\n            A np array of indices.\n        \"\"\"\n        if isinstance(instance, Instance):\n            ind = np.array([instance_lookup.index(instance)])\n\n        elif instance is None:\n            ind = np.arange(len(instance_lookup))\n\n        elif np.isscalar(instance):\n            ind = np.array([instance])\n\n        else:\n            instances = instance\n            if not [isinstance(inst, (Instance, int)) for inst in instance]:\n                raise ValueError(\n                    f\"List of indices must be `int` or `Instance`. Found {set([type(inst) for inst in instance])}\"\n                )\n            ind = np.array(\n                [\n                    (\n                        instance_lookup.index(instance)\n                        if isinstance(instance, Instance)\n                        else instance\n                    )\n                    for instance in instances\n                ]\n            )\n\n        return ind\n\n    def get_tracks(\n        self, instances: list[\"Instance\"], label: str = \"pred\"\n    ) -&gt; dict[int, list[\"Instance\"]]:\n        \"\"\"Group instances by track.\n\n        Args:\n            instances: The list of instances to group\n            label: the track id type to group by. Either `pred` or `gt`.\n\n        Returns:\n            A dictionary of track_id:instances\n        \"\"\"\n        if label == \"pred\":\n            traj_ids = set([instance.pred_track_id.item() for instance in instances])\n            traj = {\n                track_id: [\n                    instance\n                    for instance in instances\n                    if instance.pred_track_id.item() == track_id\n                ]\n                for track_id in traj_ids\n            }\n\n        elif label == \"gt\":\n            traj_ids = set(\n                [instance.gt_track_id.item() for instance in self.ref_instances]\n            )\n            traj = {\n                track_id: [\n                    instance\n                    for instance in self.ref_instances\n                    if instance.gt_track_id.item() == track_id\n                ]\n                for track_id in traj_ids\n            }\n\n        else:\n            raise ValueError(f\"Unsupported label '{label}'. Expected 'pred' or 'gt'.\")\n\n        return traj\n\n    def to(self, map_location: str | torch.device) -&gt; Self:\n        \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n        Args:\n            map_location: Either the device or dtype for the instance to be moved.\n\n        Returns:\n            self: reference to the instance moved to correct device/dtype.\n        \"\"\"\n        self.matrix = self.matrix.to(map_location)\n        self.ref_instances = [\n            instance.to(map_location) for instance in self.ref_instances\n        ]\n        self.query_instances = [\n            instance.to(map_location) for instance in self.query_instances\n        ]\n\n        return self\n</code></pre>"},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix.__getindices__","title":"<code>__getindices__(instance, instance_lookup)</code>","text":"<p>Get the indices of the instance for lookup.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>Instance | int | ArrayLike</code> <p>The instance(s) to be retrieved Can either be a single int/instance or a list of int/instances</p> required <code>instance_lookup</code> <code>list[Instance]</code> <p>A list of Instances to be used to retrieve indices</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A np array of indices.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __getindices__(\n    self,\n    instance: Instance | int | np.typing.ArrayLike,\n    instance_lookup: list[Instance],\n) -&gt; np.ndarray:\n    \"\"\"Get the indices of the instance for lookup.\n\n    Args:\n        instance: The instance(s) to be retrieved\n            Can either be a single int/instance or a list of int/instances\n        instance_lookup: A list of Instances to be used to retrieve indices\n\n    Returns:\n        A np array of indices.\n    \"\"\"\n    if isinstance(instance, Instance):\n        ind = np.array([instance_lookup.index(instance)])\n\n    elif instance is None:\n        ind = np.arange(len(instance_lookup))\n\n    elif np.isscalar(instance):\n        ind = np.array([instance])\n\n    else:\n        instances = instance\n        if not [isinstance(inst, (Instance, int)) for inst in instance]:\n            raise ValueError(\n                f\"List of indices must be `int` or `Instance`. Found {set([type(inst) for inst in instance])}\"\n            )\n        ind = np.array(\n            [\n                (\n                    instance_lookup.index(instance)\n                    if isinstance(instance, Instance)\n                    else instance\n                )\n                for instance in instances\n            ]\n        )\n\n    return ind\n</code></pre>"},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix.__getitem__","title":"<code>__getitem__(inds)</code>","text":"<p>Get elements of the association matrix.</p> <p>Parameters:</p> Name Type Description Default <code>inds</code> <code>tuple[int | Instance | list[int | Instance]]</code> <p>A tuple of query indices and reference indices. Indices can be either:     A single instance or integer.     A list of instances or integers.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>An np.ndarray containing the elements requested.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __getitem__(\n    self, inds: tuple[int | Instance | list[int | Instance]]\n) -&gt; np.ndarray:\n    \"\"\"Get elements of the association matrix.\n\n    Args:\n        inds: A tuple of query indices and reference indices.\n            Indices can be either:\n                A single instance or integer.\n                A list of instances or integers.\n\n    Returns:\n        An np.ndarray containing the elements requested.\n    \"\"\"\n    query_inst, ref_inst = inds\n\n    query_ind = self.__getindices__(query_inst, self.query_instances)\n    ref_ind = self.__getindices__(ref_inst, self.ref_instances)\n\n    try:\n        return self.numpy()[query_ind[:, None], ref_ind].squeeze()\n    except IndexError as e:\n        logger.exception(f\"Query_insts: {type(query_inst)}\")\n        logger.exception(f\"Query_inds: {query_ind}\")\n        logger.exception(f\"Ref_insts: {type(ref_inst)}\")\n        logger.exception(f\"Ref_ind: {ref_ind}\")\n        logger.exception(e)\n        raise (e)\n</code></pre>"},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix.__repr__","title":"<code>__repr__()</code>","text":"<p>Get the string representation of the Association Matrix.</p> <p>Returns:</p> Type Description <code>str</code> <p>the string representation of the association matrix.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get the string representation of the Association Matrix.\n\n    Returns:\n        the string representation of the association matrix.\n    \"\"\"\n    return (\n        f\"AssociationMatrix({self.matrix},\"\n        f\"query_instances={len(self.query_instances)},\"\n        f\"ref_instances={len(self.ref_instances)})\"\n    )\n</code></pre>"},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix.get_tracks","title":"<code>get_tracks(instances, label='pred')</code>","text":"<p>Group instances by track.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[Instance]</code> <p>The list of instances to group</p> required <code>label</code> <code>str</code> <p>the track id type to group by. Either <code>pred</code> or <code>gt</code>.</p> <code>'pred'</code> <p>Returns:</p> Type Description <code>dict[int, list[Instance]]</code> <p>A dictionary of track_id:instances</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def get_tracks(\n    self, instances: list[\"Instance\"], label: str = \"pred\"\n) -&gt; dict[int, list[\"Instance\"]]:\n    \"\"\"Group instances by track.\n\n    Args:\n        instances: The list of instances to group\n        label: the track id type to group by. Either `pred` or `gt`.\n\n    Returns:\n        A dictionary of track_id:instances\n    \"\"\"\n    if label == \"pred\":\n        traj_ids = set([instance.pred_track_id.item() for instance in instances])\n        traj = {\n            track_id: [\n                instance\n                for instance in instances\n                if instance.pred_track_id.item() == track_id\n            ]\n            for track_id in traj_ids\n        }\n\n    elif label == \"gt\":\n        traj_ids = set(\n            [instance.gt_track_id.item() for instance in self.ref_instances]\n        )\n        traj = {\n            track_id: [\n                instance\n                for instance in self.ref_instances\n                if instance.gt_track_id.item() == track_id\n            ]\n            for track_id in traj_ids\n        }\n\n    else:\n        raise ValueError(f\"Unsupported label '{label}'. Expected 'pred' or 'gt'.\")\n\n    return traj\n</code></pre>"},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix.numpy","title":"<code>numpy()</code>","text":"<p>Convert association matrix to a numpy array.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The association matrix as a numpy array.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def numpy(self) -&gt; np.ndarray:\n    \"\"\"Convert association matrix to a numpy array.\n\n    Returns:\n        The association matrix as a numpy array.\n    \"\"\"\n    if isinstance(self.matrix, torch.Tensor):\n        return self.matrix.detach().cpu().numpy()\n    return self.matrix\n</code></pre>"},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix.reduce","title":"<code>reduce(row_dims='instance', col_dims='track', row_grouping=None, col_grouping='pred', reduce_method=np.sum)</code>","text":"<p>Aggregate the association matrix by specified dimensions and grouping.</p> <p>Parameters:</p> Name Type Description Default <code>row_dims</code> <code>str</code> <p>A str indicating how to what dimensions to reduce rows to.   Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).</p> <code>'instance'</code> <code>col_dims</code> <code>str</code> <p>A str indicating how to dimensions to reduce rows to.   Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)</p> <code>'track'</code> <code>row_grouping</code> <code>str | None</code> <p>A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".</p> <code>None</code> <code>col_grouping</code> <code>str</code> <p>A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".</p> <code>'pred'</code> <code>reduce_method</code> <code>callable</code> <p>A callable function that operates on numpy matrices and can take an <code>axis</code> arg for reducing.</p> <code>sum</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def reduce(\n    self,\n    row_dims: str = \"instance\",\n    col_dims: str = \"track\",\n    row_grouping: str | None = None,\n    col_grouping: str = \"pred\",\n    reduce_method: callable = np.sum,\n) -&gt; pd.DataFrame:\n    \"\"\"Aggregate the association matrix by specified dimensions and grouping.\n\n    Args:\n       row_dims: A str indicating how to what dimensions to reduce rows to.\n            Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).\n       col_dims: A str indicating how to dimensions to reduce rows to.\n            Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)\n       row_grouping: A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".\n       col_grouping: A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".\n       reduce_method: A callable function that operates on numpy matrices and can take an `axis` arg for reducing.\n\n    Returns:\n        The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.\n    \"\"\"\n    n_rows = len(self.query_instances)\n    n_cols = len(self.ref_instances)\n\n    col_tracks = {-1: self.ref_instances}\n    row_tracks = {-1: self.query_instances}\n\n    col_inds = [i for i in range(len(self.ref_instances))]\n    row_inds = [i for i in range(len(self.query_instances))]\n\n    if col_dims == \"track\":\n        col_tracks = self.get_tracks(self.ref_instances, col_grouping)\n        col_inds = list(col_tracks.keys())\n        n_cols = len(col_inds)\n\n    if row_dims == \"track\":\n        row_tracks = self.get_tracks(self.query_instances, row_grouping)\n        row_inds = list(row_tracks.keys())\n        n_rows = len(row_inds)\n\n    reduced_matrix = []\n    for row_track, row_instances in row_tracks.items():\n        for col_track, col_instances in col_tracks.items():\n            asso_matrix = self[row_instances, col_instances]\n\n            if col_dims == \"track\":\n                asso_matrix = reduce_method(asso_matrix, axis=1)\n\n            if row_dims == \"track\":\n                asso_matrix = reduce_method(asso_matrix, axis=0)\n\n            reduced_matrix.append(asso_matrix)\n\n    reduced_matrix = np.array(reduced_matrix).reshape(n_cols, n_rows).T\n\n    return pd.DataFrame(reduced_matrix, index=row_inds, columns=col_inds)\n</code></pre>"},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix.to","title":"<code>to(map_location)</code>","text":"<p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <p>Parameters:</p> Name Type Description Default <code>map_location</code> <code>str | device</code> <p>Either the device or dtype for the instance to be moved.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>Self</code> <p>reference to the instance moved to correct device/dtype.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def to(self, map_location: str | torch.device) -&gt; Self:\n    \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n    Args:\n        map_location: Either the device or dtype for the instance to be moved.\n\n    Returns:\n        self: reference to the instance moved to correct device/dtype.\n    \"\"\"\n    self.matrix = self.matrix.to(map_location)\n    self.ref_instances = [\n        instance.to(map_location) for instance in self.ref_instances\n    ]\n    self.query_instances = [\n        instance.to(map_location) for instance in self.query_instances\n    ]\n\n    return self\n</code></pre>"},{"location":"io/asso_matrix/#dreem.io.AssociationMatrix.to_dataframe","title":"<code>to_dataframe(row_labels='gt', col_labels='gt')</code>","text":"<p>Convert the association matrix to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>row_labels</code> <code>str</code> <p>How to label the rows(queries). If list, then must match # of rows/queries If <code>\"gt\"</code> then label by gt track id. If <code>\"pred\"</code> then label by pred track id. Otherwise label by the query_instance indices</p> <code>'gt'</code> <code>col_labels</code> <code>str</code> <p>How to label the columns(references). If list, then must match # of columns/refs If <code>\"gt\"</code> then label by gt track id. If <code>\"pred\"</code> then label by pred track id. Otherwise label by the ref_instance indices</p> <code>'gt'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The association matrix as a pandas dataframe.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def to_dataframe(\n    self, row_labels: str = \"gt\", col_labels: str = \"gt\"\n) -&gt; pd.DataFrame:\n    \"\"\"Convert the association matrix to a pandas DataFrame.\n\n    Args:\n        row_labels: How to label the rows(queries).\n            If list, then must match # of rows/queries\n            If `\"gt\"` then label by gt track id.\n            If `\"pred\"` then label by pred track id.\n            Otherwise label by the query_instance indices\n        col_labels: How to label the columns(references).\n            If list, then must match # of columns/refs\n            If `\"gt\"` then label by gt track id.\n            If `\"pred\"` then label by pred track id.\n            Otherwise label by the ref_instance indices\n\n    Returns:\n        The association matrix as a pandas dataframe.\n    \"\"\"\n    matrix = self.numpy()\n\n    if not isinstance(row_labels, str):\n        if len(row_labels) == len(self.query_instances):\n            row_inds = row_labels\n\n        else:\n            raise ValueError(\n                (\n                    \"Mismatched # of rows and labels!\",\n                    f\"Found {len(row_labels)} with {len(self.query_instances)} rows\",\n                )\n            )\n\n    else:\n        if row_labels == \"gt\":\n            row_inds = [\n                instance.gt_track_id.item() for instance in self.query_instances\n            ]\n\n        elif row_labels == \"pred\":\n            row_inds = [\n                instance.pred_track_id.item() for instance in self.query_instances\n            ]\n\n        else:\n            row_inds = np.arange(len(self.query_instances))\n\n    if not isinstance(col_labels, str):\n        if len(col_labels) == len(self.ref_instances):\n            col_inds = col_labels\n\n        else:\n            raise ValueError(\n                (\n                    \"Mismatched # of columns and labels!\",\n                    f\"Found {len(col_labels)} with {len(self.ref_instances)} columns\",\n                )\n            )\n\n    else:\n        if col_labels == \"gt\":\n            col_inds = [\n                instance.gt_track_id.item() for instance in self.ref_instances\n            ]\n\n        elif col_labels == \"pred\":\n            col_inds = [\n                instance.pred_track_id.item() for instance in self.ref_instances\n            ]\n\n        else:\n            col_inds = np.arange(len(self.ref_instances))\n\n    asso_df = pd.DataFrame(matrix, index=row_inds, columns=col_inds)\n\n    return asso_df\n</code></pre>"},{"location":"io/config/","title":"<code>Config</code>","text":""},{"location":"io/config/#dreem.io.Config","title":"<code>dreem.io.Config</code>","text":"<p>Class handling loading components based on config params.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class with config from hydra/omega conf.</p> <code>__repr__</code> <p>Object representation of config class.</p> <code>__str__</code> <p>Return a string representation of config class.</p> <code>from_yaml</code> <p>Load config directly from yaml.</p> <code>get</code> <p>Get config item.</p> <code>get_checkpointing</code> <p>Getter for lightning checkpointing callback.</p> <code>get_ctc_paths</code> <p>Get file paths from directory. Only for CTC datasets.</p> <code>get_data_paths</code> <p>Get file paths from directory. Only for SLEAP datasets.</p> <code>get_dataloader</code> <p>Getter for dataloader.</p> <code>get_dataset</code> <p>Getter for datasets.</p> <code>get_early_stopping</code> <p>Getter for lightning early stopping callback.</p> <code>get_gtr_runner</code> <p>Get lightning module for training, validation, and inference.</p> <code>get_logger</code> <p>Getter for logging callback.</p> <code>get_loss</code> <p>Getter for loss functions.</p> <code>get_model</code> <p>Getter for gtr model.</p> <code>get_optimizer</code> <p>Getter for optimizer.</p> <code>get_scheduler</code> <p>Getter for lr scheduler.</p> <code>get_tracker_cfg</code> <p>Getter for tracker config params.</p> <code>get_trainer</code> <p>Getter for the lightning trainer.</p> <code>set_hparams</code> <p>Setter function for overwriting specific hparams.</p> <p>Attributes:</p> Name Type Description <code>data_paths</code> <p>Get data paths.</p> Source code in <code>dreem/io/config.py</code> <pre><code>class Config:\n    \"\"\"Class handling loading components based on config params.\"\"\"\n\n    def __init__(self, cfg: DictConfig, params_cfg: DictConfig | None = None):\n        \"\"\"Initialize the class with config from hydra/omega conf.\n\n        First uses `base_param` file then overwrites with specific `params_config`.\n\n        Args:\n            cfg: The `DictConfig` containing all the hyperparameters needed for\n                training/evaluation.\n            params_cfg: The `DictConfig` containing subset of hyperparameters to override.\n                training/evaluation\n        \"\"\"\n        base_cfg = cfg\n        logger.info(f\"Base Config: {cfg}\")\n\n        if \"params_config\" in cfg:\n            params_cfg = OmegaConf.load(cfg.params_config)\n\n        if params_cfg:\n            logger.info(f\"Overwriting base config with {params_cfg}\")\n            with open_dict(base_cfg):\n                self.cfg = OmegaConf.merge(base_cfg, params_cfg)  # merge configs\n        else:\n            self.cfg = cfg\n\n        OmegaConf.set_struct(self.cfg, False)\n\n        self._vid_files = {}\n\n    def __repr__(self):\n        \"\"\"Object representation of config class.\"\"\"\n        return f\"Config({self.cfg})\"\n\n    def __str__(self):\n        \"\"\"Return a string representation of config class.\"\"\"\n        return f\"Config({self.cfg})\"\n\n    @classmethod\n    def from_yaml(cls, base_cfg_path: str, params_cfg_path: str | None = None) -&gt; None:\n        \"\"\"Load config directly from yaml.\n\n        Args:\n            base_cfg_path: path to base config file.\n            params_cfg_path: path to override params.\n        \"\"\"\n        base_cfg = OmegaConf.load(base_cfg_path)\n        params_cfg = OmegaConf.load(params_cfg_path) if params_cfg_path else None\n        return cls(base_cfg, params_cfg)\n\n    def set_hparams(self, hparams: dict) -&gt; bool:\n        \"\"\"Setter function for overwriting specific hparams.\n\n        Useful for changing 1 or 2 hyperparameters such as dataset.\n\n        Args:\n            hparams: A dict containing the hyperparameter to be overwritten and\n                the value to be changed\n\n        Returns:\n            `True` if config is successfully updated, `False` otherwise\n        \"\"\"\n        if hparams == {} or hparams is None:\n            logger.warning(\"Nothing to update!\")\n            return False\n        for hparam, val in hparams.items():\n            try:\n                OmegaConf.update(self.cfg, hparam, val)\n            except Exception as e:\n                logger.exception(f\"Failed to update {hparam} to {val} due to {e}\")\n                return False\n        return True\n\n    def get(self, key: str, default=None, cfg: dict = None):\n        \"\"\"Get config item.\n\n        Args:\n            key: key of item to return\n            default: default value to return if key is missing.\n            cfg: the config dict from which to retrieve an item\n        \"\"\"\n        if cfg is None:\n            cfg = self.cfg\n\n        param = cfg.get(key, default)\n\n        if isinstance(param, DictConfig):\n            param = OmegaConf.to_container(param, resolve=True)\n\n        return param\n\n    def get_model(self) -&gt; \"GlobalTrackingTransformer\":\n        \"\"\"Getter for gtr model.\n\n        Returns:\n            A global tracking transformer with parameters indicated by cfg\n        \"\"\"\n        from dreem.models import GlobalTrackingTransformer, GTRRunner\n\n        model_params = self.get(\"model\", {})\n\n        ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n        if ckpt_path is not None and len(ckpt_path) &gt; 0:\n            return GTRRunner.load_from_checkpoint(ckpt_path).model\n\n        return GlobalTrackingTransformer(**model_params)\n\n    def get_tracker_cfg(self) -&gt; dict:\n        \"\"\"Getter for tracker config params.\n\n        Returns:\n            A dict containing the init params for `Tracker`.\n        \"\"\"\n        return self.get(\"tracker\", {})\n\n    def get_gtr_runner(self, ckpt_path: str | None = None) -&gt; \"GTRRunner\":\n        \"\"\"Get lightning module for training, validation, and inference.\n\n        Args:\n            ckpt_path: path to checkpoint for override\n\n        Returns:\n            a gtr runner model\n        \"\"\"\n        from dreem.models import GTRRunner\n\n        keys = [\"tracker\", \"optimizer\", \"scheduler\", \"loss\", \"runner\", \"model\"]\n        args = [key + \"_cfg\" if key != \"runner\" else key for key in keys]\n\n        params = {}\n        for key, arg in zip(keys, args):\n            sub_params = self.get(key, {})\n\n            if len(sub_params) == 0:\n                logger.warning(\n                    f\"`{key}` not found in config or is empty. Using defaults for {arg}!\"\n                )\n\n            if key == \"runner\":\n                runner_params = sub_params\n                for k, v in runner_params.items():\n                    params[k] = v\n            else:\n                params[arg] = sub_params\n\n        ckpt_path = params[\"model_cfg\"].pop(\"ckpt_path\", None)\n\n        if ckpt_path is not None and ckpt_path != \"\":\n            model = GTRRunner.load_from_checkpoint(\n                ckpt_path, tracker_cfg=params[\"tracker_cfg\"], **runner_params\n            )\n\n        else:\n            model = GTRRunner(**params)\n\n        return model\n\n    def get_ctc_paths(\n        self, list_dir_path: list[str]\n    ) -&gt; tuple[list[str], list[str], list[str]]:\n        \"\"\"Get file paths from directory. Only for CTC datasets.\n\n        Args:\n            list_dir_path: list of directories to search for labels and videos\n\n        Returns:\n            lists of labels file paths and video file paths\n        \"\"\"\n        gt_list = []\n        raw_img_list = []\n        ctc_track_meta = []\n        # user can specify a list of directories, each of which can contain several subdirectories that come in pairs of (dset_name, dset_name_GT/TRA)\n        for dir_path in list_dir_path:\n            for subdir in os.listdir(dir_path):\n                if subdir.endswith(\"_GT\"):\n                    gt_path = os.path.join(dir_path, subdir, \"TRA\")\n                    raw_img_path = os.path.join(dir_path, subdir.replace(\"_GT\", \"\"))\n                    # get filepaths for all tif files in gt_path\n                    gt_list.append(glob.glob(os.path.join(gt_path, \"*.tif\")))\n                    # get filepaths for all tif files in raw_img_path\n                    raw_img_list.append(glob.glob(os.path.join(raw_img_path, \"*.tif\")))\n                    man_track_file = glob.glob(os.path.join(gt_path, \"man_track.txt\"))\n                    if len(man_track_file) &gt; 0:\n                        ctc_track_meta.append(man_track_file[0])\n                    else:\n                        logger.debug(\n                            f\"No man_track.txt file found in {gt_path}. Continuing...\"\n                        )\n                else:\n                    continue\n\n        return gt_list, raw_img_list, ctc_track_meta\n\n    def get_data_paths(self, mode: str, data_cfg: dict) -&gt; tuple[list[str], list[str]]:\n        \"\"\"Get file paths from directory. Only for SLEAP datasets.\n\n        Args:\n            mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n                train, val, or test params for dataset\n            data_cfg: Config for the dataset containing \"dir\" key.\n\n        Returns:\n            lists of labels file paths and video file paths respectively\n        \"\"\"\n        # hack to get around the fact that for test mode, get_data_paths is called before get_dataset.\n        # also, for train/val mode, data_cfg has had the dir key popped through self.get() called in get_dataset()\n        if mode == \"test\":\n            list_dir_path = data_cfg.get(\"dir\", {}).get(\"path\", None)\n            if list_dir_path is None:\n                raise ValueError(\n                    \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n                )\n            self.labels_suffix = data_cfg.get(\"dir\", {}).get(\"labels_suffix\")\n            self.vid_suffix = data_cfg.get(\"dir\", {}).get(\"vid_suffix\")\n        else:\n            list_dir_path = self.data_dirs\n        if not isinstance(list_dir_path, list):\n            list_dir_path = [list_dir_path]\n\n        if self.labels_suffix == \".slp\":\n            label_files = []\n            vid_files = []\n            for dir_path in list_dir_path:\n                logger.debug(f\"Searching `{dir_path}` directory\")\n                labels_path = f\"{dir_path}/*{self.labels_suffix}\"\n                vid_path = f\"{dir_path}/*{self.vid_suffix}\"\n                logger.debug(f\"Searching for labels matching {labels_path}\")\n                label_files.extend(glob.glob(labels_path))\n                logger.debug(f\"Searching for videos matching {vid_path}\")\n                vid_files.extend(glob.glob(vid_path))\n\n        elif self.labels_suffix == \".tif\":\n            label_files, vid_files, ctc_track_meta = self.get_ctc_paths(list_dir_path)\n\n        logger.debug(f\"Found {len(label_files)} labels and {len(vid_files)} videos\")\n\n        # backdoor to set label files directly in the configs (i.e. bypass dir.path)\n        if data_cfg.get(\"slp_files\", None):\n            logger.debug(\"Overriding label files with user provided list\")\n            slp_files = data_cfg.get(\"slp_files\")\n            if len(slp_files) &gt; 0:\n                label_files = slp_files\n        if data_cfg.get(\"video_files\", None):\n            individual_video_files = data_cfg.get(\"video_files\")\n            if len(individual_video_files) &gt; 0:\n                vid_files = individual_video_files\n        return label_files, vid_files\n\n    def get_dataset(\n        self,\n        mode: str,\n        label_files: list[str] | None = None,\n        vid_files: list[str | list[str]] = None,\n    ) -&gt; \"SleapDataset\" | \"CellTrackingDataset\":\n        \"\"\"Getter for datasets.\n\n        Args:\n            mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n                train, val, or test params for dataset\n            label_files: path to label_files for override\n            vid_files: path to vid_files for override\n\n        Returns:\n            Either a `SleapDataset` or `CellTrackingDataset` with params indicated by cfg\n        \"\"\"\n        from dreem.datasets import CellTrackingDataset, SleapDataset\n\n        dataset_params = self.get(\"dataset\")\n        if dataset_params is None:\n            raise KeyError(\"`dataset` key is missing from cfg!\")\n\n        if mode.lower() == \"train\":\n            dataset_params = self.get(\"train_dataset\", {}, dataset_params)\n        elif mode.lower() == \"val\":\n            dataset_params = self.get(\"val_dataset\", {}, dataset_params)\n        elif mode.lower() == \"test\":\n            dataset_params = self.get(\"test_dataset\", {}, dataset_params)\n        else:\n            raise ValueError(\n                \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n            )\n\n        # input validation\n        self.data_dirs = dataset_params.get(\"dir\", {}).get(\"path\", None)\n        self.labels_suffix = dataset_params.get(\"dir\", {}).get(\"labels_suffix\")\n        self.vid_suffix = dataset_params.get(\"dir\", {}).get(\"vid_suffix\")\n        if self.data_dirs is None:\n            raise ValueError(\n                \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n            )\n        if self.labels_suffix is None or self.vid_suffix is None:\n            raise KeyError(\n                f\"Must provide a labels suffix and vid suffix to search for but found {self.labels_suffix} and {self.vid_suffix}\"\n            )\n\n        # infer dataset type from the user provided suffix\n        if self.labels_suffix == \".slp\":\n            # during training, multiple files can be used at once, so label_files is not passed in\n            # during inference, a single label_files string can be passed in as get_data_paths is\n            # called before get_dataset, hence the check\n            if label_files is None or vid_files is None:\n                label_files, vid_files = self.get_data_paths(mode, dataset_params)\n            dataset_params[\"slp_files\"] = label_files\n            dataset_params[\"video_files\"] = vid_files\n            dataset_params[\"data_dirs\"] = self.data_dirs\n            self.data_paths = (mode, vid_files)\n\n            return SleapDataset(**dataset_params)\n\n        elif self.labels_suffix == \".tif\":\n            # for CTC datasets, pass in a list of gt and raw image directories, eaech of which contain tifs\n            ctc_track_meta = None\n            list_dir_path = self.data_dirs  # don't modify self.data_dirs\n            if not isinstance(list_dir_path, list):\n                list_dir_path = [list_dir_path]\n            if label_files is None or vid_files is None:\n                label_files, vid_files, ctc_track_meta = self.get_ctc_paths(\n                    list_dir_path\n                )\n            dataset_params[\"data_dirs\"] = self.data_dirs\n            # extract filepaths of all raw images and gt images (i.e. labelled masks)\n            dataset_params[\"gt_list\"] = label_files\n            dataset_params[\"raw_img_list\"] = vid_files\n            dataset_params[\"ctc_track_meta\"] = ctc_track_meta\n\n            return CellTrackingDataset(**dataset_params)\n\n        else:\n            raise ValueError(\n                \"Could not resolve dataset type from Config! Only .slp (SLEAP) and .tif (Cell Tracking Challenge) data formats are supported.\"\n            )\n\n    @property\n    def data_paths(self):\n        \"\"\"Get data paths.\"\"\"\n        return self._vid_files\n\n    @data_paths.setter\n    def data_paths(self, paths: tuple[str, list[str]]):\n        \"\"\"Set data paths.\n\n        Args:\n            paths: A tuple containing (mode, vid_files)\n        \"\"\"\n        mode, vid_files = paths\n        self._vid_files[mode] = vid_files\n\n    def get_dataloader(\n        self,\n        dataset: \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\",\n        mode: str,\n    ) -&gt; torch.utils.data.DataLoader:\n        \"\"\"Getter for dataloader.\n\n        Args:\n            dataset: the Sleap or Microscopy Dataset used to initialize the dataloader\n            mode: either [\"train\", \"val\", or \"test\"] indicates which dataset\n                config to use\n\n        Returns:\n            A torch dataloader for `dataset` with parameters configured as specified\n        \"\"\"\n        dataloader_params = self.get(\"dataloader\", {})\n        if mode.lower() == \"train\":\n            dataloader_params = self.get(\"train_dataloader\", {}, dataloader_params)\n        elif mode.lower() == \"val\":\n            dataloader_params = self.get(\"val_dataloader\", {}, dataloader_params)\n        elif mode.lower() == \"test\":\n            dataloader_params = self.get(\"test_dataloader\", {}, dataloader_params)\n        else:\n            raise ValueError(\n                \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n            )\n        if dataloader_params.get(\"num_workers\", 0) &gt; 0:\n            # prevent too many open files error\n            pin_memory = True\n            torch.multiprocessing.set_sharing_strategy(\"file_system\")\n        else:\n            pin_memory = False\n\n        return torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size=1,\n            pin_memory=pin_memory,\n            collate_fn=dataset.no_batching_fn,\n            **dataloader_params,\n        )\n\n    def get_optimizer(self, params: Iterable) -&gt; torch.optim.Optimizer:\n        \"\"\"Getter for optimizer.\n\n        Args:\n            params: iterable of model parameters to optimize or dicts defining\n                parameter groups\n\n        Returns:\n            A torch Optimizer with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_optimizer\n\n        optimizer_params = self.get(\"optimizer\")\n\n        return init_optimizer(params, optimizer_params)\n\n    def get_scheduler(\n        self, optimizer: torch.optim.Optimizer\n    ) -&gt; torch.optim.lr_scheduler.LRScheduler | None:\n        \"\"\"Getter for lr scheduler.\n\n        Args:\n            optimizer: The optimizer to wrap the scheduler around\n\n        Returns:\n            A torch learning rate scheduler with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_scheduler\n\n        lr_scheduler_params = self.get(\"scheduler\")\n\n        if lr_scheduler_params is None:\n            logger.warning(\n                \"`scheduler` key not found in cfg or is empty. No scheduler will be returned!\"\n            )\n            return None\n        return init_scheduler(optimizer, lr_scheduler_params)\n\n    def get_loss(self) -&gt; \"dreem.training.losses.AssoLoss\":\n        \"\"\"Getter for loss functions.\n\n        Returns:\n            An AssoLoss with specified params\n        \"\"\"\n        from dreem.training.losses import AssoLoss\n\n        loss_params = self.get(\"loss\", {})\n\n        if len(loss_params) == 0:\n            logger.warning(\n                \"`loss` key not found in cfg. Using default params for `AssoLoss`\"\n            )\n\n        return AssoLoss(**loss_params)\n\n    def get_logger(self) -&gt; pl.loggers.Logger:\n        \"\"\"Getter for logging callback.\n\n        Returns:\n            A Logger with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_logger\n\n        logger_params = self.get(\"logging\", {})\n        if len(logger_params) == 0:\n            logger.warning(\n                \"`logging` key not found in cfg. No logger will be configured!\"\n            )\n\n        return init_logger(\n            logger_params, OmegaConf.to_container(self.cfg, resolve=True)\n        )\n\n    def get_early_stopping(self) -&gt; pl.callbacks.EarlyStopping:\n        \"\"\"Getter for lightning early stopping callback.\n\n        Returns:\n            A lightning early stopping callback with specified params\n        \"\"\"\n        early_stopping_params = self.get(\"early_stopping\", None)\n\n        if early_stopping_params is None:\n            logger.warning(\n                \"`early_stopping` was not found in cfg or was `null`. Early stopping will not be used!\"\n            )\n            return None\n        elif len(early_stopping_params) == 0:\n            logger.warning(\"`early_stopping` cfg is empty! Using defaults\")\n        return pl.callbacks.EarlyStopping(**early_stopping_params)\n\n    def get_checkpointing(self) -&gt; pl.callbacks.ModelCheckpoint:\n        \"\"\"Getter for lightning checkpointing callback.\n\n        Returns:\n            A lightning checkpointing callback with specified params\n        \"\"\"\n        # convert to dict to enable extracting/removing params\n        checkpoint_params = self.get(\"checkpointing\", {})\n        logging_params = self.get(\"logging\", {})\n\n        dirpath = checkpoint_params.pop(\"dirpath\", None)\n\n        if dirpath is None:\n            dirpath = f\"./models/{self.get('group', '', logging_params)}/{self.get('name', '', logging_params)}\"\n\n        dirpath = Path(dirpath).resolve()\n        if not Path(dirpath).exists():\n            try:\n                Path(dirpath).mkdir(parents=True, exist_ok=True)\n            except OSError as e:\n                logger.exception(\n                    f\"Cannot create a new folder!. Check the permissions to {dirpath}. \\n {e}\"\n                )\n\n        _ = checkpoint_params.pop(\"dirpath\", None)\n        monitor = checkpoint_params.pop(\"monitor\", [\"val_loss\"])\n        checkpointers = []\n\n        logger.info(\n            f\"Saving checkpoints to `{dirpath}` based on the following metrics: {monitor}\"\n        )\n        if len(checkpoint_params) == 0:\n            logger.warning(\n                \"\"\"`checkpointing` key was not found in cfg or was empty!\n                Configuring checkpointing to use default params!\"\"\"\n            )\n\n        for metric in monitor:\n            checkpointer = pl.callbacks.ModelCheckpoint(\n                monitor=metric,\n                dirpath=dirpath,\n                filename=f\"{{epoch}}-{{{metric}}}\",\n                **checkpoint_params,\n            )\n            checkpointer.CHECKPOINT_NAME_LAST = f\"{{epoch}}-final-{{{metric}}}\"\n            checkpointers.append(checkpointer)\n        return checkpointers\n\n    def get_trainer(\n        self,\n        callbacks: list[pl.callbacks.Callback] | None = None,\n        logger: pl.loggers.WandbLogger | None = None,\n        devices: int = 1,\n        accelerator: str = \"auto\",\n    ) -&gt; pl.Trainer:\n        \"\"\"Getter for the lightning trainer.\n\n        Args:\n            callbacks: a list of lightning callbacks preconfigured to be used\n                for training\n            logger: the Wandb logger used for logging during training\n            devices: The number of gpus to be used. 0 means cpu\n            accelerator: either \"gpu\" or \"cpu\" specifies which device to use\n\n        Returns:\n            A lightning Trainer with specified params\n        \"\"\"\n        trainer_params = self.get(\"trainer\", {})\n        profiler = trainer_params.pop(\"profiler\", None)\n        if len(trainer_params) == 0:\n            print(\n                \"`trainer` key was not found in cfg or was empty. Using defaults for `pl.Trainer`!\"\n            )\n\n        if \"accelerator\" not in trainer_params:\n            trainer_params[\"accelerator\"] = accelerator\n        if \"devices\" not in trainer_params:\n            trainer_params[\"devices\"] = devices\n\n        map_profiler = {\n            \"advanced\": pl.profilers.AdvancedProfiler,\n            \"simple\": pl.profilers.SimpleProfiler,\n            \"pytorch\": pl.profilers.PyTorchProfiler,\n            \"passthrough\": pl.profilers.PassThroughProfiler,\n            \"xla\": pl.profilers.XLAProfiler,\n        }\n\n        if profiler:\n            if profiler in map_profiler:\n                profiler = map_profiler[profiler](filename=\"profile\")\n            else:\n                raise ValueError(\n                    f\"Profiler {profiler} not supported! Please use one of {list(map_profiler.keys())}\"\n                )\n\n        return pl.Trainer(\n            callbacks=callbacks,\n            logger=logger,\n            profiler=profiler,\n            **trainer_params,\n        )\n</code></pre>"},{"location":"io/config/#dreem.io.Config.data_paths","title":"<code>data_paths</code>  <code>property</code> <code>writable</code>","text":"<p>Get data paths.</p>"},{"location":"io/config/#dreem.io.Config.__init__","title":"<code>__init__(cfg, params_cfg=None)</code>","text":"<p>Initialize the class with config from hydra/omega conf.</p> <p>First uses <code>base_param</code> file then overwrites with specific <code>params_config</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>The <code>DictConfig</code> containing all the hyperparameters needed for training/evaluation.</p> required <code>params_cfg</code> <code>DictConfig | None</code> <p>The <code>DictConfig</code> containing subset of hyperparameters to override. training/evaluation</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>def __init__(self, cfg: DictConfig, params_cfg: DictConfig | None = None):\n    \"\"\"Initialize the class with config from hydra/omega conf.\n\n    First uses `base_param` file then overwrites with specific `params_config`.\n\n    Args:\n        cfg: The `DictConfig` containing all the hyperparameters needed for\n            training/evaluation.\n        params_cfg: The `DictConfig` containing subset of hyperparameters to override.\n            training/evaluation\n    \"\"\"\n    base_cfg = cfg\n    logger.info(f\"Base Config: {cfg}\")\n\n    if \"params_config\" in cfg:\n        params_cfg = OmegaConf.load(cfg.params_config)\n\n    if params_cfg:\n        logger.info(f\"Overwriting base config with {params_cfg}\")\n        with open_dict(base_cfg):\n            self.cfg = OmegaConf.merge(base_cfg, params_cfg)  # merge configs\n    else:\n        self.cfg = cfg\n\n    OmegaConf.set_struct(self.cfg, False)\n\n    self._vid_files = {}\n</code></pre>"},{"location":"io/config/#dreem.io.Config.__repr__","title":"<code>__repr__()</code>","text":"<p>Object representation of config class.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def __repr__(self):\n    \"\"\"Object representation of config class.\"\"\"\n    return f\"Config({self.cfg})\"\n</code></pre>"},{"location":"io/config/#dreem.io.Config.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of config class.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def __str__(self):\n    \"\"\"Return a string representation of config class.\"\"\"\n    return f\"Config({self.cfg})\"\n</code></pre>"},{"location":"io/config/#dreem.io.Config.from_yaml","title":"<code>from_yaml(base_cfg_path, params_cfg_path=None)</code>  <code>classmethod</code>","text":"<p>Load config directly from yaml.</p> <p>Parameters:</p> Name Type Description Default <code>base_cfg_path</code> <code>str</code> <p>path to base config file.</p> required <code>params_cfg_path</code> <code>str | None</code> <p>path to override params.</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>@classmethod\ndef from_yaml(cls, base_cfg_path: str, params_cfg_path: str | None = None) -&gt; None:\n    \"\"\"Load config directly from yaml.\n\n    Args:\n        base_cfg_path: path to base config file.\n        params_cfg_path: path to override params.\n    \"\"\"\n    base_cfg = OmegaConf.load(base_cfg_path)\n    params_cfg = OmegaConf.load(params_cfg_path) if params_cfg_path else None\n    return cls(base_cfg, params_cfg)\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get","title":"<code>get(key, default=None, cfg=None)</code>","text":"<p>Get config item.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key of item to return</p> required <code>default</code> <p>default value to return if key is missing.</p> <code>None</code> <code>cfg</code> <code>dict</code> <p>the config dict from which to retrieve an item</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>def get(self, key: str, default=None, cfg: dict = None):\n    \"\"\"Get config item.\n\n    Args:\n        key: key of item to return\n        default: default value to return if key is missing.\n        cfg: the config dict from which to retrieve an item\n    \"\"\"\n    if cfg is None:\n        cfg = self.cfg\n\n    param = cfg.get(key, default)\n\n    if isinstance(param, DictConfig):\n        param = OmegaConf.to_container(param, resolve=True)\n\n    return param\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_checkpointing","title":"<code>get_checkpointing()</code>","text":"<p>Getter for lightning checkpointing callback.</p> <p>Returns:</p> Type Description <code>ModelCheckpoint</code> <p>A lightning checkpointing callback with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_checkpointing(self) -&gt; pl.callbacks.ModelCheckpoint:\n    \"\"\"Getter for lightning checkpointing callback.\n\n    Returns:\n        A lightning checkpointing callback with specified params\n    \"\"\"\n    # convert to dict to enable extracting/removing params\n    checkpoint_params = self.get(\"checkpointing\", {})\n    logging_params = self.get(\"logging\", {})\n\n    dirpath = checkpoint_params.pop(\"dirpath\", None)\n\n    if dirpath is None:\n        dirpath = f\"./models/{self.get('group', '', logging_params)}/{self.get('name', '', logging_params)}\"\n\n    dirpath = Path(dirpath).resolve()\n    if not Path(dirpath).exists():\n        try:\n            Path(dirpath).mkdir(parents=True, exist_ok=True)\n        except OSError as e:\n            logger.exception(\n                f\"Cannot create a new folder!. Check the permissions to {dirpath}. \\n {e}\"\n            )\n\n    _ = checkpoint_params.pop(\"dirpath\", None)\n    monitor = checkpoint_params.pop(\"monitor\", [\"val_loss\"])\n    checkpointers = []\n\n    logger.info(\n        f\"Saving checkpoints to `{dirpath}` based on the following metrics: {monitor}\"\n    )\n    if len(checkpoint_params) == 0:\n        logger.warning(\n            \"\"\"`checkpointing` key was not found in cfg or was empty!\n            Configuring checkpointing to use default params!\"\"\"\n        )\n\n    for metric in monitor:\n        checkpointer = pl.callbacks.ModelCheckpoint(\n            monitor=metric,\n            dirpath=dirpath,\n            filename=f\"{{epoch}}-{{{metric}}}\",\n            **checkpoint_params,\n        )\n        checkpointer.CHECKPOINT_NAME_LAST = f\"{{epoch}}-final-{{{metric}}}\"\n        checkpointers.append(checkpointer)\n    return checkpointers\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_ctc_paths","title":"<code>get_ctc_paths(list_dir_path)</code>","text":"<p>Get file paths from directory. Only for CTC datasets.</p> <p>Parameters:</p> Name Type Description Default <code>list_dir_path</code> <code>list[str]</code> <p>list of directories to search for labels and videos</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str], list[str]]</code> <p>lists of labels file paths and video file paths</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_ctc_paths(\n    self, list_dir_path: list[str]\n) -&gt; tuple[list[str], list[str], list[str]]:\n    \"\"\"Get file paths from directory. Only for CTC datasets.\n\n    Args:\n        list_dir_path: list of directories to search for labels and videos\n\n    Returns:\n        lists of labels file paths and video file paths\n    \"\"\"\n    gt_list = []\n    raw_img_list = []\n    ctc_track_meta = []\n    # user can specify a list of directories, each of which can contain several subdirectories that come in pairs of (dset_name, dset_name_GT/TRA)\n    for dir_path in list_dir_path:\n        for subdir in os.listdir(dir_path):\n            if subdir.endswith(\"_GT\"):\n                gt_path = os.path.join(dir_path, subdir, \"TRA\")\n                raw_img_path = os.path.join(dir_path, subdir.replace(\"_GT\", \"\"))\n                # get filepaths for all tif files in gt_path\n                gt_list.append(glob.glob(os.path.join(gt_path, \"*.tif\")))\n                # get filepaths for all tif files in raw_img_path\n                raw_img_list.append(glob.glob(os.path.join(raw_img_path, \"*.tif\")))\n                man_track_file = glob.glob(os.path.join(gt_path, \"man_track.txt\"))\n                if len(man_track_file) &gt; 0:\n                    ctc_track_meta.append(man_track_file[0])\n                else:\n                    logger.debug(\n                        f\"No man_track.txt file found in {gt_path}. Continuing...\"\n                    )\n            else:\n                continue\n\n    return gt_list, raw_img_list, ctc_track_meta\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_data_paths","title":"<code>get_data_paths(mode, data_cfg)</code>","text":"<p>Get file paths from directory. Only for SLEAP datasets.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>[None, \"train\", \"test\", \"val\"]. Indicates whether to use train, val, or test params for dataset</p> required <code>data_cfg</code> <code>dict</code> <p>Config for the dataset containing \"dir\" key.</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str]]</code> <p>lists of labels file paths and video file paths respectively</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_data_paths(self, mode: str, data_cfg: dict) -&gt; tuple[list[str], list[str]]:\n    \"\"\"Get file paths from directory. Only for SLEAP datasets.\n\n    Args:\n        mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n            train, val, or test params for dataset\n        data_cfg: Config for the dataset containing \"dir\" key.\n\n    Returns:\n        lists of labels file paths and video file paths respectively\n    \"\"\"\n    # hack to get around the fact that for test mode, get_data_paths is called before get_dataset.\n    # also, for train/val mode, data_cfg has had the dir key popped through self.get() called in get_dataset()\n    if mode == \"test\":\n        list_dir_path = data_cfg.get(\"dir\", {}).get(\"path\", None)\n        if list_dir_path is None:\n            raise ValueError(\n                \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n            )\n        self.labels_suffix = data_cfg.get(\"dir\", {}).get(\"labels_suffix\")\n        self.vid_suffix = data_cfg.get(\"dir\", {}).get(\"vid_suffix\")\n    else:\n        list_dir_path = self.data_dirs\n    if not isinstance(list_dir_path, list):\n        list_dir_path = [list_dir_path]\n\n    if self.labels_suffix == \".slp\":\n        label_files = []\n        vid_files = []\n        for dir_path in list_dir_path:\n            logger.debug(f\"Searching `{dir_path}` directory\")\n            labels_path = f\"{dir_path}/*{self.labels_suffix}\"\n            vid_path = f\"{dir_path}/*{self.vid_suffix}\"\n            logger.debug(f\"Searching for labels matching {labels_path}\")\n            label_files.extend(glob.glob(labels_path))\n            logger.debug(f\"Searching for videos matching {vid_path}\")\n            vid_files.extend(glob.glob(vid_path))\n\n    elif self.labels_suffix == \".tif\":\n        label_files, vid_files, ctc_track_meta = self.get_ctc_paths(list_dir_path)\n\n    logger.debug(f\"Found {len(label_files)} labels and {len(vid_files)} videos\")\n\n    # backdoor to set label files directly in the configs (i.e. bypass dir.path)\n    if data_cfg.get(\"slp_files\", None):\n        logger.debug(\"Overriding label files with user provided list\")\n        slp_files = data_cfg.get(\"slp_files\")\n        if len(slp_files) &gt; 0:\n            label_files = slp_files\n    if data_cfg.get(\"video_files\", None):\n        individual_video_files = data_cfg.get(\"video_files\")\n        if len(individual_video_files) &gt; 0:\n            vid_files = individual_video_files\n    return label_files, vid_files\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_dataloader","title":"<code>get_dataloader(dataset, mode)</code>","text":"<p>Getter for dataloader.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>'SleapDataset' | 'MicroscopyDataset' | 'CellTrackingDataset'</code> <p>the Sleap or Microscopy Dataset used to initialize the dataloader</p> required <code>mode</code> <code>str</code> <p>either [\"train\", \"val\", or \"test\"] indicates which dataset config to use</p> required <p>Returns:</p> Type Description <code>DataLoader</code> <p>A torch dataloader for <code>dataset</code> with parameters configured as specified</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_dataloader(\n    self,\n    dataset: \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\",\n    mode: str,\n) -&gt; torch.utils.data.DataLoader:\n    \"\"\"Getter for dataloader.\n\n    Args:\n        dataset: the Sleap or Microscopy Dataset used to initialize the dataloader\n        mode: either [\"train\", \"val\", or \"test\"] indicates which dataset\n            config to use\n\n    Returns:\n        A torch dataloader for `dataset` with parameters configured as specified\n    \"\"\"\n    dataloader_params = self.get(\"dataloader\", {})\n    if mode.lower() == \"train\":\n        dataloader_params = self.get(\"train_dataloader\", {}, dataloader_params)\n    elif mode.lower() == \"val\":\n        dataloader_params = self.get(\"val_dataloader\", {}, dataloader_params)\n    elif mode.lower() == \"test\":\n        dataloader_params = self.get(\"test_dataloader\", {}, dataloader_params)\n    else:\n        raise ValueError(\n            \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n        )\n    if dataloader_params.get(\"num_workers\", 0) &gt; 0:\n        # prevent too many open files error\n        pin_memory = True\n        torch.multiprocessing.set_sharing_strategy(\"file_system\")\n    else:\n        pin_memory = False\n\n    return torch.utils.data.DataLoader(\n        dataset=dataset,\n        batch_size=1,\n        pin_memory=pin_memory,\n        collate_fn=dataset.no_batching_fn,\n        **dataloader_params,\n    )\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_dataset","title":"<code>get_dataset(mode, label_files=None, vid_files=None)</code>","text":"<p>Getter for datasets.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>[None, \"train\", \"test\", \"val\"]. Indicates whether to use train, val, or test params for dataset</p> required <code>label_files</code> <code>list[str] | None</code> <p>path to label_files for override</p> <code>None</code> <code>vid_files</code> <code>list[str | list[str]]</code> <p>path to vid_files for override</p> <code>None</code> <p>Returns:</p> Type Description <code>'SleapDataset' | 'CellTrackingDataset'</code> <p>Either a <code>SleapDataset</code> or <code>CellTrackingDataset</code> with params indicated by cfg</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_dataset(\n    self,\n    mode: str,\n    label_files: list[str] | None = None,\n    vid_files: list[str | list[str]] = None,\n) -&gt; \"SleapDataset\" | \"CellTrackingDataset\":\n    \"\"\"Getter for datasets.\n\n    Args:\n        mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n            train, val, or test params for dataset\n        label_files: path to label_files for override\n        vid_files: path to vid_files for override\n\n    Returns:\n        Either a `SleapDataset` or `CellTrackingDataset` with params indicated by cfg\n    \"\"\"\n    from dreem.datasets import CellTrackingDataset, SleapDataset\n\n    dataset_params = self.get(\"dataset\")\n    if dataset_params is None:\n        raise KeyError(\"`dataset` key is missing from cfg!\")\n\n    if mode.lower() == \"train\":\n        dataset_params = self.get(\"train_dataset\", {}, dataset_params)\n    elif mode.lower() == \"val\":\n        dataset_params = self.get(\"val_dataset\", {}, dataset_params)\n    elif mode.lower() == \"test\":\n        dataset_params = self.get(\"test_dataset\", {}, dataset_params)\n    else:\n        raise ValueError(\n            \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n        )\n\n    # input validation\n    self.data_dirs = dataset_params.get(\"dir\", {}).get(\"path\", None)\n    self.labels_suffix = dataset_params.get(\"dir\", {}).get(\"labels_suffix\")\n    self.vid_suffix = dataset_params.get(\"dir\", {}).get(\"vid_suffix\")\n    if self.data_dirs is None:\n        raise ValueError(\n            \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n        )\n    if self.labels_suffix is None or self.vid_suffix is None:\n        raise KeyError(\n            f\"Must provide a labels suffix and vid suffix to search for but found {self.labels_suffix} and {self.vid_suffix}\"\n        )\n\n    # infer dataset type from the user provided suffix\n    if self.labels_suffix == \".slp\":\n        # during training, multiple files can be used at once, so label_files is not passed in\n        # during inference, a single label_files string can be passed in as get_data_paths is\n        # called before get_dataset, hence the check\n        if label_files is None or vid_files is None:\n            label_files, vid_files = self.get_data_paths(mode, dataset_params)\n        dataset_params[\"slp_files\"] = label_files\n        dataset_params[\"video_files\"] = vid_files\n        dataset_params[\"data_dirs\"] = self.data_dirs\n        self.data_paths = (mode, vid_files)\n\n        return SleapDataset(**dataset_params)\n\n    elif self.labels_suffix == \".tif\":\n        # for CTC datasets, pass in a list of gt and raw image directories, eaech of which contain tifs\n        ctc_track_meta = None\n        list_dir_path = self.data_dirs  # don't modify self.data_dirs\n        if not isinstance(list_dir_path, list):\n            list_dir_path = [list_dir_path]\n        if label_files is None or vid_files is None:\n            label_files, vid_files, ctc_track_meta = self.get_ctc_paths(\n                list_dir_path\n            )\n        dataset_params[\"data_dirs\"] = self.data_dirs\n        # extract filepaths of all raw images and gt images (i.e. labelled masks)\n        dataset_params[\"gt_list\"] = label_files\n        dataset_params[\"raw_img_list\"] = vid_files\n        dataset_params[\"ctc_track_meta\"] = ctc_track_meta\n\n        return CellTrackingDataset(**dataset_params)\n\n    else:\n        raise ValueError(\n            \"Could not resolve dataset type from Config! Only .slp (SLEAP) and .tif (Cell Tracking Challenge) data formats are supported.\"\n        )\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_early_stopping","title":"<code>get_early_stopping()</code>","text":"<p>Getter for lightning early stopping callback.</p> <p>Returns:</p> Type Description <code>EarlyStopping</code> <p>A lightning early stopping callback with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_early_stopping(self) -&gt; pl.callbacks.EarlyStopping:\n    \"\"\"Getter for lightning early stopping callback.\n\n    Returns:\n        A lightning early stopping callback with specified params\n    \"\"\"\n    early_stopping_params = self.get(\"early_stopping\", None)\n\n    if early_stopping_params is None:\n        logger.warning(\n            \"`early_stopping` was not found in cfg or was `null`. Early stopping will not be used!\"\n        )\n        return None\n    elif len(early_stopping_params) == 0:\n        logger.warning(\"`early_stopping` cfg is empty! Using defaults\")\n    return pl.callbacks.EarlyStopping(**early_stopping_params)\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_gtr_runner","title":"<code>get_gtr_runner(ckpt_path=None)</code>","text":"<p>Get lightning module for training, validation, and inference.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_path</code> <code>str | None</code> <p>path to checkpoint for override</p> <code>None</code> <p>Returns:</p> Type Description <code>'GTRRunner'</code> <p>a gtr runner model</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_gtr_runner(self, ckpt_path: str | None = None) -&gt; \"GTRRunner\":\n    \"\"\"Get lightning module for training, validation, and inference.\n\n    Args:\n        ckpt_path: path to checkpoint for override\n\n    Returns:\n        a gtr runner model\n    \"\"\"\n    from dreem.models import GTRRunner\n\n    keys = [\"tracker\", \"optimizer\", \"scheduler\", \"loss\", \"runner\", \"model\"]\n    args = [key + \"_cfg\" if key != \"runner\" else key for key in keys]\n\n    params = {}\n    for key, arg in zip(keys, args):\n        sub_params = self.get(key, {})\n\n        if len(sub_params) == 0:\n            logger.warning(\n                f\"`{key}` not found in config or is empty. Using defaults for {arg}!\"\n            )\n\n        if key == \"runner\":\n            runner_params = sub_params\n            for k, v in runner_params.items():\n                params[k] = v\n        else:\n            params[arg] = sub_params\n\n    ckpt_path = params[\"model_cfg\"].pop(\"ckpt_path\", None)\n\n    if ckpt_path is not None and ckpt_path != \"\":\n        model = GTRRunner.load_from_checkpoint(\n            ckpt_path, tracker_cfg=params[\"tracker_cfg\"], **runner_params\n        )\n\n    else:\n        model = GTRRunner(**params)\n\n    return model\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_logger","title":"<code>get_logger()</code>","text":"<p>Getter for logging callback.</p> <p>Returns:</p> Type Description <code>Logger</code> <p>A Logger with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_logger(self) -&gt; pl.loggers.Logger:\n    \"\"\"Getter for logging callback.\n\n    Returns:\n        A Logger with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_logger\n\n    logger_params = self.get(\"logging\", {})\n    if len(logger_params) == 0:\n        logger.warning(\n            \"`logging` key not found in cfg. No logger will be configured!\"\n        )\n\n    return init_logger(\n        logger_params, OmegaConf.to_container(self.cfg, resolve=True)\n    )\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_loss","title":"<code>get_loss()</code>","text":"<p>Getter for loss functions.</p> <p>Returns:</p> Type Description <code>'dreem.training.losses.AssoLoss'</code> <p>An AssoLoss with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_loss(self) -&gt; \"dreem.training.losses.AssoLoss\":\n    \"\"\"Getter for loss functions.\n\n    Returns:\n        An AssoLoss with specified params\n    \"\"\"\n    from dreem.training.losses import AssoLoss\n\n    loss_params = self.get(\"loss\", {})\n\n    if len(loss_params) == 0:\n        logger.warning(\n            \"`loss` key not found in cfg. Using default params for `AssoLoss`\"\n        )\n\n    return AssoLoss(**loss_params)\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_model","title":"<code>get_model()</code>","text":"<p>Getter for gtr model.</p> <p>Returns:</p> Type Description <code>'GlobalTrackingTransformer'</code> <p>A global tracking transformer with parameters indicated by cfg</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_model(self) -&gt; \"GlobalTrackingTransformer\":\n    \"\"\"Getter for gtr model.\n\n    Returns:\n        A global tracking transformer with parameters indicated by cfg\n    \"\"\"\n    from dreem.models import GlobalTrackingTransformer, GTRRunner\n\n    model_params = self.get(\"model\", {})\n\n    ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n    if ckpt_path is not None and len(ckpt_path) &gt; 0:\n        return GTRRunner.load_from_checkpoint(ckpt_path).model\n\n    return GlobalTrackingTransformer(**model_params)\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_optimizer","title":"<code>get_optimizer(params)</code>","text":"<p>Getter for optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable</code> <p>iterable of model parameters to optimize or dicts defining parameter groups</p> required <p>Returns:</p> Type Description <code>Optimizer</code> <p>A torch Optimizer with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_optimizer(self, params: Iterable) -&gt; torch.optim.Optimizer:\n    \"\"\"Getter for optimizer.\n\n    Args:\n        params: iterable of model parameters to optimize or dicts defining\n            parameter groups\n\n    Returns:\n        A torch Optimizer with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_optimizer\n\n    optimizer_params = self.get(\"optimizer\")\n\n    return init_optimizer(params, optimizer_params)\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_scheduler","title":"<code>get_scheduler(optimizer)</code>","text":"<p>Getter for lr scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to wrap the scheduler around</p> required <p>Returns:</p> Type Description <code>LRScheduler | None</code> <p>A torch learning rate scheduler with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_scheduler(\n    self, optimizer: torch.optim.Optimizer\n) -&gt; torch.optim.lr_scheduler.LRScheduler | None:\n    \"\"\"Getter for lr scheduler.\n\n    Args:\n        optimizer: The optimizer to wrap the scheduler around\n\n    Returns:\n        A torch learning rate scheduler with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_scheduler\n\n    lr_scheduler_params = self.get(\"scheduler\")\n\n    if lr_scheduler_params is None:\n        logger.warning(\n            \"`scheduler` key not found in cfg or is empty. No scheduler will be returned!\"\n        )\n        return None\n    return init_scheduler(optimizer, lr_scheduler_params)\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_tracker_cfg","title":"<code>get_tracker_cfg()</code>","text":"<p>Getter for tracker config params.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict containing the init params for <code>Tracker</code>.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_tracker_cfg(self) -&gt; dict:\n    \"\"\"Getter for tracker config params.\n\n    Returns:\n        A dict containing the init params for `Tracker`.\n    \"\"\"\n    return self.get(\"tracker\", {})\n</code></pre>"},{"location":"io/config/#dreem.io.Config.get_trainer","title":"<code>get_trainer(callbacks=None, logger=None, devices=1, accelerator='auto')</code>","text":"<p>Getter for the lightning trainer.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[Callback] | None</code> <p>a list of lightning callbacks preconfigured to be used for training</p> <code>None</code> <code>logger</code> <code>WandbLogger | None</code> <p>the Wandb logger used for logging during training</p> <code>None</code> <code>devices</code> <code>int</code> <p>The number of gpus to be used. 0 means cpu</p> <code>1</code> <code>accelerator</code> <code>str</code> <p>either \"gpu\" or \"cpu\" specifies which device to use</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>Trainer</code> <p>A lightning Trainer with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_trainer(\n    self,\n    callbacks: list[pl.callbacks.Callback] | None = None,\n    logger: pl.loggers.WandbLogger | None = None,\n    devices: int = 1,\n    accelerator: str = \"auto\",\n) -&gt; pl.Trainer:\n    \"\"\"Getter for the lightning trainer.\n\n    Args:\n        callbacks: a list of lightning callbacks preconfigured to be used\n            for training\n        logger: the Wandb logger used for logging during training\n        devices: The number of gpus to be used. 0 means cpu\n        accelerator: either \"gpu\" or \"cpu\" specifies which device to use\n\n    Returns:\n        A lightning Trainer with specified params\n    \"\"\"\n    trainer_params = self.get(\"trainer\", {})\n    profiler = trainer_params.pop(\"profiler\", None)\n    if len(trainer_params) == 0:\n        print(\n            \"`trainer` key was not found in cfg or was empty. Using defaults for `pl.Trainer`!\"\n        )\n\n    if \"accelerator\" not in trainer_params:\n        trainer_params[\"accelerator\"] = accelerator\n    if \"devices\" not in trainer_params:\n        trainer_params[\"devices\"] = devices\n\n    map_profiler = {\n        \"advanced\": pl.profilers.AdvancedProfiler,\n        \"simple\": pl.profilers.SimpleProfiler,\n        \"pytorch\": pl.profilers.PyTorchProfiler,\n        \"passthrough\": pl.profilers.PassThroughProfiler,\n        \"xla\": pl.profilers.XLAProfiler,\n    }\n\n    if profiler:\n        if profiler in map_profiler:\n            profiler = map_profiler[profiler](filename=\"profile\")\n        else:\n            raise ValueError(\n                f\"Profiler {profiler} not supported! Please use one of {list(map_profiler.keys())}\"\n            )\n\n    return pl.Trainer(\n        callbacks=callbacks,\n        logger=logger,\n        profiler=profiler,\n        **trainer_params,\n    )\n</code></pre>"},{"location":"io/config/#dreem.io.Config.set_hparams","title":"<code>set_hparams(hparams)</code>","text":"<p>Setter function for overwriting specific hparams.</p> <p>Useful for changing 1 or 2 hyperparameters such as dataset.</p> <p>Parameters:</p> Name Type Description Default <code>hparams</code> <code>dict</code> <p>A dict containing the hyperparameter to be overwritten and the value to be changed</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if config is successfully updated, <code>False</code> otherwise</p> Source code in <code>dreem/io/config.py</code> <pre><code>def set_hparams(self, hparams: dict) -&gt; bool:\n    \"\"\"Setter function for overwriting specific hparams.\n\n    Useful for changing 1 or 2 hyperparameters such as dataset.\n\n    Args:\n        hparams: A dict containing the hyperparameter to be overwritten and\n            the value to be changed\n\n    Returns:\n        `True` if config is successfully updated, `False` otherwise\n    \"\"\"\n    if hparams == {} or hparams is None:\n        logger.warning(\"Nothing to update!\")\n        return False\n    for hparam, val in hparams.items():\n        try:\n            OmegaConf.update(self.cfg, hparam, val)\n        except Exception as e:\n            logger.exception(f\"Failed to update {hparam} to {val} due to {e}\")\n            return False\n    return True\n</code></pre>"},{"location":"io/frame/","title":"<code>Frame</code>","text":""},{"location":"io/frame/#dreem.io.Frame","title":"<code>dreem.io.Frame</code>","text":"<p>Data structure containing metadata for a single frame of a video.</p> <p>Attributes:</p> Name Type Description <code>video_id</code> <code>Tensor</code> <p>The video index in the dataset.</p> <code>frame_id</code> <code>Tensor</code> <p>The index of the frame in a video.</p> <code>vid_file</code> <code>Tensor</code> <p>The path to the video the frame is from.</p> <code>img_shape</code> <code>Tensor</code> <p>The shape of the original frame (not the crop).</p> <code>instances</code> <code>list['Instance']</code> <p>A list of Instance objects that appear in the frame.</p> <code>asso_output</code> <code>'AssociationMatrix'</code> <p>The association matrix between instances output directly from the transformer.</p> <code>matches</code> <code>tuple</code> <p>matches from LSA algorithm between the instances and available trajectories during tracking.</p> <code>traj_score</code> <code>tuple</code> <p>Either a dict containing the association matrix between instances and trajectories along postprocessing pipeline or a single association matrix.</p> <code>device</code> <code>str</code> <p>The device the frame should be moved to.</p> <p>Methods:</p> Name Description <code>__attrs_post_init__</code> <p>Handle more intricate default initializations and moving to device.</p> <code>__repr__</code> <p>Return String representation of the Frame.</p> <code>add_traj_score</code> <p>Add trajectory score to dictionary.</p> <code>from_slp</code> <p>Convert <code>sio.LabeledFrame</code> to <code>dreem.io.Frame</code>.</p> <code>get_anchors</code> <p>Get the anchor names of instances in the frame.</p> <code>get_bboxes</code> <p>Get the bounding boxes of all instances in the frame.</p> <code>get_centroids</code> <p>Get the centroids around which each instance's crop was formed.</p> <code>get_crops</code> <p>Get the crops of all instances in the frame.</p> <code>get_features</code> <p>Get the reid feature vectors of all instances in the frame.</p> <code>get_gt_track_ids</code> <p>Get the gt track ids of all instances in the frame.</p> <code>get_pred_track_ids</code> <p>Get the pred track ids of all instances in the frame.</p> <code>get_traj_score</code> <p>Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.</p> <code>has_asso_output</code> <p>Determine whether the frame has an association matrix computed.</p> <code>has_bboxes</code> <p>Check if any of frames instances has a bounding box.</p> <code>has_crops</code> <p>Check if any of frames instances has a crop.</p> <code>has_features</code> <p>Check if any of frames instances has reid features already computed.</p> <code>has_gt_track_ids</code> <p>Check if any of frames instances has a gt track id.</p> <code>has_instances</code> <p>Determine whether there are instances in the frame.</p> <code>has_matches</code> <p>Check whether or not matches have been computed for frame.</p> <code>has_pred_track_ids</code> <p>Check if any of frames instances has a pred track id.</p> <code>has_traj_score</code> <p>Check if any trajectory association matrix has been saved.</p> <code>to</code> <p>Move frame to different device or dtype (See <code>torch.to</code> for more info).</p> <code>to_h5</code> <p>Convert frame to h5py group.</p> <code>to_slp</code> <p>Convert Frame to sleap_io.LabeledFrame object.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>@attrs.define(eq=False)\nclass Frame:\n    \"\"\"Data structure containing metadata for a single frame of a video.\n\n    Attributes:\n        video_id: The video index in the dataset.\n        frame_id: The index of the frame in a video.\n        vid_file: The path to the video the frame is from.\n        img_shape: The shape of the original frame (not the crop).\n        instances: A list of Instance objects that appear in the frame.\n        asso_output: The association matrix between instances\n            output directly from the transformer.\n        matches: matches from LSA algorithm between the instances and\n            available trajectories during tracking.\n        traj_score: Either a dict containing the association matrix\n            between instances and trajectories along postprocessing pipeline\n            or a single association matrix.\n        device: The device the frame should be moved to.\n    \"\"\"\n\n    _video_id: int = attrs.field(alias=\"video_id\", converter=_to_tensor)\n    _frame_id: int = attrs.field(alias=\"frame_id\", converter=_to_tensor)\n    _video: str = attrs.field(alias=\"vid_file\", default=\"\")\n    _img_shape: ArrayLike = attrs.field(\n        alias=\"img_shape\", converter=_to_tensor, factory=list\n    )\n\n    _instances: list[\"Instance\"] = attrs.field(alias=\"instances\", factory=list)\n    _asso_output: \"AssociationMatrix\" | None = attrs.field(\n        alias=\"asso_output\", default=None\n    )\n    _matches: tuple = attrs.field(alias=\"matches\", factory=tuple)\n    _traj_score: dict = attrs.field(alias=\"traj_score\", factory=dict)\n    _device: str | torch.device | None = attrs.field(alias=\"device\", default=None)\n\n    def __attrs_post_init__(self) -&gt; None:\n        \"\"\"Handle more intricate default initializations and moving to device.\"\"\"\n        if len(self.img_shape) == 0:\n            self.img_shape = torch.tensor([0, 0, 0])\n\n        for instance in self.instances:\n            instance.frame = self\n\n        self.to(self.device)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return String representation of the Frame.\n\n        Returns:\n            The string representation of the frame.\n        \"\"\"\n        return (\n            \"Frame(\"\n            f\"video={self._video.filename if isinstance(self._video, sio.Video) else self._video}, \"\n            f\"video_id={self._video_id.item()}, \"\n            f\"frame_id={self._frame_id.item()}, \"\n            f\"img_shape={self._img_shape}, \"\n            f\"num_detected={self.num_detected}, \"\n            f\"asso_output={self._asso_output}, \"\n            f\"traj_score={self._traj_score}, \"\n            f\"matches={self._matches}, \"\n            f\"instances={self._instances}, \"\n            f\"device={self._device}\"\n            \")\"\n        )\n\n    def to(self, map_location: str | torch.device) -&gt; Self:\n        \"\"\"Move frame to different device or dtype (See `torch.to` for more info).\n\n        Args:\n            map_location: A string representing the device to move to.\n\n        Returns:\n            The frame moved to a different device/dtype.\n        \"\"\"\n        self._video_id = self._video_id.to(map_location)\n        self._frame_id = self._frame_id.to(map_location)\n        self._img_shape = self._img_shape.to(map_location)\n\n        if isinstance(self._asso_output, torch.Tensor):\n            self._asso_output = self._asso_output.to(map_location)\n\n        if isinstance(self._matches, torch.Tensor):\n            self._matches = self._matches.to(map_location)\n\n        for key, val in self._traj_score.items():\n            if isinstance(val, torch.Tensor):\n                self._traj_score[key] = val.to(map_location)\n        for instance in self.instances:\n            instance = instance.to(map_location)\n\n        if isinstance(map_location, (str, torch.device)):\n            self._device = map_location\n\n        return self\n\n    @classmethod\n    def from_slp(\n        cls,\n        lf: sio.LabeledFrame,\n        video_id: int = 0,\n        device: str | None = None,\n        **kwargs,\n    ) -&gt; Self:\n        \"\"\"Convert `sio.LabeledFrame` to `dreem.io.Frame`.\n\n        Args:\n            lf: A sio.LabeledFrame object\n\n        Returns:\n            A dreem.io.Frame object\n        \"\"\"\n        from dreem.io.instance import Instance\n\n        img_shape = lf.image.shape\n        if len(img_shape) == 2:\n            img_shape = (1, *img_shape)\n        elif len(img_shape) &gt; 2 and img_shape[-1] &lt;= 3:\n            img_shape = (lf.image.shape[-1], lf.image.shape[0], lf.image.shape[1])\n        return cls(\n            video_id=video_id,\n            frame_id=(\n                lf.frame_idx.astype(np.int32)\n                if isinstance(lf.frame_idx, np.number)\n                else lf.frame_idx\n            ),\n            vid_file=lf.video.filename,\n            img_shape=img_shape,\n            instances=[Instance.from_slp(instance, **kwargs) for instance in lf],\n            device=device,\n        )\n\n    def to_slp(\n        self,\n        track_lookup: dict[int, sio.Track] | None = None,\n        video: sio.Video | None = None,\n    ) -&gt; tuple[sio.LabeledFrame, dict[int, sio.Track]]:\n        \"\"\"Convert Frame to sleap_io.LabeledFrame object.\n\n        Args:\n            track_lookup: A lookup dictionary containing the track_id and sio.Track for persistence\n            video: An sio.Video object used for overriding.\n\n        Returns: A tuple containing a LabeledFrame object with necessary metadata and\n        a lookup dictionary containing the track_id and sio.Track for persistence\n        \"\"\"\n        if track_lookup is None:\n            track_lookup = {}\n\n        slp_instances = []\n        for instance in self.instances:\n            slp_instance, track_lookup = instance.to_slp(track_lookup=track_lookup)\n            slp_instances.append(slp_instance)\n\n        if video is None:\n            video = (\n                self.video\n                if isinstance(self.video, sio.Video)\n                else sio.load_video(self.video)\n            )\n\n        return (\n            sio.LabeledFrame(\n                video=video,\n                frame_idx=self.frame_id.item(),\n                instances=slp_instances,\n            ),\n            track_lookup,\n        )\n\n    def to_h5(\n        self,\n        clip_group: h5py.Group,\n        instance_labels: list | None = None,\n        save: dict[str, bool] | None = None,\n    ) -&gt; h5py.Group:\n        \"\"\"Convert frame to h5py group.\n\n        Args:\n            clip_group: the h5py group representing the clip (e.g batch/video) the frame belongs to\n            instance_labels: the labels used to create instance group names\n            save: whether to save crops, features and embeddings for the instance\n        Returns:\n            An h5py group containing the frame\n        \"\"\"\n        if save is None:\n            save = {\"crop\": False, \"features\": False, \"embeddings\": False}\n        frame_group = clip_group.require_group(f\"frame_{self.frame_id.item()}\")\n        frame_group.attrs.create(\"frame_id\", self.frame_id.item())\n        frame_group.attrs.create(\"vid_id\", self.video_id.item())\n        frame_group.attrs.create(\"vid_name\", self.vid_name)\n\n        frame_group.create_dataset(\n            \"asso_matrix\",\n            data=self.asso_output.numpy() if self.asso_output is not None else [],\n        )\n        asso_group = frame_group.require_group(\"traj_scores\")\n        for key, value in self.get_traj_score().items():\n            asso_group.create_dataset(\n                key, data=value.to_numpy() if value is not None else []\n            )\n\n        if instance_labels is None:\n            instance_labels = self.get_gt_track_ids.cpu().numpy()\n        for instance_label, instance in zip(instance_labels, self.instances):\n            kwargs = {}\n            if save.get(\"crop\", False):\n                kwargs[\"crop\"] = instance.crop.cpu().numpy()\n            if save.get(\"features\", False):\n                kwargs[\"features\"] = instance.features.cpu().numpy()\n            if save.get(\"embeddings\", False):\n                for key, val in instance.get_embedding().items():\n                    kwargs[f\"{key}_emb\"] = val.cpu().numpy()\n            _ = instance.to_h5(frame_group, f\"instance_{instance_label}\", **kwargs)\n\n        return frame_group\n\n    @property\n    def device(self) -&gt; str:\n        \"\"\"The device the frame is on.\n\n        Returns:\n            The string representation of the device the frame is on.\n        \"\"\"\n        return self._device\n\n    @device.setter\n    def device(self, device: str) -&gt; None:\n        \"\"\"Set the device.\n\n        Note: Do not set `frame.device = device` normally. Use `frame.to(device)` instead.\n\n        Args:\n            device: the device the function should be on.\n        \"\"\"\n        self._device = device\n\n    @property\n    def video_id(self) -&gt; torch.Tensor:\n        \"\"\"The index of the video the frame comes from.\n\n        Returns:\n            A tensor containing the video index.\n        \"\"\"\n        return self._video_id\n\n    @video_id.setter\n    def video_id(self, video_id: int) -&gt; None:\n        \"\"\"Set the video index.\n\n        Note: Generally the video_id should be immutable after initialization.\n\n        Args:\n            video_id: an int representing the index of the video that the frame came from.\n        \"\"\"\n        self._video_id = torch.tensor([video_id])\n\n    @property\n    def frame_id(self) -&gt; torch.Tensor:\n        \"\"\"The index of the frame in a full video.\n\n        Returns:\n            A torch tensor containing the index of the frame in the video.\n        \"\"\"\n        return self._frame_id\n\n    @frame_id.setter\n    def frame_id(self, frame_id: int) -&gt; None:\n        \"\"\"Set the frame index of the frame.\n\n        Note: The frame_id should generally be immutable after initialization.\n\n        Args:\n            frame_id: The int index of the frame in the full video.\n        \"\"\"\n        self._frame_id = torch.tensor([frame_id])\n\n    @property\n    def video(self) -&gt; sio.Video | str:\n        \"\"\"Get the video associated with the frame.\n\n        Returns: An sio.Video object representing the video or a placeholder string\n        if it is not possible to create the sio.Video\n        \"\"\"\n        return self._video\n\n    @video.setter\n    def video(self, video: sio.Video | str) -&gt; None:\n        \"\"\"Set the video associated with the frame.\n\n        Note: we try to store the video in an sio.Video object.\n        However, if this is not possible (e.g. incompatible format or missing filepath)\n        then we simply store the string.\n\n        Args:\n            video: sio.Video containing the vid reader or string path to video_file\n        \"\"\"\n        if isinstance(video, sio.Video):\n            self._video = video\n        else:\n            try:\n                self._video = sio.load_video(video)\n            except ValueError:\n                self._video = video\n\n    @property\n    def vid_name(self) -&gt; str:\n        \"\"\"Get the path to the video corresponding to this frame.\n\n        Returns: A str file path corresponding to the frame.\n        \"\"\"\n        if isinstance(self.video, str):\n            return self.video\n        else:\n            return self.video.name\n\n    @property\n    def img_shape(self) -&gt; torch.Tensor:\n        \"\"\"The shape of the pre-cropped frame.\n\n        Returns:\n            A torch tensor containing the shape of the frame. Should generally be (c, h, w)\n        \"\"\"\n        return self._img_shape\n\n    @img_shape.setter\n    def img_shape(self, img_shape: ArrayLike) -&gt; None:\n        \"\"\"Set the shape of the frame image.\n\n        Note: the img_shape should generally be immutable after initialization.\n\n        Args:\n            img_shape: an ArrayLike object containing the shape of the frame image.\n        \"\"\"\n        self._img_shape = _to_tensor(img_shape)\n\n    @property\n    def instances(self) -&gt; list[\"Instance\"]:\n        \"\"\"A list of instances in the frame.\n\n        Returns:\n            The list of instances that appear in the frame.\n        \"\"\"\n        return self._instances\n\n    @instances.setter\n    def instances(self, instances: list[\"Instance\"]) -&gt; None:\n        \"\"\"Set the frame's instance.\n\n        Args:\n            instances: A list of Instances that appear in the frame.\n        \"\"\"\n        for instance in instances:\n            instance.frame = self\n        self._instances = instances\n\n    def has_instances(self) -&gt; bool:\n        \"\"\"Determine whether there are instances in the frame.\n\n        Returns:\n            True if there are instances in the frame, otherwise False.\n        \"\"\"\n        if self.num_detected == 0:\n            return False\n        return True\n\n    @property\n    def num_detected(self) -&gt; int:\n        \"\"\"The number of instances in the frame.\n\n        Returns:\n            the number of instances in the frame.\n        \"\"\"\n        return len(self.instances)\n\n    @property\n    def asso_output(self) -&gt; \"AssociationMatrix\":\n        \"\"\"The association matrix between instances outputted directly by transformer.\n\n        Returns:\n            An arraylike (n_query, n_nonquery) association matrix between instances.\n        \"\"\"\n        return self._asso_output\n\n    def has_asso_output(self) -&gt; bool:\n        \"\"\"Determine whether the frame has an association matrix computed.\n\n        Returns:\n            True if the frame has an association matrix otherwise, False.\n        \"\"\"\n        if self._asso_output is None or len(self._asso_output.matrix) == 0:\n            return False\n        return True\n\n    @asso_output.setter\n    def asso_output(self, asso_output: \"AssociationMatrix\") -&gt; None:\n        \"\"\"Set the association matrix of a frame.\n\n        Args:\n            asso_output: An arraylike (n_query, n_nonquery) association matrix between instances.\n        \"\"\"\n        self._asso_output = asso_output\n\n    @property\n    def matches(self) -&gt; tuple:\n        \"\"\"Matches between frame instances and available trajectories.\n\n        Returns:\n            A tuple containing the instance idx and trajectory idx for the matched instance.\n        \"\"\"\n        return self._matches\n\n    @matches.setter\n    def matches(self, matches: tuple) -&gt; None:\n        \"\"\"Set the frame matches.\n\n        Args:\n            matches: A tuple containing the instance idx and trajectory idx for the matched instance.\n        \"\"\"\n        self._matches = matches\n\n    def has_matches(self) -&gt; bool:\n        \"\"\"Check whether or not matches have been computed for frame.\n\n        Returns:\n            True if frame contains matches otherwise False.\n        \"\"\"\n        if self._matches is not None and len(self._matches) &gt; 0:\n            return True\n        return False\n\n    def get_traj_score(self, key: str | None = None) -&gt; dict | ArrayLike | None:\n        \"\"\"Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.\n\n        Args:\n            key: The key of the trajectory score to be accessed.\n                Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}\n\n        Returns:\n            - dictionary containing all trajectory scores if key is None\n            - trajectory score associated with key\n            - None if the key is not found\n        \"\"\"\n        if key is None:\n            return self._traj_score\n        else:\n            try:\n                return self._traj_score[key]\n            except KeyError as e:\n                logger.exception(f\"Could not access {key} traj_score due to {e}\")\n                return None\n\n    def add_traj_score(self, key: str, traj_score: ArrayLike) -&gt; None:\n        \"\"\"Add trajectory score to dictionary.\n\n        Args:\n            key: key associated with traj score to be used in dictionary\n            traj_score: association matrix between instances and trajectories\n        \"\"\"\n        self._traj_score[key] = traj_score\n\n    def has_traj_score(self) -&gt; bool:\n        \"\"\"Check if any trajectory association matrix has been saved.\n\n        Returns:\n            True there is at least one association matrix otherwise, false.\n        \"\"\"\n        if len(self._traj_score) == 0:\n            return False\n        return True\n\n    def has_gt_track_ids(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a gt track id.\n\n        Returns:\n            True if at least 1 instance has a gt track id otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_gt_track_id() for instance in self.instances])\n        return False\n\n    def get_gt_track_ids(self) -&gt; torch.Tensor:\n        \"\"\"Get the gt track ids of all instances in the frame.\n\n        Returns:\n            an (N,) shaped tensor with the gt track ids of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.gt_track_id for instance in self.instances])\n\n    def has_pred_track_ids(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a pred track id.\n\n        Returns:\n            True if at least 1 instance has a pred track id otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_pred_track_id() for instance in self.instances])\n        return False\n\n    def get_pred_track_ids(self) -&gt; torch.Tensor:\n        \"\"\"Get the pred track ids of all instances in the frame.\n\n        Returns:\n            an (N,) shaped tensor with the pred track ids of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.pred_track_id for instance in self.instances])\n\n    def has_bboxes(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a bounding box.\n\n        Returns:\n            True if at least 1 instance has a bounding box otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_bboxes() for instance in self.instances])\n        return False\n\n    def get_bboxes(self) -&gt; torch.Tensor:\n        \"\"\"Get the bounding boxes of all instances in the frame.\n\n        Returns:\n            an (N,4) shaped tensor with bounding boxes of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.empty(0, 4)\n        return torch.cat([instance.bbox for instance in self.instances], dim=0)\n\n    def has_crops(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a crop.\n\n        Returns:\n            True if at least 1 instance has a crop otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_crop() for instance in self.instances])\n        return False\n\n    def get_crops(self) -&gt; torch.Tensor:\n        \"\"\"Get the crops of all instances in the frame.\n\n        Returns:\n            an (N, C, H, W) shaped tensor with crops of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n\n        return torch.cat([instance.crop for instance in self.instances], dim=0)\n\n    def has_features(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has reid features already computed.\n\n        Returns:\n            True if at least 1 instance have reid features otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_features() for instance in self.instances])\n        return False\n\n    def get_features(self) -&gt; torch.Tensor:\n        \"\"\"Get the reid feature vectors of all instances in the frame.\n\n        Returns:\n            an (N, D) shaped tensor with reid feature vectors of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.features for instance in self.instances], dim=0)\n\n    def get_anchors(self) -&gt; list[str]:\n        \"\"\"Get the anchor names of instances in the frame.\n\n        Returns:\n            A list of anchor names used by the instances to get the crop.\n        \"\"\"\n        return [instance.anchor for instance in self.instances]\n\n    def get_centroids(self) -&gt; tuple[list[str], ArrayLike]:\n        \"\"\"Get the centroids around which each instance's crop was formed.\n\n        Returns:\n            anchors: the node names for the corresponding point\n            points: an n_instances x 2 array containing the centroids\n        \"\"\"\n        anchors = [\n            anchor for instance in self.instances for anchor in instance.centroid.keys()\n        ]\n\n        points = np.array(\n            [\n                point\n                for instance in self.instances\n                for point in instance.centroid.values()\n            ]\n        )\n\n        return (anchors, points)\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.asso_output","title":"<code>asso_output</code>  <code>property</code> <code>writable</code>","text":"<p>The association matrix between instances outputted directly by transformer.</p> <p>Returns:</p> Type Description <code>'AssociationMatrix'</code> <p>An arraylike (n_query, n_nonquery) association matrix between instances.</p>"},{"location":"io/frame/#dreem.io.Frame.device","title":"<code>device</code>  <code>property</code> <code>writable</code>","text":"<p>The device the frame is on.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the device the frame is on.</p>"},{"location":"io/frame/#dreem.io.Frame.frame_id","title":"<code>frame_id</code>  <code>property</code> <code>writable</code>","text":"<p>The index of the frame in a full video.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor containing the index of the frame in the video.</p>"},{"location":"io/frame/#dreem.io.Frame.img_shape","title":"<code>img_shape</code>  <code>property</code> <code>writable</code>","text":"<p>The shape of the pre-cropped frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor containing the shape of the frame. Should generally be (c, h, w)</p>"},{"location":"io/frame/#dreem.io.Frame.instances","title":"<code>instances</code>  <code>property</code> <code>writable</code>","text":"<p>A list of instances in the frame.</p> <p>Returns:</p> Type Description <code>list['Instance']</code> <p>The list of instances that appear in the frame.</p>"},{"location":"io/frame/#dreem.io.Frame.matches","title":"<code>matches</code>  <code>property</code> <code>writable</code>","text":"<p>Matches between frame instances and available trajectories.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the instance idx and trajectory idx for the matched instance.</p>"},{"location":"io/frame/#dreem.io.Frame.num_detected","title":"<code>num_detected</code>  <code>property</code>","text":"<p>The number of instances in the frame.</p> <p>Returns:</p> Type Description <code>int</code> <p>the number of instances in the frame.</p>"},{"location":"io/frame/#dreem.io.Frame.vid_name","title":"<code>vid_name</code>  <code>property</code>","text":"<p>Get the path to the video corresponding to this frame.</p> <p>Returns: A str file path corresponding to the frame.</p>"},{"location":"io/frame/#dreem.io.Frame.video","title":"<code>video</code>  <code>property</code> <code>writable</code>","text":"<p>Get the video associated with the frame.</p> <p>Returns: An sio.Video object representing the video or a placeholder string if it is not possible to create the sio.Video</p>"},{"location":"io/frame/#dreem.io.Frame.video_id","title":"<code>video_id</code>  <code>property</code> <code>writable</code>","text":"<p>The index of the video the frame comes from.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the video index.</p>"},{"location":"io/frame/#dreem.io.Frame.__attrs_post_init__","title":"<code>__attrs_post_init__()</code>","text":"<p>Handle more intricate default initializations and moving to device.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def __attrs_post_init__(self) -&gt; None:\n    \"\"\"Handle more intricate default initializations and moving to device.\"\"\"\n    if len(self.img_shape) == 0:\n        self.img_shape = torch.tensor([0, 0, 0])\n\n    for instance in self.instances:\n        instance.frame = self\n\n    self.to(self.device)\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.__repr__","title":"<code>__repr__()</code>","text":"<p>Return String representation of the Frame.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return String representation of the Frame.\n\n    Returns:\n        The string representation of the frame.\n    \"\"\"\n    return (\n        \"Frame(\"\n        f\"video={self._video.filename if isinstance(self._video, sio.Video) else self._video}, \"\n        f\"video_id={self._video_id.item()}, \"\n        f\"frame_id={self._frame_id.item()}, \"\n        f\"img_shape={self._img_shape}, \"\n        f\"num_detected={self.num_detected}, \"\n        f\"asso_output={self._asso_output}, \"\n        f\"traj_score={self._traj_score}, \"\n        f\"matches={self._matches}, \"\n        f\"instances={self._instances}, \"\n        f\"device={self._device}\"\n        \")\"\n    )\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.add_traj_score","title":"<code>add_traj_score(key, traj_score)</code>","text":"<p>Add trajectory score to dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key associated with traj score to be used in dictionary</p> required <code>traj_score</code> <code>ArrayLike</code> <p>association matrix between instances and trajectories</p> required Source code in <code>dreem/io/frame.py</code> <pre><code>def add_traj_score(self, key: str, traj_score: ArrayLike) -&gt; None:\n    \"\"\"Add trajectory score to dictionary.\n\n    Args:\n        key: key associated with traj score to be used in dictionary\n        traj_score: association matrix between instances and trajectories\n    \"\"\"\n    self._traj_score[key] = traj_score\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.from_slp","title":"<code>from_slp(lf, video_id=0, device=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Convert <code>sio.LabeledFrame</code> to <code>dreem.io.Frame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>lf</code> <code>LabeledFrame</code> <p>A sio.LabeledFrame object</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A dreem.io.Frame object</p> Source code in <code>dreem/io/frame.py</code> <pre><code>@classmethod\ndef from_slp(\n    cls,\n    lf: sio.LabeledFrame,\n    video_id: int = 0,\n    device: str | None = None,\n    **kwargs,\n) -&gt; Self:\n    \"\"\"Convert `sio.LabeledFrame` to `dreem.io.Frame`.\n\n    Args:\n        lf: A sio.LabeledFrame object\n\n    Returns:\n        A dreem.io.Frame object\n    \"\"\"\n    from dreem.io.instance import Instance\n\n    img_shape = lf.image.shape\n    if len(img_shape) == 2:\n        img_shape = (1, *img_shape)\n    elif len(img_shape) &gt; 2 and img_shape[-1] &lt;= 3:\n        img_shape = (lf.image.shape[-1], lf.image.shape[0], lf.image.shape[1])\n    return cls(\n        video_id=video_id,\n        frame_id=(\n            lf.frame_idx.astype(np.int32)\n            if isinstance(lf.frame_idx, np.number)\n            else lf.frame_idx\n        ),\n        vid_file=lf.video.filename,\n        img_shape=img_shape,\n        instances=[Instance.from_slp(instance, **kwargs) for instance in lf],\n        device=device,\n    )\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.get_anchors","title":"<code>get_anchors()</code>","text":"<p>Get the anchor names of instances in the frame.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of anchor names used by the instances to get the crop.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_anchors(self) -&gt; list[str]:\n    \"\"\"Get the anchor names of instances in the frame.\n\n    Returns:\n        A list of anchor names used by the instances to get the crop.\n    \"\"\"\n    return [instance.anchor for instance in self.instances]\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.get_bboxes","title":"<code>get_bboxes()</code>","text":"<p>Get the bounding boxes of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,4) shaped tensor with bounding boxes of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_bboxes(self) -&gt; torch.Tensor:\n    \"\"\"Get the bounding boxes of all instances in the frame.\n\n    Returns:\n        an (N,4) shaped tensor with bounding boxes of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.empty(0, 4)\n    return torch.cat([instance.bbox for instance in self.instances], dim=0)\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.get_centroids","title":"<code>get_centroids()</code>","text":"<p>Get the centroids around which each instance's crop was formed.</p> <p>Returns:</p> Name Type Description <code>anchors</code> <code>tuple[list[str], ArrayLike]</code> <p>the node names for the corresponding point points: an n_instances x 2 array containing the centroids</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_centroids(self) -&gt; tuple[list[str], ArrayLike]:\n    \"\"\"Get the centroids around which each instance's crop was formed.\n\n    Returns:\n        anchors: the node names for the corresponding point\n        points: an n_instances x 2 array containing the centroids\n    \"\"\"\n    anchors = [\n        anchor for instance in self.instances for anchor in instance.centroid.keys()\n    ]\n\n    points = np.array(\n        [\n            point\n            for instance in self.instances\n            for point in instance.centroid.values()\n        ]\n    )\n\n    return (anchors, points)\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.get_crops","title":"<code>get_crops()</code>","text":"<p>Get the crops of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N, C, H, W) shaped tensor with crops of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_crops(self) -&gt; torch.Tensor:\n    \"\"\"Get the crops of all instances in the frame.\n\n    Returns:\n        an (N, C, H, W) shaped tensor with crops of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n\n    return torch.cat([instance.crop for instance in self.instances], dim=0)\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.get_features","title":"<code>get_features()</code>","text":"<p>Get the reid feature vectors of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N, D) shaped tensor with reid feature vectors of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_features(self) -&gt; torch.Tensor:\n    \"\"\"Get the reid feature vectors of all instances in the frame.\n\n    Returns:\n        an (N, D) shaped tensor with reid feature vectors of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.features for instance in self.instances], dim=0)\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.get_gt_track_ids","title":"<code>get_gt_track_ids()</code>","text":"<p>Get the gt track ids of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,) shaped tensor with the gt track ids of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_gt_track_ids(self) -&gt; torch.Tensor:\n    \"\"\"Get the gt track ids of all instances in the frame.\n\n    Returns:\n        an (N,) shaped tensor with the gt track ids of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.gt_track_id for instance in self.instances])\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.get_pred_track_ids","title":"<code>get_pred_track_ids()</code>","text":"<p>Get the pred track ids of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,) shaped tensor with the pred track ids of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_pred_track_ids(self) -&gt; torch.Tensor:\n    \"\"\"Get the pred track ids of all instances in the frame.\n\n    Returns:\n        an (N,) shaped tensor with the pred track ids of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.pred_track_id for instance in self.instances])\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.get_traj_score","title":"<code>get_traj_score(key=None)</code>","text":"<p>Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str | None</code> <p>The key of the trajectory score to be accessed. Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}</p> <code>None</code> <p>Returns:</p> Type Description <code>dict | ArrayLike | None</code> <ul> <li>dictionary containing all trajectory scores if key is None</li> <li>trajectory score associated with key</li> <li>None if the key is not found</li> </ul> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_traj_score(self, key: str | None = None) -&gt; dict | ArrayLike | None:\n    \"\"\"Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.\n\n    Args:\n        key: The key of the trajectory score to be accessed.\n            Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}\n\n    Returns:\n        - dictionary containing all trajectory scores if key is None\n        - trajectory score associated with key\n        - None if the key is not found\n    \"\"\"\n    if key is None:\n        return self._traj_score\n    else:\n        try:\n            return self._traj_score[key]\n        except KeyError as e:\n            logger.exception(f\"Could not access {key} traj_score due to {e}\")\n            return None\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_asso_output","title":"<code>has_asso_output()</code>","text":"<p>Determine whether the frame has an association matrix computed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the frame has an association matrix otherwise, False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_asso_output(self) -&gt; bool:\n    \"\"\"Determine whether the frame has an association matrix computed.\n\n    Returns:\n        True if the frame has an association matrix otherwise, False.\n    \"\"\"\n    if self._asso_output is None or len(self._asso_output.matrix) == 0:\n        return False\n    return True\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_bboxes","title":"<code>has_bboxes()</code>","text":"<p>Check if any of frames instances has a bounding box.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a bounding box otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_bboxes(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a bounding box.\n\n    Returns:\n        True if at least 1 instance has a bounding box otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_bboxes() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_crops","title":"<code>has_crops()</code>","text":"<p>Check if any of frames instances has a crop.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a crop otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_crops(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a crop.\n\n    Returns:\n        True if at least 1 instance has a crop otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_crop() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_features","title":"<code>has_features()</code>","text":"<p>Check if any of frames instances has reid features already computed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance have reid features otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_features(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has reid features already computed.\n\n    Returns:\n        True if at least 1 instance have reid features otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_features() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_gt_track_ids","title":"<code>has_gt_track_ids()</code>","text":"<p>Check if any of frames instances has a gt track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a gt track id otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_gt_track_ids(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a gt track id.\n\n    Returns:\n        True if at least 1 instance has a gt track id otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_gt_track_id() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_instances","title":"<code>has_instances()</code>","text":"<p>Determine whether there are instances in the frame.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if there are instances in the frame, otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_instances(self) -&gt; bool:\n    \"\"\"Determine whether there are instances in the frame.\n\n    Returns:\n        True if there are instances in the frame, otherwise False.\n    \"\"\"\n    if self.num_detected == 0:\n        return False\n    return True\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_matches","title":"<code>has_matches()</code>","text":"<p>Check whether or not matches have been computed for frame.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if frame contains matches otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_matches(self) -&gt; bool:\n    \"\"\"Check whether or not matches have been computed for frame.\n\n    Returns:\n        True if frame contains matches otherwise False.\n    \"\"\"\n    if self._matches is not None and len(self._matches) &gt; 0:\n        return True\n    return False\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_pred_track_ids","title":"<code>has_pred_track_ids()</code>","text":"<p>Check if any of frames instances has a pred track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a pred track id otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_pred_track_ids(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a pred track id.\n\n    Returns:\n        True if at least 1 instance has a pred track id otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_pred_track_id() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.has_traj_score","title":"<code>has_traj_score()</code>","text":"<p>Check if any trajectory association matrix has been saved.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True there is at least one association matrix otherwise, false.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_traj_score(self) -&gt; bool:\n    \"\"\"Check if any trajectory association matrix has been saved.\n\n    Returns:\n        True there is at least one association matrix otherwise, false.\n    \"\"\"\n    if len(self._traj_score) == 0:\n        return False\n    return True\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.to","title":"<code>to(map_location)</code>","text":"<p>Move frame to different device or dtype (See <code>torch.to</code> for more info).</p> <p>Parameters:</p> Name Type Description Default <code>map_location</code> <code>str | device</code> <p>A string representing the device to move to.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The frame moved to a different device/dtype.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to(self, map_location: str | torch.device) -&gt; Self:\n    \"\"\"Move frame to different device or dtype (See `torch.to` for more info).\n\n    Args:\n        map_location: A string representing the device to move to.\n\n    Returns:\n        The frame moved to a different device/dtype.\n    \"\"\"\n    self._video_id = self._video_id.to(map_location)\n    self._frame_id = self._frame_id.to(map_location)\n    self._img_shape = self._img_shape.to(map_location)\n\n    if isinstance(self._asso_output, torch.Tensor):\n        self._asso_output = self._asso_output.to(map_location)\n\n    if isinstance(self._matches, torch.Tensor):\n        self._matches = self._matches.to(map_location)\n\n    for key, val in self._traj_score.items():\n        if isinstance(val, torch.Tensor):\n            self._traj_score[key] = val.to(map_location)\n    for instance in self.instances:\n        instance = instance.to(map_location)\n\n    if isinstance(map_location, (str, torch.device)):\n        self._device = map_location\n\n    return self\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.to_h5","title":"<code>to_h5(clip_group, instance_labels=None, save=None)</code>","text":"<p>Convert frame to h5py group.</p> <p>Parameters:</p> Name Type Description Default <code>clip_group</code> <code>Group</code> <p>the h5py group representing the clip (e.g batch/video) the frame belongs to</p> required <code>instance_labels</code> <code>list | None</code> <p>the labels used to create instance group names</p> <code>None</code> <code>save</code> <code>dict[str, bool] | None</code> <p>whether to save crops, features and embeddings for the instance</p> <code>None</code> <p>Returns:     An h5py group containing the frame</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to_h5(\n    self,\n    clip_group: h5py.Group,\n    instance_labels: list | None = None,\n    save: dict[str, bool] | None = None,\n) -&gt; h5py.Group:\n    \"\"\"Convert frame to h5py group.\n\n    Args:\n        clip_group: the h5py group representing the clip (e.g batch/video) the frame belongs to\n        instance_labels: the labels used to create instance group names\n        save: whether to save crops, features and embeddings for the instance\n    Returns:\n        An h5py group containing the frame\n    \"\"\"\n    if save is None:\n        save = {\"crop\": False, \"features\": False, \"embeddings\": False}\n    frame_group = clip_group.require_group(f\"frame_{self.frame_id.item()}\")\n    frame_group.attrs.create(\"frame_id\", self.frame_id.item())\n    frame_group.attrs.create(\"vid_id\", self.video_id.item())\n    frame_group.attrs.create(\"vid_name\", self.vid_name)\n\n    frame_group.create_dataset(\n        \"asso_matrix\",\n        data=self.asso_output.numpy() if self.asso_output is not None else [],\n    )\n    asso_group = frame_group.require_group(\"traj_scores\")\n    for key, value in self.get_traj_score().items():\n        asso_group.create_dataset(\n            key, data=value.to_numpy() if value is not None else []\n        )\n\n    if instance_labels is None:\n        instance_labels = self.get_gt_track_ids.cpu().numpy()\n    for instance_label, instance in zip(instance_labels, self.instances):\n        kwargs = {}\n        if save.get(\"crop\", False):\n            kwargs[\"crop\"] = instance.crop.cpu().numpy()\n        if save.get(\"features\", False):\n            kwargs[\"features\"] = instance.features.cpu().numpy()\n        if save.get(\"embeddings\", False):\n            for key, val in instance.get_embedding().items():\n                kwargs[f\"{key}_emb\"] = val.cpu().numpy()\n        _ = instance.to_h5(frame_group, f\"instance_{instance_label}\", **kwargs)\n\n    return frame_group\n</code></pre>"},{"location":"io/frame/#dreem.io.Frame.to_slp","title":"<code>to_slp(track_lookup=None, video=None)</code>","text":"<p>Convert Frame to sleap_io.LabeledFrame object.</p> <p>Parameters:</p> Name Type Description Default <code>track_lookup</code> <code>dict[int, Track] | None</code> <p>A lookup dictionary containing the track_id and sio.Track for persistence</p> <code>None</code> <code>video</code> <code>Video | None</code> <p>An sio.Video object used for overriding.</p> <code>None</code> <p>Returns: A tuple containing a LabeledFrame object with necessary metadata and a lookup dictionary containing the track_id and sio.Track for persistence</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to_slp(\n    self,\n    track_lookup: dict[int, sio.Track] | None = None,\n    video: sio.Video | None = None,\n) -&gt; tuple[sio.LabeledFrame, dict[int, sio.Track]]:\n    \"\"\"Convert Frame to sleap_io.LabeledFrame object.\n\n    Args:\n        track_lookup: A lookup dictionary containing the track_id and sio.Track for persistence\n        video: An sio.Video object used for overriding.\n\n    Returns: A tuple containing a LabeledFrame object with necessary metadata and\n    a lookup dictionary containing the track_id and sio.Track for persistence\n    \"\"\"\n    if track_lookup is None:\n        track_lookup = {}\n\n    slp_instances = []\n    for instance in self.instances:\n        slp_instance, track_lookup = instance.to_slp(track_lookup=track_lookup)\n        slp_instances.append(slp_instance)\n\n    if video is None:\n        video = (\n            self.video\n            if isinstance(self.video, sio.Video)\n            else sio.load_video(self.video)\n        )\n\n    return (\n        sio.LabeledFrame(\n            video=video,\n            frame_idx=self.frame_id.item(),\n            instances=slp_instances,\n        ),\n        track_lookup,\n    )\n</code></pre>"},{"location":"io/instance/","title":"<code>Instance</code>","text":""},{"location":"io/instance/#dreem.io.Instance","title":"<code>dreem.io.Instance</code>","text":"<p>Class representing a single instance to be tracked.</p> <p>Attributes:</p> Name Type Description <code>gt_track_id</code> <code>Tensor</code> <p>Ground truth track id - only used for train/eval.</p> <code>pred_track_id</code> <code>Tensor</code> <p>Predicted track id. Untracked instance is represented by -1.</p> <code>bbox</code> <code>Tensor</code> <p>The bounding box coordinate of the instance. Defaults to an empty tensor.</p> <code>crop</code> <code>Tensor</code> <p>The crop of the instance.</p> <code>centroid</code> <code>dict[str, ArrayLike]</code> <p>the centroid around which the bbox was cropped.</p> <code>features</code> <code>Tensor</code> <p>The reid features extracted from the CNN backbone used in the transformer.</p> <code>track_score</code> <code>float</code> <p>The track score output from the association matrix.</p> <code>point_scores</code> <code>ArrayLike</code> <p>The point scores from sleap.</p> <code>instance_score</code> <code>float</code> <p>The instance scores from sleap.</p> <code>skeleton</code> <code>Skeleton</code> <p>The sleap skeleton used for the instance.</p> <code>pose</code> <code>dict[str, ArrayLike]</code> <p>A dictionary containing the node name and corresponding point.</p> <code>device</code> <code>str</code> <p>String representation of the device the instance should be on.</p> <p>Methods:</p> Name Description <code>__attrs_post_init__</code> <p>Handle dimensionality and more intricate default initializations post-init.</p> <code>__repr__</code> <p>Return string representation of the Instance.</p> <code>add_embedding</code> <p>Save embedding to instance embedding dictionary.</p> <code>from_slp</code> <p>Convert a slp instance to a dreem instance.</p> <code>get_embedding</code> <p>Retrieve instance's spatial/temporal embedding.</p> <code>has_bbox</code> <p>Determine if the instance has a bbox.</p> <code>has_crop</code> <p>Determine if the instance has a crop.</p> <code>has_embedding</code> <p>Determine if the instance has embedding type requested.</p> <code>has_features</code> <p>Determine if the instance has computed reid features.</p> <code>has_gt_track_id</code> <p>Determine if instance has a gt track assignment.</p> <code>has_pose</code> <p>Check if the instance has a pose.</p> <code>has_pred_track_id</code> <p>Determine whether instance has predicted track id.</p> <code>to</code> <p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <code>to_h5</code> <p>Convert instance to an h5 group\".</p> <code>to_slp</code> <p>Convert instance to sleap_io.PredictedInstance object.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>@attrs.define(eq=False)\nclass Instance:\n    \"\"\"Class representing a single instance to be tracked.\n\n    Attributes:\n        gt_track_id: Ground truth track id - only used for train/eval.\n        pred_track_id: Predicted track id. Untracked instance is represented by -1.\n        bbox: The bounding box coordinate of the instance. Defaults to an empty tensor.\n        crop: The crop of the instance.\n        centroid: the centroid around which the bbox was cropped.\n        features: The reid features extracted from the CNN backbone used in the transformer.\n        track_score: The track score output from the association matrix.\n        point_scores: The point scores from sleap.\n        instance_score: The instance scores from sleap.\n        skeleton: The sleap skeleton used for the instance.\n        pose: A dictionary containing the node name and corresponding point.\n        device: String representation of the device the instance should be on.\n    \"\"\"\n\n    _gt_track_id: int = attrs.field(\n        alias=\"gt_track_id\", default=-1, converter=_to_tensor\n    )\n    _pred_track_id: int = attrs.field(\n        alias=\"pred_track_id\", default=-1, converter=_to_tensor\n    )\n    _bbox: ArrayLike = attrs.field(alias=\"bbox\", factory=list, converter=_to_tensor)\n    _crop: ArrayLike = attrs.field(alias=\"crop\", factory=list, converter=_to_tensor)\n    _centroid: dict[str, ArrayLike] = attrs.field(alias=\"centroid\", factory=dict)\n    _features: ArrayLike = attrs.field(\n        alias=\"features\", factory=list, converter=_to_tensor\n    )\n    _embeddings: dict = attrs.field(alias=\"embeddings\", factory=dict)\n    _track_score: float = attrs.field(alias=\"track_score\", default=-1.0)\n    _instance_score: float = attrs.field(alias=\"instance_score\", default=-1.0)\n    _point_scores: ArrayLike | None = attrs.field(alias=\"point_scores\", default=None)\n    _skeleton: sio.Skeleton | None = attrs.field(alias=\"skeleton\", default=None)\n    _mask: ArrayLike | None = attrs.field(\n        alias=\"mask\", converter=_to_tensor, default=None\n    )\n    _pose: dict[str, ArrayLike] = attrs.field(alias=\"pose\", factory=dict)\n    _device: str | torch.device | None = attrs.field(alias=\"device\", default=None)\n    _frame: \"Frame\" = None\n\n    def __attrs_post_init__(self) -&gt; None:\n        \"\"\"Handle dimensionality and more intricate default initializations post-init.\"\"\"\n        self.bbox = _expand_to_rank(self.bbox, 3)\n        self.crop = _expand_to_rank(self.crop, 4)\n        self.features = _expand_to_rank(self.features, 2)\n\n        if self.skeleton is None:\n            self.skeleton = sio.Skeleton([\"centroid\"])\n\n        if self.bbox.shape[-1] == 0:\n            self.bbox = torch.empty([1, 0, 4])\n\n        if self.crop.shape[-1] == 0 and self.bbox.shape[1] != 0:\n            y1, x1, y2, x2 = self.bbox.squeeze(dim=0).nanmean(dim=0)\n            self.centroid = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        if len(self.pose) == 0 and self.bbox.shape[1]:\n            y1, x1, y2, x2 = self.bbox.squeeze(dim=0).mean(dim=0)\n            self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        if self.point_scores is None and len(self.pose) != 0:\n            self._point_scores = np.zeros((len(self.pose), 2))\n\n        self.to(self.device)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation of the Instance.\"\"\"\n        return (\n            \"Instance(\"\n            f\"gt_track_id={self._gt_track_id.item()}, \"\n            f\"pred_track_id={self._pred_track_id.item()}, \"\n            f\"bbox={self._bbox}, \"\n            f\"centroid={self._centroid}, \"\n            f\"crop={self._crop.shape}, \"\n            f\"features={self._features.shape}, \"\n            f\"device={self._device}\"\n            \")\"\n        )\n\n    def to(self, map_location: str | torch.device) -&gt; Self:\n        \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n        Args:\n            map_location: Either the device or dtype for the instance to be moved.\n\n        Returns:\n            self: reference to the instance moved to correct device/dtype.\n        \"\"\"\n        if map_location is not None and map_location != \"\":\n            self._gt_track_id = self._gt_track_id.to(map_location)\n            self._pred_track_id = self._pred_track_id.to(map_location)\n            self._bbox = self._bbox.to(map_location)\n            self._crop = self._crop.to(map_location)\n            self._features = self._features.to(map_location)\n            if isinstance(map_location, (str, torch.device)):\n                self.device = map_location\n\n        return self\n\n    @classmethod\n    def from_slp(\n        cls,\n        slp_instance: sio.PredictedInstance | sio.Instance,\n        bbox_size: int | tuple[int, int] = 64,\n        crop: ArrayLike | None = None,\n        device: str | None = None,\n    ) -&gt; Self:\n        \"\"\"Convert a slp instance to a dreem instance.\n\n        Args:\n            slp_instance: A `sleap_io.Instance` object representing a detection\n            bbox_size: size of the pose-centered bbox to form.\n            crop: The corresponding crop of the bbox\n            device: which device to keep the instance on\n        Returns:\n            A dreem.Instance object with a pose-centered bbox and no crop.\n        \"\"\"\n        try:\n            track_id = int(slp_instance.track.name)\n        except ValueError:\n            track_id = int(\n                \"\".join([str(ord(c)) for c in slp_instance.track.name])\n            )  # better way to handle this?\n        if isinstance(bbox_size, int):\n            bbox_size = (bbox_size, bbox_size)\n\n        track_score = -1.0\n        point_scores = np.full(len(slp_instance.points), -1)\n        instance_score = -1\n        if isinstance(slp_instance, sio.PredictedInstance):\n            track_score = slp_instance.tracking_score\n            point_scores = slp_instance.numpy()[:, -1]\n            instance_score = slp_instance.score\n\n        centroid = np.nanmean(slp_instance.numpy(), axis=1)\n        bbox = [\n            centroid[1] - bbox_size[1],\n            centroid[0] - bbox_size[0],\n            centroid[1] + bbox_size[1],\n            centroid[0] + bbox_size[0],\n        ]\n        return cls(\n            gt_track_id=track_id,\n            bbox=bbox,\n            crop=crop,\n            centroid={\"centroid\": centroid},\n            track_score=track_score,\n            point_scores=point_scores,\n            instance_score=instance_score,\n            skeleton=slp_instance.skeleton,\n            pose={\n                node.name: point.numpy() for node, point in slp_instance.points.items()\n            },\n            device=device,\n        )\n\n    def to_slp(\n        self, track_lookup: dict[int, sio.Track] = {}\n    ) -&gt; tuple[sio.PredictedInstance, dict[int, sio.Track]]:\n        \"\"\"Convert instance to sleap_io.PredictedInstance object.\n\n        Args:\n            track_lookup: A track look up dictionary containing track_id:sio.Track.\n        Returns: A sleap_io.PredictedInstance with necessary metadata\n            and a track_lookup dictionary to persist tracks.\n        \"\"\"\n        try:\n            track_id = self.pred_track_id.item()\n            if track_id not in track_lookup:\n                track_lookup[track_id] = sio.Track(name=self.pred_track_id.item())\n\n            track = track_lookup[track_id]\n\n            return (\n                sio.PredictedInstance.from_numpy(\n                    points_data=np.array(list(self.pose.values())),\n                    skeleton=self.skeleton,\n                    point_scores=self.point_scores,\n                    score=self.instance_score,\n                    tracking_score=self.track_score,\n                    track=track,\n                ),\n                track_lookup,\n            )\n        except Exception as e:\n            logger.exception(\n                f\"Pose: {np.array(list(self.pose.values())).shape}, Pose score shape {self.point_scores.shape}\"\n            )\n            raise RuntimeError(f\"Failed to convert to sio.PredictedInstance: {e}\")\n\n    def to_h5(\n        self, frame_group: h5py.Group, label: Any = None, **kwargs: dict\n    ) -&gt; h5py.Group:\n        \"\"\"Convert instance to an h5 group\".\n\n        By default we always save:\n            - the gt/pred track id\n            - bbox\n            - centroid\n            - pose\n            - instance/traj/points score\n        Larger arrays (crops/features/embeddings) can be saved by passing as kwargs\n\n        Args:\n            frame_group: the h5py group representing the frame the instance appears on\n            label: the name of the instance group that will be created\n            **kwargs: additional key:value pairs to be saved as datasets.\n\n        Returns:\n            The h5 group representing this instance.\n        \"\"\"\n        if label is None:\n            if pred_track_id != -1:\n                label = f\"instance_{self.pred_track_id.item()}\"\n            else:\n                label = f\"instance_{self.gt_track_id.item()}\"\n        instance_group = frame_group.create_group(label)\n        instance_group.attrs.create(\"gt_track_id\", self.gt_track_id.item())\n        instance_group.attrs.create(\"pred_track_id\", self.pred_track_id.item())\n        instance_group.attrs.create(\"track_score\", self.track_score)\n        instance_group.attrs.create(\"instance_score\", self.instance_score)\n\n        instance_group.create_dataset(\"bbox\", data=self.bbox.cpu().numpy())\n\n        pose_group = instance_group.create_group(\"pose\")\n        pose_group.create_dataset(\"points\", data=np.array(list(self.pose.values())))\n        pose_group.attrs.create(\"nodes\", list(self.pose.keys()))\n        pose_group.create_dataset(\"scores\", data=self.point_scores)\n\n        for key, value in kwargs.items():\n            if \"emb\" in key:\n                emb_group = instance_group.require_group(\"emb\")\n                emb_group.create_dataset(key, data=value)\n            else:\n                instance_group.create_dataset(key, data=value)\n\n        return instance_group\n\n    @property\n    def device(self) -&gt; str:\n        \"\"\"The device the instance is on.\n\n        Returns:\n            The str representation of the device the gpu is on.\n        \"\"\"\n        return self._device\n\n    @device.setter\n    def device(self, device) -&gt; None:\n        \"\"\"Set for the device property.\n\n        Args:\n            device: The str representation of the device.\n        \"\"\"\n        self._device = device\n\n    @property\n    def gt_track_id(self) -&gt; torch.Tensor:\n        \"\"\"The ground truth track id of the instance.\n\n        Returns:\n            A tensor containing the ground truth track id\n        \"\"\"\n        return self._gt_track_id\n\n    @gt_track_id.setter\n    def gt_track_id(self, track: int):\n        \"\"\"Set the instance ground-truth track id.\n\n        Args:\n           track: An int representing the ground-truth track id.\n        \"\"\"\n        if track is not None:\n            self._gt_track_id = torch.tensor([track])\n        else:\n            self._gt_track_id = torch.tensor([])\n\n    def has_gt_track_id(self) -&gt; bool:\n        \"\"\"Determine if instance has a gt track assignment.\n\n        Returns:\n            True if the gt track id is set, otherwise False.\n        \"\"\"\n        if self._gt_track_id.shape[0] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def pred_track_id(self) -&gt; torch.Tensor:\n        \"\"\"The track id predicted by the tracker using asso_output from model.\n\n        Returns:\n            A tensor containing the predicted track id.\n        \"\"\"\n        return self._pred_track_id\n\n    @pred_track_id.setter\n    def pred_track_id(self, track: int) -&gt; None:\n        \"\"\"Set predicted track id.\n\n        Args:\n            track: an int representing the predicted track id.\n        \"\"\"\n        if track is not None:\n            self._pred_track_id = torch.tensor([track])\n        else:\n            self._pred_track_id = torch.tensor([])\n\n    def has_pred_track_id(self) -&gt; bool:\n        \"\"\"Determine whether instance has predicted track id.\n\n        Returns:\n            True if instance has a pred track id, False otherwise.\n        \"\"\"\n        if self._pred_track_id.item() == -1 or self._pred_track_id.shape[0] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def bbox(self) -&gt; torch.Tensor:\n        \"\"\"The bounding box coordinates of the instance in the original frame.\n\n        Returns:\n            A (1,4) tensor containing the bounding box coordinates.\n        \"\"\"\n        return self._bbox\n\n    @bbox.setter\n    def bbox(self, bbox: ArrayLike) -&gt; None:\n        \"\"\"Set the instance bounding box.\n\n        Args:\n            bbox: an arraylike object containing the bounding box coordinates.\n        \"\"\"\n        if bbox is None or len(bbox) == 0:\n            self._bbox = torch.empty((0, 4))\n        else:\n            if not isinstance(bbox, torch.Tensor):\n                self._bbox = torch.tensor(bbox)\n            else:\n                self._bbox = bbox\n\n        if self._bbox.shape[0] and len(self._bbox.shape) == 1:\n            self._bbox = self._bbox.unsqueeze(0)\n        if self._bbox.shape[1] and len(self._bbox.shape) == 2:\n            self._bbox = self._bbox.unsqueeze(0)\n\n    def has_bbox(self) -&gt; bool:\n        \"\"\"Determine if the instance has a bbox.\n\n        Returns:\n            True if the instance has a bounding box, false otherwise.\n        \"\"\"\n        if self._bbox.shape[1] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def centroid(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"The centroid around which the crop was formed.\n\n        Returns:\n            A dict containing the anchor name and the x, y bbox midpoint.\n        \"\"\"\n        return self._centroid\n\n    @centroid.setter\n    def centroid(self, centroid: dict[str, ArrayLike]) -&gt; None:\n        \"\"\"Set the centroid of the instance.\n\n        Args:\n            centroid: A dict containing the anchor name and points.\n        \"\"\"\n        self._centroid = centroid\n\n    @property\n    def anchor(self) -&gt; list[str]:\n        \"\"\"The anchor node name around which the crop was formed.\n\n        Returns:\n            the list of anchors around which each crop was formed\n            the list of anchors around which each crop was formed\n        \"\"\"\n        if self.centroid:\n            return list(self.centroid.keys())\n        return \"\"\n\n    @property\n    def mask(self) -&gt; torch.Tensor:\n        \"\"\"The mask of the instance.\n\n        Returns:\n            A (h, w) tensor containing the mask of the instance.\n        \"\"\"\n        return self._mask\n\n    @mask.setter\n    def mask(self, mask: ArrayLike) -&gt; None:\n        \"\"\"Set the mask of the instance.\n\n        Args:\n            mask: an arraylike object containing the mask of the instance.\n        \"\"\"\n        if mask is None or len(mask) == 0:\n            self._mask = torch.tensor([])\n        else:\n            if not isinstance(mask, torch.Tensor):\n                self._mask = torch.tensor(mask)\n            else:\n                self._mask = mask\n\n    @property\n    def crop(self) -&gt; torch.Tensor:\n        \"\"\"The crop of the instance.\n\n        Returns:\n            A (1, c, h , w) tensor containing the cropped image centered around the instance.\n        \"\"\"\n        return self._crop\n\n    @crop.setter\n    def crop(self, crop: ArrayLike) -&gt; None:\n        \"\"\"Set the crop of the instance.\n\n        Args:\n            crop: an arraylike object containing the cropped image of the centered instance.\n        \"\"\"\n        if crop is None or len(crop) == 0:\n            self._crop = torch.tensor([])\n        else:\n            if not isinstance(crop, torch.Tensor):\n                self._crop = torch.tensor(crop)\n            else:\n                self._crop = crop\n\n        if len(self._crop.shape) == 2:\n            self._crop = self._crop.unsqueeze(0)\n        if len(self._crop.shape) == 3:\n            self._crop = self._crop.unsqueeze(0)\n\n    def has_crop(self) -&gt; bool:\n        \"\"\"Determine if the instance has a crop.\n\n        Returns:\n            True if the instance has an image otherwise False.\n        \"\"\"\n        if self._crop.shape[-1] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def features(self) -&gt; torch.Tensor:\n        \"\"\"Re-ID feature vector from backbone model to be used as input to transformer.\n\n        Returns:\n            a (1, d) tensor containing the reid feature vector.\n        \"\"\"\n        return self._features\n\n    @features.setter\n    def features(self, features: ArrayLike) -&gt; None:\n        \"\"\"Set the reid feature vector of the instance.\n\n        Args:\n            features: a (1,d) array like object containing the reid features for the instance.\n        \"\"\"\n        if features is None or len(features) == 0:\n            self._features = torch.tensor([])\n\n        elif not isinstance(features, torch.Tensor):\n            self._features = torch.tensor(features)\n        else:\n            self._features = features\n\n        if self._features.shape[0] and len(self._features.shape) == 1:\n            self._features = self._features.unsqueeze(0)\n\n    def has_features(self) -&gt; bool:\n        \"\"\"Determine if the instance has computed reid features.\n\n        Returns:\n            True if the instance has reid features, False otherwise.\n        \"\"\"\n        if self._features.shape[-1] == 0:\n            return False\n        else:\n            return True\n\n    def has_embedding(self, emb_type: str | None = None) -&gt; bool:\n        \"\"\"Determine if the instance has embedding type requested.\n\n        Args:\n            emb_type: The key to check in the embedding dictionary.\n\n        Returns:\n            True if `emb_type` in embedding_dict else false\n        \"\"\"\n        return emb_type in self._embeddings\n\n    def get_embedding(\n        self, emb_type: str = \"all\"\n    ) -&gt; dict[str, torch.Tensor] | torch.Tensor | None:\n        \"\"\"Retrieve instance's spatial/temporal embedding.\n\n        Args:\n            emb_type: The string key of the embedding to retrieve. Should be \"pos\", \"temp\"\n\n        Returns:\n            * A torch tensor representing the spatial/temporal location of the instance.\n            * None if the embedding is not stored\n        \"\"\"\n        if emb_type.lower() == \"all\":\n            return self._embeddings\n        else:\n            try:\n                return self._embeddings[emb_type]\n            except KeyError:\n                logger.exception(\n                    f\"{emb_type} not saved! Only {list(self._embeddings.keys())} are available\"\n                )\n        return None\n\n    def add_embedding(self, emb_type: str, embedding: torch.Tensor) -&gt; None:\n        \"\"\"Save embedding to instance embedding dictionary.\n\n        Args:\n            emb_type: Key/embedding type to be saved to dictionary\n            embedding: The actual torch tensor embedding.\n        \"\"\"\n        embedding = _expand_to_rank(embedding, 2)\n        self._embeddings[emb_type] = embedding\n\n    @property\n    def frame(self) -&gt; \"Frame\":\n        \"\"\"Get the frame the instance belongs to.\n\n        Returns:\n            The back reference to the `Frame` that this `Instance` belongs to.\n        \"\"\"\n        return self._frame\n\n    @frame.setter\n    def frame(self, frame: \"Frame\") -&gt; None:\n        \"\"\"Set the back reference to the `Frame` that this `Instance` belongs to.\n\n        This field is set when instances are added to `Frame` object.\n\n        Args:\n            frame: A `Frame` object containing the metadata for the frame that the instance belongs to\n        \"\"\"\n        self._frame = frame\n\n    @property\n    def pose(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"Get the pose of the instance.\n\n        Returns:\n            A dictionary containing the node and corresponding x,y points\n        \"\"\"\n        return self._pose\n\n    @pose.setter\n    def pose(self, pose: dict[str, ArrayLike]) -&gt; None:\n        \"\"\"Set the pose of the instance.\n\n        Args:\n            pose: A nodes x 2 array containing the pose coordinates.\n        \"\"\"\n        if pose is not None:\n            self._pose = pose\n\n        elif self.bbox.shape[0]:\n            y1, x1, y2, x2 = self.bbox.squeeze()\n            self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        else:\n            self._pose = {}\n\n    def has_pose(self) -&gt; bool:\n        \"\"\"Check if the instance has a pose.\n\n        Returns True if the instance has a pose.\n        \"\"\"\n        if len(self.pose):\n            return True\n        return False\n\n    @property\n    def shown_pose(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"Get the pose with shown nodes only.\n\n        Returns: A dictionary filtered by nodes that are shown (points are not nan).\n        \"\"\"\n        pose = self.pose\n        return {node: point for node, point in pose.items() if not np.isna(point).any()}\n\n    @property\n    def skeleton(self) -&gt; sio.Skeleton:\n        \"\"\"Get the skeleton associated with the instance.\n\n        Returns: The sio.Skeleton associated with the instance.\n        \"\"\"\n        return self._skeleton\n\n    @skeleton.setter\n    def skeleton(self, skeleton: sio.Skeleton) -&gt; None:\n        \"\"\"Set the skeleton associated with the instance.\n\n        Args:\n            skeleton: The sio.Skeleton associated with the instance.\n        \"\"\"\n        self._skeleton = skeleton\n\n    @property\n    def point_scores(self) -&gt; ArrayLike:\n        \"\"\"Get the point scores associated with the pose prediction.\n\n        Returns: a vector of shape n containing the point scores outputted from sleap associated with pose predictions.\n        \"\"\"\n        return self._point_scores\n\n    @point_scores.setter\n    def point_scores(self, point_scores: ArrayLike) -&gt; None:\n        \"\"\"Set the point scores associated with the pose prediction.\n\n        Args:\n            point_scores: a vector of shape n containing the point scores\n            outputted from sleap associated with pose predictions.\n        \"\"\"\n        self._point_scores = point_scores\n\n    @property\n    def instance_score(self) -&gt; float:\n        \"\"\"Get the pose prediction score associated with the instance.\n\n        Returns: a float from 0-1 representing an instance_score.\n        \"\"\"\n        return self._instance_score\n\n    @instance_score.setter\n    def instance_score(self, instance_score: float) -&gt; None:\n        \"\"\"Set the pose prediction score associated with the instance.\n\n        Args:\n            instance_score: a float from 0-1 representing an instance_score.\n        \"\"\"\n        self._instance_score = instance_score\n\n    @property\n    def track_score(self) -&gt; float:\n        \"\"\"Get the track_score of the instance.\n\n        Returns: A float from 0-1 representing the output used in the tracker for assignment.\n        \"\"\"\n        return self._track_score\n\n    @track_score.setter\n    def track_score(self, track_score: float) -&gt; None:\n        \"\"\"Set the track_score of the instance.\n\n        Args:\n            track_score: A float from 0-1 representing the output used in the tracker for assignment.\n        \"\"\"\n        self._track_score = track_score\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.anchor","title":"<code>anchor</code>  <code>property</code>","text":"<p>The anchor node name around which the crop was formed.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>the list of anchors around which each crop was formed the list of anchors around which each crop was formed</p>"},{"location":"io/instance/#dreem.io.Instance.bbox","title":"<code>bbox</code>  <code>property</code> <code>writable</code>","text":"<p>The bounding box coordinates of the instance in the original frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A (1,4) tensor containing the bounding box coordinates.</p>"},{"location":"io/instance/#dreem.io.Instance.centroid","title":"<code>centroid</code>  <code>property</code> <code>writable</code>","text":"<p>The centroid around which the crop was formed.</p> <p>Returns:</p> Type Description <code>dict[str, ArrayLike]</code> <p>A dict containing the anchor name and the x, y bbox midpoint.</p>"},{"location":"io/instance/#dreem.io.Instance.crop","title":"<code>crop</code>  <code>property</code> <code>writable</code>","text":"<p>The crop of the instance.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A (1, c, h , w) tensor containing the cropped image centered around the instance.</p>"},{"location":"io/instance/#dreem.io.Instance.device","title":"<code>device</code>  <code>property</code> <code>writable</code>","text":"<p>The device the instance is on.</p> <p>Returns:</p> Type Description <code>str</code> <p>The str representation of the device the gpu is on.</p>"},{"location":"io/instance/#dreem.io.Instance.features","title":"<code>features</code>  <code>property</code> <code>writable</code>","text":"<p>Re-ID feature vector from backbone model to be used as input to transformer.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>a (1, d) tensor containing the reid feature vector.</p>"},{"location":"io/instance/#dreem.io.Instance.frame","title":"<code>frame</code>  <code>property</code> <code>writable</code>","text":"<p>Get the frame the instance belongs to.</p> <p>Returns:</p> Type Description <code>Frame</code> <p>The back reference to the <code>Frame</code> that this <code>Instance</code> belongs to.</p>"},{"location":"io/instance/#dreem.io.Instance.gt_track_id","title":"<code>gt_track_id</code>  <code>property</code> <code>writable</code>","text":"<p>The ground truth track id of the instance.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the ground truth track id</p>"},{"location":"io/instance/#dreem.io.Instance.instance_score","title":"<code>instance_score</code>  <code>property</code> <code>writable</code>","text":"<p>Get the pose prediction score associated with the instance.</p> <p>Returns: a float from 0-1 representing an instance_score.</p>"},{"location":"io/instance/#dreem.io.Instance.mask","title":"<code>mask</code>  <code>property</code> <code>writable</code>","text":"<p>The mask of the instance.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A (h, w) tensor containing the mask of the instance.</p>"},{"location":"io/instance/#dreem.io.Instance.point_scores","title":"<code>point_scores</code>  <code>property</code> <code>writable</code>","text":"<p>Get the point scores associated with the pose prediction.</p> <p>Returns: a vector of shape n containing the point scores outputted from sleap associated with pose predictions.</p>"},{"location":"io/instance/#dreem.io.Instance.pose","title":"<code>pose</code>  <code>property</code> <code>writable</code>","text":"<p>Get the pose of the instance.</p> <p>Returns:</p> Type Description <code>dict[str, ArrayLike]</code> <p>A dictionary containing the node and corresponding x,y points</p>"},{"location":"io/instance/#dreem.io.Instance.pred_track_id","title":"<code>pred_track_id</code>  <code>property</code> <code>writable</code>","text":"<p>The track id predicted by the tracker using asso_output from model.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the predicted track id.</p>"},{"location":"io/instance/#dreem.io.Instance.shown_pose","title":"<code>shown_pose</code>  <code>property</code>","text":"<p>Get the pose with shown nodes only.</p> <p>Returns: A dictionary filtered by nodes that are shown (points are not nan).</p>"},{"location":"io/instance/#dreem.io.Instance.skeleton","title":"<code>skeleton</code>  <code>property</code> <code>writable</code>","text":"<p>Get the skeleton associated with the instance.</p> <p>Returns: The sio.Skeleton associated with the instance.</p>"},{"location":"io/instance/#dreem.io.Instance.track_score","title":"<code>track_score</code>  <code>property</code> <code>writable</code>","text":"<p>Get the track_score of the instance.</p> <p>Returns: A float from 0-1 representing the output used in the tracker for assignment.</p>"},{"location":"io/instance/#dreem.io.Instance.__attrs_post_init__","title":"<code>__attrs_post_init__()</code>","text":"<p>Handle dimensionality and more intricate default initializations post-init.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def __attrs_post_init__(self) -&gt; None:\n    \"\"\"Handle dimensionality and more intricate default initializations post-init.\"\"\"\n    self.bbox = _expand_to_rank(self.bbox, 3)\n    self.crop = _expand_to_rank(self.crop, 4)\n    self.features = _expand_to_rank(self.features, 2)\n\n    if self.skeleton is None:\n        self.skeleton = sio.Skeleton([\"centroid\"])\n\n    if self.bbox.shape[-1] == 0:\n        self.bbox = torch.empty([1, 0, 4])\n\n    if self.crop.shape[-1] == 0 and self.bbox.shape[1] != 0:\n        y1, x1, y2, x2 = self.bbox.squeeze(dim=0).nanmean(dim=0)\n        self.centroid = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n    if len(self.pose) == 0 and self.bbox.shape[1]:\n        y1, x1, y2, x2 = self.bbox.squeeze(dim=0).mean(dim=0)\n        self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n    if self.point_scores is None and len(self.pose) != 0:\n        self._point_scores = np.zeros((len(self.pose), 2))\n\n    self.to(self.device)\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of the Instance.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation of the Instance.\"\"\"\n    return (\n        \"Instance(\"\n        f\"gt_track_id={self._gt_track_id.item()}, \"\n        f\"pred_track_id={self._pred_track_id.item()}, \"\n        f\"bbox={self._bbox}, \"\n        f\"centroid={self._centroid}, \"\n        f\"crop={self._crop.shape}, \"\n        f\"features={self._features.shape}, \"\n        f\"device={self._device}\"\n        \")\"\n    )\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.add_embedding","title":"<code>add_embedding(emb_type, embedding)</code>","text":"<p>Save embedding to instance embedding dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>Key/embedding type to be saved to dictionary</p> required <code>embedding</code> <code>Tensor</code> <p>The actual torch tensor embedding.</p> required Source code in <code>dreem/io/instance.py</code> <pre><code>def add_embedding(self, emb_type: str, embedding: torch.Tensor) -&gt; None:\n    \"\"\"Save embedding to instance embedding dictionary.\n\n    Args:\n        emb_type: Key/embedding type to be saved to dictionary\n        embedding: The actual torch tensor embedding.\n    \"\"\"\n    embedding = _expand_to_rank(embedding, 2)\n    self._embeddings[emb_type] = embedding\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.from_slp","title":"<code>from_slp(slp_instance, bbox_size=64, crop=None, device=None)</code>  <code>classmethod</code>","text":"<p>Convert a slp instance to a dreem instance.</p> <p>Parameters:</p> Name Type Description Default <code>slp_instance</code> <code>PredictedInstance | Instance</code> <p>A <code>sleap_io.Instance</code> object representing a detection</p> required <code>bbox_size</code> <code>int | tuple[int, int]</code> <p>size of the pose-centered bbox to form.</p> <code>64</code> <code>crop</code> <code>ArrayLike | None</code> <p>The corresponding crop of the bbox</p> <code>None</code> <code>device</code> <code>str | None</code> <p>which device to keep the instance on</p> <code>None</code> <p>Returns:     A dreem.Instance object with a pose-centered bbox and no crop.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>@classmethod\ndef from_slp(\n    cls,\n    slp_instance: sio.PredictedInstance | sio.Instance,\n    bbox_size: int | tuple[int, int] = 64,\n    crop: ArrayLike | None = None,\n    device: str | None = None,\n) -&gt; Self:\n    \"\"\"Convert a slp instance to a dreem instance.\n\n    Args:\n        slp_instance: A `sleap_io.Instance` object representing a detection\n        bbox_size: size of the pose-centered bbox to form.\n        crop: The corresponding crop of the bbox\n        device: which device to keep the instance on\n    Returns:\n        A dreem.Instance object with a pose-centered bbox and no crop.\n    \"\"\"\n    try:\n        track_id = int(slp_instance.track.name)\n    except ValueError:\n        track_id = int(\n            \"\".join([str(ord(c)) for c in slp_instance.track.name])\n        )  # better way to handle this?\n    if isinstance(bbox_size, int):\n        bbox_size = (bbox_size, bbox_size)\n\n    track_score = -1.0\n    point_scores = np.full(len(slp_instance.points), -1)\n    instance_score = -1\n    if isinstance(slp_instance, sio.PredictedInstance):\n        track_score = slp_instance.tracking_score\n        point_scores = slp_instance.numpy()[:, -1]\n        instance_score = slp_instance.score\n\n    centroid = np.nanmean(slp_instance.numpy(), axis=1)\n    bbox = [\n        centroid[1] - bbox_size[1],\n        centroid[0] - bbox_size[0],\n        centroid[1] + bbox_size[1],\n        centroid[0] + bbox_size[0],\n    ]\n    return cls(\n        gt_track_id=track_id,\n        bbox=bbox,\n        crop=crop,\n        centroid={\"centroid\": centroid},\n        track_score=track_score,\n        point_scores=point_scores,\n        instance_score=instance_score,\n        skeleton=slp_instance.skeleton,\n        pose={\n            node.name: point.numpy() for node, point in slp_instance.points.items()\n        },\n        device=device,\n    )\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.get_embedding","title":"<code>get_embedding(emb_type='all')</code>","text":"<p>Retrieve instance's spatial/temporal embedding.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>The string key of the embedding to retrieve. Should be \"pos\", \"temp\"</p> <code>'all'</code> <p>Returns:</p> Type Description <code>dict[str, Tensor] | Tensor | None</code> <ul> <li>A torch tensor representing the spatial/temporal location of the instance.</li> <li>None if the embedding is not stored</li> </ul> Source code in <code>dreem/io/instance.py</code> <pre><code>def get_embedding(\n    self, emb_type: str = \"all\"\n) -&gt; dict[str, torch.Tensor] | torch.Tensor | None:\n    \"\"\"Retrieve instance's spatial/temporal embedding.\n\n    Args:\n        emb_type: The string key of the embedding to retrieve. Should be \"pos\", \"temp\"\n\n    Returns:\n        * A torch tensor representing the spatial/temporal location of the instance.\n        * None if the embedding is not stored\n    \"\"\"\n    if emb_type.lower() == \"all\":\n        return self._embeddings\n    else:\n        try:\n            return self._embeddings[emb_type]\n        except KeyError:\n            logger.exception(\n                f\"{emb_type} not saved! Only {list(self._embeddings.keys())} are available\"\n            )\n    return None\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.has_bbox","title":"<code>has_bbox()</code>","text":"<p>Determine if the instance has a bbox.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has a bounding box, false otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_bbox(self) -&gt; bool:\n    \"\"\"Determine if the instance has a bbox.\n\n    Returns:\n        True if the instance has a bounding box, false otherwise.\n    \"\"\"\n    if self._bbox.shape[1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.has_crop","title":"<code>has_crop()</code>","text":"<p>Determine if the instance has a crop.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has an image otherwise False.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_crop(self) -&gt; bool:\n    \"\"\"Determine if the instance has a crop.\n\n    Returns:\n        True if the instance has an image otherwise False.\n    \"\"\"\n    if self._crop.shape[-1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.has_embedding","title":"<code>has_embedding(emb_type=None)</code>","text":"<p>Determine if the instance has embedding type requested.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str | None</code> <p>The key to check in the embedding dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if <code>emb_type</code> in embedding_dict else false</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_embedding(self, emb_type: str | None = None) -&gt; bool:\n    \"\"\"Determine if the instance has embedding type requested.\n\n    Args:\n        emb_type: The key to check in the embedding dictionary.\n\n    Returns:\n        True if `emb_type` in embedding_dict else false\n    \"\"\"\n    return emb_type in self._embeddings\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.has_features","title":"<code>has_features()</code>","text":"<p>Determine if the instance has computed reid features.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has reid features, False otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_features(self) -&gt; bool:\n    \"\"\"Determine if the instance has computed reid features.\n\n    Returns:\n        True if the instance has reid features, False otherwise.\n    \"\"\"\n    if self._features.shape[-1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.has_gt_track_id","title":"<code>has_gt_track_id()</code>","text":"<p>Determine if instance has a gt track assignment.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the gt track id is set, otherwise False.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_gt_track_id(self) -&gt; bool:\n    \"\"\"Determine if instance has a gt track assignment.\n\n    Returns:\n        True if the gt track id is set, otherwise False.\n    \"\"\"\n    if self._gt_track_id.shape[0] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.has_pose","title":"<code>has_pose()</code>","text":"<p>Check if the instance has a pose.</p> <p>Returns True if the instance has a pose.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_pose(self) -&gt; bool:\n    \"\"\"Check if the instance has a pose.\n\n    Returns True if the instance has a pose.\n    \"\"\"\n    if len(self.pose):\n        return True\n    return False\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.has_pred_track_id","title":"<code>has_pred_track_id()</code>","text":"<p>Determine whether instance has predicted track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if instance has a pred track id, False otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_pred_track_id(self) -&gt; bool:\n    \"\"\"Determine whether instance has predicted track id.\n\n    Returns:\n        True if instance has a pred track id, False otherwise.\n    \"\"\"\n    if self._pred_track_id.item() == -1 or self._pred_track_id.shape[0] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.to","title":"<code>to(map_location)</code>","text":"<p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <p>Parameters:</p> Name Type Description Default <code>map_location</code> <code>str | device</code> <p>Either the device or dtype for the instance to be moved.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>Self</code> <p>reference to the instance moved to correct device/dtype.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to(self, map_location: str | torch.device) -&gt; Self:\n    \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n    Args:\n        map_location: Either the device or dtype for the instance to be moved.\n\n    Returns:\n        self: reference to the instance moved to correct device/dtype.\n    \"\"\"\n    if map_location is not None and map_location != \"\":\n        self._gt_track_id = self._gt_track_id.to(map_location)\n        self._pred_track_id = self._pred_track_id.to(map_location)\n        self._bbox = self._bbox.to(map_location)\n        self._crop = self._crop.to(map_location)\n        self._features = self._features.to(map_location)\n        if isinstance(map_location, (str, torch.device)):\n            self.device = map_location\n\n    return self\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.to_h5","title":"<code>to_h5(frame_group, label=None, **kwargs)</code>","text":"<p>Convert instance to an h5 group\".</p> By default we always save <ul> <li>the gt/pred track id</li> <li>bbox</li> <li>centroid</li> <li>pose</li> <li>instance/traj/points score</li> </ul> <p>Larger arrays (crops/features/embeddings) can be saved by passing as kwargs</p> <p>Parameters:</p> Name Type Description Default <code>frame_group</code> <code>Group</code> <p>the h5py group representing the frame the instance appears on</p> required <code>label</code> <code>Any</code> <p>the name of the instance group that will be created</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>additional key:value pairs to be saved as datasets.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Group</code> <p>The h5 group representing this instance.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to_h5(\n    self, frame_group: h5py.Group, label: Any = None, **kwargs: dict\n) -&gt; h5py.Group:\n    \"\"\"Convert instance to an h5 group\".\n\n    By default we always save:\n        - the gt/pred track id\n        - bbox\n        - centroid\n        - pose\n        - instance/traj/points score\n    Larger arrays (crops/features/embeddings) can be saved by passing as kwargs\n\n    Args:\n        frame_group: the h5py group representing the frame the instance appears on\n        label: the name of the instance group that will be created\n        **kwargs: additional key:value pairs to be saved as datasets.\n\n    Returns:\n        The h5 group representing this instance.\n    \"\"\"\n    if label is None:\n        if pred_track_id != -1:\n            label = f\"instance_{self.pred_track_id.item()}\"\n        else:\n            label = f\"instance_{self.gt_track_id.item()}\"\n    instance_group = frame_group.create_group(label)\n    instance_group.attrs.create(\"gt_track_id\", self.gt_track_id.item())\n    instance_group.attrs.create(\"pred_track_id\", self.pred_track_id.item())\n    instance_group.attrs.create(\"track_score\", self.track_score)\n    instance_group.attrs.create(\"instance_score\", self.instance_score)\n\n    instance_group.create_dataset(\"bbox\", data=self.bbox.cpu().numpy())\n\n    pose_group = instance_group.create_group(\"pose\")\n    pose_group.create_dataset(\"points\", data=np.array(list(self.pose.values())))\n    pose_group.attrs.create(\"nodes\", list(self.pose.keys()))\n    pose_group.create_dataset(\"scores\", data=self.point_scores)\n\n    for key, value in kwargs.items():\n        if \"emb\" in key:\n            emb_group = instance_group.require_group(\"emb\")\n            emb_group.create_dataset(key, data=value)\n        else:\n            instance_group.create_dataset(key, data=value)\n\n    return instance_group\n</code></pre>"},{"location":"io/instance/#dreem.io.Instance.to_slp","title":"<code>to_slp(track_lookup={})</code>","text":"<p>Convert instance to sleap_io.PredictedInstance object.</p> <p>Parameters:</p> Name Type Description Default <code>track_lookup</code> <code>dict[int, Track]</code> <p>A track look up dictionary containing track_id:sio.Track.</p> <code>{}</code> <p>Returns: A sleap_io.PredictedInstance with necessary metadata     and a track_lookup dictionary to persist tracks.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to_slp(\n    self, track_lookup: dict[int, sio.Track] = {}\n) -&gt; tuple[sio.PredictedInstance, dict[int, sio.Track]]:\n    \"\"\"Convert instance to sleap_io.PredictedInstance object.\n\n    Args:\n        track_lookup: A track look up dictionary containing track_id:sio.Track.\n    Returns: A sleap_io.PredictedInstance with necessary metadata\n        and a track_lookup dictionary to persist tracks.\n    \"\"\"\n    try:\n        track_id = self.pred_track_id.item()\n        if track_id not in track_lookup:\n            track_lookup[track_id] = sio.Track(name=self.pred_track_id.item())\n\n        track = track_lookup[track_id]\n\n        return (\n            sio.PredictedInstance.from_numpy(\n                points_data=np.array(list(self.pose.values())),\n                skeleton=self.skeleton,\n                point_scores=self.point_scores,\n                score=self.instance_score,\n                tracking_score=self.track_score,\n                track=track,\n            ),\n            track_lookup,\n        )\n    except Exception as e:\n        logger.exception(\n            f\"Pose: {np.array(list(self.pose.values())).shape}, Pose score shape {self.point_scores.shape}\"\n        )\n        raise RuntimeError(f\"Failed to convert to sio.PredictedInstance: {e}\")\n</code></pre>"},{"location":"io/track/","title":"<code>Track</code>","text":""},{"location":"io/track/#dreem.io.Track","title":"<code>dreem.io.Track</code>","text":"<p>Object for storing instances of the same track.</p> <p>Attributes:</p> Name Type Description <code>id</code> <p>the track label.</p> <code>instances</code> <code>list['Instances']</code> <p>A list of instances belonging to the track.</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Get an instance from the track.</p> <code>__len__</code> <p>Get the length of the track.</p> <code>__repr__</code> <p>Get the string representation of the track.</p> Source code in <code>dreem/io/track.py</code> <pre><code>@attrs.define(eq=False)\nclass Track:\n    \"\"\"Object for storing instances of the same track.\n\n    Attributes:\n        id: the track label.\n        instances: A list of instances belonging to the track.\n    \"\"\"\n\n    _id: int = attrs.field(alias=\"id\")\n    _instances: list[\"Instance\"] = attrs.field(alias=\"instances\", factory=list)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get the string representation of the track.\n\n        Returns:\n            the string representation of the Track.\n        \"\"\"\n        return f\"Track(id={self.id}, len={len(self)})\"\n\n    @property\n    def track_id(self) -&gt; int:\n        \"\"\"Get the id of the track.\n\n        Returns:\n            The integer id of the track.\n        \"\"\"\n        return self._id\n\n    @track_id.setter\n    def track_id(self, track_id: int) -&gt; None:\n        \"\"\"Set the id of the track.\n\n        Args:\n            track_id: the int id of the track.\n        \"\"\"\n        self._id = track_id\n\n    @property\n    def instances(self) -&gt; list[\"Instances\"]:\n        \"\"\"Get the instances belonging to this track.\n\n        Returns:\n            A list of instances with this track id.\n        \"\"\"\n        return self._instances\n\n    @instances.setter\n    def instances(self, instances) -&gt; None:\n        \"\"\"Set the instances belonging to this track.\n\n        Args:\n            instances: A list of instances that belong to the same track.\n        \"\"\"\n        self._instances = instances\n\n    @property\n    def frames(self) -&gt; set[\"Frame\"]:\n        \"\"\"Get the frames where this track appears.\n\n        Returns:\n            A set of `Frame` objects where this track appears.\n        \"\"\"\n        return set([instance.frame for instance in self.instances])\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the length of the track.\n\n        Returns:\n            The number of instances/frames in the track.\n        \"\"\"\n        return len(self.instances)\n\n    def __getitem__(self, ind: int | list[int]) -&gt; \"Instance\" | list[\"Instance\"]:\n        \"\"\"Get an instance from the track.\n\n        Args:\n            ind: Either a single int or list of int indices.\n\n        Returns:\n            the instance at that index of the track.instances.\n        \"\"\"\n        if isinstance(ind, int):\n            return self.instances[ind]\n        elif isinstance(ind, list):\n            return [self.instances[i] for i in ind]\n        else:\n            raise ValueError(f\"Ind must be an int or list of ints, found {type(ind)}\")\n</code></pre>"},{"location":"io/track/#dreem.io.Track.frames","title":"<code>frames</code>  <code>property</code>","text":"<p>Get the frames where this track appears.</p> <p>Returns:</p> Type Description <code>set['Frame']</code> <p>A set of <code>Frame</code> objects where this track appears.</p>"},{"location":"io/track/#dreem.io.Track.instances","title":"<code>instances</code>  <code>property</code> <code>writable</code>","text":"<p>Get the instances belonging to this track.</p> <p>Returns:</p> Type Description <code>list['Instances']</code> <p>A list of instances with this track id.</p>"},{"location":"io/track/#dreem.io.Track.track_id","title":"<code>track_id</code>  <code>property</code> <code>writable</code>","text":"<p>Get the id of the track.</p> <p>Returns:</p> Type Description <code>int</code> <p>The integer id of the track.</p>"},{"location":"io/track/#dreem.io.Track.__getitem__","title":"<code>__getitem__(ind)</code>","text":"<p>Get an instance from the track.</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> <code>int | list[int]</code> <p>Either a single int or list of int indices.</p> required <p>Returns:</p> Type Description <code>'Instance' | list['Instance']</code> <p>the instance at that index of the track.instances.</p> Source code in <code>dreem/io/track.py</code> <pre><code>def __getitem__(self, ind: int | list[int]) -&gt; \"Instance\" | list[\"Instance\"]:\n    \"\"\"Get an instance from the track.\n\n    Args:\n        ind: Either a single int or list of int indices.\n\n    Returns:\n        the instance at that index of the track.instances.\n    \"\"\"\n    if isinstance(ind, int):\n        return self.instances[ind]\n    elif isinstance(ind, list):\n        return [self.instances[i] for i in ind]\n    else:\n        raise ValueError(f\"Ind must be an int or list of ints, found {type(ind)}\")\n</code></pre>"},{"location":"io/track/#dreem.io.Track.__len__","title":"<code>__len__()</code>","text":"<p>Get the length of the track.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of instances/frames in the track.</p> Source code in <code>dreem/io/track.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the length of the track.\n\n    Returns:\n        The number of instances/frames in the track.\n    \"\"\"\n    return len(self.instances)\n</code></pre>"},{"location":"io/track/#dreem.io.Track.__repr__","title":"<code>__repr__()</code>","text":"<p>Get the string representation of the track.</p> <p>Returns:</p> Type Description <code>str</code> <p>the string representation of the Track.</p> Source code in <code>dreem/io/track.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get the string representation of the track.\n\n    Returns:\n        the string representation of the Track.\n    \"\"\"\n    return f\"Track(id={self.id}, len={len(self)})\"\n</code></pre>"},{"location":"io/visualize/","title":"<code>visualize</code>","text":""},{"location":"io/visualize/#dreem.io.visualize","title":"<code>dreem.io.visualize</code>","text":"<p>Helper functions for visualizing tracking.</p> <p>Functions:</p> Name Description <code>annotate_video</code> <p>Annotate video frames with labels.</p> <code>bold</code> <p>Bold value if it is over a threshold.</p> <code>color</code> <p>Highlight value in dataframe if it is over a threshold.</p> <code>fill_missing</code> <p>Fill missing values independently along each dimension after the first.</p> <code>main</code> <p>Take in a path to a video + labels file, annotates a video and saves it to the specified path.</p> <code>save_vid</code> <p>Save video to file.</p>"},{"location":"io/visualize/#dreem.io.visualize.annotate_video","title":"<code>annotate_video(video, labels, key, color_palette=palette, trails=2, boxes=(64, 64), names=True, track_scores=0.5, centroids=4, poses=False, save_path='debug_animal.mp4', fps=30, alpha=0.2)</code>","text":"<p>Annotate video frames with labels.</p> <p>Labels video with bboxes, centroids, trajectory trails, and/or poses.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>Reader</code> <p>The video to be annotated in an ndarray</p> required <code>labels</code> <code>DataFrame</code> <p>The pandas dataframe containing the centroid and/or pose locations of the instances</p> required <code>key</code> <code>str</code> <p>The key where labels are stored in the dataframe - mostly used for choosing whether to annotate based on pred or gt labels</p> required <code>color_palette</code> <code>list | str</code> <p>The matplotlib colorpalette to use for annotating the video. Defaults to <code>tab10</code></p> <code>palette</code> <code>trails</code> <code>int</code> <p>The size of the trajectory trail. If trails size &lt;= 0 or None then it is not added</p> <code>2</code> <code>boxes</code> <code>int</code> <p>The size of the bbox. If bbox size &lt;= 0 or None then it is not added</p> <code>(64, 64)</code> <code>names</code> <code>bool</code> <p>Whether or not to annotate with name</p> <code>True</code> <code>centroids</code> <code>int</code> <p>The size of the centroid. If centroid size &lt;= 0 or None then it is not added</p> <code>4</code> <code>poses</code> <code>bool</code> <p>Whether or not to annotate with poses</p> <code>False</code> <code>fps</code> <code>int</code> <p>The frame rate of the generated video</p> <code>30</code> <code>alpha</code> <code>float</code> <p>The opacity of the annotations.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of annotated video frames</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def annotate_video(\n    video: \"imageio.core.format.Reader\",\n    labels: pd.DataFrame,\n    key: str,\n    color_palette: list | str = palette,\n    trails: int = 2,\n    boxes: int = (64, 64),\n    names: bool = True,\n    track_scores=0.5,\n    centroids: int = 4,\n    poses: bool = False,\n    save_path: str = \"debug_animal.mp4\",\n    fps: int = 30,\n    alpha: float = 0.2,\n) -&gt; list:\n    \"\"\"Annotate video frames with labels.\n\n    Labels video with bboxes, centroids, trajectory trails, and/or poses.\n\n    Args:\n        video: The video to be annotated in an ndarray\n        labels: The pandas dataframe containing the centroid and/or pose locations of the instances\n        key: The key where labels are stored in the dataframe - mostly used for choosing whether to annotate based on pred or gt labels\n        color_palette: The matplotlib colorpalette to use for annotating the video. Defaults to `tab10`\n        trails: The size of the trajectory trail. If trails size &lt;= 0 or None then it is not added\n        boxes: The size of the bbox. If bbox size &lt;= 0 or None then it is not added\n        names: Whether or not to annotate with name\n        centroids: The size of the centroid. If centroid size &lt;= 0 or None then it is not added\n        poses: Whether or not to annotate with poses\n        fps: The frame rate of the generated video\n        alpha: The opacity of the annotations.\n\n    Returns:\n        A list of annotated video frames\n    \"\"\"\n    writer = imageio.get_writer(save_path, fps=fps)\n    color_palette = (\n        sns.color_palette(color_palette)\n        if isinstance(color_palette, str)\n        else deepcopy(color_palette)\n    )\n\n    if trails:\n        track_trails = {}\n    try:\n        for i in tqdm(sorted(labels[\"Frame\"].unique()), desc=\"Frame\", unit=\"Frame\"):\n            frame = video.get_data(i)\n            if frame.shape[0] == 1 or frame.shape[-1] == 1:\n                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n            # else:\n            #     frame = frame.copy()\n\n            lf = labels[labels[\"Frame\"] == i]\n            for idx, instance in lf.iterrows():\n                if not trails:\n                    track_trails = {}\n\n                if poses:\n                    # TODO figure out best way to store poses (maybe pass a slp labels file too?)\n                    trails = False\n                    centroids = False\n                    for idx, (pose, edge) in enumerate(\n                        zip(instance[\"poses\"], instance[\"edges\"])\n                    ):\n                        pose = fill_missing(pose.numpy())\n\n                        pred_track_id = instance[key][idx].numpy().tolist()\n\n                        # Add midpt to track trail.\n                        if pred_track_id not in list(track_trails.keys()):\n                            track_trails[pred_track_id] = []\n\n                        # Select a color based on track_id.\n                        track_color_idx = pred_track_id % len(color_palette)\n                        track_color = (\n                            (np.array(color_palette[track_color_idx]) * 255)\n                            .astype(np.uint8)\n                            .tolist()[::-1]\n                        )\n\n                        for p in pose:\n                            # try:\n                            #    p = tuple([int(i) for i in p.numpy()][::-1])\n                            # except:\n                            #    continue\n\n                            p = tuple(int(i) for i in p)[::-1]\n\n                            track_trails[pred_track_id].append(p)\n\n                            frame = cv2.circle(\n                                frame, p, radius=2, color=track_color, thickness=-1\n                            )\n\n                        for e in edge:\n                            source = tuple(int(i) for i in pose[int(e[0])])[::-1]\n                            target = tuple(int(i) for i in pose[int(e[1])])[::-1]\n\n                            frame = cv2.line(frame, source, target, track_color, 1)\n\n                if (boxes) or centroids:\n                    # Get coordinates for detected objects in the current frame.\n                    if isinstance(boxes, int):\n                        boxes = (boxes, boxes)\n\n                    box_w, box_h = boxes\n                    x = instance[\"X\"]\n                    y = instance[\"Y\"]\n                    min_x, min_y, max_x, max_y = (\n                        int(x - box_w / 2),\n                        int(y - box_h / 2),\n                        int(x + box_w / 2),\n                        int(y + box_h / 2),\n                    )\n                    midpt = (int(x), int(y))\n\n                    pred_track_id = instance[key]\n\n                    if \"Track_score\" in instance.index:\n                        track_score = instance[\"Track_score\"]\n                    else:\n                        track_scores = 0\n\n                    # Add midpt to track trail.\n                    if pred_track_id not in list(track_trails.keys()):\n                        track_trails[pred_track_id] = []\n                    track_trails[pred_track_id].append(midpt)\n\n                    # Select a color based on track_id.\n                    track_color_idx = int(pred_track_id) % len(color_palette)\n                    track_color = (\n                        (np.array(color_palette[track_color_idx]) * 255)\n                        .astype(np.uint8)\n                        .tolist()[::-1]\n                    )\n\n                    # Bbox.\n                    if boxes is not None:\n                        frame = cv2.rectangle(\n                            frame,\n                            (min_x, min_y),\n                            (max_x, max_y),\n                            color=track_color,\n                            thickness=2,\n                        )\n\n                    # Track trail.\n                    if centroids:\n                        frame = cv2.circle(\n                            frame,\n                            midpt,\n                            radius=centroids,\n                            color=track_color,\n                            thickness=-1,\n                        )\n                        for i in range(0, len(track_trails[pred_track_id]) - 1):\n                            frame = cv2.addWeighted(\n                                cv2.circle(\n                                    frame,  # .copy(),\n                                    track_trails[pred_track_id][i],\n                                    radius=4,\n                                    color=track_color,\n                                    thickness=-1,\n                                ),\n                                alpha,\n                                frame,\n                                1 - alpha,\n                                0,\n                            )\n                            if trails:\n                                frame = cv2.line(\n                                    frame,\n                                    track_trails[pred_track_id][i],\n                                    track_trails[pred_track_id][i + 1],\n                                    color=track_color,\n                                    thickness=trails,\n                                )\n\n                # Track name.\n                name_str = \"\"\n\n                if names:\n                    name_str += f\"track_{pred_track_id}\"\n                if names and track_scores:\n                    name_str += \" | \"\n                if track_scores:\n                    name_str += f\"score: {track_score:0.3f}\"\n\n                if len(name_str) &gt; 0:\n                    frame = cv2.putText(\n                        frame,\n                        # f\"idx:{idx} | track_{pred_track_id}\",\n                        name_str,\n                        org=(int(min_x), max(0, int(min_y) - 10)),\n                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n                        fontScale=0.9,\n                        color=track_color,\n                        thickness=2,\n                    )\n            writer.append_data(frame)\n            # if i % fps == 0:\n            #     gc.collect()\n\n    except Exception as e:\n        writer.close()\n        logger.exception(e)\n        return False\n\n    writer.close()\n    return True\n</code></pre>"},{"location":"io/visualize/#dreem.io.visualize.bold","title":"<code>bold(val, thresh=0.01)</code>","text":"<p>Bold value if it is over a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>float</code> <p>The value to bold or not</p> required <code>thresh</code> <code>float</code> <p>The threshold the value has to exceed to be bolded</p> <code>0.01</code> <p>Returns:</p> Type Description <code>str</code> <p>A string indicating how to bold the item.</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def bold(val: float, thresh: float = 0.01) -&gt; str:\n    \"\"\"Bold value if it is over a threshold.\n\n    Args:\n        val: The value to bold or not\n        thresh: The threshold the value has to exceed to be bolded\n\n    Returns:\n        A string indicating how to bold the item.\n    \"\"\"\n    bold = \"bold\" if float(val) &gt; thresh else \"\"\n    return f\"font-weight: {bold}\"\n</code></pre>"},{"location":"io/visualize/#dreem.io.visualize.color","title":"<code>color(val, thresh=0.01)</code>","text":"<p>Highlight value in dataframe if it is over a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>float</code> <p>The value to color</p> required <code>thresh</code> <code>float</code> <p>The threshold for which to color</p> <code>0.01</code> <p>Returns:</p> Type Description <code>str</code> <p>A string containing how to highlight the value</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def color(val: float, thresh: float = 0.01) -&gt; str:\n    \"\"\"Highlight value in dataframe if it is over a threshold.\n\n    Args:\n        val: The value to color\n        thresh: The threshold for which to color\n\n    Returns:\n        A string containing how to highlight the value\n    \"\"\"\n    color = \"lightblue\" if float(val) &gt; thresh else \"\"\n    return f\"background-color: {color}\"\n</code></pre>"},{"location":"io/visualize/#dreem.io.visualize.fill_missing","title":"<code>fill_missing(data, kind='linear')</code>","text":"<p>Fill missing values independently along each dimension after the first.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>the array for which to fill missing value</p> required <code>kind</code> <code>str</code> <p>How to interpolate missing values using <code>scipy.interpolate.interp1d</code></p> <code>'linear'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The array with missing values filled in</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def fill_missing(data: np.ndarray, kind: str = \"linear\") -&gt; np.ndarray:\n    \"\"\"Fill missing values independently along each dimension after the first.\n\n    Args:\n        data: the array for which to fill missing value\n        kind: How to interpolate missing values using `scipy.interpolate.interp1d`\n\n    Returns:\n        The array with missing values filled in\n    \"\"\"\n    # Store initial shape.\n    initial_shape = data.shape\n\n    # Flatten after first dim.\n    data = data.reshape((initial_shape[0], -1))\n\n    # Interpolate along each slice.\n    for i in range(data.shape[-1]):\n        y = data[:, i]\n\n        # Build interpolant.\n        x = np.flatnonzero(~np.isnan(y))\n        f = interp1d(x, y[x], kind=kind, fill_value=np.nan, bounds_error=False)\n\n        # Fill missing\n        xq = np.flatnonzero(np.isnan(y))\n        y[xq] = f(xq)\n\n        # Fill leading or trailing NaNs with the nearest non-NaN values\n        mask = np.isnan(y)\n        y[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), y[~mask])\n\n        # Save slice\n        data[:, i] = y\n\n    # Restore to initial shape.\n    data = data.reshape(initial_shape)\n\n    return data\n</code></pre>"},{"location":"io/visualize/#dreem.io.visualize.main","title":"<code>main(cfg)</code>","text":"<p>Take in a path to a video + labels file, annotates a video and saves it to the specified path.</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>@hydra.main(config_path=None, config_name=None, version_base=None)\ndef main(cfg: DictConfig):\n    \"\"\"Take in a path to a video + labels file, annotates a video and saves it to the specified path.\"\"\"\n    labels = pd.read_csv(cfg.labels_path)\n    video = imageio.get_reader(cfg.vid_path, \"ffmpeg\")\n    frames_annotated = annotate_video(\n        video, labels, save_path=cfg.save_path, **cfg.annotate\n    )\n\n    if frames_annotated:\n        logger.info(\"Video saved to {cfg.save_path}!\")\n    else:\n        logger.error(\"Failed to annotate video!\")\n</code></pre>"},{"location":"io/visualize/#dreem.io.visualize.save_vid","title":"<code>save_vid(annotated_frames, save_path='debug_animal', fps=30)</code>","text":"<p>Save video to file.</p> <p>Parameters:</p> Name Type Description Default <code>annotated_frames</code> <code>list</code> <p>a list of frames annotated by <code>annotate_frames</code></p> required <code>save_path</code> <code>str</code> <p>The path of the annotated file.</p> <code>'debug_animal'</code> <code>fps</code> <code>int</code> <p>The frame rate in frames per second of the annotated video</p> <code>30</code> Source code in <code>dreem/io/visualize.py</code> <pre><code>def save_vid(\n    annotated_frames: list,\n    save_path: str = \"debug_animal\",\n    fps: int = 30,\n):\n    \"\"\"Save video to file.\n\n    Args:\n        annotated_frames: a list of frames annotated by `annotate_frames`\n        save_path: The path of the annotated file.\n        fps: The frame rate in frames per second of the annotated video\n    \"\"\"\n    for idx, (ds_name, data) in enumerate([(save_path, annotated_frames)]):\n        imageio.mimwrite(f\"{ds_name}.mp4\", data, fps=fps, macro_block_size=1)\n</code></pre>"},{"location":"models/","title":"DREEM Models","text":""},{"location":"models/#user-facing-models","title":"User-facing models","text":"<p>There are two main model APIs users should interact with.</p> <ol> <li><code>GlobalTrackingTransformer</code> is the underlying model architecture we use for tracking. It is made up of a <code>VisualEncoder</code> and a <code>Transformer</code> <code>Encoder-Decoder</code>. Only more advanced users who have familiarity with python and pytorch should interact with this model. For others see below</li> <li><code>GTRRunner</code> is a <code>pytorch_lightning</code> around the <code>GlobalTrackingTransformer</code>. It implements the basic routines you need for training, validation and testing. Most users will interact with this model.</li> </ol>"},{"location":"models/#model-parts","title":"Model Parts","text":"<p>For advanced users who are interested in extending our model, we have modularized each component so that its easy to compose into your own custom model. The model parts are</p> <ol> <li><code>VisualEncoder</code>: A CNN backbone used for feature extraction.</li> <li><code>Transformer</code> which is composed of a:<ul> <li>SpatioTemporal <code>Embedding</code> which computes the spatial and temporal embedding of each detection.</li> <li><code>TransformerEncoder</code>: A stack of <code>TransformerEncoderLayer</code>s</li> <li><code>TransformerDecoder</code>: A stack of <code>TransformerDecoderLayer</code>s</li> </ul> </li> <li>An <code>AttentionHead</code> which computes the association matrix from the transformer output.</li> </ol>"},{"location":"models/global_tracking_transformer/","title":"<code>GlobalTrackingTransformer</code>","text":""},{"location":"models/global_tracking_transformer/#dreem.models.GlobalTrackingTransformer","title":"<code>dreem.models.GlobalTrackingTransformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Modular GTR model composed of visual encoder + transformer used for tracking.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize GTR.</p> <code>extract_features</code> <p>Extract features from instances using visual encoder backbone.</p> <code>forward</code> <p>Execute forward pass of GTR Model to get asso matrix.</p> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>class GlobalTrackingTransformer(torch.nn.Module):\n    \"\"\"Modular GTR model composed of visual encoder + transformer used for tracking.\"\"\"\n\n    def __init__(\n        self,\n        encoder_cfg: dict | None = None,\n        d_model: int = 1024,\n        nhead: int = 8,\n        num_encoder_layers: int = 6,\n        num_decoder_layers: int = 6,\n        dropout: int = 0.1,\n        activation: str = \"relu\",\n        return_intermediate_dec: bool = False,\n        norm: bool = False,\n        num_layers_attn_head: int = 2,\n        dropout_attn_head: int = 0.1,\n        embedding_meta: dict | None = None,\n        return_embedding: bool = False,\n        decoder_self_attn: bool = False,\n    ):\n        \"\"\"Initialize GTR.\n\n        Args:\n            encoder_cfg: Dictionary of arguments to pass to the CNN constructor,\n                e.g: `cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}`\n            d_model: The number of features in the encoder/decoder inputs.\n            nhead: The number of heads in the transformer encoder/decoder.\n            num_encoder_layers: The number of encoder-layers in the encoder.\n            num_decoder_layers: The number of decoder-layers in the decoder.\n            dropout: Dropout value applied to the output of transformer layers.\n            activation: Activation function to use.\n            return_intermediate_dec: Return intermediate layers from decoder.\n            norm: If True, normalize output of encoder and decoder.\n            num_layers_attn_head: The number of layers in the attention head.\n            dropout_attn_head: Dropout value for the attention_head.\n            embedding_meta: Metadata for positional embeddings. See below.\n            return_embedding: Whether to return the positional embeddings\n            decoder_self_attn: If True, use decoder self attention.\n\n                More details on `embedding_meta`:\n                    By default this will be an empty dict and indicate\n                    that no positional embeddings should be used. To use the positional embeddings\n                    pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                    `{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},\n                    \"temp\": {'mode': 'learned', 'emb_num': 16}}`. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                    and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n        \"\"\"\n        super().__init__()\n\n        if not encoder_cfg:\n            encoder_cfg = {}\n        self.visual_encoder = create_visual_encoder(d_model=d_model, **encoder_cfg)\n\n        self.transformer = Transformer(\n            d_model=d_model,\n            nhead=nhead,\n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers,\n            dropout=dropout,\n            activation=activation,\n            return_intermediate_dec=return_intermediate_dec,\n            norm=norm,\n            num_layers_attn_head=num_layers_attn_head,\n            dropout_attn_head=dropout_attn_head,\n            embedding_meta=embedding_meta,\n            return_embedding=return_embedding,\n            decoder_self_attn=decoder_self_attn,\n            encoder_cfg=encoder_cfg,\n        )\n\n    def forward(\n        self, ref_instances: list[\"Instance\"], query_instances: list[\"Instance\"] = None\n    ) -&gt; list[\"AssociationMatrix\"]:\n        \"\"\"Execute forward pass of GTR Model to get asso matrix.\n\n        Args:\n            ref_instances: List of instances from chunk containing crops of objects + gt label info\n            query_instances: list of instances used as query in decoder.\n\n        Returns:\n            An N_T x N association matrix\n        \"\"\"\n        # Extract feature representations with pre-trained encoder.\n        self.extract_features(ref_instances)\n\n        if query_instances:\n            self.extract_features(query_instances)\n\n        asso_preds = self.transformer(ref_instances, query_instances)\n\n        return asso_preds\n\n    def extract_features(\n        self, instances: list[\"Instance\"], force_recompute: bool = False\n    ) -&gt; None:\n        \"\"\"Extract features from instances using visual encoder backbone.\n\n        Args:\n            instances: A list of instances to compute features for\n            force_recompute: indicate whether to compute features for all instances regardless of if they have instances\n        \"\"\"\n        if not force_recompute:\n            instances_to_compute = [\n                instance\n                for instance in instances\n                if instance.has_crop() and not instance.has_features()\n            ]\n        else:\n            instances_to_compute = instances\n\n        if len(instances_to_compute) == 0:\n            return\n        elif len(instances_to_compute) == 1:  # handle batch norm error when B=1\n            instances_to_compute = instances\n\n        crops = torch.concatenate([instance.crop for instance in instances_to_compute])\n\n        features = self.visual_encoder(crops)\n        features = features.to(device=instances_to_compute[0].device)\n\n        for i, z_i in enumerate(features):\n            instances_to_compute[i].features = z_i\n</code></pre>"},{"location":"models/global_tracking_transformer/#dreem.models.GlobalTrackingTransformer.__init__","title":"<code>__init__(encoder_cfg=None, d_model=1024, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dropout=0.1, activation='relu', return_intermediate_dec=False, norm=False, num_layers_attn_head=2, dropout_attn_head=0.1, embedding_meta=None, return_embedding=False, decoder_self_attn=False)</code>","text":"<p>Initialize GTR.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_cfg</code> <code>dict | None</code> <p>Dictionary of arguments to pass to the CNN constructor, e.g: <code>cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}</code></p> <code>None</code> <code>d_model</code> <code>int</code> <p>The number of features in the encoder/decoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads in the transformer encoder/decoder.</p> <code>8</code> <code>num_encoder_layers</code> <code>int</code> <p>The number of encoder-layers in the encoder.</p> <code>6</code> <code>num_decoder_layers</code> <code>int</code> <p>The number of decoder-layers in the decoder.</p> <code>6</code> <code>dropout</code> <code>int</code> <p>Dropout value applied to the output of transformer layers.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>return_intermediate_dec</code> <code>bool</code> <p>Return intermediate layers from decoder.</p> <code>False</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of encoder and decoder.</p> <code>False</code> <code>num_layers_attn_head</code> <code>int</code> <p>The number of layers in the attention head.</p> <code>2</code> <code>dropout_attn_head</code> <code>int</code> <p>Dropout value for the attention_head.</p> <code>0.1</code> <code>embedding_meta</code> <code>dict | None</code> <p>Metadata for positional embeddings. See below.</p> <code>None</code> <code>return_embedding</code> <code>bool</code> <p>Whether to return the positional embeddings</p> <code>False</code> <code>decoder_self_attn</code> <code>bool</code> <p>If True, use decoder self attention.</p> <p>More details on <code>embedding_meta</code>:     By default this will be an empty dict and indicate     that no positional embeddings should be used. To use the positional embeddings     pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:     <code>{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},     \"temp\": {'mode': 'learned', 'emb_num': 16}}</code>. (see <code>dreem.models.embeddings.Embedding.EMB_TYPES</code>     and <code>dreem.models.embeddings.Embedding.EMB_MODES</code> for embedding parameters).</p> <code>False</code> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def __init__(\n    self,\n    encoder_cfg: dict | None = None,\n    d_model: int = 1024,\n    nhead: int = 8,\n    num_encoder_layers: int = 6,\n    num_decoder_layers: int = 6,\n    dropout: int = 0.1,\n    activation: str = \"relu\",\n    return_intermediate_dec: bool = False,\n    norm: bool = False,\n    num_layers_attn_head: int = 2,\n    dropout_attn_head: int = 0.1,\n    embedding_meta: dict | None = None,\n    return_embedding: bool = False,\n    decoder_self_attn: bool = False,\n):\n    \"\"\"Initialize GTR.\n\n    Args:\n        encoder_cfg: Dictionary of arguments to pass to the CNN constructor,\n            e.g: `cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}`\n        d_model: The number of features in the encoder/decoder inputs.\n        nhead: The number of heads in the transformer encoder/decoder.\n        num_encoder_layers: The number of encoder-layers in the encoder.\n        num_decoder_layers: The number of decoder-layers in the decoder.\n        dropout: Dropout value applied to the output of transformer layers.\n        activation: Activation function to use.\n        return_intermediate_dec: Return intermediate layers from decoder.\n        norm: If True, normalize output of encoder and decoder.\n        num_layers_attn_head: The number of layers in the attention head.\n        dropout_attn_head: Dropout value for the attention_head.\n        embedding_meta: Metadata for positional embeddings. See below.\n        return_embedding: Whether to return the positional embeddings\n        decoder_self_attn: If True, use decoder self attention.\n\n            More details on `embedding_meta`:\n                By default this will be an empty dict and indicate\n                that no positional embeddings should be used. To use the positional embeddings\n                pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                `{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},\n                \"temp\": {'mode': 'learned', 'emb_num': 16}}`. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n    \"\"\"\n    super().__init__()\n\n    if not encoder_cfg:\n        encoder_cfg = {}\n    self.visual_encoder = create_visual_encoder(d_model=d_model, **encoder_cfg)\n\n    self.transformer = Transformer(\n        d_model=d_model,\n        nhead=nhead,\n        num_encoder_layers=num_encoder_layers,\n        num_decoder_layers=num_decoder_layers,\n        dropout=dropout,\n        activation=activation,\n        return_intermediate_dec=return_intermediate_dec,\n        norm=norm,\n        num_layers_attn_head=num_layers_attn_head,\n        dropout_attn_head=dropout_attn_head,\n        embedding_meta=embedding_meta,\n        return_embedding=return_embedding,\n        decoder_self_attn=decoder_self_attn,\n        encoder_cfg=encoder_cfg,\n    )\n</code></pre>"},{"location":"models/global_tracking_transformer/#dreem.models.GlobalTrackingTransformer.extract_features","title":"<code>extract_features(instances, force_recompute=False)</code>","text":"<p>Extract features from instances using visual encoder backbone.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[Instance]</code> <p>A list of instances to compute features for</p> required <code>force_recompute</code> <code>bool</code> <p>indicate whether to compute features for all instances regardless of if they have instances</p> <code>False</code> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def extract_features(\n    self, instances: list[\"Instance\"], force_recompute: bool = False\n) -&gt; None:\n    \"\"\"Extract features from instances using visual encoder backbone.\n\n    Args:\n        instances: A list of instances to compute features for\n        force_recompute: indicate whether to compute features for all instances regardless of if they have instances\n    \"\"\"\n    if not force_recompute:\n        instances_to_compute = [\n            instance\n            for instance in instances\n            if instance.has_crop() and not instance.has_features()\n        ]\n    else:\n        instances_to_compute = instances\n\n    if len(instances_to_compute) == 0:\n        return\n    elif len(instances_to_compute) == 1:  # handle batch norm error when B=1\n        instances_to_compute = instances\n\n    crops = torch.concatenate([instance.crop for instance in instances_to_compute])\n\n    features = self.visual_encoder(crops)\n    features = features.to(device=instances_to_compute[0].device)\n\n    for i, z_i in enumerate(features):\n        instances_to_compute[i].features = z_i\n</code></pre>"},{"location":"models/global_tracking_transformer/#dreem.models.GlobalTrackingTransformer.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute forward pass of GTR Model to get asso matrix.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>List of instances from chunk containing crops of objects + gt label info</p> required <code>query_instances</code> <code>list[Instance]</code> <p>list of instances used as query in decoder.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[AssociationMatrix]</code> <p>An N_T x N association matrix</p> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def forward(\n    self, ref_instances: list[\"Instance\"], query_instances: list[\"Instance\"] = None\n) -&gt; list[\"AssociationMatrix\"]:\n    \"\"\"Execute forward pass of GTR Model to get asso matrix.\n\n    Args:\n        ref_instances: List of instances from chunk containing crops of objects + gt label info\n        query_instances: list of instances used as query in decoder.\n\n    Returns:\n        An N_T x N association matrix\n    \"\"\"\n    # Extract feature representations with pre-trained encoder.\n    self.extract_features(ref_instances)\n\n    if query_instances:\n        self.extract_features(query_instances)\n\n    asso_preds = self.transformer(ref_instances, query_instances)\n\n    return asso_preds\n</code></pre>"},{"location":"models/gtr_runner/","title":"<code>GTRRunner</code>","text":""},{"location":"models/gtr_runner/#dreem.models.GTRRunner","title":"<code>dreem.models.GTRRunner</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A lightning wrapper around GTR model.</p> <p>Used for training, validation and inference.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize a lightning module for GTR.</p> <code>configure_optimizers</code> <p>Get optimizers and schedulers for training.</p> <code>forward</code> <p>Execute forward pass of the lightning module.</p> <code>log_metrics</code> <p>Log metrics computed during evaluation.</p> <code>on_test_end</code> <p>Run inference and metrics pipeline to compute metrics for test set.</p> <code>on_validation_epoch_end</code> <p>Execute hook for validation end.</p> <code>predict_step</code> <p>Run inference for model.</p> <code>test_step</code> <p>Execute single test step for model.</p> <code>training_step</code> <p>Execute single training step for model.</p> <code>validation_step</code> <p>Execute single val step for model.</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>class GTRRunner(LightningModule):\n    \"\"\"A lightning wrapper around GTR model.\n\n    Used for training, validation and inference.\n    \"\"\"\n\n    DEFAULT_METRICS = {\n        \"train\": [],\n        \"val\": [],\n        \"test\": [\"num_switches\", \"global_tracking_accuracy\"],\n    }\n    DEFAULT_TRACKING = {\n        \"train\": False,\n        \"val\": False,\n        \"test\": True,\n    }\n    DEFAULT_SAVE = {\"train\": False, \"val\": False, \"test\": False}\n\n    def __init__(\n        self,\n        model_cfg: dict | None = None,\n        tracker_cfg: dict | None = None,\n        loss_cfg: dict | None = None,\n        optimizer_cfg: dict | None = None,\n        scheduler_cfg: dict | None = None,\n        metrics: dict[str, list[str]] | None = None,\n        persistent_tracking: dict[str, bool] | None = None,\n        test_save_path: str = \"./test_results.h5\",\n    ):\n        \"\"\"Initialize a lightning module for GTR.\n\n        Args:\n            model_cfg: hyperparameters for GlobalTrackingTransformer\n            tracker_cfg: The parameters used for the tracker post-processing\n            loss_cfg: hyperparameters for AssoLoss\n            optimizer_cfg: hyper parameters used for optimizer.\n                       Only used to overwrite `configure_optimizer`\n            scheduler_cfg: hyperparameters for lr_scheduler used to overwrite `configure_optimizer\n            metrics: a dict containing the metrics to be computed during train, val, and test.\n            persistent_tracking: a dict containing whether to use persistent tracking during train, val and test inference.\n            test_save_path: path to a directory to save the eval and tracking results to\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.model_cfg = model_cfg if model_cfg else {}\n        self.loss_cfg = loss_cfg if loss_cfg else {}\n        self.tracker_cfg = tracker_cfg if tracker_cfg else {}\n\n        self.model = GlobalTrackingTransformer(**self.model_cfg)\n        self.loss = AssoLoss(**self.loss_cfg)\n        if self.tracker_cfg.get(\"tracker_type\", \"standard\") == \"batch\":\n            from dreem.inference.batch_tracker import BatchTracker\n\n            self.tracker = BatchTracker(**self.tracker_cfg)\n        else:\n            from dreem.inference.tracker import Tracker\n\n            self.tracker = Tracker(**self.tracker_cfg)\n        self.optimizer_cfg = optimizer_cfg\n        self.scheduler_cfg = scheduler_cfg\n\n        self.metrics = metrics if metrics is not None else self.DEFAULT_METRICS\n        self.persistent_tracking = (\n            persistent_tracking\n            if persistent_tracking is not None\n            else self.DEFAULT_TRACKING\n        )\n        self.test_results = {\"preds\": [], \"save_path\": test_save_path}\n\n    def forward(\n        self,\n        ref_instances: list[\"dreem.io.Instance\"],\n        query_instances: list[\"dreem.io.Instance\"] | None = None,\n    ) -&gt; list[\"AssociationMatrix\"]:\n        \"\"\"Execute forward pass of the lightning module.\n\n        Args:\n            ref_instances: a list of `Instance` objects containing crops and other data needed for transformer model\n            query_instances: a list of `Instance` objects used as queries in the decoder. Mostly used for inference.\n\n        Returns:\n            An association matrix between objects\n        \"\"\"\n        asso_preds = self.model(ref_instances, query_instances)\n        return asso_preds\n\n    def training_step(\n        self, train_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single training step for model.\n\n        Args:\n            train_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the train loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(train_batch[0], mode=\"train\")\n        self.log_metrics(result, len(train_batch[0]), \"train\")\n\n        return result\n\n    def validation_step(\n        self, val_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single val step for model.\n\n        Args:\n            val_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the val loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(val_batch[0], mode=\"val\")\n        self.log_metrics(result, len(val_batch[0]), \"val\")\n\n        return result\n\n    def test_step(\n        self, test_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single test step for model.\n\n        Args:\n            test_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the val loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(test_batch[0], mode=\"test\")\n        self.log_metrics(result, len(test_batch[0]), \"test\")\n\n        return result\n\n    def predict_step(\n        self, batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; list[\"dreem.io.Frame\"]:\n        \"\"\"Run inference for model.\n\n        Computes association + assignment.\n\n        Args:\n            batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A list of dicts where each dict is a frame containing the predicted track ids\n        \"\"\"\n        frames_pred = self.tracker(self.model, batch[0])\n        return frames_pred\n\n    def _shared_eval_step(\n        self, frames: list[\"dreem.io.Frame\"], mode: str\n    ) -&gt; dict[str, float]:\n        \"\"\"Run evaluation used by train, test, and val steps.\n\n        Args:\n            frames: A list of dicts where each dict is a frame containing gt data\n            mode: which metrics to compute and whether to use persistent tracking or not\n\n        Returns:\n            a dict containing the loss and any other metrics specified by `eval_metrics`\n        \"\"\"\n        try:\n            instances = [instance for frame in frames for instance in frame.instances]\n\n            if len(instances) == 0:\n                return None\n\n            # eval_metrics = self.metrics[mode]  # Currently unused but available for future metric computation\n\n            logits = self(instances)\n            logits = [asso.matrix for asso in logits]\n            loss = self.loss(logits, frames)\n\n            return_metrics = {\"loss\": loss}\n            if mode == \"test\":\n                self.tracker.persistent_tracking = True\n                frames_pred = self.tracker(self.model, frames)\n                self.test_results[\"preds\"].extend(\n                    [frame.to(\"cpu\") for frame in frames_pred]\n                )\n            return_metrics[\"batch_size\"] = len(frames)\n        except Exception as e:\n            logger.exception(\n                f\"Failed on frame {frames[0].frame_id} of video {frames[0].video_id}\"\n            )\n            logger.exception(e)\n            raise (e)\n\n        return return_metrics\n\n    def configure_optimizers(self) -&gt; dict:\n        \"\"\"Get optimizers and schedulers for training.\n\n        Is overridden by config but defaults to Adam + ReduceLROnPlateau.\n\n        Returns:\n            an optimizer config dict containing the optimizer, scheduler, and scheduler params\n        \"\"\"\n        # todo: init from config\n        if self.optimizer_cfg is None:\n            optimizer = torch.optim.Adam(self.parameters(), lr=1e-4, betas=(0.9, 0.999))\n        else:\n            optimizer = init_optimizer(self.parameters(), self.optimizer_cfg)\n\n        if self.scheduler_cfg is None:\n            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer, \"min\", 0.5, 10\n            )\n        else:\n            scheduler = init_scheduler(optimizer, self.scheduler_cfg)\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n                \"interval\": \"epoch\",\n                \"frequency\": 1,\n            },\n        }\n\n    def log_metrics(self, result: dict, batch_size: int, mode: str) -&gt; None:\n        \"\"\"Log metrics computed during evaluation.\n\n        Args:\n            result: A dict containing metrics to be logged.\n            batch_size: the size of the batch used to compute the metrics\n            mode: One of {'train', 'test' or 'val'}. Used as prefix while logging.\n        \"\"\"\n        if result:\n            batch_size = result.pop(\"batch_size\")\n            for metric, val in result.items():\n                if isinstance(val, torch.Tensor):\n                    val = val.item()\n                self.log(f\"{mode}_{metric}\", val, batch_size=batch_size)\n\n    def on_validation_epoch_end(self):\n        \"\"\"Execute hook for validation end.\n\n        Currently, we simply clear the gpu cache and do garbage collection.\n        \"\"\"\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def on_test_end(self):\n        \"\"\"Run inference and metrics pipeline to compute metrics for test set.\n\n        Args:\n            test_results: dict containing predictions and metrics to be filled out in metrics.evaluate\n            metrics: list of metrics to compute\n        \"\"\"\n        # input validation\n        metrics_to_compute = self.metrics[\n            \"test\"\n        ]  # list of metrics to compute, or \"all\"\n        if metrics_to_compute == \"all\":\n            metrics_to_compute = [\"motmetrics\", \"global_tracking_accuracy\"]\n        if isinstance(metrics_to_compute, str):\n            metrics_to_compute = [metrics_to_compute]\n        for metric in metrics_to_compute:\n            if metric not in [\"motmetrics\", \"global_tracking_accuracy\"]:\n                raise ValueError(\n                    f\"Metric {metric} not supported. Please select from 'motmetrics' or 'global_tracking_accuracy'\"\n                )\n\n        preds = self.test_results[\"preds\"]\n\n        # results is a dict with key being the metric name, and value being the metric value computed\n        results = metrics.evaluate(preds, metrics_to_compute)\n\n        # save metrics and frame metadata to hdf5\n\n        # Get the video name from the first frame\n        vid_name = Path(preds[0].vid_name).stem\n        # save the results to an hdf5 file\n        fname = os.path.join(\n            self.test_results[\"save_path\"], f\"{vid_name}.dreem_metrics.h5\"\n        )\n        logger.info(f\"Saving metrics to {fname}\")\n        # Check if the h5 file exists and add a suffix to prevent name collision\n        suffix_counter = 0\n        original_fname = fname\n        while os.path.exists(fname):\n            suffix_counter += 1\n            fname = original_fname.replace(\n                \".dreem_metrics.h5\", f\"_{suffix_counter}.dreem_metrics.h5\"\n            )\n\n        if suffix_counter &gt; 0:\n            logger.info(f\"File already exists. Saving to {fname} instead\")\n\n        with h5py.File(fname, \"a\") as results_file:\n            # Create a group for this video\n            vid_group = results_file.require_group(vid_name)\n            # Save each metric\n            for metric_name, value in results.items():\n                if metric_name == \"motmetrics\":\n                    # For num_switches, save mot_summary and mot_events separately\n                    mot_summary = value[0]\n                    mot_events = value[1]\n                    frame_switch_map = value[2]\n                    mot_summary_group = vid_group.require_group(\"mot_summary\")\n                    # Loop through each row in mot_summary and save as attributes\n                    for _, row in mot_summary.iterrows():\n                        mot_summary_group.attrs[row.name] = row[\"acc\"]\n                    # save extra metadata for frames in which there is a switch\n                    for frame_id, switch in frame_switch_map.items():\n                        frame = preds[frame_id]\n                        frame = frame.to(\"cpu\")\n                        if switch:\n                            _ = frame.to_h5(\n                                vid_group,\n                                frame.get_gt_track_ids().cpu().numpy(),\n                                save={\n                                    \"crop\": True,\n                                    \"features\": True,\n                                    \"embeddings\": True,\n                                },\n                            )\n                        else:\n                            _ = frame.to_h5(\n                                vid_group, frame.get_gt_track_ids().cpu().numpy()\n                            )\n                    # save motevents log to csv\n                    motevents_path = os.path.join(\n                        self.test_results[\"save_path\"], f\"{vid_name}.motevents.csv\"\n                    )\n                    logger.info(f\"Saving motevents log to {motevents_path}\")\n                    mot_events.to_csv(motevents_path, index=False)\n\n                elif metric_name == \"global_tracking_accuracy\":\n                    gta_by_gt_track = value\n                    gta_group = vid_group.require_group(\"global_tracking_accuracy\")\n                    # save as a key value pair with gt track id: gta\n                    for gt_track_id, gta in gta_by_gt_track.items():\n                        gta_group.attrs[f\"track_{gt_track_id}\"] = gta\n\n        # save the tracking results to a slp/labelled masks file\n        if isinstance(self.trainer.test_dataloaders.dataset, CellTrackingDataset):\n            outpath = os.path.join(\n                self.test_results[\"save_path\"],\n                f\"{vid_name}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.tif\",\n            )\n            pred_imgs = []\n            for frame in preds:\n                frame_masks = []\n                for instance in frame.instances:\n                    # centroid = instance.centroid[\"centroid\"]  # Currently unused but available if needed\n                    mask = instance.mask.cpu().numpy()\n                    track_id = instance.pred_track_id.cpu().numpy().item()\n                    mask = mask.astype(np.uint8)\n                    mask[mask != 0] = track_id  # label the mask with the track id\n                    frame_masks.append(mask)\n                frame_mask = np.max(frame_masks, axis=0)\n                pred_imgs.append(frame_mask)\n            pred_imgs = np.stack(pred_imgs)\n            tifffile.imwrite(outpath, pred_imgs.astype(np.uint16))\n        else:\n            outpath = os.path.join(\n                self.test_results[\"save_path\"],\n                f\"{vid_name}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.slp\",\n            )\n            pred_slp = []\n\n            logger.info(f\"Saving inference results to {outpath}\")\n            # save the tracking results to a slp file\n            tracks = {}\n            for frame in preds:\n                if frame.frame_id.item() == 0:\n                    video = (\n                        sio.Video(frame.video)\n                        if isinstance(frame.video, str)\n                        else sio.Video\n                    )\n                lf, tracks = frame.to_slp(tracks, video=video)\n                pred_slp.append(lf)\n            pred_slp = sio.Labels(pred_slp)\n\n            pred_slp.save(outpath)\n\n        # clear the preds\n        self.test_results[\"preds\"] = []\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.__init__","title":"<code>__init__(model_cfg=None, tracker_cfg=None, loss_cfg=None, optimizer_cfg=None, scheduler_cfg=None, metrics=None, persistent_tracking=None, test_save_path='./test_results.h5')</code>","text":"<p>Initialize a lightning module for GTR.</p> <p>Parameters:</p> Name Type Description Default <code>model_cfg</code> <code>dict | None</code> <p>hyperparameters for GlobalTrackingTransformer</p> <code>None</code> <code>tracker_cfg</code> <code>dict | None</code> <p>The parameters used for the tracker post-processing</p> <code>None</code> <code>loss_cfg</code> <code>dict | None</code> <p>hyperparameters for AssoLoss</p> <code>None</code> <code>optimizer_cfg</code> <code>dict | None</code> <p>hyper parameters used for optimizer.        Only used to overwrite <code>configure_optimizer</code></p> <code>None</code> <code>scheduler_cfg</code> <code>dict | None</code> <p>hyperparameters for lr_scheduler used to overwrite `configure_optimizer</p> <code>None</code> <code>metrics</code> <code>dict[str, list[str]] | None</code> <p>a dict containing the metrics to be computed during train, val, and test.</p> <code>None</code> <code>persistent_tracking</code> <code>dict[str, bool] | None</code> <p>a dict containing whether to use persistent tracking during train, val and test inference.</p> <code>None</code> <code>test_save_path</code> <code>str</code> <p>path to a directory to save the eval and tracking results to</p> <code>'./test_results.h5'</code> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def __init__(\n    self,\n    model_cfg: dict | None = None,\n    tracker_cfg: dict | None = None,\n    loss_cfg: dict | None = None,\n    optimizer_cfg: dict | None = None,\n    scheduler_cfg: dict | None = None,\n    metrics: dict[str, list[str]] | None = None,\n    persistent_tracking: dict[str, bool] | None = None,\n    test_save_path: str = \"./test_results.h5\",\n):\n    \"\"\"Initialize a lightning module for GTR.\n\n    Args:\n        model_cfg: hyperparameters for GlobalTrackingTransformer\n        tracker_cfg: The parameters used for the tracker post-processing\n        loss_cfg: hyperparameters for AssoLoss\n        optimizer_cfg: hyper parameters used for optimizer.\n                   Only used to overwrite `configure_optimizer`\n        scheduler_cfg: hyperparameters for lr_scheduler used to overwrite `configure_optimizer\n        metrics: a dict containing the metrics to be computed during train, val, and test.\n        persistent_tracking: a dict containing whether to use persistent tracking during train, val and test inference.\n        test_save_path: path to a directory to save the eval and tracking results to\n    \"\"\"\n    super().__init__()\n    self.save_hyperparameters()\n\n    self.model_cfg = model_cfg if model_cfg else {}\n    self.loss_cfg = loss_cfg if loss_cfg else {}\n    self.tracker_cfg = tracker_cfg if tracker_cfg else {}\n\n    self.model = GlobalTrackingTransformer(**self.model_cfg)\n    self.loss = AssoLoss(**self.loss_cfg)\n    if self.tracker_cfg.get(\"tracker_type\", \"standard\") == \"batch\":\n        from dreem.inference.batch_tracker import BatchTracker\n\n        self.tracker = BatchTracker(**self.tracker_cfg)\n    else:\n        from dreem.inference.tracker import Tracker\n\n        self.tracker = Tracker(**self.tracker_cfg)\n    self.optimizer_cfg = optimizer_cfg\n    self.scheduler_cfg = scheduler_cfg\n\n    self.metrics = metrics if metrics is not None else self.DEFAULT_METRICS\n    self.persistent_tracking = (\n        persistent_tracking\n        if persistent_tracking is not None\n        else self.DEFAULT_TRACKING\n    )\n    self.test_results = {\"preds\": [], \"save_path\": test_save_path}\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Get optimizers and schedulers for training.</p> <p>Is overridden by config but defaults to Adam + ReduceLROnPlateau.</p> <p>Returns:</p> Type Description <code>dict</code> <p>an optimizer config dict containing the optimizer, scheduler, and scheduler params</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def configure_optimizers(self) -&gt; dict:\n    \"\"\"Get optimizers and schedulers for training.\n\n    Is overridden by config but defaults to Adam + ReduceLROnPlateau.\n\n    Returns:\n        an optimizer config dict containing the optimizer, scheduler, and scheduler params\n    \"\"\"\n    # todo: init from config\n    if self.optimizer_cfg is None:\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4, betas=(0.9, 0.999))\n    else:\n        optimizer = init_optimizer(self.parameters(), self.optimizer_cfg)\n\n    if self.scheduler_cfg is None:\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, \"min\", 0.5, 10\n        )\n    else:\n        scheduler = init_scheduler(optimizer, self.scheduler_cfg)\n\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"monitor\": \"val_loss\",\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        },\n    }\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute forward pass of the lightning module.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>a list of <code>Instance</code> objects containing crops and other data needed for transformer model</p> required <code>query_instances</code> <code>list[Instance] | None</code> <p>a list of <code>Instance</code> objects used as queries in the decoder. Mostly used for inference.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[AssociationMatrix]</code> <p>An association matrix between objects</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def forward(\n    self,\n    ref_instances: list[\"dreem.io.Instance\"],\n    query_instances: list[\"dreem.io.Instance\"] | None = None,\n) -&gt; list[\"AssociationMatrix\"]:\n    \"\"\"Execute forward pass of the lightning module.\n\n    Args:\n        ref_instances: a list of `Instance` objects containing crops and other data needed for transformer model\n        query_instances: a list of `Instance` objects used as queries in the decoder. Mostly used for inference.\n\n    Returns:\n        An association matrix between objects\n    \"\"\"\n    asso_preds = self.model(ref_instances, query_instances)\n    return asso_preds\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.log_metrics","title":"<code>log_metrics(result, batch_size, mode)</code>","text":"<p>Log metrics computed during evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>dict</code> <p>A dict containing metrics to be logged.</p> required <code>batch_size</code> <code>int</code> <p>the size of the batch used to compute the metrics</p> required <code>mode</code> <code>str</code> <p>One of {'train', 'test' or 'val'}. Used as prefix while logging.</p> required Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def log_metrics(self, result: dict, batch_size: int, mode: str) -&gt; None:\n    \"\"\"Log metrics computed during evaluation.\n\n    Args:\n        result: A dict containing metrics to be logged.\n        batch_size: the size of the batch used to compute the metrics\n        mode: One of {'train', 'test' or 'val'}. Used as prefix while logging.\n    \"\"\"\n    if result:\n        batch_size = result.pop(\"batch_size\")\n        for metric, val in result.items():\n            if isinstance(val, torch.Tensor):\n                val = val.item()\n            self.log(f\"{mode}_{metric}\", val, batch_size=batch_size)\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.on_test_end","title":"<code>on_test_end()</code>","text":"<p>Run inference and metrics pipeline to compute metrics for test set.</p> <p>Parameters:</p> Name Type Description Default <code>test_results</code> <p>dict containing predictions and metrics to be filled out in metrics.evaluate</p> required <code>metrics</code> <p>list of metrics to compute</p> required Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def on_test_end(self):\n    \"\"\"Run inference and metrics pipeline to compute metrics for test set.\n\n    Args:\n        test_results: dict containing predictions and metrics to be filled out in metrics.evaluate\n        metrics: list of metrics to compute\n    \"\"\"\n    # input validation\n    metrics_to_compute = self.metrics[\n        \"test\"\n    ]  # list of metrics to compute, or \"all\"\n    if metrics_to_compute == \"all\":\n        metrics_to_compute = [\"motmetrics\", \"global_tracking_accuracy\"]\n    if isinstance(metrics_to_compute, str):\n        metrics_to_compute = [metrics_to_compute]\n    for metric in metrics_to_compute:\n        if metric not in [\"motmetrics\", \"global_tracking_accuracy\"]:\n            raise ValueError(\n                f\"Metric {metric} not supported. Please select from 'motmetrics' or 'global_tracking_accuracy'\"\n            )\n\n    preds = self.test_results[\"preds\"]\n\n    # results is a dict with key being the metric name, and value being the metric value computed\n    results = metrics.evaluate(preds, metrics_to_compute)\n\n    # save metrics and frame metadata to hdf5\n\n    # Get the video name from the first frame\n    vid_name = Path(preds[0].vid_name).stem\n    # save the results to an hdf5 file\n    fname = os.path.join(\n        self.test_results[\"save_path\"], f\"{vid_name}.dreem_metrics.h5\"\n    )\n    logger.info(f\"Saving metrics to {fname}\")\n    # Check if the h5 file exists and add a suffix to prevent name collision\n    suffix_counter = 0\n    original_fname = fname\n    while os.path.exists(fname):\n        suffix_counter += 1\n        fname = original_fname.replace(\n            \".dreem_metrics.h5\", f\"_{suffix_counter}.dreem_metrics.h5\"\n        )\n\n    if suffix_counter &gt; 0:\n        logger.info(f\"File already exists. Saving to {fname} instead\")\n\n    with h5py.File(fname, \"a\") as results_file:\n        # Create a group for this video\n        vid_group = results_file.require_group(vid_name)\n        # Save each metric\n        for metric_name, value in results.items():\n            if metric_name == \"motmetrics\":\n                # For num_switches, save mot_summary and mot_events separately\n                mot_summary = value[0]\n                mot_events = value[1]\n                frame_switch_map = value[2]\n                mot_summary_group = vid_group.require_group(\"mot_summary\")\n                # Loop through each row in mot_summary and save as attributes\n                for _, row in mot_summary.iterrows():\n                    mot_summary_group.attrs[row.name] = row[\"acc\"]\n                # save extra metadata for frames in which there is a switch\n                for frame_id, switch in frame_switch_map.items():\n                    frame = preds[frame_id]\n                    frame = frame.to(\"cpu\")\n                    if switch:\n                        _ = frame.to_h5(\n                            vid_group,\n                            frame.get_gt_track_ids().cpu().numpy(),\n                            save={\n                                \"crop\": True,\n                                \"features\": True,\n                                \"embeddings\": True,\n                            },\n                        )\n                    else:\n                        _ = frame.to_h5(\n                            vid_group, frame.get_gt_track_ids().cpu().numpy()\n                        )\n                # save motevents log to csv\n                motevents_path = os.path.join(\n                    self.test_results[\"save_path\"], f\"{vid_name}.motevents.csv\"\n                )\n                logger.info(f\"Saving motevents log to {motevents_path}\")\n                mot_events.to_csv(motevents_path, index=False)\n\n            elif metric_name == \"global_tracking_accuracy\":\n                gta_by_gt_track = value\n                gta_group = vid_group.require_group(\"global_tracking_accuracy\")\n                # save as a key value pair with gt track id: gta\n                for gt_track_id, gta in gta_by_gt_track.items():\n                    gta_group.attrs[f\"track_{gt_track_id}\"] = gta\n\n    # save the tracking results to a slp/labelled masks file\n    if isinstance(self.trainer.test_dataloaders.dataset, CellTrackingDataset):\n        outpath = os.path.join(\n            self.test_results[\"save_path\"],\n            f\"{vid_name}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.tif\",\n        )\n        pred_imgs = []\n        for frame in preds:\n            frame_masks = []\n            for instance in frame.instances:\n                # centroid = instance.centroid[\"centroid\"]  # Currently unused but available if needed\n                mask = instance.mask.cpu().numpy()\n                track_id = instance.pred_track_id.cpu().numpy().item()\n                mask = mask.astype(np.uint8)\n                mask[mask != 0] = track_id  # label the mask with the track id\n                frame_masks.append(mask)\n            frame_mask = np.max(frame_masks, axis=0)\n            pred_imgs.append(frame_mask)\n        pred_imgs = np.stack(pred_imgs)\n        tifffile.imwrite(outpath, pred_imgs.astype(np.uint16))\n    else:\n        outpath = os.path.join(\n            self.test_results[\"save_path\"],\n            f\"{vid_name}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.slp\",\n        )\n        pred_slp = []\n\n        logger.info(f\"Saving inference results to {outpath}\")\n        # save the tracking results to a slp file\n        tracks = {}\n        for frame in preds:\n            if frame.frame_id.item() == 0:\n                video = (\n                    sio.Video(frame.video)\n                    if isinstance(frame.video, str)\n                    else sio.Video\n                )\n            lf, tracks = frame.to_slp(tracks, video=video)\n            pred_slp.append(lf)\n        pred_slp = sio.Labels(pred_slp)\n\n        pred_slp.save(outpath)\n\n    # clear the preds\n    self.test_results[\"preds\"] = []\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.on_validation_epoch_end","title":"<code>on_validation_epoch_end()</code>","text":"<p>Execute hook for validation end.</p> <p>Currently, we simply clear the gpu cache and do garbage collection.</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Execute hook for validation end.\n\n    Currently, we simply clear the gpu cache and do garbage collection.\n    \"\"\"\n    gc.collect()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.predict_step","title":"<code>predict_step(batch, batch_idx)</code>","text":"<p>Run inference for model.</p> <p>Computes association + assignment.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects     with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A list of dicts where each dict is a frame containing the predicted track ids</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def predict_step(\n    self, batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; list[\"dreem.io.Frame\"]:\n    \"\"\"Run inference for model.\n\n    Computes association + assignment.\n\n    Args:\n        batch: A single batch from the dataset which is a list of `Frame` objects\n                with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A list of dicts where each dict is a frame containing the predicted track ids\n    \"\"\"\n    frames_pred = self.tracker(self.model, batch[0])\n    return frames_pred\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.test_step","title":"<code>test_step(test_batch, batch_idx)</code>","text":"<p>Execute single test step for model.</p> <p>Parameters:</p> Name Type Description Default <code>test_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the val loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def test_step(\n    self, test_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single test step for model.\n\n    Args:\n        test_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the val loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(test_batch[0], mode=\"test\")\n    self.log_metrics(result, len(test_batch[0]), \"test\")\n\n    return result\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.training_step","title":"<code>training_step(train_batch, batch_idx)</code>","text":"<p>Execute single training step for model.</p> <p>Parameters:</p> Name Type Description Default <code>train_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the train loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def training_step(\n    self, train_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single training step for model.\n\n    Args:\n        train_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the train loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(train_batch[0], mode=\"train\")\n    self.log_metrics(result, len(train_batch[0]), \"train\")\n\n    return result\n</code></pre>"},{"location":"models/gtr_runner/#dreem.models.GTRRunner.validation_step","title":"<code>validation_step(val_batch, batch_idx)</code>","text":"<p>Execute single val step for model.</p> <p>Parameters:</p> Name Type Description Default <code>val_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the val loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def validation_step(\n    self, val_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single val step for model.\n\n    Args:\n        val_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the val loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(val_batch[0], mode=\"val\")\n    self.log_metrics(result, len(val_batch[0]), \"val\")\n\n    return result\n</code></pre>"},{"location":"models/model_parts/","title":"Model Parts","text":""},{"location":"models/model_parts/#dreem.models.VisualEncoder","title":"<code>dreem.models.VisualEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Class wrapping around a visual feature extractor backbone.</p> <p>Currently CNN only.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize Visual Encoder.</p> <code>encoder_dim</code> <p>Compute dummy forward pass of encoder model and get embedding dimension.</p> <code>forward</code> <p>Forward pass of feature extractor to get feature vector.</p> <code>select_feature_extractor</code> <p>Select the appropriate feature extractor based on config.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>class VisualEncoder(torch.nn.Module):\n    \"\"\"Class wrapping around a visual feature extractor backbone.\n\n    Currently CNN only.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"resnet18\",\n        d_model: int = 512,\n        in_chans: int = 3,\n        backend: int = \"timm\",\n        **kwargs: Any | None,\n    ):\n        \"\"\"Initialize Visual Encoder.\n\n        Args:\n            model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n            d_model (int): Output embedding dimension.\n            in_chans: the number of input channels of the image.\n            backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n            kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n        \"\"\"\n        super().__init__()\n\n        self.model_name = model_name.lower()\n        self.d_model = d_model\n        self.backend = backend\n        if in_chans == 1:\n            self.in_chans = 3\n        else:\n            self.in_chans = in_chans\n\n        self.feature_extractor = self.select_feature_extractor(\n            model_name=self.model_name,\n            in_chans=self.in_chans,\n            backend=self.backend,\n            **kwargs,\n        )\n\n        self.out_layer = torch.nn.Linear(\n            self.encoder_dim(self.feature_extractor), self.d_model\n        )\n\n    def select_feature_extractor(\n        self, model_name: str, in_chans: int, backend: str, **kwargs: Any\n    ) -&gt; torch.nn.Module:\n        \"\"\"Select the appropriate feature extractor based on config.\n\n        Args:\n            model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n            in_chans: the number of input channels of the image.\n            backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n            kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n\n        Returns:\n            a CNN encoder based on the config and backend selected.\n        \"\"\"\n        if \"timm\" in backend.lower():\n            feature_extractor = timm.create_model(\n                model_name=self.model_name,\n                in_chans=self.in_chans,\n                num_classes=0,\n                **kwargs,\n            )\n        elif \"torch\" in backend.lower():\n            if model_name.lower() == \"resnet18\":\n                feature_extractor = torchvision.models.resnet18(**kwargs)\n\n            elif model_name.lower() == \"resnet50\":\n                feature_extractor = torchvision.models.resnet50(**kwargs)\n\n            else:\n                raise ValueError(\n                    f\"Only `[resnet18, resnet50]` are available when backend is {backend}. Found {model_name}\"\n                )\n            feature_extractor = torch.nn.Sequential(\n                *list(feature_extractor.children())[:-1]\n            )\n            input_layer = feature_extractor[0]\n            if in_chans != 3:\n                feature_extractor[0] = torch.nn.Conv2d(\n                    in_channels=in_chans,\n                    out_channels=input_layer.out_channels,\n                    kernel_size=input_layer.kernel_size,\n                    stride=input_layer.stride,\n                    padding=input_layer.padding,\n                    dilation=input_layer.dilation,\n                    groups=input_layer.groups,\n                    bias=input_layer.bias,\n                    padding_mode=input_layer.padding_mode,\n                )\n\n        else:\n            raise ValueError(\n                f\"Only ['timm', 'torch'] backends are available! Found {backend}.\"\n            )\n        return feature_extractor\n\n    def encoder_dim(self, model: torch.nn.Module) -&gt; int:\n        \"\"\"Compute dummy forward pass of encoder model and get embedding dimension.\n\n        Args:\n            model: a vision encoder model.\n\n        Returns:\n            The embedding dimension size.\n        \"\"\"\n        _ = model.eval()\n        dummy_output = model(torch.randn(1, self.in_chans, 224, 224)).squeeze()\n        _ = model.train()  # to be safe\n        return dummy_output.shape[-1]\n\n    def forward(self, img: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of feature extractor to get feature vector.\n\n        Args:\n            img: Input image tensor of shape (B, C, H, W).\n\n        Returns:\n            feats: Normalized output tensor of shape (B, d_model).\n        \"\"\"\n        # If grayscale, tile the image to 3 channels.\n        if img.shape[1] == 1:\n            img = img.repeat([1, 3, 1, 1])  # (B, nc=3, H, W)\n\n        b, c, h, w = img.shape\n\n        if c != self.in_chans:\n            raise ValueError(\n                f\"\"\"Found {c} channels in image but model was configured for {self.in_chans} channels! \\n\n                    Hint: have you set the number of anchors in your dataset &gt; 1? \\n\n                    If so, make sure to set `in_chans=3 * n_anchors`\"\"\"\n            )\n        feats = self.feature_extractor(\n            img\n        )  # (B, out_dim, 1, 1) if using resnet18 backbone.\n\n        # Reshape feature vectors\n        feats = feats.reshape([img.shape[0], -1])  # (B, out_dim)\n        # Map feature vectors to output dimension using linear layer.\n        feats = self.out_layer(feats)  # (B, d_model)\n        # Normalize output feature vectors.\n        feats = F.normalize(feats)  # (B, d_model)\n        return feats\n</code></pre>"},{"location":"models/model_parts/#dreem.models.VisualEncoder.__init__","title":"<code>__init__(model_name='resnet18', d_model=512, in_chans=3, backend='timm', **kwargs)</code>","text":"<p>Initialize Visual Encoder.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").</p> <code>'resnet18'</code> <code>d_model</code> <code>int</code> <p>Output embedding dimension.</p> <code>512</code> <code>in_chans</code> <code>int</code> <p>the number of input channels of the image.</p> <code>3</code> <code>backend</code> <code>int</code> <p>Which model backend to use. One of {\"timm\", \"torchvision\"}</p> <code>'timm'</code> <code>kwargs</code> <code>Any | None</code> <p>see <code>timm.create_model</code> and <code>torchvision.models.resnetX</code> for kwargs.</p> <code>{}</code> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"resnet18\",\n    d_model: int = 512,\n    in_chans: int = 3,\n    backend: int = \"timm\",\n    **kwargs: Any | None,\n):\n    \"\"\"Initialize Visual Encoder.\n\n    Args:\n        model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n        d_model (int): Output embedding dimension.\n        in_chans: the number of input channels of the image.\n        backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n        kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n    \"\"\"\n    super().__init__()\n\n    self.model_name = model_name.lower()\n    self.d_model = d_model\n    self.backend = backend\n    if in_chans == 1:\n        self.in_chans = 3\n    else:\n        self.in_chans = in_chans\n\n    self.feature_extractor = self.select_feature_extractor(\n        model_name=self.model_name,\n        in_chans=self.in_chans,\n        backend=self.backend,\n        **kwargs,\n    )\n\n    self.out_layer = torch.nn.Linear(\n        self.encoder_dim(self.feature_extractor), self.d_model\n    )\n</code></pre>"},{"location":"models/model_parts/#dreem.models.VisualEncoder.encoder_dim","title":"<code>encoder_dim(model)</code>","text":"<p>Compute dummy forward pass of encoder model and get embedding dimension.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>a vision encoder model.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The embedding dimension size.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def encoder_dim(self, model: torch.nn.Module) -&gt; int:\n    \"\"\"Compute dummy forward pass of encoder model and get embedding dimension.\n\n    Args:\n        model: a vision encoder model.\n\n    Returns:\n        The embedding dimension size.\n    \"\"\"\n    _ = model.eval()\n    dummy_output = model(torch.randn(1, self.in_chans, 224, 224)).squeeze()\n    _ = model.train()  # to be safe\n    return dummy_output.shape[-1]\n</code></pre>"},{"location":"models/model_parts/#dreem.models.VisualEncoder.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of feature extractor to get feature vector.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor</code> <p>Input image tensor of shape (B, C, H, W).</p> required <p>Returns:</p> Name Type Description <code>feats</code> <code>Tensor</code> <p>Normalized output tensor of shape (B, d_model).</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def forward(self, img: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of feature extractor to get feature vector.\n\n    Args:\n        img: Input image tensor of shape (B, C, H, W).\n\n    Returns:\n        feats: Normalized output tensor of shape (B, d_model).\n    \"\"\"\n    # If grayscale, tile the image to 3 channels.\n    if img.shape[1] == 1:\n        img = img.repeat([1, 3, 1, 1])  # (B, nc=3, H, W)\n\n    b, c, h, w = img.shape\n\n    if c != self.in_chans:\n        raise ValueError(\n            f\"\"\"Found {c} channels in image but model was configured for {self.in_chans} channels! \\n\n                Hint: have you set the number of anchors in your dataset &gt; 1? \\n\n                If so, make sure to set `in_chans=3 * n_anchors`\"\"\"\n        )\n    feats = self.feature_extractor(\n        img\n    )  # (B, out_dim, 1, 1) if using resnet18 backbone.\n\n    # Reshape feature vectors\n    feats = feats.reshape([img.shape[0], -1])  # (B, out_dim)\n    # Map feature vectors to output dimension using linear layer.\n    feats = self.out_layer(feats)  # (B, d_model)\n    # Normalize output feature vectors.\n    feats = F.normalize(feats)  # (B, d_model)\n    return feats\n</code></pre>"},{"location":"models/model_parts/#dreem.models.VisualEncoder.select_feature_extractor","title":"<code>select_feature_extractor(model_name, in_chans, backend, **kwargs)</code>","text":"<p>Select the appropriate feature extractor based on config.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").</p> required <code>in_chans</code> <code>int</code> <p>the number of input channels of the image.</p> required <code>backend</code> <code>str</code> <p>Which model backend to use. One of {\"timm\", \"torchvision\"}</p> required <code>kwargs</code> <code>Any</code> <p>see <code>timm.create_model</code> and <code>torchvision.models.resnetX</code> for kwargs.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Module</code> <p>a CNN encoder based on the config and backend selected.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def select_feature_extractor(\n    self, model_name: str, in_chans: int, backend: str, **kwargs: Any\n) -&gt; torch.nn.Module:\n    \"\"\"Select the appropriate feature extractor based on config.\n\n    Args:\n        model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n        in_chans: the number of input channels of the image.\n        backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n        kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n\n    Returns:\n        a CNN encoder based on the config and backend selected.\n    \"\"\"\n    if \"timm\" in backend.lower():\n        feature_extractor = timm.create_model(\n            model_name=self.model_name,\n            in_chans=self.in_chans,\n            num_classes=0,\n            **kwargs,\n        )\n    elif \"torch\" in backend.lower():\n        if model_name.lower() == \"resnet18\":\n            feature_extractor = torchvision.models.resnet18(**kwargs)\n\n        elif model_name.lower() == \"resnet50\":\n            feature_extractor = torchvision.models.resnet50(**kwargs)\n\n        else:\n            raise ValueError(\n                f\"Only `[resnet18, resnet50]` are available when backend is {backend}. Found {model_name}\"\n            )\n        feature_extractor = torch.nn.Sequential(\n            *list(feature_extractor.children())[:-1]\n        )\n        input_layer = feature_extractor[0]\n        if in_chans != 3:\n            feature_extractor[0] = torch.nn.Conv2d(\n                in_channels=in_chans,\n                out_channels=input_layer.out_channels,\n                kernel_size=input_layer.kernel_size,\n                stride=input_layer.stride,\n                padding=input_layer.padding,\n                dilation=input_layer.dilation,\n                groups=input_layer.groups,\n                bias=input_layer.bias,\n                padding_mode=input_layer.padding_mode,\n            )\n\n    else:\n        raise ValueError(\n            f\"Only ['timm', 'torch'] backends are available! Found {backend}.\"\n        )\n    return feature_extractor\n</code></pre>"},{"location":"models/model_parts/#dreem.models.Transformer","title":"<code>dreem.models.Transformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer class.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize Transformer.</p> <code>forward</code> <p>Execute a forward pass through the transformer and attention head.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class Transformer(torch.nn.Module):\n    \"\"\"Transformer class.\"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 1024,\n        nhead: int = 8,\n        num_encoder_layers: int = 6,\n        num_decoder_layers: int = 6,\n        dropout: float = 0.1,\n        activation: str = \"relu\",\n        return_intermediate_dec: bool = False,\n        norm: bool = False,\n        num_layers_attn_head: int = 2,\n        dropout_attn_head: float = 0.1,\n        embedding_meta: dict | None = None,\n        return_embedding: bool = False,\n        decoder_self_attn: bool = False,\n        encoder_cfg: dict | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize Transformer.\n\n        Args:\n            d_model: The number of features in the encoder/decoder inputs.\n            nhead: The number of heads in the transformer encoder/decoder.\n            num_encoder_layers: The number of encoder-layers in the encoder.\n            num_decoder_layers: The number of decoder-layers in the decoder.\n            dropout: Dropout value applied to the output of transformer layers.\n            activation: Activation function to use.\n            return_intermediate_dec: Return intermediate layers from decoder.\n            norm: If True, normalize output of encoder and decoder.\n            num_layers_attn_head: The number of layers in the attention head.\n            dropout_attn_head: Dropout value for the attention_head.\n            embedding_meta: Metadata for positional embeddings. See below.\n            return_embedding: Whether to return the positional embeddings\n            decoder_self_attn: If True, use decoder self attention.\n            encoder_cfg: Encoder configuration.\n\n                More details on `embedding_meta`:\n                    By default this will be an empty dict and indicate\n                    that no positional embeddings should be used. To use the positional embeddings\n                    pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                    {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},\n                    \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                    and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n        \"\"\"\n        super().__init__()\n\n        self.d_model = dim_feedforward = feature_dim_attn_head = d_model\n\n        self.embedding_meta = embedding_meta\n        self.return_embedding = return_embedding\n        self.encoder_cfg = encoder_cfg\n\n        self.pos_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n        self.temp_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n\n        if self.embedding_meta:\n            if \"pos\" in self.embedding_meta:\n                pos_emb_cfg = self.embedding_meta[\"pos\"]\n                if pos_emb_cfg:\n                    self.pos_emb = Embedding(\n                        emb_type=\"pos\", features=self.d_model, **pos_emb_cfg\n                    )\n            if \"temp\" in self.embedding_meta:\n                temp_emb_cfg = self.embedding_meta[\"temp\"]\n                if temp_emb_cfg:\n                    self.temp_emb = Embedding(\n                        emb_type=\"temp\", features=self.d_model, **temp_emb_cfg\n                    )\n\n        self.fourier_embeddings = FourierPositionalEmbeddings(\n            n_components=8, d_model=d_model\n        )\n\n        # Transformer Encoder\n        encoder_layer = TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, activation, norm\n        )\n\n        encoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n        # only used if using descriptor visual encoder; default resnet encoder uses d_model directly\n        if self.encoder_cfg and \"encoder_type\" in self.encoder_cfg:\n            self.visual_feat_dim = (\n                self.encoder_cfg[\"ndim\"] if \"ndim\" in self.encoder_cfg else 5\n            )  # 5 is default for descriptor\n            self.fourier_proj = nn.Linear(self.d_model + self.visual_feat_dim, d_model)\n            self.fourier_norm = nn.LayerNorm(self.d_model)\n\n        self.encoder = TransformerEncoder(\n            encoder_layer, num_encoder_layers, encoder_norm\n        )\n\n        # Transformer Decoder\n        decoder_layer = TransformerDecoderLayer(\n            d_model,\n            nhead,\n            dim_feedforward,\n            dropout,\n            activation,\n            norm,\n            decoder_self_attn,\n        )\n\n        decoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n        self.decoder = TransformerDecoder(\n            decoder_layer, num_decoder_layers, return_intermediate_dec, decoder_norm\n        )\n\n        # Transformer attention head\n        self.attn_head = ATTWeightHead(\n            feature_dim=feature_dim_attn_head,\n            num_layers=num_layers_attn_head,\n            dropout=dropout_attn_head,\n        )\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        \"\"\"Initialize model weights from xavier distribution.\"\"\"\n        for p in self.parameters():\n            if not torch.nn.parameter.is_lazy(p) and p.dim() &gt; 1:\n                try:\n                    nn.init.xavier_uniform_(p)\n                except ValueError as e:\n                    print(f\"Failed Trying to initialize {p}\")\n                    raise (e)\n\n    def forward(\n        self,\n        ref_instances: list[\"dreem.io.Instance\"],\n        query_instances: list[\"dreem.io.Instance\"] | None = None,\n    ) -&gt; list[AssociationMatrix]:\n        \"\"\"Execute a forward pass through the transformer and attention head.\n\n        Args:\n            ref_instances: A list of instance objects (See `dreem.io.Instance` for more info.)\n            query_instances: An set of instances to be used as decoder queries.\n\n        Returns:\n            asso_output: A list of torch.Tensors of shape (L, n_query, total_instances) where:\n                L: number of decoder blocks\n                n_query: number of instances in current query/frame\n                total_instances: number of instances in window\n        \"\"\"\n        ref_features = torch.cat(\n            [instance.features for instance in ref_instances], dim=0\n        ).unsqueeze(0)\n\n        # window_length = len(frames)\n        # instances_per_frame = [frame.num_detected for frame in frames]\n        total_instances = len(ref_instances)\n        embed_dim = self.d_model\n        # print(f'T: {window_length}; N: {total_instances}; N_t: {instances_per_frame} n_reid: {reid_features.shape}')\n        ref_boxes = get_boxes(ref_instances)  # total_instances, 4\n        ref_boxes = torch.nan_to_num(ref_boxes, -1.0)\n        ref_times, query_times = get_times(ref_instances, query_instances)\n\n        # window_length = len(ref_times.unique())  # Currently unused but may be useful for debugging\n\n        ref_temp_emb = self.temp_emb(ref_times)\n\n        ref_pos_emb = self.pos_emb(ref_boxes)\n\n        if self.return_embedding:\n            for i, instance in enumerate(ref_instances):\n                instance.add_embedding(\"pos\", ref_pos_emb[i])\n                instance.add_embedding(\"temp\", ref_temp_emb[i])\n\n        ref_emb = (ref_pos_emb + ref_temp_emb) / 2.0\n\n        ref_emb = ref_emb.view(1, total_instances, embed_dim)\n\n        ref_emb = ref_emb.permute(1, 0, 2)  # (total_instances, batch_size, embed_dim)\n\n        batch_size, total_instances = ref_features.shape[:-1]\n\n        ref_features = ref_features.permute(\n            1, 0, 2\n        )  # (total_instances, batch_size, embed_dim)\n\n        encoder_queries = ref_features\n\n        # apply fourier embeddings if using fourier rope, OR if using descriptor (compact) visual encoder\n        if (\n            self.embedding_meta\n            and \"use_fourier\" in self.embedding_meta\n            and self.embedding_meta[\"use_fourier\"]\n        ) or (\n            self.encoder_cfg\n            and \"encoder_type\" in self.encoder_cfg\n            and self.encoder_cfg[\"encoder_type\"] == \"descriptor\"\n        ):\n            encoder_queries = apply_fourier_embeddings(\n                encoder_queries,\n                ref_times,\n                self.d_model,\n                self.fourier_embeddings,\n                self.fourier_proj,\n                self.fourier_norm,\n            )\n\n        encoder_features = self.encoder(\n            encoder_queries, pos_emb=ref_emb\n        )  # (total_instances, batch_size, embed_dim)\n\n        n_query = total_instances\n\n        query_features = ref_features\n        query_pos_emb = ref_pos_emb\n        query_temp_emb = ref_temp_emb\n        query_emb = ref_emb\n\n        if query_instances is not None:\n            n_query = len(query_instances)\n\n            query_features = torch.cat(\n                [instance.features for instance in query_instances], dim=0\n            ).unsqueeze(0)\n\n            query_features = query_features.permute(\n                1, 0, 2\n            )  # (n_query, batch_size, embed_dim)\n\n            query_boxes = get_boxes(query_instances)\n            query_boxes = torch.nan_to_num(query_boxes, -1.0)\n            query_temp_emb = self.temp_emb(query_times)\n\n            query_pos_emb = self.pos_emb(query_boxes)\n\n            query_emb = (query_pos_emb + query_temp_emb) / 2.0\n            query_emb = query_emb.view(1, n_query, embed_dim)\n            query_emb = query_emb.permute(1, 0, 2)  # (n_query, batch_size, embed_dim)\n\n        else:\n            query_instances = ref_instances\n            query_times = ref_times\n\n        if self.return_embedding:\n            for i, instance in enumerate(query_instances):\n                instance.add_embedding(\"pos\", query_pos_emb[i])\n                instance.add_embedding(\"temp\", query_temp_emb[i])\n\n        # apply fourier embeddings if using fourier rope, OR if using descriptor (compact) visual encoder\n        if (\n            self.embedding_meta\n            and \"use_fourier\" in self.embedding_meta\n            and self.embedding_meta[\"use_fourier\"]\n        ) or (\n            self.encoder_cfg\n            and \"encoder_type\" in self.encoder_cfg\n            and self.encoder_cfg[\"encoder_type\"] == \"descriptor\"\n        ):\n            query_features = apply_fourier_embeddings(\n                query_features,\n                query_times,\n                self.d_model,\n                self.fourier_embeddings,\n                self.fourier_proj,\n                self.fourier_norm,\n            )\n\n        decoder_features = self.decoder(\n            query_features,\n            encoder_features,\n            ref_pos_emb=ref_emb,\n            query_pos_emb=query_emb,\n        )  # (L, n_query, batch_size, embed_dim)\n\n        decoder_features = decoder_features.transpose(\n            1, 2\n        )  # # (L, batch_size, n_query, embed_dim)\n        encoder_features = encoder_features.permute(1, 0, 2).view(\n            batch_size, total_instances, embed_dim\n        )  # (batch_size, total_instances, embed_dim)\n\n        asso_output = []\n        for frame_features in decoder_features:\n            asso_matrix = self.attn_head(frame_features, encoder_features).view(\n                n_query, total_instances\n            )\n            asso_matrix = AssociationMatrix(asso_matrix, ref_instances, query_instances)\n\n            asso_output.append(asso_matrix)\n\n        # (L=1, n_query, total_instances)\n        return asso_output\n</code></pre>"},{"location":"models/model_parts/#dreem.models.Transformer.__init__","title":"<code>__init__(d_model=1024, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dropout=0.1, activation='relu', return_intermediate_dec=False, norm=False, num_layers_attn_head=2, dropout_attn_head=0.1, embedding_meta=None, return_embedding=False, decoder_self_attn=False, encoder_cfg=None)</code>","text":"<p>Initialize Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of features in the encoder/decoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads in the transformer encoder/decoder.</p> <code>8</code> <code>num_encoder_layers</code> <code>int</code> <p>The number of encoder-layers in the encoder.</p> <code>6</code> <code>num_decoder_layers</code> <code>int</code> <p>The number of decoder-layers in the decoder.</p> <code>6</code> <code>dropout</code> <code>float</code> <p>Dropout value applied to the output of transformer layers.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>return_intermediate_dec</code> <code>bool</code> <p>Return intermediate layers from decoder.</p> <code>False</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of encoder and decoder.</p> <code>False</code> <code>num_layers_attn_head</code> <code>int</code> <p>The number of layers in the attention head.</p> <code>2</code> <code>dropout_attn_head</code> <code>float</code> <p>Dropout value for the attention_head.</p> <code>0.1</code> <code>embedding_meta</code> <code>dict | None</code> <p>Metadata for positional embeddings. See below.</p> <code>None</code> <code>return_embedding</code> <code>bool</code> <p>Whether to return the positional embeddings</p> <code>False</code> <code>decoder_self_attn</code> <code>bool</code> <p>If True, use decoder self attention.</p> <code>False</code> <code>encoder_cfg</code> <code>dict | None</code> <p>Encoder configuration.</p> <p>More details on <code>embedding_meta</code>:     By default this will be an empty dict and indicate     that no positional embeddings should be used. To use the positional embeddings     pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:     {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},     \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see <code>dreem.models.embeddings.Embedding.EMB_TYPES</code>     and <code>dreem.models.embeddings.Embedding.EMB_MODES</code> for embedding parameters).</p> <code>None</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    d_model: int = 1024,\n    nhead: int = 8,\n    num_encoder_layers: int = 6,\n    num_decoder_layers: int = 6,\n    dropout: float = 0.1,\n    activation: str = \"relu\",\n    return_intermediate_dec: bool = False,\n    norm: bool = False,\n    num_layers_attn_head: int = 2,\n    dropout_attn_head: float = 0.1,\n    embedding_meta: dict | None = None,\n    return_embedding: bool = False,\n    decoder_self_attn: bool = False,\n    encoder_cfg: dict | None = None,\n) -&gt; None:\n    \"\"\"Initialize Transformer.\n\n    Args:\n        d_model: The number of features in the encoder/decoder inputs.\n        nhead: The number of heads in the transformer encoder/decoder.\n        num_encoder_layers: The number of encoder-layers in the encoder.\n        num_decoder_layers: The number of decoder-layers in the decoder.\n        dropout: Dropout value applied to the output of transformer layers.\n        activation: Activation function to use.\n        return_intermediate_dec: Return intermediate layers from decoder.\n        norm: If True, normalize output of encoder and decoder.\n        num_layers_attn_head: The number of layers in the attention head.\n        dropout_attn_head: Dropout value for the attention_head.\n        embedding_meta: Metadata for positional embeddings. See below.\n        return_embedding: Whether to return the positional embeddings\n        decoder_self_attn: If True, use decoder self attention.\n        encoder_cfg: Encoder configuration.\n\n            More details on `embedding_meta`:\n                By default this will be an empty dict and indicate\n                that no positional embeddings should be used. To use the positional embeddings\n                pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},\n                \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n    \"\"\"\n    super().__init__()\n\n    self.d_model = dim_feedforward = feature_dim_attn_head = d_model\n\n    self.embedding_meta = embedding_meta\n    self.return_embedding = return_embedding\n    self.encoder_cfg = encoder_cfg\n\n    self.pos_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n    self.temp_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n\n    if self.embedding_meta:\n        if \"pos\" in self.embedding_meta:\n            pos_emb_cfg = self.embedding_meta[\"pos\"]\n            if pos_emb_cfg:\n                self.pos_emb = Embedding(\n                    emb_type=\"pos\", features=self.d_model, **pos_emb_cfg\n                )\n        if \"temp\" in self.embedding_meta:\n            temp_emb_cfg = self.embedding_meta[\"temp\"]\n            if temp_emb_cfg:\n                self.temp_emb = Embedding(\n                    emb_type=\"temp\", features=self.d_model, **temp_emb_cfg\n                )\n\n    self.fourier_embeddings = FourierPositionalEmbeddings(\n        n_components=8, d_model=d_model\n    )\n\n    # Transformer Encoder\n    encoder_layer = TransformerEncoderLayer(\n        d_model, nhead, dim_feedforward, dropout, activation, norm\n    )\n\n    encoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n    # only used if using descriptor visual encoder; default resnet encoder uses d_model directly\n    if self.encoder_cfg and \"encoder_type\" in self.encoder_cfg:\n        self.visual_feat_dim = (\n            self.encoder_cfg[\"ndim\"] if \"ndim\" in self.encoder_cfg else 5\n        )  # 5 is default for descriptor\n        self.fourier_proj = nn.Linear(self.d_model + self.visual_feat_dim, d_model)\n        self.fourier_norm = nn.LayerNorm(self.d_model)\n\n    self.encoder = TransformerEncoder(\n        encoder_layer, num_encoder_layers, encoder_norm\n    )\n\n    # Transformer Decoder\n    decoder_layer = TransformerDecoderLayer(\n        d_model,\n        nhead,\n        dim_feedforward,\n        dropout,\n        activation,\n        norm,\n        decoder_self_attn,\n    )\n\n    decoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n    self.decoder = TransformerDecoder(\n        decoder_layer, num_decoder_layers, return_intermediate_dec, decoder_norm\n    )\n\n    # Transformer attention head\n    self.attn_head = ATTWeightHead(\n        feature_dim=feature_dim_attn_head,\n        num_layers=num_layers_attn_head,\n        dropout=dropout_attn_head,\n    )\n\n    self._reset_parameters()\n</code></pre>"},{"location":"models/model_parts/#dreem.models.Transformer.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute a forward pass through the transformer and attention head.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>A list of instance objects (See <code>dreem.io.Instance</code> for more info.)</p> required <code>query_instances</code> <code>list[Instance] | None</code> <p>An set of instances to be used as decoder queries.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>asso_output</code> <code>list[AssociationMatrix]</code> <p>A list of torch.Tensors of shape (L, n_query, total_instances) where:     L: number of decoder blocks     n_query: number of instances in current query/frame     total_instances: number of instances in window</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self,\n    ref_instances: list[\"dreem.io.Instance\"],\n    query_instances: list[\"dreem.io.Instance\"] | None = None,\n) -&gt; list[AssociationMatrix]:\n    \"\"\"Execute a forward pass through the transformer and attention head.\n\n    Args:\n        ref_instances: A list of instance objects (See `dreem.io.Instance` for more info.)\n        query_instances: An set of instances to be used as decoder queries.\n\n    Returns:\n        asso_output: A list of torch.Tensors of shape (L, n_query, total_instances) where:\n            L: number of decoder blocks\n            n_query: number of instances in current query/frame\n            total_instances: number of instances in window\n    \"\"\"\n    ref_features = torch.cat(\n        [instance.features for instance in ref_instances], dim=0\n    ).unsqueeze(0)\n\n    # window_length = len(frames)\n    # instances_per_frame = [frame.num_detected for frame in frames]\n    total_instances = len(ref_instances)\n    embed_dim = self.d_model\n    # print(f'T: {window_length}; N: {total_instances}; N_t: {instances_per_frame} n_reid: {reid_features.shape}')\n    ref_boxes = get_boxes(ref_instances)  # total_instances, 4\n    ref_boxes = torch.nan_to_num(ref_boxes, -1.0)\n    ref_times, query_times = get_times(ref_instances, query_instances)\n\n    # window_length = len(ref_times.unique())  # Currently unused but may be useful for debugging\n\n    ref_temp_emb = self.temp_emb(ref_times)\n\n    ref_pos_emb = self.pos_emb(ref_boxes)\n\n    if self.return_embedding:\n        for i, instance in enumerate(ref_instances):\n            instance.add_embedding(\"pos\", ref_pos_emb[i])\n            instance.add_embedding(\"temp\", ref_temp_emb[i])\n\n    ref_emb = (ref_pos_emb + ref_temp_emb) / 2.0\n\n    ref_emb = ref_emb.view(1, total_instances, embed_dim)\n\n    ref_emb = ref_emb.permute(1, 0, 2)  # (total_instances, batch_size, embed_dim)\n\n    batch_size, total_instances = ref_features.shape[:-1]\n\n    ref_features = ref_features.permute(\n        1, 0, 2\n    )  # (total_instances, batch_size, embed_dim)\n\n    encoder_queries = ref_features\n\n    # apply fourier embeddings if using fourier rope, OR if using descriptor (compact) visual encoder\n    if (\n        self.embedding_meta\n        and \"use_fourier\" in self.embedding_meta\n        and self.embedding_meta[\"use_fourier\"]\n    ) or (\n        self.encoder_cfg\n        and \"encoder_type\" in self.encoder_cfg\n        and self.encoder_cfg[\"encoder_type\"] == \"descriptor\"\n    ):\n        encoder_queries = apply_fourier_embeddings(\n            encoder_queries,\n            ref_times,\n            self.d_model,\n            self.fourier_embeddings,\n            self.fourier_proj,\n            self.fourier_norm,\n        )\n\n    encoder_features = self.encoder(\n        encoder_queries, pos_emb=ref_emb\n    )  # (total_instances, batch_size, embed_dim)\n\n    n_query = total_instances\n\n    query_features = ref_features\n    query_pos_emb = ref_pos_emb\n    query_temp_emb = ref_temp_emb\n    query_emb = ref_emb\n\n    if query_instances is not None:\n        n_query = len(query_instances)\n\n        query_features = torch.cat(\n            [instance.features for instance in query_instances], dim=0\n        ).unsqueeze(0)\n\n        query_features = query_features.permute(\n            1, 0, 2\n        )  # (n_query, batch_size, embed_dim)\n\n        query_boxes = get_boxes(query_instances)\n        query_boxes = torch.nan_to_num(query_boxes, -1.0)\n        query_temp_emb = self.temp_emb(query_times)\n\n        query_pos_emb = self.pos_emb(query_boxes)\n\n        query_emb = (query_pos_emb + query_temp_emb) / 2.0\n        query_emb = query_emb.view(1, n_query, embed_dim)\n        query_emb = query_emb.permute(1, 0, 2)  # (n_query, batch_size, embed_dim)\n\n    else:\n        query_instances = ref_instances\n        query_times = ref_times\n\n    if self.return_embedding:\n        for i, instance in enumerate(query_instances):\n            instance.add_embedding(\"pos\", query_pos_emb[i])\n            instance.add_embedding(\"temp\", query_temp_emb[i])\n\n    # apply fourier embeddings if using fourier rope, OR if using descriptor (compact) visual encoder\n    if (\n        self.embedding_meta\n        and \"use_fourier\" in self.embedding_meta\n        and self.embedding_meta[\"use_fourier\"]\n    ) or (\n        self.encoder_cfg\n        and \"encoder_type\" in self.encoder_cfg\n        and self.encoder_cfg[\"encoder_type\"] == \"descriptor\"\n    ):\n        query_features = apply_fourier_embeddings(\n            query_features,\n            query_times,\n            self.d_model,\n            self.fourier_embeddings,\n            self.fourier_proj,\n            self.fourier_norm,\n        )\n\n    decoder_features = self.decoder(\n        query_features,\n        encoder_features,\n        ref_pos_emb=ref_emb,\n        query_pos_emb=query_emb,\n    )  # (L, n_query, batch_size, embed_dim)\n\n    decoder_features = decoder_features.transpose(\n        1, 2\n    )  # # (L, batch_size, n_query, embed_dim)\n    encoder_features = encoder_features.permute(1, 0, 2).view(\n        batch_size, total_instances, embed_dim\n    )  # (batch_size, total_instances, embed_dim)\n\n    asso_output = []\n    for frame_features in decoder_features:\n        asso_matrix = self.attn_head(frame_features, encoder_features).view(\n            n_query, total_instances\n        )\n        asso_matrix = AssociationMatrix(asso_matrix, ref_instances, query_instances)\n\n        asso_output.append(asso_matrix)\n\n    # (L=1, n_query, total_instances)\n    return asso_output\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerEncoder","title":"<code>dreem.models.transformer.TransformerEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>A transformer encoder block composed of encoder layers.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize transformer encoder.</p> <code>forward</code> <p>Execute a forward pass of encoder layer.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class TransformerEncoder(nn.Module):\n    \"\"\"A transformer encoder block composed of encoder layers.\"\"\"\n\n    def __init__(\n        self,\n        encoder_layer: TransformerEncoderLayer,\n        num_layers: int,\n        norm: nn.Module | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize transformer encoder.\n\n        Args:\n            encoder_layer: An instance of the TransformerEncoderLayer.\n            num_layers: The number of encoder layers to be stacked.\n            norm: The normalization layer to be applied.\n        \"\"\"\n        super().__init__()\n\n        self.layers = _get_clones(encoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm if norm is not None else nn.Identity()\n\n    def forward(\n        self, queries: torch.Tensor, pos_emb: torch.Tensor = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute a forward pass of encoder layer.\n\n        Args:\n            queries: The input tensor of shape (n_query, batch_size, embed_dim).\n            pos_emb: The positional embedding tensor of shape (n_query, embed_dim).\n\n        Returns:\n            The output tensor of shape (n_query, batch_size, embed_dim).\n        \"\"\"\n        for layer in self.layers:\n            queries = layer(queries, pos_emb=pos_emb)\n\n        encoder_features = self.norm(queries)\n        return encoder_features\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerEncoder.__init__","title":"<code>__init__(encoder_layer, num_layers, norm=None)</code>","text":"<p>Initialize transformer encoder.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_layer</code> <code>TransformerEncoderLayer</code> <p>An instance of the TransformerEncoderLayer.</p> required <code>num_layers</code> <code>int</code> <p>The number of encoder layers to be stacked.</p> required <code>norm</code> <code>Module | None</code> <p>The normalization layer to be applied.</p> <code>None</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    encoder_layer: TransformerEncoderLayer,\n    num_layers: int,\n    norm: nn.Module | None = None,\n) -&gt; None:\n    \"\"\"Initialize transformer encoder.\n\n    Args:\n        encoder_layer: An instance of the TransformerEncoderLayer.\n        num_layers: The number of encoder layers to be stacked.\n        norm: The normalization layer to be applied.\n    \"\"\"\n    super().__init__()\n\n    self.layers = _get_clones(encoder_layer, num_layers)\n    self.num_layers = num_layers\n    self.norm = norm if norm is not None else nn.Identity()\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerEncoder.forward","title":"<code>forward(queries, pos_emb=None)</code>","text":"<p>Execute a forward pass of encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Tensor</code> <p>The input tensor of shape (n_query, batch_size, embed_dim).</p> required <code>pos_emb</code> <code>Tensor</code> <p>The positional embedding tensor of shape (n_query, embed_dim).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self, queries: torch.Tensor, pos_emb: torch.Tensor = None\n) -&gt; torch.Tensor:\n    \"\"\"Execute a forward pass of encoder layer.\n\n    Args:\n        queries: The input tensor of shape (n_query, batch_size, embed_dim).\n        pos_emb: The positional embedding tensor of shape (n_query, embed_dim).\n\n    Returns:\n        The output tensor of shape (n_query, batch_size, embed_dim).\n    \"\"\"\n    for layer in self.layers:\n        queries = layer(queries, pos_emb=pos_emb)\n\n    encoder_features = self.norm(queries)\n    return encoder_features\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerEncoderLayer","title":"<code>dreem.models.transformer.TransformerEncoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A single transformer encoder layer.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize a transformer encoder layer.</p> <code>forward</code> <p>Execute a forward pass of the encoder layer.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class TransformerEncoderLayer(nn.Module):\n    \"\"\"A single transformer encoder layer.\"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 1024,\n        nhead: int = 6,\n        dim_feedforward: int = 1024,\n        dropout: float = 0.1,\n        activation: str = \"relu\",\n        norm: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize a transformer encoder layer.\n\n        Args:\n            d_model: The number of features in the encoder inputs.\n            nhead: The number of heads for the encoder.\n            dim_feedforward: Dimension of the feedforward layers of encoder.\n            dropout: Dropout value applied to the output of encoder.\n            activation: Activation function to use.\n            norm: If True, normalize output of encoder.\n        \"\"\"\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model) if norm else nn.Identity()\n        self.norm2 = nn.LayerNorm(d_model) if norm else nn.Identity()\n\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n\n    def forward(\n        self, queries: torch.Tensor, pos_emb: torch.Tensor = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute a forward pass of the encoder layer.\n\n        Args:\n            queries: Input sequence for encoder (n_query, batch_size, embed_dim).\n            pos_emb: Position embedding, if provided is added to src\n\n        Returns:\n            The output tensor of shape (n_query, batch_size, embed_dim).\n        \"\"\"\n        if pos_emb is None:\n            pos_emb = torch.zeros_like(queries)\n\n        queries = queries + pos_emb\n\n        # q = k = src\n\n        attn_features = self.self_attn(\n            query=queries,\n            key=queries,\n            value=queries,\n        )[0]\n\n        queries = queries + self.dropout1(attn_features)\n        queries = self.norm1(queries)\n        projection = self.linear2(self.dropout(self.activation(self.linear1(queries))))\n        queries = queries + self.dropout2(projection)\n        encoder_features = self.norm2(queries)\n\n        return encoder_features\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerEncoderLayer.__init__","title":"<code>__init__(d_model=1024, nhead=6, dim_feedforward=1024, dropout=0.1, activation='relu', norm=False)</code>","text":"<p>Initialize a transformer encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of features in the encoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads for the encoder.</p> <code>6</code> <code>dim_feedforward</code> <code>int</code> <p>Dimension of the feedforward layers of encoder.</p> <code>1024</code> <code>dropout</code> <code>float</code> <p>Dropout value applied to the output of encoder.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of encoder.</p> <code>False</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    d_model: int = 1024,\n    nhead: int = 6,\n    dim_feedforward: int = 1024,\n    dropout: float = 0.1,\n    activation: str = \"relu\",\n    norm: bool = False,\n) -&gt; None:\n    \"\"\"Initialize a transformer encoder layer.\n\n    Args:\n        d_model: The number of features in the encoder inputs.\n        nhead: The number of heads for the encoder.\n        dim_feedforward: Dimension of the feedforward layers of encoder.\n        dropout: Dropout value applied to the output of encoder.\n        activation: Activation function to use.\n        norm: If True, normalize output of encoder.\n    \"\"\"\n    super().__init__()\n    self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n    self.linear1 = nn.Linear(d_model, dim_feedforward)\n    self.dropout = nn.Dropout(dropout)\n    self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n    self.norm1 = nn.LayerNorm(d_model) if norm else nn.Identity()\n    self.norm2 = nn.LayerNorm(d_model) if norm else nn.Identity()\n\n    self.dropout1 = nn.Dropout(dropout)\n    self.dropout2 = nn.Dropout(dropout)\n\n    self.activation = _get_activation_fn(activation)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerEncoderLayer.forward","title":"<code>forward(queries, pos_emb=None)</code>","text":"<p>Execute a forward pass of the encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Tensor</code> <p>Input sequence for encoder (n_query, batch_size, embed_dim).</p> required <code>pos_emb</code> <code>Tensor</code> <p>Position embedding, if provided is added to src</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self, queries: torch.Tensor, pos_emb: torch.Tensor = None\n) -&gt; torch.Tensor:\n    \"\"\"Execute a forward pass of the encoder layer.\n\n    Args:\n        queries: Input sequence for encoder (n_query, batch_size, embed_dim).\n        pos_emb: Position embedding, if provided is added to src\n\n    Returns:\n        The output tensor of shape (n_query, batch_size, embed_dim).\n    \"\"\"\n    if pos_emb is None:\n        pos_emb = torch.zeros_like(queries)\n\n    queries = queries + pos_emb\n\n    # q = k = src\n\n    attn_features = self.self_attn(\n        query=queries,\n        key=queries,\n        value=queries,\n    )[0]\n\n    queries = queries + self.dropout1(attn_features)\n    queries = self.norm1(queries)\n    projection = self.linear2(self.dropout(self.activation(self.linear1(queries))))\n    queries = queries + self.dropout2(projection)\n    encoder_features = self.norm2(queries)\n\n    return encoder_features\n</code></pre>"},{"location":"models/model_parts/#dreem.models.Embedding","title":"<code>dreem.models.Embedding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Class that wraps around different embedding types.</p> <p>Used for both learned and fixed embeddings.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize embeddings.</p> <code>forward</code> <p>Get the sequence positional embeddings.</p> Source code in <code>dreem/models/embedding.py</code> <pre><code>class Embedding(torch.nn.Module):\n    \"\"\"Class that wraps around different embedding types.\n\n    Used for both learned and fixed embeddings.\n    \"\"\"\n\n    EMB_TYPES = {\n        \"temp\": {},\n        \"pos\": {\"over_boxes\"},\n        \"off\": {},\n        None: {},\n    }  # dict of valid args:keyword params\n    EMB_MODES = {\n        \"fixed\": {\"temperature\", \"scale\", \"normalize\"},\n        \"learned\": {\"emb_num\"},\n        \"off\": {},\n    }  # dict of valid args:keyword params\n\n    def __init__(\n        self,\n        emb_type: str,\n        mode: str,\n        features: int,\n        n_points: int = 1,\n        emb_num: int = 16,\n        over_boxes: bool = True,\n        temperature: int = 10000,\n        normalize: bool = False,\n        scale: float | None = None,\n        mlp_cfg: dict | None = None,\n    ):\n        \"\"\"Initialize embeddings.\n\n        Args:\n            emb_type: The type of embedding to compute.\n                Must be one of `{\"temp\", \"pos\", \"off\"}`\n            mode: The mode or function used to map positions to vector embeddings.\n                  Must be one of `{\"fixed\", \"learned\", \"off\"}`\n            features: The embedding dimensions. Must match the dimension of the\n                      input vectors for the transformer model.\n            n_points: the number of points that will be embedded.\n            emb_num: the number of embeddings in the `self.lookup` table\n                (Only used in learned embeddings).\n            over_boxes: Whether to compute the position embedding for each bbox\n                coordinate (y1x1y2x2) or the centroid + bbox size (yxwh).\n            temperature: the temperature constant to be used when computing\n                the sinusoidal position embedding\n            normalize: whether or not to normalize the positions\n                (Only used in fixed embeddings).\n            scale: factor by which to scale the positions after normalizing\n                (Only used in fixed embeddings).\n            mlp_cfg: A dictionary of mlp hyperparameters for projecting\n                embedding to correct space.\n                    Example: {\"hidden_dims\": 256, \"num_layers\":3, \"dropout\": 0.3}\n        \"\"\"\n        self._check_init_args(emb_type, mode)\n\n        super().__init__()\n\n        self.emb_type = emb_type\n        self.mode = mode\n        self.features = features\n        self.emb_num = emb_num\n        self.over_boxes = over_boxes\n        self.temperature = temperature\n        self.normalize = normalize\n        self.scale = scale\n        self.n_points = n_points\n\n        if self.normalize and self.scale is None:\n            self.scale = 2 * math.pi\n\n        if self.emb_type == \"pos\" and mlp_cfg is not None and mlp_cfg[\"num_layers\"] &gt; 0:\n            if self.mode == \"fixed\":\n                self.mlp = MLP(\n                    input_dim=n_points * self.features,\n                    output_dim=self.features,\n                    **mlp_cfg,\n                )\n            else:\n                in_dim = (self.features // (4 * n_points)) * (4 * n_points)\n                self.mlp = MLP(\n                    input_dim=in_dim,\n                    output_dim=self.features,\n                    **mlp_cfg,\n                )\n        else:\n            self.mlp = torch.nn.Identity()\n\n        self._emb_func = lambda tensor: torch.zeros(\n            (tensor.shape[0], self.features), dtype=tensor.dtype, device=tensor.device\n        )  # turn off embedding by returning zeros\n\n        self.lookup = None\n\n        if self.mode == \"learned\":\n            if self.emb_type == \"pos\":\n                self.lookup = torch.nn.Embedding(\n                    self.emb_num * 4 * self.n_points, self.features // (4 * n_points)\n                )\n                self._emb_func = self._learned_pos_embedding\n            elif self.emb_type == \"temp\":\n                self.lookup = torch.nn.Embedding(self.emb_num, self.features)\n                self._emb_func = self._learned_temp_embedding\n\n        elif self.mode == \"fixed\":\n            if self.emb_type == \"pos\":\n                self._emb_func = self._sine_box_embedding\n            elif self.emb_type == \"temp\":\n                self._emb_func = self._sine_temp_embedding\n\n    def _check_init_args(self, emb_type: str, mode: str):\n        \"\"\"Check whether the correct arguments were passed to initialization.\n\n        Args:\n            emb_type: The type of embedding to compute. Must be one of `{\"temp\", \"pos\", \"\"}`\n            mode: The mode or function used to map positions to vector embeddings.\n                Must be one of `{\"fixed\", \"learned\"}`\n\n        Raises:\n            ValueError:\n              * if the incorrect `emb_type` or `mode` string are passed\n            NotImplementedError: if `emb_type` is `temp` and `mode` is `fixed`.\n        \"\"\"\n        if emb_type.lower() not in self.EMB_TYPES:\n            raise ValueError(\n                f\"Embedding `emb_type` must be one of {self.EMB_TYPES} not {emb_type}\"\n            )\n\n        if mode.lower() not in self.EMB_MODES:\n            raise ValueError(\n                f\"Embedding `mode` must be one of {self.EMB_MODES} not {mode}\"\n            )\n\n    def forward(self, seq_positions: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Get the sequence positional embeddings.\n\n        Args:\n            seq_positions:\n                * An (`N`, 1) tensor where seq_positions[i] represents the temporal position of instance_i in the sequence.\n                * An (`N`, n_anchors x 4) tensor where seq_positions[i, j, :] represents the [y1, x1, y2, x2] spatial locations of jth point of instance_i in the sequence.\n\n        Returns:\n            An `N` x `self.features` tensor representing the corresponding spatial or temporal embedding.\n        \"\"\"\n        emb = self._emb_func(seq_positions)\n\n        if emb.shape[-1] != self.features:\n            raise RuntimeError(\n                (\n                    f\"Output embedding dimension is {emb.shape[-1]} but requested {self.features} dimensions! \\n\"\n                    f\"hint: Try turning the MLP on by passing `mlp_cfg` to the constructor to project to the correct embedding dimensions.\"\n                )\n            )\n        return emb\n\n    def _torch_int_div(\n        self, tensor1: torch.Tensor, tensor2: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Perform integer division of two tensors.\n\n        Args:\n            tensor1: dividend tensor.\n            tensor2: divisor tensor.\n\n        Returns:\n            torch.Tensor, resulting tensor.\n        \"\"\"\n        return torch.div(tensor1, tensor2, rounding_mode=\"floor\")\n\n    def _sine_box_embedding(self, boxes: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute sine positional embeddings for boxes using given parameters.\n\n        Args:\n             boxes: the input boxes of shape N, n_anchors, 4 or B, N, n_anchors, 4\n                    where the last dimension is the bbox coords in [y1, x1, y2, x2].\n                    (Note currently `B=batch_size=1`).\n\n        Returns:\n             torch.Tensor, the sine positional embeddings\n             (embedding[:, 4i] = sin(x)\n              embedding[:, 4i+1] = cos(x)\n              embedding[:, 4i+2] = sin(y)\n              embedding[:, 4i+3] = cos(y)\n              )\n        \"\"\"\n        if self.scale is not None and self.normalize is False:\n            raise ValueError(\"normalize should be True if scale is passed\")\n\n        if len(boxes.size()) == 3:\n            boxes = boxes.unsqueeze(0)\n\n        if self.normalize:\n            boxes = boxes / (boxes[:, :, -1:] + 1e-6) * self.scale\n\n        dim_t = torch.arange(self.features // 4, dtype=torch.float32)\n\n        dim_t = self.temperature ** (\n            2 * self._torch_int_div(dim_t, 2) / (self.features // 4)\n        )\n\n        # (b, n_t, n_anchors, 4, D//4)\n        pos_emb = boxes[:, :, :, :, None] / dim_t.to(boxes.device)\n\n        pos_emb = torch.stack(\n            (pos_emb[:, :, :, :, 0::2].sin(), pos_emb[:, :, :, :, 1::2].cos()), dim=4\n        )\n        pos_emb = pos_emb.flatten(2).squeeze(0)  # (N_t, n_anchors * D)\n\n        pos_emb = self.mlp(pos_emb)\n\n        pos_emb = pos_emb.view(boxes.shape[1], self.features)\n\n        return pos_emb\n\n    def _sine_temp_embedding(self, times: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute fixed sine temporal embeddings.\n\n        Args:\n            times: the input times of shape (N,) or (N,1) where N = (sum(instances_per_frame))\n            which is the frame index of the instance relative\n            to the batch size\n            (e.g. `torch.tensor([0, 0, ..., 0, 1, 1, ..., 1, 2, 2, ..., 2,..., B, B, ...B])`).\n\n        Returns:\n            an n_instances x D embedding representing the temporal embedding.\n        \"\"\"\n        T = times.int().max().item() + 1\n        d = self.features\n        n = self.temperature\n\n        positions = torch.arange(0, T).unsqueeze(1)\n        temp_lookup = torch.zeros(T, d, device=times.device)\n\n        denominators = torch.pow(\n            n, 2 * torch.arange(0, d // 2) / d\n        )  # 10000^(2i/d_model), i is the index of embedding\n        temp_lookup[:, 0::2] = torch.sin(\n            positions / denominators\n        )  # sin(pos/10000^(2i/d_model))\n        temp_lookup[:, 1::2] = torch.cos(\n            positions / denominators\n        )  # cos(pos/10000^(2i/d_model))\n\n        temp_emb = temp_lookup[times.int()]\n        return temp_emb  # .view(len(times), self.features)\n\n    def _learned_pos_embedding(self, boxes: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute learned positional embeddings for boxes using given parameters.\n\n        Args:\n            boxes: the input boxes of shape N x 4 or B x N x 4\n                   where the last dimension is the bbox coords in [y1, x1, y2, x2].\n                   (Note currently `B=batch_size=1`).\n\n        Returns:\n            torch.Tensor, the learned positional embeddings.\n        \"\"\"\n        pos_lookup = self.lookup\n\n        N, n_anchors, _ = boxes.shape\n        boxes = boxes.view(N, n_anchors, 4)\n\n        if self.over_boxes:\n            xywh = boxes\n        else:\n            xywh = torch.cat(\n                [\n                    (boxes[:, :, 2:] + boxes[:, :, :2]) / 2,\n                    (boxes[:, :, 2:] - boxes[:, :, :2]),\n                ],\n                dim=1,\n            )\n\n        left_ind, right_ind, left_weight, right_weight = self._compute_weights(xywh)\n        f = pos_lookup.weight.shape[1]  # self.features // 4\n\n        try:\n            pos_emb_table = pos_lookup.weight.view(\n                self.emb_num, n_anchors, 4, f\n            )  # T x 4 x (D * 4)\n        except RuntimeError as e:\n            logger.exception(\n                f\"Hint: `n_points` ({self.n_points}) may be set incorrectly!\"\n            )\n            logger.exception(e)\n            raise (e)\n\n        left_emb = pos_emb_table.gather(\n            0,\n            left_ind[:, :, :, None].to(pos_emb_table.device).expand(N, n_anchors, 4, f),\n        )  # N x 4 x d\n        right_emb = pos_emb_table.gather(\n            0,\n            right_ind[:, :, :, None]\n            .to(pos_emb_table.device)\n            .expand(N, n_anchors, 4, f),\n        )  # N x 4 x d\n        pos_emb = left_weight[:, :, :, None] * right_emb.to(\n            left_weight.device\n        ) + right_weight[:, :, :, None] * left_emb.to(right_weight.device)\n\n        pos_emb = pos_emb.flatten(1)\n        pos_emb = self.mlp(pos_emb)\n\n        return pos_emb.view(N, self.features)\n\n    def _learned_temp_embedding(self, times: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute learned temporal embeddings for times using given parameters.\n\n        Args:\n            times: the input times of shape (N,) or (N,1) where N = (sum(instances_per_frame))\n            which is the frame index of the instance relative\n            to the batch size\n            (e.g. `torch.tensor([0, 0, ..., 0, 1, 1, ..., 1, 2, 2, ..., 2,..., B, B, ...B])`).\n\n        Returns:\n            torch.Tensor, the learned temporal embeddings.\n        \"\"\"\n        temp_lookup = self.lookup\n        N = times.shape[0]\n\n        left_ind, right_ind, left_weight, right_weight = self._compute_weights(times)\n\n        left_emb = temp_lookup.weight[\n            left_ind.to(temp_lookup.weight.device)\n        ]  # T x D --&gt; N x D\n        right_emb = temp_lookup.weight[right_ind.to(temp_lookup.weight.device)]\n\n        temp_emb = left_weight[:, None] * right_emb.to(\n            left_weight.device\n        ) + right_weight[:, None] * left_emb.to(right_weight.device)\n\n        return temp_emb.view(N, self.features)\n\n    def _compute_weights(self, data: torch.Tensor) -&gt; tuple[torch.Tensor, ...]:\n        \"\"\"Compute left and right learned embedding weights.\n\n        Args:\n            data: the input data (e.g boxes or times).\n\n        Returns:\n            A torch.Tensor for each of the left/right indices and weights, respectively\n        \"\"\"\n        data = data * self.emb_num\n\n        left_ind = data.clamp(min=0, max=self.emb_num - 1).long()  # N x 4\n        right_ind = (left_ind + 1).clamp(min=0, max=self.emb_num - 1).long()  # N x 4\n\n        left_weight = data - left_ind.float()  # N x 4\n\n        right_weight = 1.0 - left_weight\n\n        return left_ind, right_ind, left_weight, right_weight\n</code></pre>"},{"location":"models/model_parts/#dreem.models.Embedding.__init__","title":"<code>__init__(emb_type, mode, features, n_points=1, emb_num=16, over_boxes=True, temperature=10000, normalize=False, scale=None, mlp_cfg=None)</code>","text":"<p>Initialize embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>The type of embedding to compute. Must be one of <code>{\"temp\", \"pos\", \"off\"}</code></p> required <code>mode</code> <code>str</code> <p>The mode or function used to map positions to vector embeddings.   Must be one of <code>{\"fixed\", \"learned\", \"off\"}</code></p> required <code>features</code> <code>int</code> <p>The embedding dimensions. Must match the dimension of the       input vectors for the transformer model.</p> required <code>n_points</code> <code>int</code> <p>the number of points that will be embedded.</p> <code>1</code> <code>emb_num</code> <code>int</code> <p>the number of embeddings in the <code>self.lookup</code> table (Only used in learned embeddings).</p> <code>16</code> <code>over_boxes</code> <code>bool</code> <p>Whether to compute the position embedding for each bbox coordinate (y1x1y2x2) or the centroid + bbox size (yxwh).</p> <code>True</code> <code>temperature</code> <code>int</code> <p>the temperature constant to be used when computing the sinusoidal position embedding</p> <code>10000</code> <code>normalize</code> <code>bool</code> <p>whether or not to normalize the positions (Only used in fixed embeddings).</p> <code>False</code> <code>scale</code> <code>float | None</code> <p>factor by which to scale the positions after normalizing (Only used in fixed embeddings).</p> <code>None</code> <code>mlp_cfg</code> <code>dict | None</code> <p>A dictionary of mlp hyperparameters for projecting embedding to correct space.     Example: {\"hidden_dims\": 256, \"num_layers\":3, \"dropout\": 0.3}</p> <code>None</code> Source code in <code>dreem/models/embedding.py</code> <pre><code>def __init__(\n    self,\n    emb_type: str,\n    mode: str,\n    features: int,\n    n_points: int = 1,\n    emb_num: int = 16,\n    over_boxes: bool = True,\n    temperature: int = 10000,\n    normalize: bool = False,\n    scale: float | None = None,\n    mlp_cfg: dict | None = None,\n):\n    \"\"\"Initialize embeddings.\n\n    Args:\n        emb_type: The type of embedding to compute.\n            Must be one of `{\"temp\", \"pos\", \"off\"}`\n        mode: The mode or function used to map positions to vector embeddings.\n              Must be one of `{\"fixed\", \"learned\", \"off\"}`\n        features: The embedding dimensions. Must match the dimension of the\n                  input vectors for the transformer model.\n        n_points: the number of points that will be embedded.\n        emb_num: the number of embeddings in the `self.lookup` table\n            (Only used in learned embeddings).\n        over_boxes: Whether to compute the position embedding for each bbox\n            coordinate (y1x1y2x2) or the centroid + bbox size (yxwh).\n        temperature: the temperature constant to be used when computing\n            the sinusoidal position embedding\n        normalize: whether or not to normalize the positions\n            (Only used in fixed embeddings).\n        scale: factor by which to scale the positions after normalizing\n            (Only used in fixed embeddings).\n        mlp_cfg: A dictionary of mlp hyperparameters for projecting\n            embedding to correct space.\n                Example: {\"hidden_dims\": 256, \"num_layers\":3, \"dropout\": 0.3}\n    \"\"\"\n    self._check_init_args(emb_type, mode)\n\n    super().__init__()\n\n    self.emb_type = emb_type\n    self.mode = mode\n    self.features = features\n    self.emb_num = emb_num\n    self.over_boxes = over_boxes\n    self.temperature = temperature\n    self.normalize = normalize\n    self.scale = scale\n    self.n_points = n_points\n\n    if self.normalize and self.scale is None:\n        self.scale = 2 * math.pi\n\n    if self.emb_type == \"pos\" and mlp_cfg is not None and mlp_cfg[\"num_layers\"] &gt; 0:\n        if self.mode == \"fixed\":\n            self.mlp = MLP(\n                input_dim=n_points * self.features,\n                output_dim=self.features,\n                **mlp_cfg,\n            )\n        else:\n            in_dim = (self.features // (4 * n_points)) * (4 * n_points)\n            self.mlp = MLP(\n                input_dim=in_dim,\n                output_dim=self.features,\n                **mlp_cfg,\n            )\n    else:\n        self.mlp = torch.nn.Identity()\n\n    self._emb_func = lambda tensor: torch.zeros(\n        (tensor.shape[0], self.features), dtype=tensor.dtype, device=tensor.device\n    )  # turn off embedding by returning zeros\n\n    self.lookup = None\n\n    if self.mode == \"learned\":\n        if self.emb_type == \"pos\":\n            self.lookup = torch.nn.Embedding(\n                self.emb_num * 4 * self.n_points, self.features // (4 * n_points)\n            )\n            self._emb_func = self._learned_pos_embedding\n        elif self.emb_type == \"temp\":\n            self.lookup = torch.nn.Embedding(self.emb_num, self.features)\n            self._emb_func = self._learned_temp_embedding\n\n    elif self.mode == \"fixed\":\n        if self.emb_type == \"pos\":\n            self._emb_func = self._sine_box_embedding\n        elif self.emb_type == \"temp\":\n            self._emb_func = self._sine_temp_embedding\n</code></pre>"},{"location":"models/model_parts/#dreem.models.Embedding.forward","title":"<code>forward(seq_positions)</code>","text":"<p>Get the sequence positional embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>seq_positions</code> <code>Tensor</code> <ul> <li>An (<code>N</code>, 1) tensor where seq_positions[i] represents the temporal position of instance_i in the sequence.</li> <li>An (<code>N</code>, n_anchors x 4) tensor where seq_positions[i, j, :] represents the [y1, x1, y2, x2] spatial locations of jth point of instance_i in the sequence.</li> </ul> required <p>Returns:</p> Type Description <code>Tensor</code> <p>An <code>N</code> x <code>self.features</code> tensor representing the corresponding spatial or temporal embedding.</p> Source code in <code>dreem/models/embedding.py</code> <pre><code>def forward(self, seq_positions: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Get the sequence positional embeddings.\n\n    Args:\n        seq_positions:\n            * An (`N`, 1) tensor where seq_positions[i] represents the temporal position of instance_i in the sequence.\n            * An (`N`, n_anchors x 4) tensor where seq_positions[i, j, :] represents the [y1, x1, y2, x2] spatial locations of jth point of instance_i in the sequence.\n\n    Returns:\n        An `N` x `self.features` tensor representing the corresponding spatial or temporal embedding.\n    \"\"\"\n    emb = self._emb_func(seq_positions)\n\n    if emb.shape[-1] != self.features:\n        raise RuntimeError(\n            (\n                f\"Output embedding dimension is {emb.shape[-1]} but requested {self.features} dimensions! \\n\"\n                f\"hint: Try turning the MLP on by passing `mlp_cfg` to the constructor to project to the correct embedding dimensions.\"\n            )\n        )\n    return emb\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerDecoder","title":"<code>dreem.models.transformer.TransformerDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer Decoder Block composed of Transformer Decoder Layers.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize transformer decoder block.</p> <code>forward</code> <p>Execute a forward pass of the decoder block.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class TransformerDecoder(nn.Module):\n    \"\"\"Transformer Decoder Block composed of Transformer Decoder Layers.\"\"\"\n\n    def __init__(\n        self,\n        decoder_layer: TransformerDecoderLayer,\n        num_layers: int,\n        return_intermediate: bool = False,\n        norm: nn.Module | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize transformer decoder block.\n\n        Args:\n            decoder_layer: An instance of TransformerDecoderLayer.\n            num_layers: The number of decoder layers to be stacked.\n            return_intermediate: Return intermediate layers from decoder.\n            norm: The normalization layer to be applied.\n        \"\"\"\n        super().__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.return_intermediate = return_intermediate\n        self.norm = norm if norm is not None else nn.Identity()\n\n    def forward(\n        self,\n        decoder_queries: torch.Tensor,\n        encoder_features: torch.Tensor,\n        ref_pos_emb: torch.Tensor | None = None,\n        query_pos_emb: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute a forward pass of the decoder block.\n\n        Args:\n            decoder_queries: Query sequence for decoder to generate (n_query, batch_size, embed_dim).\n            encoder_features: Output from encoder, that decoder uses to attend to relevant\n                parts of input sequence (total_instances, batch_size, embed_dim)\n            ref_pos_emb: The input positional embedding tensor of shape (total_instances, batch_size, embed_dim).\n            query_pos_emb: The query positional embedding of shape (n_query, batch_size, embed_dim)\n\n        Returns:\n            The output tensor of shape (L, n_query, batch_size, embed_dim).\n        \"\"\"\n        decoder_features = decoder_queries\n\n        intermediate = []\n\n        for layer in self.layers:\n            decoder_features = layer(\n                decoder_features,\n                encoder_features,\n                ref_pos_emb=ref_pos_emb,\n                query_pos_emb=query_pos_emb,\n            )\n            if self.return_intermediate:\n                intermediate.append(self.norm(decoder_features))\n\n        decoder_features = self.norm(decoder_features)\n        if self.return_intermediate:\n            intermediate.pop()\n            intermediate.append(decoder_features)\n\n            return torch.stack(intermediate)\n\n        return decoder_features.unsqueeze(0)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerDecoder.__init__","title":"<code>__init__(decoder_layer, num_layers, return_intermediate=False, norm=None)</code>","text":"<p>Initialize transformer decoder block.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_layer</code> <code>TransformerDecoderLayer</code> <p>An instance of TransformerDecoderLayer.</p> required <code>num_layers</code> <code>int</code> <p>The number of decoder layers to be stacked.</p> required <code>return_intermediate</code> <code>bool</code> <p>Return intermediate layers from decoder.</p> <code>False</code> <code>norm</code> <code>Module | None</code> <p>The normalization layer to be applied.</p> <code>None</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    decoder_layer: TransformerDecoderLayer,\n    num_layers: int,\n    return_intermediate: bool = False,\n    norm: nn.Module | None = None,\n) -&gt; None:\n    \"\"\"Initialize transformer decoder block.\n\n    Args:\n        decoder_layer: An instance of TransformerDecoderLayer.\n        num_layers: The number of decoder layers to be stacked.\n        return_intermediate: Return intermediate layers from decoder.\n        norm: The normalization layer to be applied.\n    \"\"\"\n    super().__init__()\n    self.layers = _get_clones(decoder_layer, num_layers)\n    self.num_layers = num_layers\n    self.return_intermediate = return_intermediate\n    self.norm = norm if norm is not None else nn.Identity()\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerDecoder.forward","title":"<code>forward(decoder_queries, encoder_features, ref_pos_emb=None, query_pos_emb=None)</code>","text":"<p>Execute a forward pass of the decoder block.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_queries</code> <code>Tensor</code> <p>Query sequence for decoder to generate (n_query, batch_size, embed_dim).</p> required <code>encoder_features</code> <code>Tensor</code> <p>Output from encoder, that decoder uses to attend to relevant parts of input sequence (total_instances, batch_size, embed_dim)</p> required <code>ref_pos_emb</code> <code>Tensor | None</code> <p>The input positional embedding tensor of shape (total_instances, batch_size, embed_dim).</p> <code>None</code> <code>query_pos_emb</code> <code>Tensor | None</code> <p>The query positional embedding of shape (n_query, batch_size, embed_dim)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (L, n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self,\n    decoder_queries: torch.Tensor,\n    encoder_features: torch.Tensor,\n    ref_pos_emb: torch.Tensor | None = None,\n    query_pos_emb: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Execute a forward pass of the decoder block.\n\n    Args:\n        decoder_queries: Query sequence for decoder to generate (n_query, batch_size, embed_dim).\n        encoder_features: Output from encoder, that decoder uses to attend to relevant\n            parts of input sequence (total_instances, batch_size, embed_dim)\n        ref_pos_emb: The input positional embedding tensor of shape (total_instances, batch_size, embed_dim).\n        query_pos_emb: The query positional embedding of shape (n_query, batch_size, embed_dim)\n\n    Returns:\n        The output tensor of shape (L, n_query, batch_size, embed_dim).\n    \"\"\"\n    decoder_features = decoder_queries\n\n    intermediate = []\n\n    for layer in self.layers:\n        decoder_features = layer(\n            decoder_features,\n            encoder_features,\n            ref_pos_emb=ref_pos_emb,\n            query_pos_emb=query_pos_emb,\n        )\n        if self.return_intermediate:\n            intermediate.append(self.norm(decoder_features))\n\n    decoder_features = self.norm(decoder_features)\n    if self.return_intermediate:\n        intermediate.pop()\n        intermediate.append(decoder_features)\n\n        return torch.stack(intermediate)\n\n    return decoder_features.unsqueeze(0)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerDecoderLayer","title":"<code>dreem.models.transformer.TransformerDecoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A single transformer decoder layer.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize transformer decoder layer.</p> <code>forward</code> <p>Execute forward pass of decoder layer.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class TransformerDecoderLayer(nn.Module):\n    \"\"\"A single transformer decoder layer.\"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 1024,\n        nhead: int = 6,\n        dim_feedforward: int = 1024,\n        dropout: float = 0.1,\n        activation: str = \"relu\",\n        norm: bool = False,\n        decoder_self_attn: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize transformer decoder layer.\n\n        Args:\n            d_model: The number of features in the decoder inputs.\n            nhead: The number of heads for the decoder.\n            dim_feedforward: Dimension of the feedforward layers of decoder.\n            dropout: Dropout value applied to the output of decoder.\n            activation: Activation function to use.\n            norm: If True, normalize output of decoder.\n            decoder_self_attn: If True, use decoder self attention\n        \"\"\"\n        super().__init__()\n\n        self.decoder_self_attn = decoder_self_attn\n\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        if self.decoder_self_attn:\n            self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n\n        self.norm1 = nn.LayerNorm(d_model) if norm else nn.Identity()\n        self.norm2 = nn.LayerNorm(d_model) if norm else nn.Identity()\n        self.norm3 = nn.LayerNorm(d_model) if norm else nn.Identity()\n\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n\n    def forward(\n        self,\n        decoder_queries: torch.Tensor,\n        encoder_features: torch.Tensor,\n        ref_pos_emb: torch.Tensor | None = None,\n        query_pos_emb: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute forward pass of decoder layer.\n\n        Args:\n            decoder_queries: Target sequence for decoder to generate (n_query, batch_size, embed_dim).\n            encoder_features: Output from encoder, that decoder uses to attend to relevant\n                parts of input sequence (total_instances, batch_size, embed_dim)\n            ref_pos_emb: The input positional embedding tensor of shape (n_query, embed_dim).\n            query_pos_emb: The target positional embedding of shape (n_query, embed_dim)\n\n        Returns:\n            The output tensor of shape (n_query, batch_size, embed_dim).\n        \"\"\"\n        if query_pos_emb is None:\n            query_pos_emb = torch.zeros_like(decoder_queries)\n        if ref_pos_emb is None:\n            ref_pos_emb = torch.zeros_like(encoder_features)\n\n        decoder_queries = decoder_queries + query_pos_emb\n        encoder_features = encoder_features + ref_pos_emb\n\n        if self.decoder_self_attn:\n            self_attn_features = self.self_attn(\n                query=decoder_queries, key=decoder_queries, value=decoder_queries\n            )[0]\n            decoder_queries = decoder_queries + self.dropout1(self_attn_features)\n            decoder_queries = self.norm1(decoder_queries)\n\n        x_attn_features = self.multihead_attn(\n            query=decoder_queries,  # (n_query, batch_size, embed_dim)\n            key=encoder_features,  # (total_instances, batch_size, embed_dim)\n            value=encoder_features,  # (total_instances, batch_size, embed_dim)\n        )[0]  # (n_query, batch_size, embed_dim)\n\n        decoder_queries = decoder_queries + self.dropout2(\n            x_attn_features\n        )  # (n_query, batch_size, embed_dim)\n        decoder_queries = self.norm2(\n            decoder_queries\n        )  # (n_query, batch_size, embed_dim)\n        projection = self.linear2(\n            self.dropout(self.activation(self.linear1(decoder_queries)))\n        )  # (n_query, batch_size, embed_dim)\n        decoder_queries = decoder_queries + self.dropout3(\n            projection\n        )  # (n_query, batch_size, embed_dim)\n        decoder_features = self.norm3(decoder_queries)\n\n        return decoder_features  # (n_query, batch_size, embed_dim)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerDecoderLayer.__init__","title":"<code>__init__(d_model=1024, nhead=6, dim_feedforward=1024, dropout=0.1, activation='relu', norm=False, decoder_self_attn=False)</code>","text":"<p>Initialize transformer decoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of features in the decoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads for the decoder.</p> <code>6</code> <code>dim_feedforward</code> <code>int</code> <p>Dimension of the feedforward layers of decoder.</p> <code>1024</code> <code>dropout</code> <code>float</code> <p>Dropout value applied to the output of decoder.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of decoder.</p> <code>False</code> <code>decoder_self_attn</code> <code>bool</code> <p>If True, use decoder self attention</p> <code>False</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    d_model: int = 1024,\n    nhead: int = 6,\n    dim_feedforward: int = 1024,\n    dropout: float = 0.1,\n    activation: str = \"relu\",\n    norm: bool = False,\n    decoder_self_attn: bool = False,\n) -&gt; None:\n    \"\"\"Initialize transformer decoder layer.\n\n    Args:\n        d_model: The number of features in the decoder inputs.\n        nhead: The number of heads for the decoder.\n        dim_feedforward: Dimension of the feedforward layers of decoder.\n        dropout: Dropout value applied to the output of decoder.\n        activation: Activation function to use.\n        norm: If True, normalize output of decoder.\n        decoder_self_attn: If True, use decoder self attention\n    \"\"\"\n    super().__init__()\n\n    self.decoder_self_attn = decoder_self_attn\n\n    self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n    self.linear1 = nn.Linear(d_model, dim_feedforward)\n    self.dropout = nn.Dropout(dropout)\n    self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n    if self.decoder_self_attn:\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n\n    self.norm1 = nn.LayerNorm(d_model) if norm else nn.Identity()\n    self.norm2 = nn.LayerNorm(d_model) if norm else nn.Identity()\n    self.norm3 = nn.LayerNorm(d_model) if norm else nn.Identity()\n\n    self.dropout1 = nn.Dropout(dropout)\n    self.dropout2 = nn.Dropout(dropout)\n    self.dropout3 = nn.Dropout(dropout)\n\n    self.activation = _get_activation_fn(activation)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.transformer.TransformerDecoderLayer.forward","title":"<code>forward(decoder_queries, encoder_features, ref_pos_emb=None, query_pos_emb=None)</code>","text":"<p>Execute forward pass of decoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_queries</code> <code>Tensor</code> <p>Target sequence for decoder to generate (n_query, batch_size, embed_dim).</p> required <code>encoder_features</code> <code>Tensor</code> <p>Output from encoder, that decoder uses to attend to relevant parts of input sequence (total_instances, batch_size, embed_dim)</p> required <code>ref_pos_emb</code> <code>Tensor | None</code> <p>The input positional embedding tensor of shape (n_query, embed_dim).</p> <code>None</code> <code>query_pos_emb</code> <code>Tensor | None</code> <p>The target positional embedding of shape (n_query, embed_dim)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self,\n    decoder_queries: torch.Tensor,\n    encoder_features: torch.Tensor,\n    ref_pos_emb: torch.Tensor | None = None,\n    query_pos_emb: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Execute forward pass of decoder layer.\n\n    Args:\n        decoder_queries: Target sequence for decoder to generate (n_query, batch_size, embed_dim).\n        encoder_features: Output from encoder, that decoder uses to attend to relevant\n            parts of input sequence (total_instances, batch_size, embed_dim)\n        ref_pos_emb: The input positional embedding tensor of shape (n_query, embed_dim).\n        query_pos_emb: The target positional embedding of shape (n_query, embed_dim)\n\n    Returns:\n        The output tensor of shape (n_query, batch_size, embed_dim).\n    \"\"\"\n    if query_pos_emb is None:\n        query_pos_emb = torch.zeros_like(decoder_queries)\n    if ref_pos_emb is None:\n        ref_pos_emb = torch.zeros_like(encoder_features)\n\n    decoder_queries = decoder_queries + query_pos_emb\n    encoder_features = encoder_features + ref_pos_emb\n\n    if self.decoder_self_attn:\n        self_attn_features = self.self_attn(\n            query=decoder_queries, key=decoder_queries, value=decoder_queries\n        )[0]\n        decoder_queries = decoder_queries + self.dropout1(self_attn_features)\n        decoder_queries = self.norm1(decoder_queries)\n\n    x_attn_features = self.multihead_attn(\n        query=decoder_queries,  # (n_query, batch_size, embed_dim)\n        key=encoder_features,  # (total_instances, batch_size, embed_dim)\n        value=encoder_features,  # (total_instances, batch_size, embed_dim)\n    )[0]  # (n_query, batch_size, embed_dim)\n\n    decoder_queries = decoder_queries + self.dropout2(\n        x_attn_features\n    )  # (n_query, batch_size, embed_dim)\n    decoder_queries = self.norm2(\n        decoder_queries\n    )  # (n_query, batch_size, embed_dim)\n    projection = self.linear2(\n        self.dropout(self.activation(self.linear1(decoder_queries)))\n    )  # (n_query, batch_size, embed_dim)\n    decoder_queries = decoder_queries + self.dropout3(\n        projection\n    )  # (n_query, batch_size, embed_dim)\n    decoder_features = self.norm3(decoder_queries)\n\n    return decoder_features  # (n_query, batch_size, embed_dim)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.attention_head.ATTWeightHead","title":"<code>dreem.models.attention_head.ATTWeightHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Single attention head.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize an instance of ATTWeightHead.</p> <code>forward</code> <p>Compute the attention weights of a query tensor using the key tensor.</p> Source code in <code>dreem/models/attention_head.py</code> <pre><code>class ATTWeightHead(torch.nn.Module):\n    \"\"\"Single attention head.\"\"\"\n\n    def __init__(\n        self,\n        feature_dim: int,\n        num_layers: int,\n        dropout: float,\n    ):\n        \"\"\"Initialize an instance of ATTWeightHead.\n\n        Args:\n            feature_dim: The dimensionality of input features.\n            num_layers: The number of hidden layers in the MLP.\n            dropout: Dropout probability.\n        \"\"\"\n        super().__init__()\n\n        self.q_proj = MLP(feature_dim, feature_dim, feature_dim, num_layers, dropout)\n        self.k_proj = MLP(feature_dim, feature_dim, feature_dim, num_layers, dropout)\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the attention weights of a query tensor using the key tensor.\n\n        Args:\n            query: Input tensor of shape (batch_size, num_frame_instances, feature_dim).\n            key: Input tensor of shape (batch_size, num_window_instances, feature_dim).\n\n        Returns:\n            Output tensor of shape\n            (batch_size, num_frame_instances, num_window_instances).\n        \"\"\"\n        k = self.k_proj(key)\n        q = self.q_proj(query)\n        attn_weights = torch.bmm(q, k.transpose(1, 2))\n\n        return attn_weights  # (B, N_t, N)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.attention_head.ATTWeightHead.__init__","title":"<code>__init__(feature_dim, num_layers, dropout)</code>","text":"<p>Initialize an instance of ATTWeightHead.</p> <p>Parameters:</p> Name Type Description Default <code>feature_dim</code> <code>int</code> <p>The dimensionality of input features.</p> required <code>num_layers</code> <code>int</code> <p>The number of hidden layers in the MLP.</p> required <code>dropout</code> <code>float</code> <p>Dropout probability.</p> required Source code in <code>dreem/models/attention_head.py</code> <pre><code>def __init__(\n    self,\n    feature_dim: int,\n    num_layers: int,\n    dropout: float,\n):\n    \"\"\"Initialize an instance of ATTWeightHead.\n\n    Args:\n        feature_dim: The dimensionality of input features.\n        num_layers: The number of hidden layers in the MLP.\n        dropout: Dropout probability.\n    \"\"\"\n    super().__init__()\n\n    self.q_proj = MLP(feature_dim, feature_dim, feature_dim, num_layers, dropout)\n    self.k_proj = MLP(feature_dim, feature_dim, feature_dim, num_layers, dropout)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.attention_head.ATTWeightHead.forward","title":"<code>forward(query, key)</code>","text":"<p>Compute the attention weights of a query tensor using the key tensor.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, num_frame_instances, feature_dim).</p> required <code>key</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, num_window_instances, feature_dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, num_frame_instances, num_window_instances).</p> Source code in <code>dreem/models/attention_head.py</code> <pre><code>def forward(\n    self,\n    query: torch.Tensor,\n    key: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the attention weights of a query tensor using the key tensor.\n\n    Args:\n        query: Input tensor of shape (batch_size, num_frame_instances, feature_dim).\n        key: Input tensor of shape (batch_size, num_window_instances, feature_dim).\n\n    Returns:\n        Output tensor of shape\n        (batch_size, num_frame_instances, num_window_instances).\n    \"\"\"\n    k = self.k_proj(key)\n    q = self.q_proj(query)\n    attn_weights = torch.bmm(q, k.transpose(1, 2))\n\n    return attn_weights  # (B, N_t, N)\n</code></pre>"},{"location":"models/model_parts/#dreem.models.mlp.MLP","title":"<code>dreem.models.mlp.MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>Multi-Layer Perceptron (MLP) module.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize MLP.</p> <code>forward</code> <p>Forward pass of the MLP.</p> Source code in <code>dreem/models/mlp.py</code> <pre><code>class MLP(torch.nn.Module):\n    \"\"\"Multi-Layer Perceptron (MLP) module.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        output_dim: int,\n        num_layers: int,\n        dropout: float = 0.0,\n    ):\n        \"\"\"Initialize MLP.\n\n        Args:\n            input_dim: Dimensionality of the input features.\n            hidden_dim: Number of units in the hidden layers.\n            output_dim: Dimensionality of the output features.\n            num_layers: Number of hidden layers.\n            dropout: Dropout probability.\n        \"\"\"\n        super().__init__()\n\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        if self.num_layers &gt; 0:\n            h = [hidden_dim] * (num_layers - 1)\n            self.layers = torch.nn.ModuleList(\n                [\n                    torch.nn.Linear(n, k)\n                    for n, k in zip([input_dim] + h, h + [output_dim])\n                ]\n            )\n            if self.dropout &gt; 0.0:\n                self.dropouts = torch.nn.ModuleList(\n                    [torch.nn.Dropout(dropout) for _ in range(self.num_layers - 1)]\n                )\n        else:\n            self.layers = []\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of the MLP.\n\n        Args:\n            x: Input tensor of shape (batch_size, num_instances, input_dim).\n\n        Returns:\n            Output tensor of shape (batch_size, num_instances, output_dim).\n        \"\"\"\n        for i, layer in enumerate(self.layers):\n            x = F.relu(layer(x)) if i &lt; self.num_layers - 1 else layer(x)\n            if i &lt; self.num_layers - 1 and self.dropout &gt; 0.0:\n                x = self.dropouts[i](x)\n\n        return x\n</code></pre>"},{"location":"models/model_parts/#dreem.models.mlp.MLP.__init__","title":"<code>__init__(input_dim, hidden_dim, output_dim, num_layers, dropout=0.0)</code>","text":"<p>Initialize MLP.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of the input features.</p> required <code>hidden_dim</code> <code>int</code> <p>Number of units in the hidden layers.</p> required <code>output_dim</code> <code>int</code> <p>Dimensionality of the output features.</p> required <code>num_layers</code> <code>int</code> <p>Number of hidden layers.</p> required <code>dropout</code> <code>float</code> <p>Dropout probability.</p> <code>0.0</code> Source code in <code>dreem/models/mlp.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    hidden_dim: int,\n    output_dim: int,\n    num_layers: int,\n    dropout: float = 0.0,\n):\n    \"\"\"Initialize MLP.\n\n    Args:\n        input_dim: Dimensionality of the input features.\n        hidden_dim: Number of units in the hidden layers.\n        output_dim: Dimensionality of the output features.\n        num_layers: Number of hidden layers.\n        dropout: Dropout probability.\n    \"\"\"\n    super().__init__()\n\n    self.num_layers = num_layers\n    self.dropout = dropout\n\n    if self.num_layers &gt; 0:\n        h = [hidden_dim] * (num_layers - 1)\n        self.layers = torch.nn.ModuleList(\n            [\n                torch.nn.Linear(n, k)\n                for n, k in zip([input_dim] + h, h + [output_dim])\n            ]\n        )\n        if self.dropout &gt; 0.0:\n            self.dropouts = torch.nn.ModuleList(\n                [torch.nn.Dropout(dropout) for _ in range(self.num_layers - 1)]\n            )\n    else:\n        self.layers = []\n</code></pre>"},{"location":"models/model_parts/#dreem.models.mlp.MLP.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the MLP.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, num_instances, input_dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, num_instances, output_dim).</p> Source code in <code>dreem/models/mlp.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the MLP.\n\n    Args:\n        x: Input tensor of shape (batch_size, num_instances, input_dim).\n\n    Returns:\n        Output tensor of shape (batch_size, num_instances, output_dim).\n    \"\"\"\n    for i, layer in enumerate(self.layers):\n        x = F.relu(layer(x)) if i &lt; self.num_layers - 1 else layer(x)\n        if i &lt; self.num_layers - 1 and self.dropout &gt; 0.0:\n            x = self.dropouts[i](x)\n\n    return x\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>dreem<ul> <li>cli</li> <li>datasets<ul> <li>base_dataset</li> <li>cell_tracking_dataset</li> <li>data_utils</li> <li>microscopy_dataset</li> <li>sleap_dataset</li> <li>tracking_dataset</li> </ul> </li> <li>inference<ul> <li>batch_tracker</li> <li>boxes</li> <li>eval</li> <li>metrics</li> <li>post_processing</li> <li>track</li> <li>track_queue</li> <li>tracker</li> </ul> </li> <li>io<ul> <li>association_matrix</li> <li>config</li> <li>frame</li> <li>instance</li> <li>track</li> <li>visualize</li> </ul> </li> <li>models<ul> <li>attention_head</li> <li>embedding</li> <li>global_tracking_transformer</li> <li>gtr_runner</li> <li>mlp</li> <li>model_utils</li> <li>transformer</li> <li>visual_encoder</li> </ul> </li> <li>training<ul> <li>losses</li> <li>train</li> </ul> </li> <li>version</li> </ul> </li> </ul>"},{"location":"reference/dreem/","title":"dreem","text":""},{"location":"reference/dreem/#dreem","title":"<code>dreem</code>","text":"<p>Top-level package for dreem.</p> <p>Modules:</p> Name Description <code>cli</code> <p>This module contains the command line interfaces for the dreem package.</p> <code>datasets</code> <p>Data loading and preprocessing.</p> <code>inference</code> <p>Tracking Inference using GTR Model.</p> <code>io</code> <p>Module containing input/output data structures for easy storage and manipulation.</p> <code>models</code> <p>Model architectures and layers.</p> <code>training</code> <p>Initialize training module.</p> <code>version</code> <p>Central location for version information.</p> <p>Classes:</p> Name Description <code>AssociationMatrix</code> <p>Class representing the associations between detections.</p> <code>Config</code> <p>Class handling loading components based on config params.</p> <code>Frame</code> <p>Data structure containing metadata for a single frame of a video.</p> <code>GTRRunner</code> <p>A lightning wrapper around GTR model.</p> <code>GlobalTrackingTransformer</code> <p>Modular GTR model composed of visual encoder + transformer used for tracking.</p> <code>Instance</code> <p>Class representing a single instance to be tracked.</p> <code>Tracker</code> <p>Tracker class used for assignment based on sliding inference from GTR.</p> <code>Transformer</code> <p>Transformer class.</p> <code>VisualEncoder</code> <p>Class wrapping around a visual feature extractor backbone.</p> <p>Functions:</p> Name Description <code>annotate_video</code> <p>Annotate video frames with labels.</p>"},{"location":"reference/dreem/#dreem.AssociationMatrix","title":"<code>AssociationMatrix</code>","text":"<p>Class representing the associations between detections.</p> <p>Attributes:</p> Name Type Description <code>matrix</code> <code>ndarray | Tensor</code> <p>the <code>n_query x n_ref</code> association matrix`</p> <code>ref_instances</code> <code>list[Instance]</code> <p>all instances used to associate against.</p> <code>query_instances</code> <code>list[Instance]</code> <p>query instances that were associated against ref instances.</p> <p>Methods:</p> Name Description <code>__getindices__</code> <p>Get the indices of the instance for lookup.</p> <code>__getitem__</code> <p>Get elements of the association matrix.</p> <code>__repr__</code> <p>Get the string representation of the Association Matrix.</p> <code>get_tracks</code> <p>Group instances by track.</p> <code>numpy</code> <p>Convert association matrix to a numpy array.</p> <code>reduce</code> <p>Aggregate the association matrix by specified dimensions and grouping.</p> <code>to</code> <p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <code>to_dataframe</code> <p>Convert the association matrix to a pandas DataFrame.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>@attrs.define\nclass AssociationMatrix:\n    \"\"\"Class representing the associations between detections.\n\n    Attributes:\n        matrix: the `n_query x n_ref` association matrix`\n        ref_instances: all instances used to associate against.\n        query_instances: query instances that were associated against ref instances.\n    \"\"\"\n\n    matrix: np.ndarray | torch.Tensor\n    ref_instances: list[Instance] = attrs.field()\n    query_instances: list[Instance] = attrs.field()\n\n    @ref_instances.validator\n    def _check_ref_instances(self, attribute, value):\n        \"\"\"Check to ensure that the number of association matrix columns and reference instances match.\n\n        Args:\n            attribute: The ref instances.\n            value: the list of ref instances.\n\n        Raises:\n            ValueError if the number of columns and reference instances don't match.\n        \"\"\"\n        if len(value) != self.matrix.shape[-1]:\n            raise ValueError(\n                (\n                    \"Ref instances must equal number of columns in Association matrix\"\n                    f\"Found {len(value)} ref instances but {self.matrix.shape[-1]} columns.\"\n                )\n            )\n\n    @query_instances.validator\n    def _check_query_instances(self, attribute, value):\n        \"\"\"Check to ensure that the number of association matrix rows and query instances match.\n\n        Args:\n            attribute: The query instances.\n            value: the list of query instances.\n\n        Raises:\n            ValueError if the number of rows and query instances don't match.\n        \"\"\"\n        if len(value) != self.matrix.shape[0]:\n            raise ValueError(\n                (\n                    \"Query instances must equal number of rows in Association matrix\"\n                    f\"Found {len(value)} query instances but {self.matrix.shape[0]} rows.\"\n                )\n            )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get the string representation of the Association Matrix.\n\n        Returns:\n            the string representation of the association matrix.\n        \"\"\"\n        return (\n            f\"AssociationMatrix({self.matrix},\"\n            f\"query_instances={len(self.query_instances)},\"\n            f\"ref_instances={len(self.ref_instances)})\"\n        )\n\n    def numpy(self) -&gt; np.ndarray:\n        \"\"\"Convert association matrix to a numpy array.\n\n        Returns:\n            The association matrix as a numpy array.\n        \"\"\"\n        if isinstance(self.matrix, torch.Tensor):\n            return self.matrix.detach().cpu().numpy()\n        return self.matrix\n\n    def to_dataframe(\n        self, row_labels: str = \"gt\", col_labels: str = \"gt\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Convert the association matrix to a pandas DataFrame.\n\n        Args:\n            row_labels: How to label the rows(queries).\n                If list, then must match # of rows/queries\n                If `\"gt\"` then label by gt track id.\n                If `\"pred\"` then label by pred track id.\n                Otherwise label by the query_instance indices\n            col_labels: How to label the columns(references).\n                If list, then must match # of columns/refs\n                If `\"gt\"` then label by gt track id.\n                If `\"pred\"` then label by pred track id.\n                Otherwise label by the ref_instance indices\n\n        Returns:\n            The association matrix as a pandas dataframe.\n        \"\"\"\n        matrix = self.numpy()\n\n        if not isinstance(row_labels, str):\n            if len(row_labels) == len(self.query_instances):\n                row_inds = row_labels\n\n            else:\n                raise ValueError(\n                    (\n                        \"Mismatched # of rows and labels!\",\n                        f\"Found {len(row_labels)} with {len(self.query_instances)} rows\",\n                    )\n                )\n\n        else:\n            if row_labels == \"gt\":\n                row_inds = [\n                    instance.gt_track_id.item() for instance in self.query_instances\n                ]\n\n            elif row_labels == \"pred\":\n                row_inds = [\n                    instance.pred_track_id.item() for instance in self.query_instances\n                ]\n\n            else:\n                row_inds = np.arange(len(self.query_instances))\n\n        if not isinstance(col_labels, str):\n            if len(col_labels) == len(self.ref_instances):\n                col_inds = col_labels\n\n            else:\n                raise ValueError(\n                    (\n                        \"Mismatched # of columns and labels!\",\n                        f\"Found {len(col_labels)} with {len(self.ref_instances)} columns\",\n                    )\n                )\n\n        else:\n            if col_labels == \"gt\":\n                col_inds = [\n                    instance.gt_track_id.item() for instance in self.ref_instances\n                ]\n\n            elif col_labels == \"pred\":\n                col_inds = [\n                    instance.pred_track_id.item() for instance in self.ref_instances\n                ]\n\n            else:\n                col_inds = np.arange(len(self.ref_instances))\n\n        asso_df = pd.DataFrame(matrix, index=row_inds, columns=col_inds)\n\n        return asso_df\n\n    def reduce(\n        self,\n        row_dims: str = \"instance\",\n        col_dims: str = \"track\",\n        row_grouping: str | None = None,\n        col_grouping: str = \"pred\",\n        reduce_method: callable = np.sum,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Aggregate the association matrix by specified dimensions and grouping.\n\n        Args:\n           row_dims: A str indicating how to what dimensions to reduce rows to.\n                Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).\n           col_dims: A str indicating how to dimensions to reduce rows to.\n                Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)\n           row_grouping: A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".\n           col_grouping: A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".\n           reduce_method: A callable function that operates on numpy matrices and can take an `axis` arg for reducing.\n\n        Returns:\n            The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.\n        \"\"\"\n        n_rows = len(self.query_instances)\n        n_cols = len(self.ref_instances)\n\n        col_tracks = {-1: self.ref_instances}\n        row_tracks = {-1: self.query_instances}\n\n        col_inds = [i for i in range(len(self.ref_instances))]\n        row_inds = [i for i in range(len(self.query_instances))]\n\n        if col_dims == \"track\":\n            col_tracks = self.get_tracks(self.ref_instances, col_grouping)\n            col_inds = list(col_tracks.keys())\n            n_cols = len(col_inds)\n\n        if row_dims == \"track\":\n            row_tracks = self.get_tracks(self.query_instances, row_grouping)\n            row_inds = list(row_tracks.keys())\n            n_rows = len(row_inds)\n\n        reduced_matrix = []\n        for row_track, row_instances in row_tracks.items():\n            for col_track, col_instances in col_tracks.items():\n                asso_matrix = self[row_instances, col_instances]\n\n                if col_dims == \"track\":\n                    asso_matrix = reduce_method(asso_matrix, axis=1)\n\n                if row_dims == \"track\":\n                    asso_matrix = reduce_method(asso_matrix, axis=0)\n\n                reduced_matrix.append(asso_matrix)\n\n        reduced_matrix = np.array(reduced_matrix).reshape(n_cols, n_rows).T\n\n        return pd.DataFrame(reduced_matrix, index=row_inds, columns=col_inds)\n\n    def __getitem__(\n        self, inds: tuple[int | Instance | list[int | Instance]]\n    ) -&gt; np.ndarray:\n        \"\"\"Get elements of the association matrix.\n\n        Args:\n            inds: A tuple of query indices and reference indices.\n                Indices can be either:\n                    A single instance or integer.\n                    A list of instances or integers.\n\n        Returns:\n            An np.ndarray containing the elements requested.\n        \"\"\"\n        query_inst, ref_inst = inds\n\n        query_ind = self.__getindices__(query_inst, self.query_instances)\n        ref_ind = self.__getindices__(ref_inst, self.ref_instances)\n\n        try:\n            return self.numpy()[query_ind[:, None], ref_ind].squeeze()\n        except IndexError as e:\n            logger.exception(f\"Query_insts: {type(query_inst)}\")\n            logger.exception(f\"Query_inds: {query_ind}\")\n            logger.exception(f\"Ref_insts: {type(ref_inst)}\")\n            logger.exception(f\"Ref_ind: {ref_ind}\")\n            logger.exception(e)\n            raise (e)\n\n    def __getindices__(\n        self,\n        instance: Instance | int | np.typing.ArrayLike,\n        instance_lookup: list[Instance],\n    ) -&gt; np.ndarray:\n        \"\"\"Get the indices of the instance for lookup.\n\n        Args:\n            instance: The instance(s) to be retrieved\n                Can either be a single int/instance or a list of int/instances\n            instance_lookup: A list of Instances to be used to retrieve indices\n\n        Returns:\n            A np array of indices.\n        \"\"\"\n        if isinstance(instance, Instance):\n            ind = np.array([instance_lookup.index(instance)])\n\n        elif instance is None:\n            ind = np.arange(len(instance_lookup))\n\n        elif np.isscalar(instance):\n            ind = np.array([instance])\n\n        else:\n            instances = instance\n            if not [isinstance(inst, (Instance, int)) for inst in instance]:\n                raise ValueError(\n                    f\"List of indices must be `int` or `Instance`. Found {set([type(inst) for inst in instance])}\"\n                )\n            ind = np.array(\n                [\n                    (\n                        instance_lookup.index(instance)\n                        if isinstance(instance, Instance)\n                        else instance\n                    )\n                    for instance in instances\n                ]\n            )\n\n        return ind\n\n    def get_tracks(\n        self, instances: list[\"Instance\"], label: str = \"pred\"\n    ) -&gt; dict[int, list[\"Instance\"]]:\n        \"\"\"Group instances by track.\n\n        Args:\n            instances: The list of instances to group\n            label: the track id type to group by. Either `pred` or `gt`.\n\n        Returns:\n            A dictionary of track_id:instances\n        \"\"\"\n        if label == \"pred\":\n            traj_ids = set([instance.pred_track_id.item() for instance in instances])\n            traj = {\n                track_id: [\n                    instance\n                    for instance in instances\n                    if instance.pred_track_id.item() == track_id\n                ]\n                for track_id in traj_ids\n            }\n\n        elif label == \"gt\":\n            traj_ids = set(\n                [instance.gt_track_id.item() for instance in self.ref_instances]\n            )\n            traj = {\n                track_id: [\n                    instance\n                    for instance in self.ref_instances\n                    if instance.gt_track_id.item() == track_id\n                ]\n                for track_id in traj_ids\n            }\n\n        else:\n            raise ValueError(f\"Unsupported label '{label}'. Expected 'pred' or 'gt'.\")\n\n        return traj\n\n    def to(self, map_location: str | torch.device) -&gt; Self:\n        \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n        Args:\n            map_location: Either the device or dtype for the instance to be moved.\n\n        Returns:\n            self: reference to the instance moved to correct device/dtype.\n        \"\"\"\n        self.matrix = self.matrix.to(map_location)\n        self.ref_instances = [\n            instance.to(map_location) for instance in self.ref_instances\n        ]\n        self.query_instances = [\n            instance.to(map_location) for instance in self.query_instances\n        ]\n\n        return self\n</code></pre>"},{"location":"reference/dreem/#dreem.AssociationMatrix.__getindices__","title":"<code>__getindices__(instance, instance_lookup)</code>","text":"<p>Get the indices of the instance for lookup.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>Instance | int | ArrayLike</code> <p>The instance(s) to be retrieved Can either be a single int/instance or a list of int/instances</p> required <code>instance_lookup</code> <code>list[Instance]</code> <p>A list of Instances to be used to retrieve indices</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A np array of indices.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __getindices__(\n    self,\n    instance: Instance | int | np.typing.ArrayLike,\n    instance_lookup: list[Instance],\n) -&gt; np.ndarray:\n    \"\"\"Get the indices of the instance for lookup.\n\n    Args:\n        instance: The instance(s) to be retrieved\n            Can either be a single int/instance or a list of int/instances\n        instance_lookup: A list of Instances to be used to retrieve indices\n\n    Returns:\n        A np array of indices.\n    \"\"\"\n    if isinstance(instance, Instance):\n        ind = np.array([instance_lookup.index(instance)])\n\n    elif instance is None:\n        ind = np.arange(len(instance_lookup))\n\n    elif np.isscalar(instance):\n        ind = np.array([instance])\n\n    else:\n        instances = instance\n        if not [isinstance(inst, (Instance, int)) for inst in instance]:\n            raise ValueError(\n                f\"List of indices must be `int` or `Instance`. Found {set([type(inst) for inst in instance])}\"\n            )\n        ind = np.array(\n            [\n                (\n                    instance_lookup.index(instance)\n                    if isinstance(instance, Instance)\n                    else instance\n                )\n                for instance in instances\n            ]\n        )\n\n    return ind\n</code></pre>"},{"location":"reference/dreem/#dreem.AssociationMatrix.__getitem__","title":"<code>__getitem__(inds)</code>","text":"<p>Get elements of the association matrix.</p> <p>Parameters:</p> Name Type Description Default <code>inds</code> <code>tuple[int | Instance | list[int | Instance]]</code> <p>A tuple of query indices and reference indices. Indices can be either:     A single instance or integer.     A list of instances or integers.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>An np.ndarray containing the elements requested.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __getitem__(\n    self, inds: tuple[int | Instance | list[int | Instance]]\n) -&gt; np.ndarray:\n    \"\"\"Get elements of the association matrix.\n\n    Args:\n        inds: A tuple of query indices and reference indices.\n            Indices can be either:\n                A single instance or integer.\n                A list of instances or integers.\n\n    Returns:\n        An np.ndarray containing the elements requested.\n    \"\"\"\n    query_inst, ref_inst = inds\n\n    query_ind = self.__getindices__(query_inst, self.query_instances)\n    ref_ind = self.__getindices__(ref_inst, self.ref_instances)\n\n    try:\n        return self.numpy()[query_ind[:, None], ref_ind].squeeze()\n    except IndexError as e:\n        logger.exception(f\"Query_insts: {type(query_inst)}\")\n        logger.exception(f\"Query_inds: {query_ind}\")\n        logger.exception(f\"Ref_insts: {type(ref_inst)}\")\n        logger.exception(f\"Ref_ind: {ref_ind}\")\n        logger.exception(e)\n        raise (e)\n</code></pre>"},{"location":"reference/dreem/#dreem.AssociationMatrix.__repr__","title":"<code>__repr__()</code>","text":"<p>Get the string representation of the Association Matrix.</p> <p>Returns:</p> Type Description <code>str</code> <p>the string representation of the association matrix.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get the string representation of the Association Matrix.\n\n    Returns:\n        the string representation of the association matrix.\n    \"\"\"\n    return (\n        f\"AssociationMatrix({self.matrix},\"\n        f\"query_instances={len(self.query_instances)},\"\n        f\"ref_instances={len(self.ref_instances)})\"\n    )\n</code></pre>"},{"location":"reference/dreem/#dreem.AssociationMatrix.get_tracks","title":"<code>get_tracks(instances, label='pred')</code>","text":"<p>Group instances by track.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[Instance]</code> <p>The list of instances to group</p> required <code>label</code> <code>str</code> <p>the track id type to group by. Either <code>pred</code> or <code>gt</code>.</p> <code>'pred'</code> <p>Returns:</p> Type Description <code>dict[int, list[Instance]]</code> <p>A dictionary of track_id:instances</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def get_tracks(\n    self, instances: list[\"Instance\"], label: str = \"pred\"\n) -&gt; dict[int, list[\"Instance\"]]:\n    \"\"\"Group instances by track.\n\n    Args:\n        instances: The list of instances to group\n        label: the track id type to group by. Either `pred` or `gt`.\n\n    Returns:\n        A dictionary of track_id:instances\n    \"\"\"\n    if label == \"pred\":\n        traj_ids = set([instance.pred_track_id.item() for instance in instances])\n        traj = {\n            track_id: [\n                instance\n                for instance in instances\n                if instance.pred_track_id.item() == track_id\n            ]\n            for track_id in traj_ids\n        }\n\n    elif label == \"gt\":\n        traj_ids = set(\n            [instance.gt_track_id.item() for instance in self.ref_instances]\n        )\n        traj = {\n            track_id: [\n                instance\n                for instance in self.ref_instances\n                if instance.gt_track_id.item() == track_id\n            ]\n            for track_id in traj_ids\n        }\n\n    else:\n        raise ValueError(f\"Unsupported label '{label}'. Expected 'pred' or 'gt'.\")\n\n    return traj\n</code></pre>"},{"location":"reference/dreem/#dreem.AssociationMatrix.numpy","title":"<code>numpy()</code>","text":"<p>Convert association matrix to a numpy array.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The association matrix as a numpy array.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def numpy(self) -&gt; np.ndarray:\n    \"\"\"Convert association matrix to a numpy array.\n\n    Returns:\n        The association matrix as a numpy array.\n    \"\"\"\n    if isinstance(self.matrix, torch.Tensor):\n        return self.matrix.detach().cpu().numpy()\n    return self.matrix\n</code></pre>"},{"location":"reference/dreem/#dreem.AssociationMatrix.reduce","title":"<code>reduce(row_dims='instance', col_dims='track', row_grouping=None, col_grouping='pred', reduce_method=np.sum)</code>","text":"<p>Aggregate the association matrix by specified dimensions and grouping.</p> <p>Parameters:</p> Name Type Description Default <code>row_dims</code> <code>str</code> <p>A str indicating how to what dimensions to reduce rows to.   Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).</p> <code>'instance'</code> <code>col_dims</code> <code>str</code> <p>A str indicating how to dimensions to reduce rows to.   Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)</p> <code>'track'</code> <code>row_grouping</code> <code>str | None</code> <p>A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".</p> <code>None</code> <code>col_grouping</code> <code>str</code> <p>A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".</p> <code>'pred'</code> <code>reduce_method</code> <code>callable</code> <p>A callable function that operates on numpy matrices and can take an <code>axis</code> arg for reducing.</p> <code>sum</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def reduce(\n    self,\n    row_dims: str = \"instance\",\n    col_dims: str = \"track\",\n    row_grouping: str | None = None,\n    col_grouping: str = \"pred\",\n    reduce_method: callable = np.sum,\n) -&gt; pd.DataFrame:\n    \"\"\"Aggregate the association matrix by specified dimensions and grouping.\n\n    Args:\n       row_dims: A str indicating how to what dimensions to reduce rows to.\n            Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).\n       col_dims: A str indicating how to dimensions to reduce rows to.\n            Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)\n       row_grouping: A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".\n       col_grouping: A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".\n       reduce_method: A callable function that operates on numpy matrices and can take an `axis` arg for reducing.\n\n    Returns:\n        The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.\n    \"\"\"\n    n_rows = len(self.query_instances)\n    n_cols = len(self.ref_instances)\n\n    col_tracks = {-1: self.ref_instances}\n    row_tracks = {-1: self.query_instances}\n\n    col_inds = [i for i in range(len(self.ref_instances))]\n    row_inds = [i for i in range(len(self.query_instances))]\n\n    if col_dims == \"track\":\n        col_tracks = self.get_tracks(self.ref_instances, col_grouping)\n        col_inds = list(col_tracks.keys())\n        n_cols = len(col_inds)\n\n    if row_dims == \"track\":\n        row_tracks = self.get_tracks(self.query_instances, row_grouping)\n        row_inds = list(row_tracks.keys())\n        n_rows = len(row_inds)\n\n    reduced_matrix = []\n    for row_track, row_instances in row_tracks.items():\n        for col_track, col_instances in col_tracks.items():\n            asso_matrix = self[row_instances, col_instances]\n\n            if col_dims == \"track\":\n                asso_matrix = reduce_method(asso_matrix, axis=1)\n\n            if row_dims == \"track\":\n                asso_matrix = reduce_method(asso_matrix, axis=0)\n\n            reduced_matrix.append(asso_matrix)\n\n    reduced_matrix = np.array(reduced_matrix).reshape(n_cols, n_rows).T\n\n    return pd.DataFrame(reduced_matrix, index=row_inds, columns=col_inds)\n</code></pre>"},{"location":"reference/dreem/#dreem.AssociationMatrix.to","title":"<code>to(map_location)</code>","text":"<p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <p>Parameters:</p> Name Type Description Default <code>map_location</code> <code>str | device</code> <p>Either the device or dtype for the instance to be moved.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>Self</code> <p>reference to the instance moved to correct device/dtype.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def to(self, map_location: str | torch.device) -&gt; Self:\n    \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n    Args:\n        map_location: Either the device or dtype for the instance to be moved.\n\n    Returns:\n        self: reference to the instance moved to correct device/dtype.\n    \"\"\"\n    self.matrix = self.matrix.to(map_location)\n    self.ref_instances = [\n        instance.to(map_location) for instance in self.ref_instances\n    ]\n    self.query_instances = [\n        instance.to(map_location) for instance in self.query_instances\n    ]\n\n    return self\n</code></pre>"},{"location":"reference/dreem/#dreem.AssociationMatrix.to_dataframe","title":"<code>to_dataframe(row_labels='gt', col_labels='gt')</code>","text":"<p>Convert the association matrix to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>row_labels</code> <code>str</code> <p>How to label the rows(queries). If list, then must match # of rows/queries If <code>\"gt\"</code> then label by gt track id. If <code>\"pred\"</code> then label by pred track id. Otherwise label by the query_instance indices</p> <code>'gt'</code> <code>col_labels</code> <code>str</code> <p>How to label the columns(references). If list, then must match # of columns/refs If <code>\"gt\"</code> then label by gt track id. If <code>\"pred\"</code> then label by pred track id. Otherwise label by the ref_instance indices</p> <code>'gt'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The association matrix as a pandas dataframe.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def to_dataframe(\n    self, row_labels: str = \"gt\", col_labels: str = \"gt\"\n) -&gt; pd.DataFrame:\n    \"\"\"Convert the association matrix to a pandas DataFrame.\n\n    Args:\n        row_labels: How to label the rows(queries).\n            If list, then must match # of rows/queries\n            If `\"gt\"` then label by gt track id.\n            If `\"pred\"` then label by pred track id.\n            Otherwise label by the query_instance indices\n        col_labels: How to label the columns(references).\n            If list, then must match # of columns/refs\n            If `\"gt\"` then label by gt track id.\n            If `\"pred\"` then label by pred track id.\n            Otherwise label by the ref_instance indices\n\n    Returns:\n        The association matrix as a pandas dataframe.\n    \"\"\"\n    matrix = self.numpy()\n\n    if not isinstance(row_labels, str):\n        if len(row_labels) == len(self.query_instances):\n            row_inds = row_labels\n\n        else:\n            raise ValueError(\n                (\n                    \"Mismatched # of rows and labels!\",\n                    f\"Found {len(row_labels)} with {len(self.query_instances)} rows\",\n                )\n            )\n\n    else:\n        if row_labels == \"gt\":\n            row_inds = [\n                instance.gt_track_id.item() for instance in self.query_instances\n            ]\n\n        elif row_labels == \"pred\":\n            row_inds = [\n                instance.pred_track_id.item() for instance in self.query_instances\n            ]\n\n        else:\n            row_inds = np.arange(len(self.query_instances))\n\n    if not isinstance(col_labels, str):\n        if len(col_labels) == len(self.ref_instances):\n            col_inds = col_labels\n\n        else:\n            raise ValueError(\n                (\n                    \"Mismatched # of columns and labels!\",\n                    f\"Found {len(col_labels)} with {len(self.ref_instances)} columns\",\n                )\n            )\n\n    else:\n        if col_labels == \"gt\":\n            col_inds = [\n                instance.gt_track_id.item() for instance in self.ref_instances\n            ]\n\n        elif col_labels == \"pred\":\n            col_inds = [\n                instance.pred_track_id.item() for instance in self.ref_instances\n            ]\n\n        else:\n            col_inds = np.arange(len(self.ref_instances))\n\n    asso_df = pd.DataFrame(matrix, index=row_inds, columns=col_inds)\n\n    return asso_df\n</code></pre>"},{"location":"reference/dreem/#dreem.Config","title":"<code>Config</code>","text":"<p>Class handling loading components based on config params.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class with config from hydra/omega conf.</p> <code>__repr__</code> <p>Object representation of config class.</p> <code>__str__</code> <p>Return a string representation of config class.</p> <code>from_yaml</code> <p>Load config directly from yaml.</p> <code>get</code> <p>Get config item.</p> <code>get_checkpointing</code> <p>Getter for lightning checkpointing callback.</p> <code>get_ctc_paths</code> <p>Get file paths from directory. Only for CTC datasets.</p> <code>get_data_paths</code> <p>Get file paths from directory. Only for SLEAP datasets.</p> <code>get_dataloader</code> <p>Getter for dataloader.</p> <code>get_dataset</code> <p>Getter for datasets.</p> <code>get_early_stopping</code> <p>Getter for lightning early stopping callback.</p> <code>get_gtr_runner</code> <p>Get lightning module for training, validation, and inference.</p> <code>get_logger</code> <p>Getter for logging callback.</p> <code>get_loss</code> <p>Getter for loss functions.</p> <code>get_model</code> <p>Getter for gtr model.</p> <code>get_optimizer</code> <p>Getter for optimizer.</p> <code>get_scheduler</code> <p>Getter for lr scheduler.</p> <code>get_tracker_cfg</code> <p>Getter for tracker config params.</p> <code>get_trainer</code> <p>Getter for the lightning trainer.</p> <code>set_hparams</code> <p>Setter function for overwriting specific hparams.</p> <p>Attributes:</p> Name Type Description <code>data_paths</code> <p>Get data paths.</p> Source code in <code>dreem/io/config.py</code> <pre><code>class Config:\n    \"\"\"Class handling loading components based on config params.\"\"\"\n\n    def __init__(self, cfg: DictConfig, params_cfg: DictConfig | None = None):\n        \"\"\"Initialize the class with config from hydra/omega conf.\n\n        First uses `base_param` file then overwrites with specific `params_config`.\n\n        Args:\n            cfg: The `DictConfig` containing all the hyperparameters needed for\n                training/evaluation.\n            params_cfg: The `DictConfig` containing subset of hyperparameters to override.\n                training/evaluation\n        \"\"\"\n        base_cfg = cfg\n        logger.info(f\"Base Config: {cfg}\")\n\n        if \"params_config\" in cfg:\n            params_cfg = OmegaConf.load(cfg.params_config)\n\n        if params_cfg:\n            logger.info(f\"Overwriting base config with {params_cfg}\")\n            with open_dict(base_cfg):\n                self.cfg = OmegaConf.merge(base_cfg, params_cfg)  # merge configs\n        else:\n            self.cfg = cfg\n\n        OmegaConf.set_struct(self.cfg, False)\n\n        self._vid_files = {}\n\n    def __repr__(self):\n        \"\"\"Object representation of config class.\"\"\"\n        return f\"Config({self.cfg})\"\n\n    def __str__(self):\n        \"\"\"Return a string representation of config class.\"\"\"\n        return f\"Config({self.cfg})\"\n\n    @classmethod\n    def from_yaml(cls, base_cfg_path: str, params_cfg_path: str | None = None) -&gt; None:\n        \"\"\"Load config directly from yaml.\n\n        Args:\n            base_cfg_path: path to base config file.\n            params_cfg_path: path to override params.\n        \"\"\"\n        base_cfg = OmegaConf.load(base_cfg_path)\n        params_cfg = OmegaConf.load(params_cfg_path) if params_cfg_path else None\n        return cls(base_cfg, params_cfg)\n\n    def set_hparams(self, hparams: dict) -&gt; bool:\n        \"\"\"Setter function for overwriting specific hparams.\n\n        Useful for changing 1 or 2 hyperparameters such as dataset.\n\n        Args:\n            hparams: A dict containing the hyperparameter to be overwritten and\n                the value to be changed\n\n        Returns:\n            `True` if config is successfully updated, `False` otherwise\n        \"\"\"\n        if hparams == {} or hparams is None:\n            logger.warning(\"Nothing to update!\")\n            return False\n        for hparam, val in hparams.items():\n            try:\n                OmegaConf.update(self.cfg, hparam, val)\n            except Exception as e:\n                logger.exception(f\"Failed to update {hparam} to {val} due to {e}\")\n                return False\n        return True\n\n    def get(self, key: str, default=None, cfg: dict = None):\n        \"\"\"Get config item.\n\n        Args:\n            key: key of item to return\n            default: default value to return if key is missing.\n            cfg: the config dict from which to retrieve an item\n        \"\"\"\n        if cfg is None:\n            cfg = self.cfg\n\n        param = cfg.get(key, default)\n\n        if isinstance(param, DictConfig):\n            param = OmegaConf.to_container(param, resolve=True)\n\n        return param\n\n    def get_model(self) -&gt; \"GlobalTrackingTransformer\":\n        \"\"\"Getter for gtr model.\n\n        Returns:\n            A global tracking transformer with parameters indicated by cfg\n        \"\"\"\n        from dreem.models import GlobalTrackingTransformer, GTRRunner\n\n        model_params = self.get(\"model\", {})\n\n        ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n        if ckpt_path is not None and len(ckpt_path) &gt; 0:\n            return GTRRunner.load_from_checkpoint(ckpt_path).model\n\n        return GlobalTrackingTransformer(**model_params)\n\n    def get_tracker_cfg(self) -&gt; dict:\n        \"\"\"Getter for tracker config params.\n\n        Returns:\n            A dict containing the init params for `Tracker`.\n        \"\"\"\n        return self.get(\"tracker\", {})\n\n    def get_gtr_runner(self, ckpt_path: str | None = None) -&gt; \"GTRRunner\":\n        \"\"\"Get lightning module for training, validation, and inference.\n\n        Args:\n            ckpt_path: path to checkpoint for override\n\n        Returns:\n            a gtr runner model\n        \"\"\"\n        from dreem.models import GTRRunner\n\n        keys = [\"tracker\", \"optimizer\", \"scheduler\", \"loss\", \"runner\", \"model\"]\n        args = [key + \"_cfg\" if key != \"runner\" else key for key in keys]\n\n        params = {}\n        for key, arg in zip(keys, args):\n            sub_params = self.get(key, {})\n\n            if len(sub_params) == 0:\n                logger.warning(\n                    f\"`{key}` not found in config or is empty. Using defaults for {arg}!\"\n                )\n\n            if key == \"runner\":\n                runner_params = sub_params\n                for k, v in runner_params.items():\n                    params[k] = v\n            else:\n                params[arg] = sub_params\n\n        ckpt_path = params[\"model_cfg\"].pop(\"ckpt_path\", None)\n\n        if ckpt_path is not None and ckpt_path != \"\":\n            model = GTRRunner.load_from_checkpoint(\n                ckpt_path, tracker_cfg=params[\"tracker_cfg\"], **runner_params\n            )\n\n        else:\n            model = GTRRunner(**params)\n\n        return model\n\n    def get_ctc_paths(\n        self, list_dir_path: list[str]\n    ) -&gt; tuple[list[str], list[str], list[str]]:\n        \"\"\"Get file paths from directory. Only for CTC datasets.\n\n        Args:\n            list_dir_path: list of directories to search for labels and videos\n\n        Returns:\n            lists of labels file paths and video file paths\n        \"\"\"\n        gt_list = []\n        raw_img_list = []\n        ctc_track_meta = []\n        # user can specify a list of directories, each of which can contain several subdirectories that come in pairs of (dset_name, dset_name_GT/TRA)\n        for dir_path in list_dir_path:\n            for subdir in os.listdir(dir_path):\n                if subdir.endswith(\"_GT\"):\n                    gt_path = os.path.join(dir_path, subdir, \"TRA\")\n                    raw_img_path = os.path.join(dir_path, subdir.replace(\"_GT\", \"\"))\n                    # get filepaths for all tif files in gt_path\n                    gt_list.append(glob.glob(os.path.join(gt_path, \"*.tif\")))\n                    # get filepaths for all tif files in raw_img_path\n                    raw_img_list.append(glob.glob(os.path.join(raw_img_path, \"*.tif\")))\n                    man_track_file = glob.glob(os.path.join(gt_path, \"man_track.txt\"))\n                    if len(man_track_file) &gt; 0:\n                        ctc_track_meta.append(man_track_file[0])\n                    else:\n                        logger.debug(\n                            f\"No man_track.txt file found in {gt_path}. Continuing...\"\n                        )\n                else:\n                    continue\n\n        return gt_list, raw_img_list, ctc_track_meta\n\n    def get_data_paths(self, mode: str, data_cfg: dict) -&gt; tuple[list[str], list[str]]:\n        \"\"\"Get file paths from directory. Only for SLEAP datasets.\n\n        Args:\n            mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n                train, val, or test params for dataset\n            data_cfg: Config for the dataset containing \"dir\" key.\n\n        Returns:\n            lists of labels file paths and video file paths respectively\n        \"\"\"\n        # hack to get around the fact that for test mode, get_data_paths is called before get_dataset.\n        # also, for train/val mode, data_cfg has had the dir key popped through self.get() called in get_dataset()\n        if mode == \"test\":\n            list_dir_path = data_cfg.get(\"dir\", {}).get(\"path\", None)\n            if list_dir_path is None:\n                raise ValueError(\n                    \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n                )\n            self.labels_suffix = data_cfg.get(\"dir\", {}).get(\"labels_suffix\")\n            self.vid_suffix = data_cfg.get(\"dir\", {}).get(\"vid_suffix\")\n        else:\n            list_dir_path = self.data_dirs\n        if not isinstance(list_dir_path, list):\n            list_dir_path = [list_dir_path]\n\n        if self.labels_suffix == \".slp\":\n            label_files = []\n            vid_files = []\n            for dir_path in list_dir_path:\n                logger.debug(f\"Searching `{dir_path}` directory\")\n                labels_path = f\"{dir_path}/*{self.labels_suffix}\"\n                vid_path = f\"{dir_path}/*{self.vid_suffix}\"\n                logger.debug(f\"Searching for labels matching {labels_path}\")\n                label_files.extend(glob.glob(labels_path))\n                logger.debug(f\"Searching for videos matching {vid_path}\")\n                vid_files.extend(glob.glob(vid_path))\n\n        elif self.labels_suffix == \".tif\":\n            label_files, vid_files, ctc_track_meta = self.get_ctc_paths(list_dir_path)\n\n        logger.debug(f\"Found {len(label_files)} labels and {len(vid_files)} videos\")\n\n        # backdoor to set label files directly in the configs (i.e. bypass dir.path)\n        if data_cfg.get(\"slp_files\", None):\n            logger.debug(\"Overriding label files with user provided list\")\n            slp_files = data_cfg.get(\"slp_files\")\n            if len(slp_files) &gt; 0:\n                label_files = slp_files\n        if data_cfg.get(\"video_files\", None):\n            individual_video_files = data_cfg.get(\"video_files\")\n            if len(individual_video_files) &gt; 0:\n                vid_files = individual_video_files\n        return label_files, vid_files\n\n    def get_dataset(\n        self,\n        mode: str,\n        label_files: list[str] | None = None,\n        vid_files: list[str | list[str]] = None,\n    ) -&gt; \"SleapDataset\" | \"CellTrackingDataset\":\n        \"\"\"Getter for datasets.\n\n        Args:\n            mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n                train, val, or test params for dataset\n            label_files: path to label_files for override\n            vid_files: path to vid_files for override\n\n        Returns:\n            Either a `SleapDataset` or `CellTrackingDataset` with params indicated by cfg\n        \"\"\"\n        from dreem.datasets import CellTrackingDataset, SleapDataset\n\n        dataset_params = self.get(\"dataset\")\n        if dataset_params is None:\n            raise KeyError(\"`dataset` key is missing from cfg!\")\n\n        if mode.lower() == \"train\":\n            dataset_params = self.get(\"train_dataset\", {}, dataset_params)\n        elif mode.lower() == \"val\":\n            dataset_params = self.get(\"val_dataset\", {}, dataset_params)\n        elif mode.lower() == \"test\":\n            dataset_params = self.get(\"test_dataset\", {}, dataset_params)\n        else:\n            raise ValueError(\n                \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n            )\n\n        # input validation\n        self.data_dirs = dataset_params.get(\"dir\", {}).get(\"path\", None)\n        self.labels_suffix = dataset_params.get(\"dir\", {}).get(\"labels_suffix\")\n        self.vid_suffix = dataset_params.get(\"dir\", {}).get(\"vid_suffix\")\n        if self.data_dirs is None:\n            raise ValueError(\n                \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n            )\n        if self.labels_suffix is None or self.vid_suffix is None:\n            raise KeyError(\n                f\"Must provide a labels suffix and vid suffix to search for but found {self.labels_suffix} and {self.vid_suffix}\"\n            )\n\n        # infer dataset type from the user provided suffix\n        if self.labels_suffix == \".slp\":\n            # during training, multiple files can be used at once, so label_files is not passed in\n            # during inference, a single label_files string can be passed in as get_data_paths is\n            # called before get_dataset, hence the check\n            if label_files is None or vid_files is None:\n                label_files, vid_files = self.get_data_paths(mode, dataset_params)\n            dataset_params[\"slp_files\"] = label_files\n            dataset_params[\"video_files\"] = vid_files\n            dataset_params[\"data_dirs\"] = self.data_dirs\n            self.data_paths = (mode, vid_files)\n\n            return SleapDataset(**dataset_params)\n\n        elif self.labels_suffix == \".tif\":\n            # for CTC datasets, pass in a list of gt and raw image directories, eaech of which contain tifs\n            ctc_track_meta = None\n            list_dir_path = self.data_dirs  # don't modify self.data_dirs\n            if not isinstance(list_dir_path, list):\n                list_dir_path = [list_dir_path]\n            if label_files is None or vid_files is None:\n                label_files, vid_files, ctc_track_meta = self.get_ctc_paths(\n                    list_dir_path\n                )\n            dataset_params[\"data_dirs\"] = self.data_dirs\n            # extract filepaths of all raw images and gt images (i.e. labelled masks)\n            dataset_params[\"gt_list\"] = label_files\n            dataset_params[\"raw_img_list\"] = vid_files\n            dataset_params[\"ctc_track_meta\"] = ctc_track_meta\n\n            return CellTrackingDataset(**dataset_params)\n\n        else:\n            raise ValueError(\n                \"Could not resolve dataset type from Config! Only .slp (SLEAP) and .tif (Cell Tracking Challenge) data formats are supported.\"\n            )\n\n    @property\n    def data_paths(self):\n        \"\"\"Get data paths.\"\"\"\n        return self._vid_files\n\n    @data_paths.setter\n    def data_paths(self, paths: tuple[str, list[str]]):\n        \"\"\"Set data paths.\n\n        Args:\n            paths: A tuple containing (mode, vid_files)\n        \"\"\"\n        mode, vid_files = paths\n        self._vid_files[mode] = vid_files\n\n    def get_dataloader(\n        self,\n        dataset: \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\",\n        mode: str,\n    ) -&gt; torch.utils.data.DataLoader:\n        \"\"\"Getter for dataloader.\n\n        Args:\n            dataset: the Sleap or Microscopy Dataset used to initialize the dataloader\n            mode: either [\"train\", \"val\", or \"test\"] indicates which dataset\n                config to use\n\n        Returns:\n            A torch dataloader for `dataset` with parameters configured as specified\n        \"\"\"\n        dataloader_params = self.get(\"dataloader\", {})\n        if mode.lower() == \"train\":\n            dataloader_params = self.get(\"train_dataloader\", {}, dataloader_params)\n        elif mode.lower() == \"val\":\n            dataloader_params = self.get(\"val_dataloader\", {}, dataloader_params)\n        elif mode.lower() == \"test\":\n            dataloader_params = self.get(\"test_dataloader\", {}, dataloader_params)\n        else:\n            raise ValueError(\n                \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n            )\n        if dataloader_params.get(\"num_workers\", 0) &gt; 0:\n            # prevent too many open files error\n            pin_memory = True\n            torch.multiprocessing.set_sharing_strategy(\"file_system\")\n        else:\n            pin_memory = False\n\n        return torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size=1,\n            pin_memory=pin_memory,\n            collate_fn=dataset.no_batching_fn,\n            **dataloader_params,\n        )\n\n    def get_optimizer(self, params: Iterable) -&gt; torch.optim.Optimizer:\n        \"\"\"Getter for optimizer.\n\n        Args:\n            params: iterable of model parameters to optimize or dicts defining\n                parameter groups\n\n        Returns:\n            A torch Optimizer with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_optimizer\n\n        optimizer_params = self.get(\"optimizer\")\n\n        return init_optimizer(params, optimizer_params)\n\n    def get_scheduler(\n        self, optimizer: torch.optim.Optimizer\n    ) -&gt; torch.optim.lr_scheduler.LRScheduler | None:\n        \"\"\"Getter for lr scheduler.\n\n        Args:\n            optimizer: The optimizer to wrap the scheduler around\n\n        Returns:\n            A torch learning rate scheduler with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_scheduler\n\n        lr_scheduler_params = self.get(\"scheduler\")\n\n        if lr_scheduler_params is None:\n            logger.warning(\n                \"`scheduler` key not found in cfg or is empty. No scheduler will be returned!\"\n            )\n            return None\n        return init_scheduler(optimizer, lr_scheduler_params)\n\n    def get_loss(self) -&gt; \"dreem.training.losses.AssoLoss\":\n        \"\"\"Getter for loss functions.\n\n        Returns:\n            An AssoLoss with specified params\n        \"\"\"\n        from dreem.training.losses import AssoLoss\n\n        loss_params = self.get(\"loss\", {})\n\n        if len(loss_params) == 0:\n            logger.warning(\n                \"`loss` key not found in cfg. Using default params for `AssoLoss`\"\n            )\n\n        return AssoLoss(**loss_params)\n\n    def get_logger(self) -&gt; pl.loggers.Logger:\n        \"\"\"Getter for logging callback.\n\n        Returns:\n            A Logger with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_logger\n\n        logger_params = self.get(\"logging\", {})\n        if len(logger_params) == 0:\n            logger.warning(\n                \"`logging` key not found in cfg. No logger will be configured!\"\n            )\n\n        return init_logger(\n            logger_params, OmegaConf.to_container(self.cfg, resolve=True)\n        )\n\n    def get_early_stopping(self) -&gt; pl.callbacks.EarlyStopping:\n        \"\"\"Getter for lightning early stopping callback.\n\n        Returns:\n            A lightning early stopping callback with specified params\n        \"\"\"\n        early_stopping_params = self.get(\"early_stopping\", None)\n\n        if early_stopping_params is None:\n            logger.warning(\n                \"`early_stopping` was not found in cfg or was `null`. Early stopping will not be used!\"\n            )\n            return None\n        elif len(early_stopping_params) == 0:\n            logger.warning(\"`early_stopping` cfg is empty! Using defaults\")\n        return pl.callbacks.EarlyStopping(**early_stopping_params)\n\n    def get_checkpointing(self) -&gt; pl.callbacks.ModelCheckpoint:\n        \"\"\"Getter for lightning checkpointing callback.\n\n        Returns:\n            A lightning checkpointing callback with specified params\n        \"\"\"\n        # convert to dict to enable extracting/removing params\n        checkpoint_params = self.get(\"checkpointing\", {})\n        logging_params = self.get(\"logging\", {})\n\n        dirpath = checkpoint_params.pop(\"dirpath\", None)\n\n        if dirpath is None:\n            dirpath = f\"./models/{self.get('group', '', logging_params)}/{self.get('name', '', logging_params)}\"\n\n        dirpath = Path(dirpath).resolve()\n        if not Path(dirpath).exists():\n            try:\n                Path(dirpath).mkdir(parents=True, exist_ok=True)\n            except OSError as e:\n                logger.exception(\n                    f\"Cannot create a new folder!. Check the permissions to {dirpath}. \\n {e}\"\n                )\n\n        _ = checkpoint_params.pop(\"dirpath\", None)\n        monitor = checkpoint_params.pop(\"monitor\", [\"val_loss\"])\n        checkpointers = []\n\n        logger.info(\n            f\"Saving checkpoints to `{dirpath}` based on the following metrics: {monitor}\"\n        )\n        if len(checkpoint_params) == 0:\n            logger.warning(\n                \"\"\"`checkpointing` key was not found in cfg or was empty!\n                Configuring checkpointing to use default params!\"\"\"\n            )\n\n        for metric in monitor:\n            checkpointer = pl.callbacks.ModelCheckpoint(\n                monitor=metric,\n                dirpath=dirpath,\n                filename=f\"{{epoch}}-{{{metric}}}\",\n                **checkpoint_params,\n            )\n            checkpointer.CHECKPOINT_NAME_LAST = f\"{{epoch}}-final-{{{metric}}}\"\n            checkpointers.append(checkpointer)\n        return checkpointers\n\n    def get_trainer(\n        self,\n        callbacks: list[pl.callbacks.Callback] | None = None,\n        logger: pl.loggers.WandbLogger | None = None,\n        devices: int = 1,\n        accelerator: str = \"auto\",\n    ) -&gt; pl.Trainer:\n        \"\"\"Getter for the lightning trainer.\n\n        Args:\n            callbacks: a list of lightning callbacks preconfigured to be used\n                for training\n            logger: the Wandb logger used for logging during training\n            devices: The number of gpus to be used. 0 means cpu\n            accelerator: either \"gpu\" or \"cpu\" specifies which device to use\n\n        Returns:\n            A lightning Trainer with specified params\n        \"\"\"\n        trainer_params = self.get(\"trainer\", {})\n        profiler = trainer_params.pop(\"profiler\", None)\n        if len(trainer_params) == 0:\n            print(\n                \"`trainer` key was not found in cfg or was empty. Using defaults for `pl.Trainer`!\"\n            )\n\n        if \"accelerator\" not in trainer_params:\n            trainer_params[\"accelerator\"] = accelerator\n        if \"devices\" not in trainer_params:\n            trainer_params[\"devices\"] = devices\n\n        map_profiler = {\n            \"advanced\": pl.profilers.AdvancedProfiler,\n            \"simple\": pl.profilers.SimpleProfiler,\n            \"pytorch\": pl.profilers.PyTorchProfiler,\n            \"passthrough\": pl.profilers.PassThroughProfiler,\n            \"xla\": pl.profilers.XLAProfiler,\n        }\n\n        if profiler:\n            if profiler in map_profiler:\n                profiler = map_profiler[profiler](filename=\"profile\")\n            else:\n                raise ValueError(\n                    f\"Profiler {profiler} not supported! Please use one of {list(map_profiler.keys())}\"\n                )\n\n        return pl.Trainer(\n            callbacks=callbacks,\n            logger=logger,\n            profiler=profiler,\n            **trainer_params,\n        )\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.data_paths","title":"<code>data_paths</code>  <code>property</code> <code>writable</code>","text":"<p>Get data paths.</p>"},{"location":"reference/dreem/#dreem.Config.__init__","title":"<code>__init__(cfg, params_cfg=None)</code>","text":"<p>Initialize the class with config from hydra/omega conf.</p> <p>First uses <code>base_param</code> file then overwrites with specific <code>params_config</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>The <code>DictConfig</code> containing all the hyperparameters needed for training/evaluation.</p> required <code>params_cfg</code> <code>DictConfig | None</code> <p>The <code>DictConfig</code> containing subset of hyperparameters to override. training/evaluation</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>def __init__(self, cfg: DictConfig, params_cfg: DictConfig | None = None):\n    \"\"\"Initialize the class with config from hydra/omega conf.\n\n    First uses `base_param` file then overwrites with specific `params_config`.\n\n    Args:\n        cfg: The `DictConfig` containing all the hyperparameters needed for\n            training/evaluation.\n        params_cfg: The `DictConfig` containing subset of hyperparameters to override.\n            training/evaluation\n    \"\"\"\n    base_cfg = cfg\n    logger.info(f\"Base Config: {cfg}\")\n\n    if \"params_config\" in cfg:\n        params_cfg = OmegaConf.load(cfg.params_config)\n\n    if params_cfg:\n        logger.info(f\"Overwriting base config with {params_cfg}\")\n        with open_dict(base_cfg):\n            self.cfg = OmegaConf.merge(base_cfg, params_cfg)  # merge configs\n    else:\n        self.cfg = cfg\n\n    OmegaConf.set_struct(self.cfg, False)\n\n    self._vid_files = {}\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.__repr__","title":"<code>__repr__()</code>","text":"<p>Object representation of config class.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def __repr__(self):\n    \"\"\"Object representation of config class.\"\"\"\n    return f\"Config({self.cfg})\"\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of config class.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def __str__(self):\n    \"\"\"Return a string representation of config class.\"\"\"\n    return f\"Config({self.cfg})\"\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.from_yaml","title":"<code>from_yaml(base_cfg_path, params_cfg_path=None)</code>  <code>classmethod</code>","text":"<p>Load config directly from yaml.</p> <p>Parameters:</p> Name Type Description Default <code>base_cfg_path</code> <code>str</code> <p>path to base config file.</p> required <code>params_cfg_path</code> <code>str | None</code> <p>path to override params.</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>@classmethod\ndef from_yaml(cls, base_cfg_path: str, params_cfg_path: str | None = None) -&gt; None:\n    \"\"\"Load config directly from yaml.\n\n    Args:\n        base_cfg_path: path to base config file.\n        params_cfg_path: path to override params.\n    \"\"\"\n    base_cfg = OmegaConf.load(base_cfg_path)\n    params_cfg = OmegaConf.load(params_cfg_path) if params_cfg_path else None\n    return cls(base_cfg, params_cfg)\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.get","title":"<code>get(key, default=None, cfg=None)</code>","text":"<p>Get config item.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key of item to return</p> required <code>default</code> <p>default value to return if key is missing.</p> <code>None</code> <code>cfg</code> <code>dict</code> <p>the config dict from which to retrieve an item</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>def get(self, key: str, default=None, cfg: dict = None):\n    \"\"\"Get config item.\n\n    Args:\n        key: key of item to return\n        default: default value to return if key is missing.\n        cfg: the config dict from which to retrieve an item\n    \"\"\"\n    if cfg is None:\n        cfg = self.cfg\n\n    param = cfg.get(key, default)\n\n    if isinstance(param, DictConfig):\n        param = OmegaConf.to_container(param, resolve=True)\n\n    return param\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.get_checkpointing","title":"<code>get_checkpointing()</code>","text":"<p>Getter for lightning checkpointing callback.</p> <p>Returns:</p> Type Description <code>ModelCheckpoint</code> <p>A lightning checkpointing callback with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_checkpointing(self) -&gt; pl.callbacks.ModelCheckpoint:\n    \"\"\"Getter for lightning checkpointing callback.\n\n    Returns:\n        A lightning checkpointing callback with specified params\n    \"\"\"\n    # convert to dict to enable extracting/removing params\n    checkpoint_params = self.get(\"checkpointing\", {})\n    logging_params = self.get(\"logging\", {})\n\n    dirpath = checkpoint_params.pop(\"dirpath\", None)\n\n    if dirpath is None:\n        dirpath = f\"./models/{self.get('group', '', logging_params)}/{self.get('name', '', logging_params)}\"\n\n    dirpath = Path(dirpath).resolve()\n    if not Path(dirpath).exists():\n        try:\n            Path(dirpath).mkdir(parents=True, exist_ok=True)\n        except OSError as e:\n            logger.exception(\n                f\"Cannot create a new folder!. Check the permissions to {dirpath}. \\n {e}\"\n            )\n\n    _ = checkpoint_params.pop(\"dirpath\", None)\n    monitor = checkpoint_params.pop(\"monitor\", [\"val_loss\"])\n    checkpointers = []\n\n    logger.info(\n        f\"Saving checkpoints to `{dirpath}` based on the following metrics: {monitor}\"\n    )\n    if len(checkpoint_params) == 0:\n        logger.warning(\n            \"\"\"`checkpointing` key was not found in cfg or was empty!\n            Configuring checkpointing to use default params!\"\"\"\n        )\n\n    for metric in monitor:\n        checkpointer = pl.callbacks.ModelCheckpoint(\n            monitor=metric,\n            dirpath=dirpath,\n            filename=f\"{{epoch}}-{{{metric}}}\",\n            **checkpoint_params,\n        )\n        checkpointer.CHECKPOINT_NAME_LAST = f\"{{epoch}}-final-{{{metric}}}\"\n        checkpointers.append(checkpointer)\n    return checkpointers\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.get_ctc_paths","title":"<code>get_ctc_paths(list_dir_path)</code>","text":"<p>Get file paths from directory. Only for CTC datasets.</p> <p>Parameters:</p> Name Type Description Default <code>list_dir_path</code> <code>list[str]</code> <p>list of directories to search for labels and videos</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str], list[str]]</code> <p>lists of labels file paths and video file paths</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_ctc_paths(\n    self, list_dir_path: list[str]\n) -&gt; tuple[list[str], list[str], list[str]]:\n    \"\"\"Get file paths from directory. Only for CTC datasets.\n\n    Args:\n        list_dir_path: list of directories to search for labels and videos\n\n    Returns:\n        lists of labels file paths and video file paths\n    \"\"\"\n    gt_list = []\n    raw_img_list = []\n    ctc_track_meta = []\n    # user can specify a list of directories, each of which can contain several subdirectories that come in pairs of (dset_name, dset_name_GT/TRA)\n    for dir_path in list_dir_path:\n        for subdir in os.listdir(dir_path):\n            if subdir.endswith(\"_GT\"):\n                gt_path = os.path.join(dir_path, subdir, \"TRA\")\n                raw_img_path = os.path.join(dir_path, subdir.replace(\"_GT\", \"\"))\n                # get filepaths for all tif files in gt_path\n                gt_list.append(glob.glob(os.path.join(gt_path, \"*.tif\")))\n                # get filepaths for all tif files in raw_img_path\n                raw_img_list.append(glob.glob(os.path.join(raw_img_path, \"*.tif\")))\n                man_track_file = glob.glob(os.path.join(gt_path, \"man_track.txt\"))\n                if len(man_track_file) &gt; 0:\n                    ctc_track_meta.append(man_track_file[0])\n                else:\n                    logger.debug(\n                        f\"No man_track.txt file found in {gt_path}. Continuing...\"\n                    )\n            else:\n                continue\n\n    return gt_list, raw_img_list, ctc_track_meta\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.get_data_paths","title":"<code>get_data_paths(mode, data_cfg)</code>","text":"<p>Get file paths from directory. Only for SLEAP datasets.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>[None, \"train\", \"test\", \"val\"]. Indicates whether to use train, val, or test params for dataset</p> required <code>data_cfg</code> <code>dict</code> <p>Config for the dataset containing \"dir\" key.</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str]]</code> <p>lists of labels file paths and video file paths respectively</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_data_paths(self, mode: str, data_cfg: dict) -&gt; tuple[list[str], list[str]]:\n    \"\"\"Get file paths from directory. Only for SLEAP datasets.\n\n    Args:\n        mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n            train, val, or test params for dataset\n        data_cfg: Config for the dataset containing \"dir\" key.\n\n    Returns:\n        lists of labels file paths and video file paths respectively\n    \"\"\"\n    # hack to get around the fact that for test mode, get_data_paths is called before get_dataset.\n    # also, for train/val mode, data_cfg has had the dir key popped through self.get() called in get_dataset()\n    if mode == \"test\":\n        list_dir_path = data_cfg.get(\"dir\", {}).get(\"path\", None)\n        if list_dir_path is None:\n            raise ValueError(\n                \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n            )\n        self.labels_suffix = data_cfg.get(\"dir\", {}).get(\"labels_suffix\")\n        self.vid_suffix = data_cfg.get(\"dir\", {}).get(\"vid_suffix\")\n    else:\n        list_dir_path = self.data_dirs\n    if not isinstance(list_dir_path, list):\n        list_dir_path = [list_dir_path]\n\n    if self.labels_suffix == \".slp\":\n        label_files = []\n        vid_files = []\n        for dir_path in list_dir_path:\n            logger.debug(f\"Searching `{dir_path}` directory\")\n            labels_path = f\"{dir_path}/*{self.labels_suffix}\"\n            vid_path = f\"{dir_path}/*{self.vid_suffix}\"\n            logger.debug(f\"Searching for labels matching {labels_path}\")\n            label_files.extend(glob.glob(labels_path))\n            logger.debug(f\"Searching for videos matching {vid_path}\")\n            vid_files.extend(glob.glob(vid_path))\n\n    elif self.labels_suffix == \".tif\":\n        label_files, vid_files, ctc_track_meta = self.get_ctc_paths(list_dir_path)\n\n    logger.debug(f\"Found {len(label_files)} labels and {len(vid_files)} videos\")\n\n    # backdoor to set label files directly in the configs (i.e. bypass dir.path)\n    if data_cfg.get(\"slp_files\", None):\n        logger.debug(\"Overriding label files with user provided list\")\n        slp_files = data_cfg.get(\"slp_files\")\n        if len(slp_files) &gt; 0:\n            label_files = slp_files\n    if data_cfg.get(\"video_files\", None):\n        individual_video_files = data_cfg.get(\"video_files\")\n        if len(individual_video_files) &gt; 0:\n            vid_files = individual_video_files\n    return label_files, vid_files\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.get_dataloader","title":"<code>get_dataloader(dataset, mode)</code>","text":"<p>Getter for dataloader.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>'SleapDataset' | 'MicroscopyDataset' | 'CellTrackingDataset'</code> <p>the Sleap or Microscopy Dataset used to initialize the dataloader</p> required <code>mode</code> <code>str</code> <p>either [\"train\", \"val\", or \"test\"] indicates which dataset config to use</p> required <p>Returns:</p> Type Description <code>DataLoader</code> <p>A torch dataloader for <code>dataset</code> with parameters configured as specified</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_dataloader(\n    self,\n    dataset: \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\",\n    mode: str,\n) -&gt; torch.utils.data.DataLoader:\n    \"\"\"Getter for dataloader.\n\n    Args:\n        dataset: the Sleap or Microscopy Dataset used to initialize the dataloader\n        mode: either [\"train\", \"val\", or \"test\"] indicates which dataset\n            config to use\n\n    Returns:\n        A torch dataloader for `dataset` with parameters configured as specified\n    \"\"\"\n    dataloader_params = self.get(\"dataloader\", {})\n    if mode.lower() == \"train\":\n        dataloader_params = self.get(\"train_dataloader\", {}, dataloader_params)\n    elif mode.lower() == \"val\":\n        dataloader_params = self.get(\"val_dataloader\", {}, dataloader_params)\n    elif mode.lower() == \"test\":\n        dataloader_params = self.get(\"test_dataloader\", {}, dataloader_params)\n    else:\n        raise ValueError(\n            \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n        )\n    if dataloader_params.get(\"num_workers\", 0) &gt; 0:\n        # prevent too many open files error\n        pin_memory = True\n        torch.multiprocessing.set_sharing_strategy(\"file_system\")\n    else:\n        pin_memory = False\n\n    return torch.utils.data.DataLoader(\n        dataset=dataset,\n        batch_size=1,\n        pin_memory=pin_memory,\n        collate_fn=dataset.no_batching_fn,\n        **dataloader_params,\n    )\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.get_dataset","title":"<code>get_dataset(mode, label_files=None, vid_files=None)</code>","text":"<p>Getter for datasets.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>[None, \"train\", \"test\", \"val\"]. Indicates whether to use train, val, or test params for dataset</p> required <code>label_files</code> <code>list[str] | None</code> <p>path to label_files for override</p> <code>None</code> <code>vid_files</code> <code>list[str | list[str]]</code> <p>path to vid_files for override</p> <code>None</code> <p>Returns:</p> Type Description <code>'SleapDataset' | 'CellTrackingDataset'</code> <p>Either a <code>SleapDataset</code> or <code>CellTrackingDataset</code> with params indicated by cfg</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_dataset(\n    self,\n    mode: str,\n    label_files: list[str] | None = None,\n    vid_files: list[str | list[str]] = None,\n) -&gt; \"SleapDataset\" | \"CellTrackingDataset\":\n    \"\"\"Getter for datasets.\n\n    Args:\n        mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n            train, val, or test params for dataset\n        label_files: path to label_files for override\n        vid_files: path to vid_files for override\n\n    Returns:\n        Either a `SleapDataset` or `CellTrackingDataset` with params indicated by cfg\n    \"\"\"\n    from dreem.datasets import CellTrackingDataset, SleapDataset\n\n    dataset_params = self.get(\"dataset\")\n    if dataset_params is None:\n        raise KeyError(\"`dataset` key is missing from cfg!\")\n\n    if mode.lower() == \"train\":\n        dataset_params = self.get(\"train_dataset\", {}, dataset_params)\n    elif mode.lower() == \"val\":\n        dataset_params = self.get(\"val_dataset\", {}, dataset_params)\n    elif mode.lower() == \"test\":\n        dataset_params = self.get(\"test_dataset\", {}, dataset_params)\n    else:\n        raise ValueError(\n            \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n        )\n\n    # input validation\n    self.data_dirs = dataset_params.get(\"dir\", {}).get(\"path\", None)\n    self.labels_suffix = dataset_params.get(\"dir\", {}).get(\"labels_suffix\")\n    self.vid_suffix = dataset_params.get(\"dir\", {}).get(\"vid_suffix\")\n    if self.data_dirs is None:\n        raise ValueError(\n            \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n        )\n    if self.labels_suffix is None or self.vid_suffix is None:\n        raise KeyError(\n            f\"Must provide a labels suffix and vid suffix to search for but found {self.labels_suffix} and {self.vid_suffix}\"\n        )\n\n    # infer dataset type from the user provided suffix\n    if self.labels_suffix == \".slp\":\n        # during training, multiple files can be used at once, so label_files is not passed in\n        # during inference, a single label_files string can be passed in as get_data_paths is\n        # called before get_dataset, hence the check\n        if label_files is None or vid_files is None:\n            label_files, vid_files = self.get_data_paths(mode, dataset_params)\n        dataset_params[\"slp_files\"] = label_files\n        dataset_params[\"video_files\"] = vid_files\n        dataset_params[\"data_dirs\"] = self.data_dirs\n        self.data_paths = (mode, vid_files)\n\n        return SleapDataset(**dataset_params)\n\n    elif self.labels_suffix == \".tif\":\n        # for CTC datasets, pass in a list of gt and raw image directories, eaech of which contain tifs\n        ctc_track_meta = None\n        list_dir_path = self.data_dirs  # don't modify self.data_dirs\n        if not isinstance(list_dir_path, list):\n            list_dir_path = [list_dir_path]\n        if label_files is None or vid_files is None:\n            label_files, vid_files, ctc_track_meta = self.get_ctc_paths(\n                list_dir_path\n            )\n        dataset_params[\"data_dirs\"] = self.data_dirs\n        # extract filepaths of all raw images and gt images (i.e. labelled masks)\n        dataset_params[\"gt_list\"] = label_files\n        dataset_params[\"raw_img_list\"] = vid_files\n        dataset_params[\"ctc_track_meta\"] = ctc_track_meta\n\n        return CellTrackingDataset(**dataset_params)\n\n    else:\n        raise ValueError(\n            \"Could not resolve dataset type from Config! Only .slp (SLEAP) and .tif (Cell Tracking Challenge) data formats are supported.\"\n        )\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.get_early_stopping","title":"<code>get_early_stopping()</code>","text":"<p>Getter for lightning early stopping callback.</p> <p>Returns:</p> Type Description <code>EarlyStopping</code> <p>A lightning early stopping callback with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_early_stopping(self) -&gt; pl.callbacks.EarlyStopping:\n    \"\"\"Getter for lightning early stopping callback.\n\n    Returns:\n        A lightning early stopping callback with specified params\n    \"\"\"\n    early_stopping_params = self.get(\"early_stopping\", None)\n\n    if early_stopping_params is None:\n        logger.warning(\n            \"`early_stopping` was not found in cfg or was `null`. Early stopping will not be used!\"\n        )\n        return None\n    elif len(early_stopping_params) == 0:\n        logger.warning(\"`early_stopping` cfg is empty! Using defaults\")\n    return pl.callbacks.EarlyStopping(**early_stopping_params)\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.get_gtr_runner","title":"<code>get_gtr_runner(ckpt_path=None)</code>","text":"<p>Get lightning module for training, validation, and inference.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_path</code> <code>str | None</code> <p>path to checkpoint for override</p> <code>None</code> <p>Returns:</p> Type Description <code>'GTRRunner'</code> <p>a gtr runner model</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_gtr_runner(self, ckpt_path: str | None = None) -&gt; \"GTRRunner\":\n    \"\"\"Get lightning module for training, validation, and inference.\n\n    Args:\n        ckpt_path: path to checkpoint for override\n\n    Returns:\n        a gtr runner model\n    \"\"\"\n    from dreem.models import GTRRunner\n\n    keys = [\"tracker\", \"optimizer\", \"scheduler\", \"loss\", \"runner\", \"model\"]\n    args = [key + \"_cfg\" if key != \"runner\" else key for key in keys]\n\n    params = {}\n    for key, arg in zip(keys, args):\n        sub_params = self.get(key, {})\n\n        if len(sub_params) == 0:\n            logger.warning(\n                f\"`{key}` not found in config or is empty. Using defaults for {arg}!\"\n            )\n\n        if key == \"runner\":\n            runner_params = sub_params\n            for k, v in runner_params.items():\n                params[k] = v\n        else:\n            params[arg] = sub_params\n\n    ckpt_path = params[\"model_cfg\"].pop(\"ckpt_path\", None)\n\n    if ckpt_path is not None and ckpt_path != \"\":\n        model = GTRRunner.load_from_checkpoint(\n            ckpt_path, tracker_cfg=params[\"tracker_cfg\"], **runner_params\n        )\n\n    else:\n        model = GTRRunner(**params)\n\n    return model\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.get_logger","title":"<code>get_logger()</code>","text":"<p>Getter for logging callback.</p> <p>Returns:</p> Type Description <code>Logger</code> <p>A Logger with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_logger(self) -&gt; pl.loggers.Logger:\n    \"\"\"Getter for logging callback.\n\n    Returns:\n        A Logger with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_logger\n\n    logger_params = self.get(\"logging\", {})\n    if len(logger_params) == 0:\n        logger.warning(\n            \"`logging` key not found in cfg. No logger will be configured!\"\n        )\n\n    return init_logger(\n        logger_params, OmegaConf.to_container(self.cfg, resolve=True)\n    )\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.get_loss","title":"<code>get_loss()</code>","text":"<p>Getter for loss functions.</p> <p>Returns:</p> Type Description <code>'dreem.training.losses.AssoLoss'</code> <p>An AssoLoss with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_loss(self) -&gt; \"dreem.training.losses.AssoLoss\":\n    \"\"\"Getter for loss functions.\n\n    Returns:\n        An AssoLoss with specified params\n    \"\"\"\n    from dreem.training.losses import AssoLoss\n\n    loss_params = self.get(\"loss\", {})\n\n    if len(loss_params) == 0:\n        logger.warning(\n            \"`loss` key not found in cfg. Using default params for `AssoLoss`\"\n        )\n\n    return AssoLoss(**loss_params)\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.get_model","title":"<code>get_model()</code>","text":"<p>Getter for gtr model.</p> <p>Returns:</p> Type Description <code>'GlobalTrackingTransformer'</code> <p>A global tracking transformer with parameters indicated by cfg</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_model(self) -&gt; \"GlobalTrackingTransformer\":\n    \"\"\"Getter for gtr model.\n\n    Returns:\n        A global tracking transformer with parameters indicated by cfg\n    \"\"\"\n    from dreem.models import GlobalTrackingTransformer, GTRRunner\n\n    model_params = self.get(\"model\", {})\n\n    ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n    if ckpt_path is not None and len(ckpt_path) &gt; 0:\n        return GTRRunner.load_from_checkpoint(ckpt_path).model\n\n    return GlobalTrackingTransformer(**model_params)\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.get_optimizer","title":"<code>get_optimizer(params)</code>","text":"<p>Getter for optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable</code> <p>iterable of model parameters to optimize or dicts defining parameter groups</p> required <p>Returns:</p> Type Description <code>Optimizer</code> <p>A torch Optimizer with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_optimizer(self, params: Iterable) -&gt; torch.optim.Optimizer:\n    \"\"\"Getter for optimizer.\n\n    Args:\n        params: iterable of model parameters to optimize or dicts defining\n            parameter groups\n\n    Returns:\n        A torch Optimizer with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_optimizer\n\n    optimizer_params = self.get(\"optimizer\")\n\n    return init_optimizer(params, optimizer_params)\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.get_scheduler","title":"<code>get_scheduler(optimizer)</code>","text":"<p>Getter for lr scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to wrap the scheduler around</p> required <p>Returns:</p> Type Description <code>LRScheduler | None</code> <p>A torch learning rate scheduler with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_scheduler(\n    self, optimizer: torch.optim.Optimizer\n) -&gt; torch.optim.lr_scheduler.LRScheduler | None:\n    \"\"\"Getter for lr scheduler.\n\n    Args:\n        optimizer: The optimizer to wrap the scheduler around\n\n    Returns:\n        A torch learning rate scheduler with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_scheduler\n\n    lr_scheduler_params = self.get(\"scheduler\")\n\n    if lr_scheduler_params is None:\n        logger.warning(\n            \"`scheduler` key not found in cfg or is empty. No scheduler will be returned!\"\n        )\n        return None\n    return init_scheduler(optimizer, lr_scheduler_params)\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.get_tracker_cfg","title":"<code>get_tracker_cfg()</code>","text":"<p>Getter for tracker config params.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict containing the init params for <code>Tracker</code>.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_tracker_cfg(self) -&gt; dict:\n    \"\"\"Getter for tracker config params.\n\n    Returns:\n        A dict containing the init params for `Tracker`.\n    \"\"\"\n    return self.get(\"tracker\", {})\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.get_trainer","title":"<code>get_trainer(callbacks=None, logger=None, devices=1, accelerator='auto')</code>","text":"<p>Getter for the lightning trainer.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[Callback] | None</code> <p>a list of lightning callbacks preconfigured to be used for training</p> <code>None</code> <code>logger</code> <code>WandbLogger | None</code> <p>the Wandb logger used for logging during training</p> <code>None</code> <code>devices</code> <code>int</code> <p>The number of gpus to be used. 0 means cpu</p> <code>1</code> <code>accelerator</code> <code>str</code> <p>either \"gpu\" or \"cpu\" specifies which device to use</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>Trainer</code> <p>A lightning Trainer with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_trainer(\n    self,\n    callbacks: list[pl.callbacks.Callback] | None = None,\n    logger: pl.loggers.WandbLogger | None = None,\n    devices: int = 1,\n    accelerator: str = \"auto\",\n) -&gt; pl.Trainer:\n    \"\"\"Getter for the lightning trainer.\n\n    Args:\n        callbacks: a list of lightning callbacks preconfigured to be used\n            for training\n        logger: the Wandb logger used for logging during training\n        devices: The number of gpus to be used. 0 means cpu\n        accelerator: either \"gpu\" or \"cpu\" specifies which device to use\n\n    Returns:\n        A lightning Trainer with specified params\n    \"\"\"\n    trainer_params = self.get(\"trainer\", {})\n    profiler = trainer_params.pop(\"profiler\", None)\n    if len(trainer_params) == 0:\n        print(\n            \"`trainer` key was not found in cfg or was empty. Using defaults for `pl.Trainer`!\"\n        )\n\n    if \"accelerator\" not in trainer_params:\n        trainer_params[\"accelerator\"] = accelerator\n    if \"devices\" not in trainer_params:\n        trainer_params[\"devices\"] = devices\n\n    map_profiler = {\n        \"advanced\": pl.profilers.AdvancedProfiler,\n        \"simple\": pl.profilers.SimpleProfiler,\n        \"pytorch\": pl.profilers.PyTorchProfiler,\n        \"passthrough\": pl.profilers.PassThroughProfiler,\n        \"xla\": pl.profilers.XLAProfiler,\n    }\n\n    if profiler:\n        if profiler in map_profiler:\n            profiler = map_profiler[profiler](filename=\"profile\")\n        else:\n            raise ValueError(\n                f\"Profiler {profiler} not supported! Please use one of {list(map_profiler.keys())}\"\n            )\n\n    return pl.Trainer(\n        callbacks=callbacks,\n        logger=logger,\n        profiler=profiler,\n        **trainer_params,\n    )\n</code></pre>"},{"location":"reference/dreem/#dreem.Config.set_hparams","title":"<code>set_hparams(hparams)</code>","text":"<p>Setter function for overwriting specific hparams.</p> <p>Useful for changing 1 or 2 hyperparameters such as dataset.</p> <p>Parameters:</p> Name Type Description Default <code>hparams</code> <code>dict</code> <p>A dict containing the hyperparameter to be overwritten and the value to be changed</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if config is successfully updated, <code>False</code> otherwise</p> Source code in <code>dreem/io/config.py</code> <pre><code>def set_hparams(self, hparams: dict) -&gt; bool:\n    \"\"\"Setter function for overwriting specific hparams.\n\n    Useful for changing 1 or 2 hyperparameters such as dataset.\n\n    Args:\n        hparams: A dict containing the hyperparameter to be overwritten and\n            the value to be changed\n\n    Returns:\n        `True` if config is successfully updated, `False` otherwise\n    \"\"\"\n    if hparams == {} or hparams is None:\n        logger.warning(\"Nothing to update!\")\n        return False\n    for hparam, val in hparams.items():\n        try:\n            OmegaConf.update(self.cfg, hparam, val)\n        except Exception as e:\n            logger.exception(f\"Failed to update {hparam} to {val} due to {e}\")\n            return False\n    return True\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame","title":"<code>Frame</code>","text":"<p>Data structure containing metadata for a single frame of a video.</p> <p>Attributes:</p> Name Type Description <code>video_id</code> <code>Tensor</code> <p>The video index in the dataset.</p> <code>frame_id</code> <code>Tensor</code> <p>The index of the frame in a video.</p> <code>vid_file</code> <code>Tensor</code> <p>The path to the video the frame is from.</p> <code>img_shape</code> <code>Tensor</code> <p>The shape of the original frame (not the crop).</p> <code>instances</code> <code>list['Instance']</code> <p>A list of Instance objects that appear in the frame.</p> <code>asso_output</code> <code>'AssociationMatrix'</code> <p>The association matrix between instances output directly from the transformer.</p> <code>matches</code> <code>tuple</code> <p>matches from LSA algorithm between the instances and available trajectories during tracking.</p> <code>traj_score</code> <code>tuple</code> <p>Either a dict containing the association matrix between instances and trajectories along postprocessing pipeline or a single association matrix.</p> <code>device</code> <code>str</code> <p>The device the frame should be moved to.</p> <p>Methods:</p> Name Description <code>__attrs_post_init__</code> <p>Handle more intricate default initializations and moving to device.</p> <code>__repr__</code> <p>Return String representation of the Frame.</p> <code>add_traj_score</code> <p>Add trajectory score to dictionary.</p> <code>from_slp</code> <p>Convert <code>sio.LabeledFrame</code> to <code>dreem.io.Frame</code>.</p> <code>get_anchors</code> <p>Get the anchor names of instances in the frame.</p> <code>get_bboxes</code> <p>Get the bounding boxes of all instances in the frame.</p> <code>get_centroids</code> <p>Get the centroids around which each instance's crop was formed.</p> <code>get_crops</code> <p>Get the crops of all instances in the frame.</p> <code>get_features</code> <p>Get the reid feature vectors of all instances in the frame.</p> <code>get_gt_track_ids</code> <p>Get the gt track ids of all instances in the frame.</p> <code>get_pred_track_ids</code> <p>Get the pred track ids of all instances in the frame.</p> <code>get_traj_score</code> <p>Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.</p> <code>has_asso_output</code> <p>Determine whether the frame has an association matrix computed.</p> <code>has_bboxes</code> <p>Check if any of frames instances has a bounding box.</p> <code>has_crops</code> <p>Check if any of frames instances has a crop.</p> <code>has_features</code> <p>Check if any of frames instances has reid features already computed.</p> <code>has_gt_track_ids</code> <p>Check if any of frames instances has a gt track id.</p> <code>has_instances</code> <p>Determine whether there are instances in the frame.</p> <code>has_matches</code> <p>Check whether or not matches have been computed for frame.</p> <code>has_pred_track_ids</code> <p>Check if any of frames instances has a pred track id.</p> <code>has_traj_score</code> <p>Check if any trajectory association matrix has been saved.</p> <code>to</code> <p>Move frame to different device or dtype (See <code>torch.to</code> for more info).</p> <code>to_h5</code> <p>Convert frame to h5py group.</p> <code>to_slp</code> <p>Convert Frame to sleap_io.LabeledFrame object.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>@attrs.define(eq=False)\nclass Frame:\n    \"\"\"Data structure containing metadata for a single frame of a video.\n\n    Attributes:\n        video_id: The video index in the dataset.\n        frame_id: The index of the frame in a video.\n        vid_file: The path to the video the frame is from.\n        img_shape: The shape of the original frame (not the crop).\n        instances: A list of Instance objects that appear in the frame.\n        asso_output: The association matrix between instances\n            output directly from the transformer.\n        matches: matches from LSA algorithm between the instances and\n            available trajectories during tracking.\n        traj_score: Either a dict containing the association matrix\n            between instances and trajectories along postprocessing pipeline\n            or a single association matrix.\n        device: The device the frame should be moved to.\n    \"\"\"\n\n    _video_id: int = attrs.field(alias=\"video_id\", converter=_to_tensor)\n    _frame_id: int = attrs.field(alias=\"frame_id\", converter=_to_tensor)\n    _video: str = attrs.field(alias=\"vid_file\", default=\"\")\n    _img_shape: ArrayLike = attrs.field(\n        alias=\"img_shape\", converter=_to_tensor, factory=list\n    )\n\n    _instances: list[\"Instance\"] = attrs.field(alias=\"instances\", factory=list)\n    _asso_output: \"AssociationMatrix\" | None = attrs.field(\n        alias=\"asso_output\", default=None\n    )\n    _matches: tuple = attrs.field(alias=\"matches\", factory=tuple)\n    _traj_score: dict = attrs.field(alias=\"traj_score\", factory=dict)\n    _device: str | torch.device | None = attrs.field(alias=\"device\", default=None)\n\n    def __attrs_post_init__(self) -&gt; None:\n        \"\"\"Handle more intricate default initializations and moving to device.\"\"\"\n        if len(self.img_shape) == 0:\n            self.img_shape = torch.tensor([0, 0, 0])\n\n        for instance in self.instances:\n            instance.frame = self\n\n        self.to(self.device)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return String representation of the Frame.\n\n        Returns:\n            The string representation of the frame.\n        \"\"\"\n        return (\n            \"Frame(\"\n            f\"video={self._video.filename if isinstance(self._video, sio.Video) else self._video}, \"\n            f\"video_id={self._video_id.item()}, \"\n            f\"frame_id={self._frame_id.item()}, \"\n            f\"img_shape={self._img_shape}, \"\n            f\"num_detected={self.num_detected}, \"\n            f\"asso_output={self._asso_output}, \"\n            f\"traj_score={self._traj_score}, \"\n            f\"matches={self._matches}, \"\n            f\"instances={self._instances}, \"\n            f\"device={self._device}\"\n            \")\"\n        )\n\n    def to(self, map_location: str | torch.device) -&gt; Self:\n        \"\"\"Move frame to different device or dtype (See `torch.to` for more info).\n\n        Args:\n            map_location: A string representing the device to move to.\n\n        Returns:\n            The frame moved to a different device/dtype.\n        \"\"\"\n        self._video_id = self._video_id.to(map_location)\n        self._frame_id = self._frame_id.to(map_location)\n        self._img_shape = self._img_shape.to(map_location)\n\n        if isinstance(self._asso_output, torch.Tensor):\n            self._asso_output = self._asso_output.to(map_location)\n\n        if isinstance(self._matches, torch.Tensor):\n            self._matches = self._matches.to(map_location)\n\n        for key, val in self._traj_score.items():\n            if isinstance(val, torch.Tensor):\n                self._traj_score[key] = val.to(map_location)\n        for instance in self.instances:\n            instance = instance.to(map_location)\n\n        if isinstance(map_location, (str, torch.device)):\n            self._device = map_location\n\n        return self\n\n    @classmethod\n    def from_slp(\n        cls,\n        lf: sio.LabeledFrame,\n        video_id: int = 0,\n        device: str | None = None,\n        **kwargs,\n    ) -&gt; Self:\n        \"\"\"Convert `sio.LabeledFrame` to `dreem.io.Frame`.\n\n        Args:\n            lf: A sio.LabeledFrame object\n\n        Returns:\n            A dreem.io.Frame object\n        \"\"\"\n        from dreem.io.instance import Instance\n\n        img_shape = lf.image.shape\n        if len(img_shape) == 2:\n            img_shape = (1, *img_shape)\n        elif len(img_shape) &gt; 2 and img_shape[-1] &lt;= 3:\n            img_shape = (lf.image.shape[-1], lf.image.shape[0], lf.image.shape[1])\n        return cls(\n            video_id=video_id,\n            frame_id=(\n                lf.frame_idx.astype(np.int32)\n                if isinstance(lf.frame_idx, np.number)\n                else lf.frame_idx\n            ),\n            vid_file=lf.video.filename,\n            img_shape=img_shape,\n            instances=[Instance.from_slp(instance, **kwargs) for instance in lf],\n            device=device,\n        )\n\n    def to_slp(\n        self,\n        track_lookup: dict[int, sio.Track] | None = None,\n        video: sio.Video | None = None,\n    ) -&gt; tuple[sio.LabeledFrame, dict[int, sio.Track]]:\n        \"\"\"Convert Frame to sleap_io.LabeledFrame object.\n\n        Args:\n            track_lookup: A lookup dictionary containing the track_id and sio.Track for persistence\n            video: An sio.Video object used for overriding.\n\n        Returns: A tuple containing a LabeledFrame object with necessary metadata and\n        a lookup dictionary containing the track_id and sio.Track for persistence\n        \"\"\"\n        if track_lookup is None:\n            track_lookup = {}\n\n        slp_instances = []\n        for instance in self.instances:\n            slp_instance, track_lookup = instance.to_slp(track_lookup=track_lookup)\n            slp_instances.append(slp_instance)\n\n        if video is None:\n            video = (\n                self.video\n                if isinstance(self.video, sio.Video)\n                else sio.load_video(self.video)\n            )\n\n        return (\n            sio.LabeledFrame(\n                video=video,\n                frame_idx=self.frame_id.item(),\n                instances=slp_instances,\n            ),\n            track_lookup,\n        )\n\n    def to_h5(\n        self,\n        clip_group: h5py.Group,\n        instance_labels: list | None = None,\n        save: dict[str, bool] | None = None,\n    ) -&gt; h5py.Group:\n        \"\"\"Convert frame to h5py group.\n\n        Args:\n            clip_group: the h5py group representing the clip (e.g batch/video) the frame belongs to\n            instance_labels: the labels used to create instance group names\n            save: whether to save crops, features and embeddings for the instance\n        Returns:\n            An h5py group containing the frame\n        \"\"\"\n        if save is None:\n            save = {\"crop\": False, \"features\": False, \"embeddings\": False}\n        frame_group = clip_group.require_group(f\"frame_{self.frame_id.item()}\")\n        frame_group.attrs.create(\"frame_id\", self.frame_id.item())\n        frame_group.attrs.create(\"vid_id\", self.video_id.item())\n        frame_group.attrs.create(\"vid_name\", self.vid_name)\n\n        frame_group.create_dataset(\n            \"asso_matrix\",\n            data=self.asso_output.numpy() if self.asso_output is not None else [],\n        )\n        asso_group = frame_group.require_group(\"traj_scores\")\n        for key, value in self.get_traj_score().items():\n            asso_group.create_dataset(\n                key, data=value.to_numpy() if value is not None else []\n            )\n\n        if instance_labels is None:\n            instance_labels = self.get_gt_track_ids.cpu().numpy()\n        for instance_label, instance in zip(instance_labels, self.instances):\n            kwargs = {}\n            if save.get(\"crop\", False):\n                kwargs[\"crop\"] = instance.crop.cpu().numpy()\n            if save.get(\"features\", False):\n                kwargs[\"features\"] = instance.features.cpu().numpy()\n            if save.get(\"embeddings\", False):\n                for key, val in instance.get_embedding().items():\n                    kwargs[f\"{key}_emb\"] = val.cpu().numpy()\n            _ = instance.to_h5(frame_group, f\"instance_{instance_label}\", **kwargs)\n\n        return frame_group\n\n    @property\n    def device(self) -&gt; str:\n        \"\"\"The device the frame is on.\n\n        Returns:\n            The string representation of the device the frame is on.\n        \"\"\"\n        return self._device\n\n    @device.setter\n    def device(self, device: str) -&gt; None:\n        \"\"\"Set the device.\n\n        Note: Do not set `frame.device = device` normally. Use `frame.to(device)` instead.\n\n        Args:\n            device: the device the function should be on.\n        \"\"\"\n        self._device = device\n\n    @property\n    def video_id(self) -&gt; torch.Tensor:\n        \"\"\"The index of the video the frame comes from.\n\n        Returns:\n            A tensor containing the video index.\n        \"\"\"\n        return self._video_id\n\n    @video_id.setter\n    def video_id(self, video_id: int) -&gt; None:\n        \"\"\"Set the video index.\n\n        Note: Generally the video_id should be immutable after initialization.\n\n        Args:\n            video_id: an int representing the index of the video that the frame came from.\n        \"\"\"\n        self._video_id = torch.tensor([video_id])\n\n    @property\n    def frame_id(self) -&gt; torch.Tensor:\n        \"\"\"The index of the frame in a full video.\n\n        Returns:\n            A torch tensor containing the index of the frame in the video.\n        \"\"\"\n        return self._frame_id\n\n    @frame_id.setter\n    def frame_id(self, frame_id: int) -&gt; None:\n        \"\"\"Set the frame index of the frame.\n\n        Note: The frame_id should generally be immutable after initialization.\n\n        Args:\n            frame_id: The int index of the frame in the full video.\n        \"\"\"\n        self._frame_id = torch.tensor([frame_id])\n\n    @property\n    def video(self) -&gt; sio.Video | str:\n        \"\"\"Get the video associated with the frame.\n\n        Returns: An sio.Video object representing the video or a placeholder string\n        if it is not possible to create the sio.Video\n        \"\"\"\n        return self._video\n\n    @video.setter\n    def video(self, video: sio.Video | str) -&gt; None:\n        \"\"\"Set the video associated with the frame.\n\n        Note: we try to store the video in an sio.Video object.\n        However, if this is not possible (e.g. incompatible format or missing filepath)\n        then we simply store the string.\n\n        Args:\n            video: sio.Video containing the vid reader or string path to video_file\n        \"\"\"\n        if isinstance(video, sio.Video):\n            self._video = video\n        else:\n            try:\n                self._video = sio.load_video(video)\n            except ValueError:\n                self._video = video\n\n    @property\n    def vid_name(self) -&gt; str:\n        \"\"\"Get the path to the video corresponding to this frame.\n\n        Returns: A str file path corresponding to the frame.\n        \"\"\"\n        if isinstance(self.video, str):\n            return self.video\n        else:\n            return self.video.name\n\n    @property\n    def img_shape(self) -&gt; torch.Tensor:\n        \"\"\"The shape of the pre-cropped frame.\n\n        Returns:\n            A torch tensor containing the shape of the frame. Should generally be (c, h, w)\n        \"\"\"\n        return self._img_shape\n\n    @img_shape.setter\n    def img_shape(self, img_shape: ArrayLike) -&gt; None:\n        \"\"\"Set the shape of the frame image.\n\n        Note: the img_shape should generally be immutable after initialization.\n\n        Args:\n            img_shape: an ArrayLike object containing the shape of the frame image.\n        \"\"\"\n        self._img_shape = _to_tensor(img_shape)\n\n    @property\n    def instances(self) -&gt; list[\"Instance\"]:\n        \"\"\"A list of instances in the frame.\n\n        Returns:\n            The list of instances that appear in the frame.\n        \"\"\"\n        return self._instances\n\n    @instances.setter\n    def instances(self, instances: list[\"Instance\"]) -&gt; None:\n        \"\"\"Set the frame's instance.\n\n        Args:\n            instances: A list of Instances that appear in the frame.\n        \"\"\"\n        for instance in instances:\n            instance.frame = self\n        self._instances = instances\n\n    def has_instances(self) -&gt; bool:\n        \"\"\"Determine whether there are instances in the frame.\n\n        Returns:\n            True if there are instances in the frame, otherwise False.\n        \"\"\"\n        if self.num_detected == 0:\n            return False\n        return True\n\n    @property\n    def num_detected(self) -&gt; int:\n        \"\"\"The number of instances in the frame.\n\n        Returns:\n            the number of instances in the frame.\n        \"\"\"\n        return len(self.instances)\n\n    @property\n    def asso_output(self) -&gt; \"AssociationMatrix\":\n        \"\"\"The association matrix between instances outputted directly by transformer.\n\n        Returns:\n            An arraylike (n_query, n_nonquery) association matrix between instances.\n        \"\"\"\n        return self._asso_output\n\n    def has_asso_output(self) -&gt; bool:\n        \"\"\"Determine whether the frame has an association matrix computed.\n\n        Returns:\n            True if the frame has an association matrix otherwise, False.\n        \"\"\"\n        if self._asso_output is None or len(self._asso_output.matrix) == 0:\n            return False\n        return True\n\n    @asso_output.setter\n    def asso_output(self, asso_output: \"AssociationMatrix\") -&gt; None:\n        \"\"\"Set the association matrix of a frame.\n\n        Args:\n            asso_output: An arraylike (n_query, n_nonquery) association matrix between instances.\n        \"\"\"\n        self._asso_output = asso_output\n\n    @property\n    def matches(self) -&gt; tuple:\n        \"\"\"Matches between frame instances and available trajectories.\n\n        Returns:\n            A tuple containing the instance idx and trajectory idx for the matched instance.\n        \"\"\"\n        return self._matches\n\n    @matches.setter\n    def matches(self, matches: tuple) -&gt; None:\n        \"\"\"Set the frame matches.\n\n        Args:\n            matches: A tuple containing the instance idx and trajectory idx for the matched instance.\n        \"\"\"\n        self._matches = matches\n\n    def has_matches(self) -&gt; bool:\n        \"\"\"Check whether or not matches have been computed for frame.\n\n        Returns:\n            True if frame contains matches otherwise False.\n        \"\"\"\n        if self._matches is not None and len(self._matches) &gt; 0:\n            return True\n        return False\n\n    def get_traj_score(self, key: str | None = None) -&gt; dict | ArrayLike | None:\n        \"\"\"Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.\n\n        Args:\n            key: The key of the trajectory score to be accessed.\n                Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}\n\n        Returns:\n            - dictionary containing all trajectory scores if key is None\n            - trajectory score associated with key\n            - None if the key is not found\n        \"\"\"\n        if key is None:\n            return self._traj_score\n        else:\n            try:\n                return self._traj_score[key]\n            except KeyError as e:\n                logger.exception(f\"Could not access {key} traj_score due to {e}\")\n                return None\n\n    def add_traj_score(self, key: str, traj_score: ArrayLike) -&gt; None:\n        \"\"\"Add trajectory score to dictionary.\n\n        Args:\n            key: key associated with traj score to be used in dictionary\n            traj_score: association matrix between instances and trajectories\n        \"\"\"\n        self._traj_score[key] = traj_score\n\n    def has_traj_score(self) -&gt; bool:\n        \"\"\"Check if any trajectory association matrix has been saved.\n\n        Returns:\n            True there is at least one association matrix otherwise, false.\n        \"\"\"\n        if len(self._traj_score) == 0:\n            return False\n        return True\n\n    def has_gt_track_ids(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a gt track id.\n\n        Returns:\n            True if at least 1 instance has a gt track id otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_gt_track_id() for instance in self.instances])\n        return False\n\n    def get_gt_track_ids(self) -&gt; torch.Tensor:\n        \"\"\"Get the gt track ids of all instances in the frame.\n\n        Returns:\n            an (N,) shaped tensor with the gt track ids of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.gt_track_id for instance in self.instances])\n\n    def has_pred_track_ids(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a pred track id.\n\n        Returns:\n            True if at least 1 instance has a pred track id otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_pred_track_id() for instance in self.instances])\n        return False\n\n    def get_pred_track_ids(self) -&gt; torch.Tensor:\n        \"\"\"Get the pred track ids of all instances in the frame.\n\n        Returns:\n            an (N,) shaped tensor with the pred track ids of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.pred_track_id for instance in self.instances])\n\n    def has_bboxes(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a bounding box.\n\n        Returns:\n            True if at least 1 instance has a bounding box otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_bboxes() for instance in self.instances])\n        return False\n\n    def get_bboxes(self) -&gt; torch.Tensor:\n        \"\"\"Get the bounding boxes of all instances in the frame.\n\n        Returns:\n            an (N,4) shaped tensor with bounding boxes of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.empty(0, 4)\n        return torch.cat([instance.bbox for instance in self.instances], dim=0)\n\n    def has_crops(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a crop.\n\n        Returns:\n            True if at least 1 instance has a crop otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_crop() for instance in self.instances])\n        return False\n\n    def get_crops(self) -&gt; torch.Tensor:\n        \"\"\"Get the crops of all instances in the frame.\n\n        Returns:\n            an (N, C, H, W) shaped tensor with crops of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n\n        return torch.cat([instance.crop for instance in self.instances], dim=0)\n\n    def has_features(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has reid features already computed.\n\n        Returns:\n            True if at least 1 instance have reid features otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_features() for instance in self.instances])\n        return False\n\n    def get_features(self) -&gt; torch.Tensor:\n        \"\"\"Get the reid feature vectors of all instances in the frame.\n\n        Returns:\n            an (N, D) shaped tensor with reid feature vectors of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.features for instance in self.instances], dim=0)\n\n    def get_anchors(self) -&gt; list[str]:\n        \"\"\"Get the anchor names of instances in the frame.\n\n        Returns:\n            A list of anchor names used by the instances to get the crop.\n        \"\"\"\n        return [instance.anchor for instance in self.instances]\n\n    def get_centroids(self) -&gt; tuple[list[str], ArrayLike]:\n        \"\"\"Get the centroids around which each instance's crop was formed.\n\n        Returns:\n            anchors: the node names for the corresponding point\n            points: an n_instances x 2 array containing the centroids\n        \"\"\"\n        anchors = [\n            anchor for instance in self.instances for anchor in instance.centroid.keys()\n        ]\n\n        points = np.array(\n            [\n                point\n                for instance in self.instances\n                for point in instance.centroid.values()\n            ]\n        )\n\n        return (anchors, points)\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.asso_output","title":"<code>asso_output</code>  <code>property</code> <code>writable</code>","text":"<p>The association matrix between instances outputted directly by transformer.</p> <p>Returns:</p> Type Description <code>'AssociationMatrix'</code> <p>An arraylike (n_query, n_nonquery) association matrix between instances.</p>"},{"location":"reference/dreem/#dreem.Frame.device","title":"<code>device</code>  <code>property</code> <code>writable</code>","text":"<p>The device the frame is on.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the device the frame is on.</p>"},{"location":"reference/dreem/#dreem.Frame.frame_id","title":"<code>frame_id</code>  <code>property</code> <code>writable</code>","text":"<p>The index of the frame in a full video.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor containing the index of the frame in the video.</p>"},{"location":"reference/dreem/#dreem.Frame.img_shape","title":"<code>img_shape</code>  <code>property</code> <code>writable</code>","text":"<p>The shape of the pre-cropped frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor containing the shape of the frame. Should generally be (c, h, w)</p>"},{"location":"reference/dreem/#dreem.Frame.instances","title":"<code>instances</code>  <code>property</code> <code>writable</code>","text":"<p>A list of instances in the frame.</p> <p>Returns:</p> Type Description <code>list['Instance']</code> <p>The list of instances that appear in the frame.</p>"},{"location":"reference/dreem/#dreem.Frame.matches","title":"<code>matches</code>  <code>property</code> <code>writable</code>","text":"<p>Matches between frame instances and available trajectories.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the instance idx and trajectory idx for the matched instance.</p>"},{"location":"reference/dreem/#dreem.Frame.num_detected","title":"<code>num_detected</code>  <code>property</code>","text":"<p>The number of instances in the frame.</p> <p>Returns:</p> Type Description <code>int</code> <p>the number of instances in the frame.</p>"},{"location":"reference/dreem/#dreem.Frame.vid_name","title":"<code>vid_name</code>  <code>property</code>","text":"<p>Get the path to the video corresponding to this frame.</p> <p>Returns: A str file path corresponding to the frame.</p>"},{"location":"reference/dreem/#dreem.Frame.video","title":"<code>video</code>  <code>property</code> <code>writable</code>","text":"<p>Get the video associated with the frame.</p> <p>Returns: An sio.Video object representing the video or a placeholder string if it is not possible to create the sio.Video</p>"},{"location":"reference/dreem/#dreem.Frame.video_id","title":"<code>video_id</code>  <code>property</code> <code>writable</code>","text":"<p>The index of the video the frame comes from.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the video index.</p>"},{"location":"reference/dreem/#dreem.Frame.__attrs_post_init__","title":"<code>__attrs_post_init__()</code>","text":"<p>Handle more intricate default initializations and moving to device.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def __attrs_post_init__(self) -&gt; None:\n    \"\"\"Handle more intricate default initializations and moving to device.\"\"\"\n    if len(self.img_shape) == 0:\n        self.img_shape = torch.tensor([0, 0, 0])\n\n    for instance in self.instances:\n        instance.frame = self\n\n    self.to(self.device)\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.__repr__","title":"<code>__repr__()</code>","text":"<p>Return String representation of the Frame.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return String representation of the Frame.\n\n    Returns:\n        The string representation of the frame.\n    \"\"\"\n    return (\n        \"Frame(\"\n        f\"video={self._video.filename if isinstance(self._video, sio.Video) else self._video}, \"\n        f\"video_id={self._video_id.item()}, \"\n        f\"frame_id={self._frame_id.item()}, \"\n        f\"img_shape={self._img_shape}, \"\n        f\"num_detected={self.num_detected}, \"\n        f\"asso_output={self._asso_output}, \"\n        f\"traj_score={self._traj_score}, \"\n        f\"matches={self._matches}, \"\n        f\"instances={self._instances}, \"\n        f\"device={self._device}\"\n        \")\"\n    )\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.add_traj_score","title":"<code>add_traj_score(key, traj_score)</code>","text":"<p>Add trajectory score to dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key associated with traj score to be used in dictionary</p> required <code>traj_score</code> <code>ArrayLike</code> <p>association matrix between instances and trajectories</p> required Source code in <code>dreem/io/frame.py</code> <pre><code>def add_traj_score(self, key: str, traj_score: ArrayLike) -&gt; None:\n    \"\"\"Add trajectory score to dictionary.\n\n    Args:\n        key: key associated with traj score to be used in dictionary\n        traj_score: association matrix between instances and trajectories\n    \"\"\"\n    self._traj_score[key] = traj_score\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.from_slp","title":"<code>from_slp(lf, video_id=0, device=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Convert <code>sio.LabeledFrame</code> to <code>dreem.io.Frame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>lf</code> <code>LabeledFrame</code> <p>A sio.LabeledFrame object</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A dreem.io.Frame object</p> Source code in <code>dreem/io/frame.py</code> <pre><code>@classmethod\ndef from_slp(\n    cls,\n    lf: sio.LabeledFrame,\n    video_id: int = 0,\n    device: str | None = None,\n    **kwargs,\n) -&gt; Self:\n    \"\"\"Convert `sio.LabeledFrame` to `dreem.io.Frame`.\n\n    Args:\n        lf: A sio.LabeledFrame object\n\n    Returns:\n        A dreem.io.Frame object\n    \"\"\"\n    from dreem.io.instance import Instance\n\n    img_shape = lf.image.shape\n    if len(img_shape) == 2:\n        img_shape = (1, *img_shape)\n    elif len(img_shape) &gt; 2 and img_shape[-1] &lt;= 3:\n        img_shape = (lf.image.shape[-1], lf.image.shape[0], lf.image.shape[1])\n    return cls(\n        video_id=video_id,\n        frame_id=(\n            lf.frame_idx.astype(np.int32)\n            if isinstance(lf.frame_idx, np.number)\n            else lf.frame_idx\n        ),\n        vid_file=lf.video.filename,\n        img_shape=img_shape,\n        instances=[Instance.from_slp(instance, **kwargs) for instance in lf],\n        device=device,\n    )\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.get_anchors","title":"<code>get_anchors()</code>","text":"<p>Get the anchor names of instances in the frame.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of anchor names used by the instances to get the crop.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_anchors(self) -&gt; list[str]:\n    \"\"\"Get the anchor names of instances in the frame.\n\n    Returns:\n        A list of anchor names used by the instances to get the crop.\n    \"\"\"\n    return [instance.anchor for instance in self.instances]\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.get_bboxes","title":"<code>get_bboxes()</code>","text":"<p>Get the bounding boxes of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,4) shaped tensor with bounding boxes of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_bboxes(self) -&gt; torch.Tensor:\n    \"\"\"Get the bounding boxes of all instances in the frame.\n\n    Returns:\n        an (N,4) shaped tensor with bounding boxes of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.empty(0, 4)\n    return torch.cat([instance.bbox for instance in self.instances], dim=0)\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.get_centroids","title":"<code>get_centroids()</code>","text":"<p>Get the centroids around which each instance's crop was formed.</p> <p>Returns:</p> Name Type Description <code>anchors</code> <code>tuple[list[str], ArrayLike]</code> <p>the node names for the corresponding point points: an n_instances x 2 array containing the centroids</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_centroids(self) -&gt; tuple[list[str], ArrayLike]:\n    \"\"\"Get the centroids around which each instance's crop was formed.\n\n    Returns:\n        anchors: the node names for the corresponding point\n        points: an n_instances x 2 array containing the centroids\n    \"\"\"\n    anchors = [\n        anchor for instance in self.instances for anchor in instance.centroid.keys()\n    ]\n\n    points = np.array(\n        [\n            point\n            for instance in self.instances\n            for point in instance.centroid.values()\n        ]\n    )\n\n    return (anchors, points)\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.get_crops","title":"<code>get_crops()</code>","text":"<p>Get the crops of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N, C, H, W) shaped tensor with crops of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_crops(self) -&gt; torch.Tensor:\n    \"\"\"Get the crops of all instances in the frame.\n\n    Returns:\n        an (N, C, H, W) shaped tensor with crops of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n\n    return torch.cat([instance.crop for instance in self.instances], dim=0)\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.get_features","title":"<code>get_features()</code>","text":"<p>Get the reid feature vectors of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N, D) shaped tensor with reid feature vectors of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_features(self) -&gt; torch.Tensor:\n    \"\"\"Get the reid feature vectors of all instances in the frame.\n\n    Returns:\n        an (N, D) shaped tensor with reid feature vectors of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.features for instance in self.instances], dim=0)\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.get_gt_track_ids","title":"<code>get_gt_track_ids()</code>","text":"<p>Get the gt track ids of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,) shaped tensor with the gt track ids of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_gt_track_ids(self) -&gt; torch.Tensor:\n    \"\"\"Get the gt track ids of all instances in the frame.\n\n    Returns:\n        an (N,) shaped tensor with the gt track ids of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.gt_track_id for instance in self.instances])\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.get_pred_track_ids","title":"<code>get_pred_track_ids()</code>","text":"<p>Get the pred track ids of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,) shaped tensor with the pred track ids of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_pred_track_ids(self) -&gt; torch.Tensor:\n    \"\"\"Get the pred track ids of all instances in the frame.\n\n    Returns:\n        an (N,) shaped tensor with the pred track ids of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.pred_track_id for instance in self.instances])\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.get_traj_score","title":"<code>get_traj_score(key=None)</code>","text":"<p>Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str | None</code> <p>The key of the trajectory score to be accessed. Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}</p> <code>None</code> <p>Returns:</p> Type Description <code>dict | ArrayLike | None</code> <ul> <li>dictionary containing all trajectory scores if key is None</li> <li>trajectory score associated with key</li> <li>None if the key is not found</li> </ul> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_traj_score(self, key: str | None = None) -&gt; dict | ArrayLike | None:\n    \"\"\"Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.\n\n    Args:\n        key: The key of the trajectory score to be accessed.\n            Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}\n\n    Returns:\n        - dictionary containing all trajectory scores if key is None\n        - trajectory score associated with key\n        - None if the key is not found\n    \"\"\"\n    if key is None:\n        return self._traj_score\n    else:\n        try:\n            return self._traj_score[key]\n        except KeyError as e:\n            logger.exception(f\"Could not access {key} traj_score due to {e}\")\n            return None\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.has_asso_output","title":"<code>has_asso_output()</code>","text":"<p>Determine whether the frame has an association matrix computed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the frame has an association matrix otherwise, False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_asso_output(self) -&gt; bool:\n    \"\"\"Determine whether the frame has an association matrix computed.\n\n    Returns:\n        True if the frame has an association matrix otherwise, False.\n    \"\"\"\n    if self._asso_output is None or len(self._asso_output.matrix) == 0:\n        return False\n    return True\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.has_bboxes","title":"<code>has_bboxes()</code>","text":"<p>Check if any of frames instances has a bounding box.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a bounding box otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_bboxes(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a bounding box.\n\n    Returns:\n        True if at least 1 instance has a bounding box otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_bboxes() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.has_crops","title":"<code>has_crops()</code>","text":"<p>Check if any of frames instances has a crop.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a crop otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_crops(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a crop.\n\n    Returns:\n        True if at least 1 instance has a crop otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_crop() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.has_features","title":"<code>has_features()</code>","text":"<p>Check if any of frames instances has reid features already computed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance have reid features otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_features(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has reid features already computed.\n\n    Returns:\n        True if at least 1 instance have reid features otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_features() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.has_gt_track_ids","title":"<code>has_gt_track_ids()</code>","text":"<p>Check if any of frames instances has a gt track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a gt track id otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_gt_track_ids(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a gt track id.\n\n    Returns:\n        True if at least 1 instance has a gt track id otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_gt_track_id() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.has_instances","title":"<code>has_instances()</code>","text":"<p>Determine whether there are instances in the frame.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if there are instances in the frame, otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_instances(self) -&gt; bool:\n    \"\"\"Determine whether there are instances in the frame.\n\n    Returns:\n        True if there are instances in the frame, otherwise False.\n    \"\"\"\n    if self.num_detected == 0:\n        return False\n    return True\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.has_matches","title":"<code>has_matches()</code>","text":"<p>Check whether or not matches have been computed for frame.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if frame contains matches otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_matches(self) -&gt; bool:\n    \"\"\"Check whether or not matches have been computed for frame.\n\n    Returns:\n        True if frame contains matches otherwise False.\n    \"\"\"\n    if self._matches is not None and len(self._matches) &gt; 0:\n        return True\n    return False\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.has_pred_track_ids","title":"<code>has_pred_track_ids()</code>","text":"<p>Check if any of frames instances has a pred track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a pred track id otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_pred_track_ids(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a pred track id.\n\n    Returns:\n        True if at least 1 instance has a pred track id otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_pred_track_id() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.has_traj_score","title":"<code>has_traj_score()</code>","text":"<p>Check if any trajectory association matrix has been saved.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True there is at least one association matrix otherwise, false.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_traj_score(self) -&gt; bool:\n    \"\"\"Check if any trajectory association matrix has been saved.\n\n    Returns:\n        True there is at least one association matrix otherwise, false.\n    \"\"\"\n    if len(self._traj_score) == 0:\n        return False\n    return True\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.to","title":"<code>to(map_location)</code>","text":"<p>Move frame to different device or dtype (See <code>torch.to</code> for more info).</p> <p>Parameters:</p> Name Type Description Default <code>map_location</code> <code>str | device</code> <p>A string representing the device to move to.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The frame moved to a different device/dtype.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to(self, map_location: str | torch.device) -&gt; Self:\n    \"\"\"Move frame to different device or dtype (See `torch.to` for more info).\n\n    Args:\n        map_location: A string representing the device to move to.\n\n    Returns:\n        The frame moved to a different device/dtype.\n    \"\"\"\n    self._video_id = self._video_id.to(map_location)\n    self._frame_id = self._frame_id.to(map_location)\n    self._img_shape = self._img_shape.to(map_location)\n\n    if isinstance(self._asso_output, torch.Tensor):\n        self._asso_output = self._asso_output.to(map_location)\n\n    if isinstance(self._matches, torch.Tensor):\n        self._matches = self._matches.to(map_location)\n\n    for key, val in self._traj_score.items():\n        if isinstance(val, torch.Tensor):\n            self._traj_score[key] = val.to(map_location)\n    for instance in self.instances:\n        instance = instance.to(map_location)\n\n    if isinstance(map_location, (str, torch.device)):\n        self._device = map_location\n\n    return self\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.to_h5","title":"<code>to_h5(clip_group, instance_labels=None, save=None)</code>","text":"<p>Convert frame to h5py group.</p> <p>Parameters:</p> Name Type Description Default <code>clip_group</code> <code>Group</code> <p>the h5py group representing the clip (e.g batch/video) the frame belongs to</p> required <code>instance_labels</code> <code>list | None</code> <p>the labels used to create instance group names</p> <code>None</code> <code>save</code> <code>dict[str, bool] | None</code> <p>whether to save crops, features and embeddings for the instance</p> <code>None</code> <p>Returns:     An h5py group containing the frame</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to_h5(\n    self,\n    clip_group: h5py.Group,\n    instance_labels: list | None = None,\n    save: dict[str, bool] | None = None,\n) -&gt; h5py.Group:\n    \"\"\"Convert frame to h5py group.\n\n    Args:\n        clip_group: the h5py group representing the clip (e.g batch/video) the frame belongs to\n        instance_labels: the labels used to create instance group names\n        save: whether to save crops, features and embeddings for the instance\n    Returns:\n        An h5py group containing the frame\n    \"\"\"\n    if save is None:\n        save = {\"crop\": False, \"features\": False, \"embeddings\": False}\n    frame_group = clip_group.require_group(f\"frame_{self.frame_id.item()}\")\n    frame_group.attrs.create(\"frame_id\", self.frame_id.item())\n    frame_group.attrs.create(\"vid_id\", self.video_id.item())\n    frame_group.attrs.create(\"vid_name\", self.vid_name)\n\n    frame_group.create_dataset(\n        \"asso_matrix\",\n        data=self.asso_output.numpy() if self.asso_output is not None else [],\n    )\n    asso_group = frame_group.require_group(\"traj_scores\")\n    for key, value in self.get_traj_score().items():\n        asso_group.create_dataset(\n            key, data=value.to_numpy() if value is not None else []\n        )\n\n    if instance_labels is None:\n        instance_labels = self.get_gt_track_ids.cpu().numpy()\n    for instance_label, instance in zip(instance_labels, self.instances):\n        kwargs = {}\n        if save.get(\"crop\", False):\n            kwargs[\"crop\"] = instance.crop.cpu().numpy()\n        if save.get(\"features\", False):\n            kwargs[\"features\"] = instance.features.cpu().numpy()\n        if save.get(\"embeddings\", False):\n            for key, val in instance.get_embedding().items():\n                kwargs[f\"{key}_emb\"] = val.cpu().numpy()\n        _ = instance.to_h5(frame_group, f\"instance_{instance_label}\", **kwargs)\n\n    return frame_group\n</code></pre>"},{"location":"reference/dreem/#dreem.Frame.to_slp","title":"<code>to_slp(track_lookup=None, video=None)</code>","text":"<p>Convert Frame to sleap_io.LabeledFrame object.</p> <p>Parameters:</p> Name Type Description Default <code>track_lookup</code> <code>dict[int, Track] | None</code> <p>A lookup dictionary containing the track_id and sio.Track for persistence</p> <code>None</code> <code>video</code> <code>Video | None</code> <p>An sio.Video object used for overriding.</p> <code>None</code> <p>Returns: A tuple containing a LabeledFrame object with necessary metadata and a lookup dictionary containing the track_id and sio.Track for persistence</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to_slp(\n    self,\n    track_lookup: dict[int, sio.Track] | None = None,\n    video: sio.Video | None = None,\n) -&gt; tuple[sio.LabeledFrame, dict[int, sio.Track]]:\n    \"\"\"Convert Frame to sleap_io.LabeledFrame object.\n\n    Args:\n        track_lookup: A lookup dictionary containing the track_id and sio.Track for persistence\n        video: An sio.Video object used for overriding.\n\n    Returns: A tuple containing a LabeledFrame object with necessary metadata and\n    a lookup dictionary containing the track_id and sio.Track for persistence\n    \"\"\"\n    if track_lookup is None:\n        track_lookup = {}\n\n    slp_instances = []\n    for instance in self.instances:\n        slp_instance, track_lookup = instance.to_slp(track_lookup=track_lookup)\n        slp_instances.append(slp_instance)\n\n    if video is None:\n        video = (\n            self.video\n            if isinstance(self.video, sio.Video)\n            else sio.load_video(self.video)\n        )\n\n    return (\n        sio.LabeledFrame(\n            video=video,\n            frame_idx=self.frame_id.item(),\n            instances=slp_instances,\n        ),\n        track_lookup,\n    )\n</code></pre>"},{"location":"reference/dreem/#dreem.GTRRunner","title":"<code>GTRRunner</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A lightning wrapper around GTR model.</p> <p>Used for training, validation and inference.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize a lightning module for GTR.</p> <code>configure_optimizers</code> <p>Get optimizers and schedulers for training.</p> <code>forward</code> <p>Execute forward pass of the lightning module.</p> <code>log_metrics</code> <p>Log metrics computed during evaluation.</p> <code>on_test_end</code> <p>Run inference and metrics pipeline to compute metrics for test set.</p> <code>on_validation_epoch_end</code> <p>Execute hook for validation end.</p> <code>predict_step</code> <p>Run inference for model.</p> <code>test_step</code> <p>Execute single test step for model.</p> <code>training_step</code> <p>Execute single training step for model.</p> <code>validation_step</code> <p>Execute single val step for model.</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>class GTRRunner(LightningModule):\n    \"\"\"A lightning wrapper around GTR model.\n\n    Used for training, validation and inference.\n    \"\"\"\n\n    DEFAULT_METRICS = {\n        \"train\": [],\n        \"val\": [],\n        \"test\": [\"num_switches\", \"global_tracking_accuracy\"],\n    }\n    DEFAULT_TRACKING = {\n        \"train\": False,\n        \"val\": False,\n        \"test\": True,\n    }\n    DEFAULT_SAVE = {\"train\": False, \"val\": False, \"test\": False}\n\n    def __init__(\n        self,\n        model_cfg: dict | None = None,\n        tracker_cfg: dict | None = None,\n        loss_cfg: dict | None = None,\n        optimizer_cfg: dict | None = None,\n        scheduler_cfg: dict | None = None,\n        metrics: dict[str, list[str]] | None = None,\n        persistent_tracking: dict[str, bool] | None = None,\n        test_save_path: str = \"./test_results.h5\",\n    ):\n        \"\"\"Initialize a lightning module for GTR.\n\n        Args:\n            model_cfg: hyperparameters for GlobalTrackingTransformer\n            tracker_cfg: The parameters used for the tracker post-processing\n            loss_cfg: hyperparameters for AssoLoss\n            optimizer_cfg: hyper parameters used for optimizer.\n                       Only used to overwrite `configure_optimizer`\n            scheduler_cfg: hyperparameters for lr_scheduler used to overwrite `configure_optimizer\n            metrics: a dict containing the metrics to be computed during train, val, and test.\n            persistent_tracking: a dict containing whether to use persistent tracking during train, val and test inference.\n            test_save_path: path to a directory to save the eval and tracking results to\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.model_cfg = model_cfg if model_cfg else {}\n        self.loss_cfg = loss_cfg if loss_cfg else {}\n        self.tracker_cfg = tracker_cfg if tracker_cfg else {}\n\n        self.model = GlobalTrackingTransformer(**self.model_cfg)\n        self.loss = AssoLoss(**self.loss_cfg)\n        if self.tracker_cfg.get(\"tracker_type\", \"standard\") == \"batch\":\n            from dreem.inference.batch_tracker import BatchTracker\n\n            self.tracker = BatchTracker(**self.tracker_cfg)\n        else:\n            from dreem.inference.tracker import Tracker\n\n            self.tracker = Tracker(**self.tracker_cfg)\n        self.optimizer_cfg = optimizer_cfg\n        self.scheduler_cfg = scheduler_cfg\n\n        self.metrics = metrics if metrics is not None else self.DEFAULT_METRICS\n        self.persistent_tracking = (\n            persistent_tracking\n            if persistent_tracking is not None\n            else self.DEFAULT_TRACKING\n        )\n        self.test_results = {\"preds\": [], \"save_path\": test_save_path}\n\n    def forward(\n        self,\n        ref_instances: list[\"dreem.io.Instance\"],\n        query_instances: list[\"dreem.io.Instance\"] | None = None,\n    ) -&gt; list[\"AssociationMatrix\"]:\n        \"\"\"Execute forward pass of the lightning module.\n\n        Args:\n            ref_instances: a list of `Instance` objects containing crops and other data needed for transformer model\n            query_instances: a list of `Instance` objects used as queries in the decoder. Mostly used for inference.\n\n        Returns:\n            An association matrix between objects\n        \"\"\"\n        asso_preds = self.model(ref_instances, query_instances)\n        return asso_preds\n\n    def training_step(\n        self, train_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single training step for model.\n\n        Args:\n            train_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the train loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(train_batch[0], mode=\"train\")\n        self.log_metrics(result, len(train_batch[0]), \"train\")\n\n        return result\n\n    def validation_step(\n        self, val_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single val step for model.\n\n        Args:\n            val_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the val loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(val_batch[0], mode=\"val\")\n        self.log_metrics(result, len(val_batch[0]), \"val\")\n\n        return result\n\n    def test_step(\n        self, test_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single test step for model.\n\n        Args:\n            test_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the val loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(test_batch[0], mode=\"test\")\n        self.log_metrics(result, len(test_batch[0]), \"test\")\n\n        return result\n\n    def predict_step(\n        self, batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; list[\"dreem.io.Frame\"]:\n        \"\"\"Run inference for model.\n\n        Computes association + assignment.\n\n        Args:\n            batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A list of dicts where each dict is a frame containing the predicted track ids\n        \"\"\"\n        frames_pred = self.tracker(self.model, batch[0])\n        return frames_pred\n\n    def _shared_eval_step(\n        self, frames: list[\"dreem.io.Frame\"], mode: str\n    ) -&gt; dict[str, float]:\n        \"\"\"Run evaluation used by train, test, and val steps.\n\n        Args:\n            frames: A list of dicts where each dict is a frame containing gt data\n            mode: which metrics to compute and whether to use persistent tracking or not\n\n        Returns:\n            a dict containing the loss and any other metrics specified by `eval_metrics`\n        \"\"\"\n        try:\n            instances = [instance for frame in frames for instance in frame.instances]\n\n            if len(instances) == 0:\n                return None\n\n            # eval_metrics = self.metrics[mode]  # Currently unused but available for future metric computation\n\n            logits = self(instances)\n            logits = [asso.matrix for asso in logits]\n            loss = self.loss(logits, frames)\n\n            return_metrics = {\"loss\": loss}\n            if mode == \"test\":\n                self.tracker.persistent_tracking = True\n                frames_pred = self.tracker(self.model, frames)\n                self.test_results[\"preds\"].extend(\n                    [frame.to(\"cpu\") for frame in frames_pred]\n                )\n            return_metrics[\"batch_size\"] = len(frames)\n        except Exception as e:\n            logger.exception(\n                f\"Failed on frame {frames[0].frame_id} of video {frames[0].video_id}\"\n            )\n            logger.exception(e)\n            raise (e)\n\n        return return_metrics\n\n    def configure_optimizers(self) -&gt; dict:\n        \"\"\"Get optimizers and schedulers for training.\n\n        Is overridden by config but defaults to Adam + ReduceLROnPlateau.\n\n        Returns:\n            an optimizer config dict containing the optimizer, scheduler, and scheduler params\n        \"\"\"\n        # todo: init from config\n        if self.optimizer_cfg is None:\n            optimizer = torch.optim.Adam(self.parameters(), lr=1e-4, betas=(0.9, 0.999))\n        else:\n            optimizer = init_optimizer(self.parameters(), self.optimizer_cfg)\n\n        if self.scheduler_cfg is None:\n            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer, \"min\", 0.5, 10\n            )\n        else:\n            scheduler = init_scheduler(optimizer, self.scheduler_cfg)\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n                \"interval\": \"epoch\",\n                \"frequency\": 1,\n            },\n        }\n\n    def log_metrics(self, result: dict, batch_size: int, mode: str) -&gt; None:\n        \"\"\"Log metrics computed during evaluation.\n\n        Args:\n            result: A dict containing metrics to be logged.\n            batch_size: the size of the batch used to compute the metrics\n            mode: One of {'train', 'test' or 'val'}. Used as prefix while logging.\n        \"\"\"\n        if result:\n            batch_size = result.pop(\"batch_size\")\n            for metric, val in result.items():\n                if isinstance(val, torch.Tensor):\n                    val = val.item()\n                self.log(f\"{mode}_{metric}\", val, batch_size=batch_size)\n\n    def on_validation_epoch_end(self):\n        \"\"\"Execute hook for validation end.\n\n        Currently, we simply clear the gpu cache and do garbage collection.\n        \"\"\"\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def on_test_end(self):\n        \"\"\"Run inference and metrics pipeline to compute metrics for test set.\n\n        Args:\n            test_results: dict containing predictions and metrics to be filled out in metrics.evaluate\n            metrics: list of metrics to compute\n        \"\"\"\n        # input validation\n        metrics_to_compute = self.metrics[\n            \"test\"\n        ]  # list of metrics to compute, or \"all\"\n        if metrics_to_compute == \"all\":\n            metrics_to_compute = [\"motmetrics\", \"global_tracking_accuracy\"]\n        if isinstance(metrics_to_compute, str):\n            metrics_to_compute = [metrics_to_compute]\n        for metric in metrics_to_compute:\n            if metric not in [\"motmetrics\", \"global_tracking_accuracy\"]:\n                raise ValueError(\n                    f\"Metric {metric} not supported. Please select from 'motmetrics' or 'global_tracking_accuracy'\"\n                )\n\n        preds = self.test_results[\"preds\"]\n\n        # results is a dict with key being the metric name, and value being the metric value computed\n        results = metrics.evaluate(preds, metrics_to_compute)\n\n        # save metrics and frame metadata to hdf5\n\n        # Get the video name from the first frame\n        vid_name = Path(preds[0].vid_name).stem\n        # save the results to an hdf5 file\n        fname = os.path.join(\n            self.test_results[\"save_path\"], f\"{vid_name}.dreem_metrics.h5\"\n        )\n        logger.info(f\"Saving metrics to {fname}\")\n        # Check if the h5 file exists and add a suffix to prevent name collision\n        suffix_counter = 0\n        original_fname = fname\n        while os.path.exists(fname):\n            suffix_counter += 1\n            fname = original_fname.replace(\n                \".dreem_metrics.h5\", f\"_{suffix_counter}.dreem_metrics.h5\"\n            )\n\n        if suffix_counter &gt; 0:\n            logger.info(f\"File already exists. Saving to {fname} instead\")\n\n        with h5py.File(fname, \"a\") as results_file:\n            # Create a group for this video\n            vid_group = results_file.require_group(vid_name)\n            # Save each metric\n            for metric_name, value in results.items():\n                if metric_name == \"motmetrics\":\n                    # For num_switches, save mot_summary and mot_events separately\n                    mot_summary = value[0]\n                    mot_events = value[1]\n                    frame_switch_map = value[2]\n                    mot_summary_group = vid_group.require_group(\"mot_summary\")\n                    # Loop through each row in mot_summary and save as attributes\n                    for _, row in mot_summary.iterrows():\n                        mot_summary_group.attrs[row.name] = row[\"acc\"]\n                    # save extra metadata for frames in which there is a switch\n                    for frame_id, switch in frame_switch_map.items():\n                        frame = preds[frame_id]\n                        frame = frame.to(\"cpu\")\n                        if switch:\n                            _ = frame.to_h5(\n                                vid_group,\n                                frame.get_gt_track_ids().cpu().numpy(),\n                                save={\n                                    \"crop\": True,\n                                    \"features\": True,\n                                    \"embeddings\": True,\n                                },\n                            )\n                        else:\n                            _ = frame.to_h5(\n                                vid_group, frame.get_gt_track_ids().cpu().numpy()\n                            )\n                    # save motevents log to csv\n                    motevents_path = os.path.join(\n                        self.test_results[\"save_path\"], f\"{vid_name}.motevents.csv\"\n                    )\n                    logger.info(f\"Saving motevents log to {motevents_path}\")\n                    mot_events.to_csv(motevents_path, index=False)\n\n                elif metric_name == \"global_tracking_accuracy\":\n                    gta_by_gt_track = value\n                    gta_group = vid_group.require_group(\"global_tracking_accuracy\")\n                    # save as a key value pair with gt track id: gta\n                    for gt_track_id, gta in gta_by_gt_track.items():\n                        gta_group.attrs[f\"track_{gt_track_id}\"] = gta\n\n        # save the tracking results to a slp/labelled masks file\n        if isinstance(self.trainer.test_dataloaders.dataset, CellTrackingDataset):\n            outpath = os.path.join(\n                self.test_results[\"save_path\"],\n                f\"{vid_name}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.tif\",\n            )\n            pred_imgs = []\n            for frame in preds:\n                frame_masks = []\n                for instance in frame.instances:\n                    # centroid = instance.centroid[\"centroid\"]  # Currently unused but available if needed\n                    mask = instance.mask.cpu().numpy()\n                    track_id = instance.pred_track_id.cpu().numpy().item()\n                    mask = mask.astype(np.uint8)\n                    mask[mask != 0] = track_id  # label the mask with the track id\n                    frame_masks.append(mask)\n                frame_mask = np.max(frame_masks, axis=0)\n                pred_imgs.append(frame_mask)\n            pred_imgs = np.stack(pred_imgs)\n            tifffile.imwrite(outpath, pred_imgs.astype(np.uint16))\n        else:\n            outpath = os.path.join(\n                self.test_results[\"save_path\"],\n                f\"{vid_name}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.slp\",\n            )\n            pred_slp = []\n\n            logger.info(f\"Saving inference results to {outpath}\")\n            # save the tracking results to a slp file\n            tracks = {}\n            for frame in preds:\n                if frame.frame_id.item() == 0:\n                    video = (\n                        sio.Video(frame.video)\n                        if isinstance(frame.video, str)\n                        else sio.Video\n                    )\n                lf, tracks = frame.to_slp(tracks, video=video)\n                pred_slp.append(lf)\n            pred_slp = sio.Labels(pred_slp)\n\n            pred_slp.save(outpath)\n\n        # clear the preds\n        self.test_results[\"preds\"] = []\n</code></pre>"},{"location":"reference/dreem/#dreem.GTRRunner.__init__","title":"<code>__init__(model_cfg=None, tracker_cfg=None, loss_cfg=None, optimizer_cfg=None, scheduler_cfg=None, metrics=None, persistent_tracking=None, test_save_path='./test_results.h5')</code>","text":"<p>Initialize a lightning module for GTR.</p> <p>Parameters:</p> Name Type Description Default <code>model_cfg</code> <code>dict | None</code> <p>hyperparameters for GlobalTrackingTransformer</p> <code>None</code> <code>tracker_cfg</code> <code>dict | None</code> <p>The parameters used for the tracker post-processing</p> <code>None</code> <code>loss_cfg</code> <code>dict | None</code> <p>hyperparameters for AssoLoss</p> <code>None</code> <code>optimizer_cfg</code> <code>dict | None</code> <p>hyper parameters used for optimizer.        Only used to overwrite <code>configure_optimizer</code></p> <code>None</code> <code>scheduler_cfg</code> <code>dict | None</code> <p>hyperparameters for lr_scheduler used to overwrite `configure_optimizer</p> <code>None</code> <code>metrics</code> <code>dict[str, list[str]] | None</code> <p>a dict containing the metrics to be computed during train, val, and test.</p> <code>None</code> <code>persistent_tracking</code> <code>dict[str, bool] | None</code> <p>a dict containing whether to use persistent tracking during train, val and test inference.</p> <code>None</code> <code>test_save_path</code> <code>str</code> <p>path to a directory to save the eval and tracking results to</p> <code>'./test_results.h5'</code> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def __init__(\n    self,\n    model_cfg: dict | None = None,\n    tracker_cfg: dict | None = None,\n    loss_cfg: dict | None = None,\n    optimizer_cfg: dict | None = None,\n    scheduler_cfg: dict | None = None,\n    metrics: dict[str, list[str]] | None = None,\n    persistent_tracking: dict[str, bool] | None = None,\n    test_save_path: str = \"./test_results.h5\",\n):\n    \"\"\"Initialize a lightning module for GTR.\n\n    Args:\n        model_cfg: hyperparameters for GlobalTrackingTransformer\n        tracker_cfg: The parameters used for the tracker post-processing\n        loss_cfg: hyperparameters for AssoLoss\n        optimizer_cfg: hyper parameters used for optimizer.\n                   Only used to overwrite `configure_optimizer`\n        scheduler_cfg: hyperparameters for lr_scheduler used to overwrite `configure_optimizer\n        metrics: a dict containing the metrics to be computed during train, val, and test.\n        persistent_tracking: a dict containing whether to use persistent tracking during train, val and test inference.\n        test_save_path: path to a directory to save the eval and tracking results to\n    \"\"\"\n    super().__init__()\n    self.save_hyperparameters()\n\n    self.model_cfg = model_cfg if model_cfg else {}\n    self.loss_cfg = loss_cfg if loss_cfg else {}\n    self.tracker_cfg = tracker_cfg if tracker_cfg else {}\n\n    self.model = GlobalTrackingTransformer(**self.model_cfg)\n    self.loss = AssoLoss(**self.loss_cfg)\n    if self.tracker_cfg.get(\"tracker_type\", \"standard\") == \"batch\":\n        from dreem.inference.batch_tracker import BatchTracker\n\n        self.tracker = BatchTracker(**self.tracker_cfg)\n    else:\n        from dreem.inference.tracker import Tracker\n\n        self.tracker = Tracker(**self.tracker_cfg)\n    self.optimizer_cfg = optimizer_cfg\n    self.scheduler_cfg = scheduler_cfg\n\n    self.metrics = metrics if metrics is not None else self.DEFAULT_METRICS\n    self.persistent_tracking = (\n        persistent_tracking\n        if persistent_tracking is not None\n        else self.DEFAULT_TRACKING\n    )\n    self.test_results = {\"preds\": [], \"save_path\": test_save_path}\n</code></pre>"},{"location":"reference/dreem/#dreem.GTRRunner.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Get optimizers and schedulers for training.</p> <p>Is overridden by config but defaults to Adam + ReduceLROnPlateau.</p> <p>Returns:</p> Type Description <code>dict</code> <p>an optimizer config dict containing the optimizer, scheduler, and scheduler params</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def configure_optimizers(self) -&gt; dict:\n    \"\"\"Get optimizers and schedulers for training.\n\n    Is overridden by config but defaults to Adam + ReduceLROnPlateau.\n\n    Returns:\n        an optimizer config dict containing the optimizer, scheduler, and scheduler params\n    \"\"\"\n    # todo: init from config\n    if self.optimizer_cfg is None:\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4, betas=(0.9, 0.999))\n    else:\n        optimizer = init_optimizer(self.parameters(), self.optimizer_cfg)\n\n    if self.scheduler_cfg is None:\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, \"min\", 0.5, 10\n        )\n    else:\n        scheduler = init_scheduler(optimizer, self.scheduler_cfg)\n\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"monitor\": \"val_loss\",\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        },\n    }\n</code></pre>"},{"location":"reference/dreem/#dreem.GTRRunner.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute forward pass of the lightning module.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>a list of <code>Instance</code> objects containing crops and other data needed for transformer model</p> required <code>query_instances</code> <code>list[Instance] | None</code> <p>a list of <code>Instance</code> objects used as queries in the decoder. Mostly used for inference.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[AssociationMatrix]</code> <p>An association matrix between objects</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def forward(\n    self,\n    ref_instances: list[\"dreem.io.Instance\"],\n    query_instances: list[\"dreem.io.Instance\"] | None = None,\n) -&gt; list[\"AssociationMatrix\"]:\n    \"\"\"Execute forward pass of the lightning module.\n\n    Args:\n        ref_instances: a list of `Instance` objects containing crops and other data needed for transformer model\n        query_instances: a list of `Instance` objects used as queries in the decoder. Mostly used for inference.\n\n    Returns:\n        An association matrix between objects\n    \"\"\"\n    asso_preds = self.model(ref_instances, query_instances)\n    return asso_preds\n</code></pre>"},{"location":"reference/dreem/#dreem.GTRRunner.log_metrics","title":"<code>log_metrics(result, batch_size, mode)</code>","text":"<p>Log metrics computed during evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>dict</code> <p>A dict containing metrics to be logged.</p> required <code>batch_size</code> <code>int</code> <p>the size of the batch used to compute the metrics</p> required <code>mode</code> <code>str</code> <p>One of {'train', 'test' or 'val'}. Used as prefix while logging.</p> required Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def log_metrics(self, result: dict, batch_size: int, mode: str) -&gt; None:\n    \"\"\"Log metrics computed during evaluation.\n\n    Args:\n        result: A dict containing metrics to be logged.\n        batch_size: the size of the batch used to compute the metrics\n        mode: One of {'train', 'test' or 'val'}. Used as prefix while logging.\n    \"\"\"\n    if result:\n        batch_size = result.pop(\"batch_size\")\n        for metric, val in result.items():\n            if isinstance(val, torch.Tensor):\n                val = val.item()\n            self.log(f\"{mode}_{metric}\", val, batch_size=batch_size)\n</code></pre>"},{"location":"reference/dreem/#dreem.GTRRunner.on_test_end","title":"<code>on_test_end()</code>","text":"<p>Run inference and metrics pipeline to compute metrics for test set.</p> <p>Parameters:</p> Name Type Description Default <code>test_results</code> <p>dict containing predictions and metrics to be filled out in metrics.evaluate</p> required <code>metrics</code> <p>list of metrics to compute</p> required Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def on_test_end(self):\n    \"\"\"Run inference and metrics pipeline to compute metrics for test set.\n\n    Args:\n        test_results: dict containing predictions and metrics to be filled out in metrics.evaluate\n        metrics: list of metrics to compute\n    \"\"\"\n    # input validation\n    metrics_to_compute = self.metrics[\n        \"test\"\n    ]  # list of metrics to compute, or \"all\"\n    if metrics_to_compute == \"all\":\n        metrics_to_compute = [\"motmetrics\", \"global_tracking_accuracy\"]\n    if isinstance(metrics_to_compute, str):\n        metrics_to_compute = [metrics_to_compute]\n    for metric in metrics_to_compute:\n        if metric not in [\"motmetrics\", \"global_tracking_accuracy\"]:\n            raise ValueError(\n                f\"Metric {metric} not supported. Please select from 'motmetrics' or 'global_tracking_accuracy'\"\n            )\n\n    preds = self.test_results[\"preds\"]\n\n    # results is a dict with key being the metric name, and value being the metric value computed\n    results = metrics.evaluate(preds, metrics_to_compute)\n\n    # save metrics and frame metadata to hdf5\n\n    # Get the video name from the first frame\n    vid_name = Path(preds[0].vid_name).stem\n    # save the results to an hdf5 file\n    fname = os.path.join(\n        self.test_results[\"save_path\"], f\"{vid_name}.dreem_metrics.h5\"\n    )\n    logger.info(f\"Saving metrics to {fname}\")\n    # Check if the h5 file exists and add a suffix to prevent name collision\n    suffix_counter = 0\n    original_fname = fname\n    while os.path.exists(fname):\n        suffix_counter += 1\n        fname = original_fname.replace(\n            \".dreem_metrics.h5\", f\"_{suffix_counter}.dreem_metrics.h5\"\n        )\n\n    if suffix_counter &gt; 0:\n        logger.info(f\"File already exists. Saving to {fname} instead\")\n\n    with h5py.File(fname, \"a\") as results_file:\n        # Create a group for this video\n        vid_group = results_file.require_group(vid_name)\n        # Save each metric\n        for metric_name, value in results.items():\n            if metric_name == \"motmetrics\":\n                # For num_switches, save mot_summary and mot_events separately\n                mot_summary = value[0]\n                mot_events = value[1]\n                frame_switch_map = value[2]\n                mot_summary_group = vid_group.require_group(\"mot_summary\")\n                # Loop through each row in mot_summary and save as attributes\n                for _, row in mot_summary.iterrows():\n                    mot_summary_group.attrs[row.name] = row[\"acc\"]\n                # save extra metadata for frames in which there is a switch\n                for frame_id, switch in frame_switch_map.items():\n                    frame = preds[frame_id]\n                    frame = frame.to(\"cpu\")\n                    if switch:\n                        _ = frame.to_h5(\n                            vid_group,\n                            frame.get_gt_track_ids().cpu().numpy(),\n                            save={\n                                \"crop\": True,\n                                \"features\": True,\n                                \"embeddings\": True,\n                            },\n                        )\n                    else:\n                        _ = frame.to_h5(\n                            vid_group, frame.get_gt_track_ids().cpu().numpy()\n                        )\n                # save motevents log to csv\n                motevents_path = os.path.join(\n                    self.test_results[\"save_path\"], f\"{vid_name}.motevents.csv\"\n                )\n                logger.info(f\"Saving motevents log to {motevents_path}\")\n                mot_events.to_csv(motevents_path, index=False)\n\n            elif metric_name == \"global_tracking_accuracy\":\n                gta_by_gt_track = value\n                gta_group = vid_group.require_group(\"global_tracking_accuracy\")\n                # save as a key value pair with gt track id: gta\n                for gt_track_id, gta in gta_by_gt_track.items():\n                    gta_group.attrs[f\"track_{gt_track_id}\"] = gta\n\n    # save the tracking results to a slp/labelled masks file\n    if isinstance(self.trainer.test_dataloaders.dataset, CellTrackingDataset):\n        outpath = os.path.join(\n            self.test_results[\"save_path\"],\n            f\"{vid_name}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.tif\",\n        )\n        pred_imgs = []\n        for frame in preds:\n            frame_masks = []\n            for instance in frame.instances:\n                # centroid = instance.centroid[\"centroid\"]  # Currently unused but available if needed\n                mask = instance.mask.cpu().numpy()\n                track_id = instance.pred_track_id.cpu().numpy().item()\n                mask = mask.astype(np.uint8)\n                mask[mask != 0] = track_id  # label the mask with the track id\n                frame_masks.append(mask)\n            frame_mask = np.max(frame_masks, axis=0)\n            pred_imgs.append(frame_mask)\n        pred_imgs = np.stack(pred_imgs)\n        tifffile.imwrite(outpath, pred_imgs.astype(np.uint16))\n    else:\n        outpath = os.path.join(\n            self.test_results[\"save_path\"],\n            f\"{vid_name}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.slp\",\n        )\n        pred_slp = []\n\n        logger.info(f\"Saving inference results to {outpath}\")\n        # save the tracking results to a slp file\n        tracks = {}\n        for frame in preds:\n            if frame.frame_id.item() == 0:\n                video = (\n                    sio.Video(frame.video)\n                    if isinstance(frame.video, str)\n                    else sio.Video\n                )\n            lf, tracks = frame.to_slp(tracks, video=video)\n            pred_slp.append(lf)\n        pred_slp = sio.Labels(pred_slp)\n\n        pred_slp.save(outpath)\n\n    # clear the preds\n    self.test_results[\"preds\"] = []\n</code></pre>"},{"location":"reference/dreem/#dreem.GTRRunner.on_validation_epoch_end","title":"<code>on_validation_epoch_end()</code>","text":"<p>Execute hook for validation end.</p> <p>Currently, we simply clear the gpu cache and do garbage collection.</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Execute hook for validation end.\n\n    Currently, we simply clear the gpu cache and do garbage collection.\n    \"\"\"\n    gc.collect()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"reference/dreem/#dreem.GTRRunner.predict_step","title":"<code>predict_step(batch, batch_idx)</code>","text":"<p>Run inference for model.</p> <p>Computes association + assignment.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects     with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A list of dicts where each dict is a frame containing the predicted track ids</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def predict_step(\n    self, batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; list[\"dreem.io.Frame\"]:\n    \"\"\"Run inference for model.\n\n    Computes association + assignment.\n\n    Args:\n        batch: A single batch from the dataset which is a list of `Frame` objects\n                with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A list of dicts where each dict is a frame containing the predicted track ids\n    \"\"\"\n    frames_pred = self.tracker(self.model, batch[0])\n    return frames_pred\n</code></pre>"},{"location":"reference/dreem/#dreem.GTRRunner.test_step","title":"<code>test_step(test_batch, batch_idx)</code>","text":"<p>Execute single test step for model.</p> <p>Parameters:</p> Name Type Description Default <code>test_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the val loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def test_step(\n    self, test_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single test step for model.\n\n    Args:\n        test_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the val loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(test_batch[0], mode=\"test\")\n    self.log_metrics(result, len(test_batch[0]), \"test\")\n\n    return result\n</code></pre>"},{"location":"reference/dreem/#dreem.GTRRunner.training_step","title":"<code>training_step(train_batch, batch_idx)</code>","text":"<p>Execute single training step for model.</p> <p>Parameters:</p> Name Type Description Default <code>train_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the train loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def training_step(\n    self, train_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single training step for model.\n\n    Args:\n        train_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the train loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(train_batch[0], mode=\"train\")\n    self.log_metrics(result, len(train_batch[0]), \"train\")\n\n    return result\n</code></pre>"},{"location":"reference/dreem/#dreem.GTRRunner.validation_step","title":"<code>validation_step(val_batch, batch_idx)</code>","text":"<p>Execute single val step for model.</p> <p>Parameters:</p> Name Type Description Default <code>val_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the val loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def validation_step(\n    self, val_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single val step for model.\n\n    Args:\n        val_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the val loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(val_batch[0], mode=\"val\")\n    self.log_metrics(result, len(val_batch[0]), \"val\")\n\n    return result\n</code></pre>"},{"location":"reference/dreem/#dreem.GlobalTrackingTransformer","title":"<code>GlobalTrackingTransformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Modular GTR model composed of visual encoder + transformer used for tracking.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize GTR.</p> <code>extract_features</code> <p>Extract features from instances using visual encoder backbone.</p> <code>forward</code> <p>Execute forward pass of GTR Model to get asso matrix.</p> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>class GlobalTrackingTransformer(torch.nn.Module):\n    \"\"\"Modular GTR model composed of visual encoder + transformer used for tracking.\"\"\"\n\n    def __init__(\n        self,\n        encoder_cfg: dict | None = None,\n        d_model: int = 1024,\n        nhead: int = 8,\n        num_encoder_layers: int = 6,\n        num_decoder_layers: int = 6,\n        dropout: int = 0.1,\n        activation: str = \"relu\",\n        return_intermediate_dec: bool = False,\n        norm: bool = False,\n        num_layers_attn_head: int = 2,\n        dropout_attn_head: int = 0.1,\n        embedding_meta: dict | None = None,\n        return_embedding: bool = False,\n        decoder_self_attn: bool = False,\n    ):\n        \"\"\"Initialize GTR.\n\n        Args:\n            encoder_cfg: Dictionary of arguments to pass to the CNN constructor,\n                e.g: `cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}`\n            d_model: The number of features in the encoder/decoder inputs.\n            nhead: The number of heads in the transformer encoder/decoder.\n            num_encoder_layers: The number of encoder-layers in the encoder.\n            num_decoder_layers: The number of decoder-layers in the decoder.\n            dropout: Dropout value applied to the output of transformer layers.\n            activation: Activation function to use.\n            return_intermediate_dec: Return intermediate layers from decoder.\n            norm: If True, normalize output of encoder and decoder.\n            num_layers_attn_head: The number of layers in the attention head.\n            dropout_attn_head: Dropout value for the attention_head.\n            embedding_meta: Metadata for positional embeddings. See below.\n            return_embedding: Whether to return the positional embeddings\n            decoder_self_attn: If True, use decoder self attention.\n\n                More details on `embedding_meta`:\n                    By default this will be an empty dict and indicate\n                    that no positional embeddings should be used. To use the positional embeddings\n                    pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                    `{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},\n                    \"temp\": {'mode': 'learned', 'emb_num': 16}}`. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                    and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n        \"\"\"\n        super().__init__()\n\n        if not encoder_cfg:\n            encoder_cfg = {}\n        self.visual_encoder = create_visual_encoder(d_model=d_model, **encoder_cfg)\n\n        self.transformer = Transformer(\n            d_model=d_model,\n            nhead=nhead,\n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers,\n            dropout=dropout,\n            activation=activation,\n            return_intermediate_dec=return_intermediate_dec,\n            norm=norm,\n            num_layers_attn_head=num_layers_attn_head,\n            dropout_attn_head=dropout_attn_head,\n            embedding_meta=embedding_meta,\n            return_embedding=return_embedding,\n            decoder_self_attn=decoder_self_attn,\n            encoder_cfg=encoder_cfg,\n        )\n\n    def forward(\n        self, ref_instances: list[\"Instance\"], query_instances: list[\"Instance\"] = None\n    ) -&gt; list[\"AssociationMatrix\"]:\n        \"\"\"Execute forward pass of GTR Model to get asso matrix.\n\n        Args:\n            ref_instances: List of instances from chunk containing crops of objects + gt label info\n            query_instances: list of instances used as query in decoder.\n\n        Returns:\n            An N_T x N association matrix\n        \"\"\"\n        # Extract feature representations with pre-trained encoder.\n        self.extract_features(ref_instances)\n\n        if query_instances:\n            self.extract_features(query_instances)\n\n        asso_preds = self.transformer(ref_instances, query_instances)\n\n        return asso_preds\n\n    def extract_features(\n        self, instances: list[\"Instance\"], force_recompute: bool = False\n    ) -&gt; None:\n        \"\"\"Extract features from instances using visual encoder backbone.\n\n        Args:\n            instances: A list of instances to compute features for\n            force_recompute: indicate whether to compute features for all instances regardless of if they have instances\n        \"\"\"\n        if not force_recompute:\n            instances_to_compute = [\n                instance\n                for instance in instances\n                if instance.has_crop() and not instance.has_features()\n            ]\n        else:\n            instances_to_compute = instances\n\n        if len(instances_to_compute) == 0:\n            return\n        elif len(instances_to_compute) == 1:  # handle batch norm error when B=1\n            instances_to_compute = instances\n\n        crops = torch.concatenate([instance.crop for instance in instances_to_compute])\n\n        features = self.visual_encoder(crops)\n        features = features.to(device=instances_to_compute[0].device)\n\n        for i, z_i in enumerate(features):\n            instances_to_compute[i].features = z_i\n</code></pre>"},{"location":"reference/dreem/#dreem.GlobalTrackingTransformer.__init__","title":"<code>__init__(encoder_cfg=None, d_model=1024, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dropout=0.1, activation='relu', return_intermediate_dec=False, norm=False, num_layers_attn_head=2, dropout_attn_head=0.1, embedding_meta=None, return_embedding=False, decoder_self_attn=False)</code>","text":"<p>Initialize GTR.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_cfg</code> <code>dict | None</code> <p>Dictionary of arguments to pass to the CNN constructor, e.g: <code>cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}</code></p> <code>None</code> <code>d_model</code> <code>int</code> <p>The number of features in the encoder/decoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads in the transformer encoder/decoder.</p> <code>8</code> <code>num_encoder_layers</code> <code>int</code> <p>The number of encoder-layers in the encoder.</p> <code>6</code> <code>num_decoder_layers</code> <code>int</code> <p>The number of decoder-layers in the decoder.</p> <code>6</code> <code>dropout</code> <code>int</code> <p>Dropout value applied to the output of transformer layers.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>return_intermediate_dec</code> <code>bool</code> <p>Return intermediate layers from decoder.</p> <code>False</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of encoder and decoder.</p> <code>False</code> <code>num_layers_attn_head</code> <code>int</code> <p>The number of layers in the attention head.</p> <code>2</code> <code>dropout_attn_head</code> <code>int</code> <p>Dropout value for the attention_head.</p> <code>0.1</code> <code>embedding_meta</code> <code>dict | None</code> <p>Metadata for positional embeddings. See below.</p> <code>None</code> <code>return_embedding</code> <code>bool</code> <p>Whether to return the positional embeddings</p> <code>False</code> <code>decoder_self_attn</code> <code>bool</code> <p>If True, use decoder self attention.</p> <p>More details on <code>embedding_meta</code>:     By default this will be an empty dict and indicate     that no positional embeddings should be used. To use the positional embeddings     pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:     <code>{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},     \"temp\": {'mode': 'learned', 'emb_num': 16}}</code>. (see <code>dreem.models.embeddings.Embedding.EMB_TYPES</code>     and <code>dreem.models.embeddings.Embedding.EMB_MODES</code> for embedding parameters).</p> <code>False</code> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def __init__(\n    self,\n    encoder_cfg: dict | None = None,\n    d_model: int = 1024,\n    nhead: int = 8,\n    num_encoder_layers: int = 6,\n    num_decoder_layers: int = 6,\n    dropout: int = 0.1,\n    activation: str = \"relu\",\n    return_intermediate_dec: bool = False,\n    norm: bool = False,\n    num_layers_attn_head: int = 2,\n    dropout_attn_head: int = 0.1,\n    embedding_meta: dict | None = None,\n    return_embedding: bool = False,\n    decoder_self_attn: bool = False,\n):\n    \"\"\"Initialize GTR.\n\n    Args:\n        encoder_cfg: Dictionary of arguments to pass to the CNN constructor,\n            e.g: `cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}`\n        d_model: The number of features in the encoder/decoder inputs.\n        nhead: The number of heads in the transformer encoder/decoder.\n        num_encoder_layers: The number of encoder-layers in the encoder.\n        num_decoder_layers: The number of decoder-layers in the decoder.\n        dropout: Dropout value applied to the output of transformer layers.\n        activation: Activation function to use.\n        return_intermediate_dec: Return intermediate layers from decoder.\n        norm: If True, normalize output of encoder and decoder.\n        num_layers_attn_head: The number of layers in the attention head.\n        dropout_attn_head: Dropout value for the attention_head.\n        embedding_meta: Metadata for positional embeddings. See below.\n        return_embedding: Whether to return the positional embeddings\n        decoder_self_attn: If True, use decoder self attention.\n\n            More details on `embedding_meta`:\n                By default this will be an empty dict and indicate\n                that no positional embeddings should be used. To use the positional embeddings\n                pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                `{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},\n                \"temp\": {'mode': 'learned', 'emb_num': 16}}`. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n    \"\"\"\n    super().__init__()\n\n    if not encoder_cfg:\n        encoder_cfg = {}\n    self.visual_encoder = create_visual_encoder(d_model=d_model, **encoder_cfg)\n\n    self.transformer = Transformer(\n        d_model=d_model,\n        nhead=nhead,\n        num_encoder_layers=num_encoder_layers,\n        num_decoder_layers=num_decoder_layers,\n        dropout=dropout,\n        activation=activation,\n        return_intermediate_dec=return_intermediate_dec,\n        norm=norm,\n        num_layers_attn_head=num_layers_attn_head,\n        dropout_attn_head=dropout_attn_head,\n        embedding_meta=embedding_meta,\n        return_embedding=return_embedding,\n        decoder_self_attn=decoder_self_attn,\n        encoder_cfg=encoder_cfg,\n    )\n</code></pre>"},{"location":"reference/dreem/#dreem.GlobalTrackingTransformer.extract_features","title":"<code>extract_features(instances, force_recompute=False)</code>","text":"<p>Extract features from instances using visual encoder backbone.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[Instance]</code> <p>A list of instances to compute features for</p> required <code>force_recompute</code> <code>bool</code> <p>indicate whether to compute features for all instances regardless of if they have instances</p> <code>False</code> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def extract_features(\n    self, instances: list[\"Instance\"], force_recompute: bool = False\n) -&gt; None:\n    \"\"\"Extract features from instances using visual encoder backbone.\n\n    Args:\n        instances: A list of instances to compute features for\n        force_recompute: indicate whether to compute features for all instances regardless of if they have instances\n    \"\"\"\n    if not force_recompute:\n        instances_to_compute = [\n            instance\n            for instance in instances\n            if instance.has_crop() and not instance.has_features()\n        ]\n    else:\n        instances_to_compute = instances\n\n    if len(instances_to_compute) == 0:\n        return\n    elif len(instances_to_compute) == 1:  # handle batch norm error when B=1\n        instances_to_compute = instances\n\n    crops = torch.concatenate([instance.crop for instance in instances_to_compute])\n\n    features = self.visual_encoder(crops)\n    features = features.to(device=instances_to_compute[0].device)\n\n    for i, z_i in enumerate(features):\n        instances_to_compute[i].features = z_i\n</code></pre>"},{"location":"reference/dreem/#dreem.GlobalTrackingTransformer.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute forward pass of GTR Model to get asso matrix.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>List of instances from chunk containing crops of objects + gt label info</p> required <code>query_instances</code> <code>list[Instance]</code> <p>list of instances used as query in decoder.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[AssociationMatrix]</code> <p>An N_T x N association matrix</p> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def forward(\n    self, ref_instances: list[\"Instance\"], query_instances: list[\"Instance\"] = None\n) -&gt; list[\"AssociationMatrix\"]:\n    \"\"\"Execute forward pass of GTR Model to get asso matrix.\n\n    Args:\n        ref_instances: List of instances from chunk containing crops of objects + gt label info\n        query_instances: list of instances used as query in decoder.\n\n    Returns:\n        An N_T x N association matrix\n    \"\"\"\n    # Extract feature representations with pre-trained encoder.\n    self.extract_features(ref_instances)\n\n    if query_instances:\n        self.extract_features(query_instances)\n\n    asso_preds = self.transformer(ref_instances, query_instances)\n\n    return asso_preds\n</code></pre>"},{"location":"reference/dreem/#dreem.Instance","title":"<code>Instance</code>","text":"<p>Class representing a single instance to be tracked.</p> <p>Attributes:</p> Name Type Description <code>gt_track_id</code> <code>Tensor</code> <p>Ground truth track id - only used for train/eval.</p> <code>pred_track_id</code> <code>Tensor</code> <p>Predicted track id. Untracked instance is represented by -1.</p> <code>bbox</code> <code>Tensor</code> <p>The bounding box coordinate of the instance. Defaults to an empty tensor.</p> <code>crop</code> <code>Tensor</code> <p>The crop of the instance.</p> <code>centroid</code> <code>dict[str, ArrayLike]</code> <p>the centroid around which the bbox was cropped.</p> <code>features</code> <code>Tensor</code> <p>The reid features extracted from the CNN backbone used in the transformer.</p> <code>track_score</code> <code>float</code> <p>The track score output from the association matrix.</p> <code>point_scores</code> <code>ArrayLike</code> <p>The point scores from sleap.</p> <code>instance_score</code> <code>float</code> <p>The instance scores from sleap.</p> <code>skeleton</code> <code>Skeleton</code> <p>The sleap skeleton used for the instance.</p> <code>pose</code> <code>dict[str, ArrayLike]</code> <p>A dictionary containing the node name and corresponding point.</p> <code>device</code> <code>str</code> <p>String representation of the device the instance should be on.</p> <p>Methods:</p> Name Description <code>__attrs_post_init__</code> <p>Handle dimensionality and more intricate default initializations post-init.</p> <code>__repr__</code> <p>Return string representation of the Instance.</p> <code>add_embedding</code> <p>Save embedding to instance embedding dictionary.</p> <code>from_slp</code> <p>Convert a slp instance to a dreem instance.</p> <code>get_embedding</code> <p>Retrieve instance's spatial/temporal embedding.</p> <code>has_bbox</code> <p>Determine if the instance has a bbox.</p> <code>has_crop</code> <p>Determine if the instance has a crop.</p> <code>has_embedding</code> <p>Determine if the instance has embedding type requested.</p> <code>has_features</code> <p>Determine if the instance has computed reid features.</p> <code>has_gt_track_id</code> <p>Determine if instance has a gt track assignment.</p> <code>has_pose</code> <p>Check if the instance has a pose.</p> <code>has_pred_track_id</code> <p>Determine whether instance has predicted track id.</p> <code>to</code> <p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <code>to_h5</code> <p>Convert instance to an h5 group\".</p> <code>to_slp</code> <p>Convert instance to sleap_io.PredictedInstance object.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>@attrs.define(eq=False)\nclass Instance:\n    \"\"\"Class representing a single instance to be tracked.\n\n    Attributes:\n        gt_track_id: Ground truth track id - only used for train/eval.\n        pred_track_id: Predicted track id. Untracked instance is represented by -1.\n        bbox: The bounding box coordinate of the instance. Defaults to an empty tensor.\n        crop: The crop of the instance.\n        centroid: the centroid around which the bbox was cropped.\n        features: The reid features extracted from the CNN backbone used in the transformer.\n        track_score: The track score output from the association matrix.\n        point_scores: The point scores from sleap.\n        instance_score: The instance scores from sleap.\n        skeleton: The sleap skeleton used for the instance.\n        pose: A dictionary containing the node name and corresponding point.\n        device: String representation of the device the instance should be on.\n    \"\"\"\n\n    _gt_track_id: int = attrs.field(\n        alias=\"gt_track_id\", default=-1, converter=_to_tensor\n    )\n    _pred_track_id: int = attrs.field(\n        alias=\"pred_track_id\", default=-1, converter=_to_tensor\n    )\n    _bbox: ArrayLike = attrs.field(alias=\"bbox\", factory=list, converter=_to_tensor)\n    _crop: ArrayLike = attrs.field(alias=\"crop\", factory=list, converter=_to_tensor)\n    _centroid: dict[str, ArrayLike] = attrs.field(alias=\"centroid\", factory=dict)\n    _features: ArrayLike = attrs.field(\n        alias=\"features\", factory=list, converter=_to_tensor\n    )\n    _embeddings: dict = attrs.field(alias=\"embeddings\", factory=dict)\n    _track_score: float = attrs.field(alias=\"track_score\", default=-1.0)\n    _instance_score: float = attrs.field(alias=\"instance_score\", default=-1.0)\n    _point_scores: ArrayLike | None = attrs.field(alias=\"point_scores\", default=None)\n    _skeleton: sio.Skeleton | None = attrs.field(alias=\"skeleton\", default=None)\n    _mask: ArrayLike | None = attrs.field(\n        alias=\"mask\", converter=_to_tensor, default=None\n    )\n    _pose: dict[str, ArrayLike] = attrs.field(alias=\"pose\", factory=dict)\n    _device: str | torch.device | None = attrs.field(alias=\"device\", default=None)\n    _frame: \"Frame\" = None\n\n    def __attrs_post_init__(self) -&gt; None:\n        \"\"\"Handle dimensionality and more intricate default initializations post-init.\"\"\"\n        self.bbox = _expand_to_rank(self.bbox, 3)\n        self.crop = _expand_to_rank(self.crop, 4)\n        self.features = _expand_to_rank(self.features, 2)\n\n        if self.skeleton is None:\n            self.skeleton = sio.Skeleton([\"centroid\"])\n\n        if self.bbox.shape[-1] == 0:\n            self.bbox = torch.empty([1, 0, 4])\n\n        if self.crop.shape[-1] == 0 and self.bbox.shape[1] != 0:\n            y1, x1, y2, x2 = self.bbox.squeeze(dim=0).nanmean(dim=0)\n            self.centroid = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        if len(self.pose) == 0 and self.bbox.shape[1]:\n            y1, x1, y2, x2 = self.bbox.squeeze(dim=0).mean(dim=0)\n            self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        if self.point_scores is None and len(self.pose) != 0:\n            self._point_scores = np.zeros((len(self.pose), 2))\n\n        self.to(self.device)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation of the Instance.\"\"\"\n        return (\n            \"Instance(\"\n            f\"gt_track_id={self._gt_track_id.item()}, \"\n            f\"pred_track_id={self._pred_track_id.item()}, \"\n            f\"bbox={self._bbox}, \"\n            f\"centroid={self._centroid}, \"\n            f\"crop={self._crop.shape}, \"\n            f\"features={self._features.shape}, \"\n            f\"device={self._device}\"\n            \")\"\n        )\n\n    def to(self, map_location: str | torch.device) -&gt; Self:\n        \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n        Args:\n            map_location: Either the device or dtype for the instance to be moved.\n\n        Returns:\n            self: reference to the instance moved to correct device/dtype.\n        \"\"\"\n        if map_location is not None and map_location != \"\":\n            self._gt_track_id = self._gt_track_id.to(map_location)\n            self._pred_track_id = self._pred_track_id.to(map_location)\n            self._bbox = self._bbox.to(map_location)\n            self._crop = self._crop.to(map_location)\n            self._features = self._features.to(map_location)\n            if isinstance(map_location, (str, torch.device)):\n                self.device = map_location\n\n        return self\n\n    @classmethod\n    def from_slp(\n        cls,\n        slp_instance: sio.PredictedInstance | sio.Instance,\n        bbox_size: int | tuple[int, int] = 64,\n        crop: ArrayLike | None = None,\n        device: str | None = None,\n    ) -&gt; Self:\n        \"\"\"Convert a slp instance to a dreem instance.\n\n        Args:\n            slp_instance: A `sleap_io.Instance` object representing a detection\n            bbox_size: size of the pose-centered bbox to form.\n            crop: The corresponding crop of the bbox\n            device: which device to keep the instance on\n        Returns:\n            A dreem.Instance object with a pose-centered bbox and no crop.\n        \"\"\"\n        try:\n            track_id = int(slp_instance.track.name)\n        except ValueError:\n            track_id = int(\n                \"\".join([str(ord(c)) for c in slp_instance.track.name])\n            )  # better way to handle this?\n        if isinstance(bbox_size, int):\n            bbox_size = (bbox_size, bbox_size)\n\n        track_score = -1.0\n        point_scores = np.full(len(slp_instance.points), -1)\n        instance_score = -1\n        if isinstance(slp_instance, sio.PredictedInstance):\n            track_score = slp_instance.tracking_score\n            point_scores = slp_instance.numpy()[:, -1]\n            instance_score = slp_instance.score\n\n        centroid = np.nanmean(slp_instance.numpy(), axis=1)\n        bbox = [\n            centroid[1] - bbox_size[1],\n            centroid[0] - bbox_size[0],\n            centroid[1] + bbox_size[1],\n            centroid[0] + bbox_size[0],\n        ]\n        return cls(\n            gt_track_id=track_id,\n            bbox=bbox,\n            crop=crop,\n            centroid={\"centroid\": centroid},\n            track_score=track_score,\n            point_scores=point_scores,\n            instance_score=instance_score,\n            skeleton=slp_instance.skeleton,\n            pose={\n                node.name: point.numpy() for node, point in slp_instance.points.items()\n            },\n            device=device,\n        )\n\n    def to_slp(\n        self, track_lookup: dict[int, sio.Track] = {}\n    ) -&gt; tuple[sio.PredictedInstance, dict[int, sio.Track]]:\n        \"\"\"Convert instance to sleap_io.PredictedInstance object.\n\n        Args:\n            track_lookup: A track look up dictionary containing track_id:sio.Track.\n        Returns: A sleap_io.PredictedInstance with necessary metadata\n            and a track_lookup dictionary to persist tracks.\n        \"\"\"\n        try:\n            track_id = self.pred_track_id.item()\n            if track_id not in track_lookup:\n                track_lookup[track_id] = sio.Track(name=self.pred_track_id.item())\n\n            track = track_lookup[track_id]\n\n            return (\n                sio.PredictedInstance.from_numpy(\n                    points_data=np.array(list(self.pose.values())),\n                    skeleton=self.skeleton,\n                    point_scores=self.point_scores,\n                    score=self.instance_score,\n                    tracking_score=self.track_score,\n                    track=track,\n                ),\n                track_lookup,\n            )\n        except Exception as e:\n            logger.exception(\n                f\"Pose: {np.array(list(self.pose.values())).shape}, Pose score shape {self.point_scores.shape}\"\n            )\n            raise RuntimeError(f\"Failed to convert to sio.PredictedInstance: {e}\")\n\n    def to_h5(\n        self, frame_group: h5py.Group, label: Any = None, **kwargs: dict\n    ) -&gt; h5py.Group:\n        \"\"\"Convert instance to an h5 group\".\n\n        By default we always save:\n            - the gt/pred track id\n            - bbox\n            - centroid\n            - pose\n            - instance/traj/points score\n        Larger arrays (crops/features/embeddings) can be saved by passing as kwargs\n\n        Args:\n            frame_group: the h5py group representing the frame the instance appears on\n            label: the name of the instance group that will be created\n            **kwargs: additional key:value pairs to be saved as datasets.\n\n        Returns:\n            The h5 group representing this instance.\n        \"\"\"\n        if label is None:\n            if pred_track_id != -1:\n                label = f\"instance_{self.pred_track_id.item()}\"\n            else:\n                label = f\"instance_{self.gt_track_id.item()}\"\n        instance_group = frame_group.create_group(label)\n        instance_group.attrs.create(\"gt_track_id\", self.gt_track_id.item())\n        instance_group.attrs.create(\"pred_track_id\", self.pred_track_id.item())\n        instance_group.attrs.create(\"track_score\", self.track_score)\n        instance_group.attrs.create(\"instance_score\", self.instance_score)\n\n        instance_group.create_dataset(\"bbox\", data=self.bbox.cpu().numpy())\n\n        pose_group = instance_group.create_group(\"pose\")\n        pose_group.create_dataset(\"points\", data=np.array(list(self.pose.values())))\n        pose_group.attrs.create(\"nodes\", list(self.pose.keys()))\n        pose_group.create_dataset(\"scores\", data=self.point_scores)\n\n        for key, value in kwargs.items():\n            if \"emb\" in key:\n                emb_group = instance_group.require_group(\"emb\")\n                emb_group.create_dataset(key, data=value)\n            else:\n                instance_group.create_dataset(key, data=value)\n\n        return instance_group\n\n    @property\n    def device(self) -&gt; str:\n        \"\"\"The device the instance is on.\n\n        Returns:\n            The str representation of the device the gpu is on.\n        \"\"\"\n        return self._device\n\n    @device.setter\n    def device(self, device) -&gt; None:\n        \"\"\"Set for the device property.\n\n        Args:\n            device: The str representation of the device.\n        \"\"\"\n        self._device = device\n\n    @property\n    def gt_track_id(self) -&gt; torch.Tensor:\n        \"\"\"The ground truth track id of the instance.\n\n        Returns:\n            A tensor containing the ground truth track id\n        \"\"\"\n        return self._gt_track_id\n\n    @gt_track_id.setter\n    def gt_track_id(self, track: int):\n        \"\"\"Set the instance ground-truth track id.\n\n        Args:\n           track: An int representing the ground-truth track id.\n        \"\"\"\n        if track is not None:\n            self._gt_track_id = torch.tensor([track])\n        else:\n            self._gt_track_id = torch.tensor([])\n\n    def has_gt_track_id(self) -&gt; bool:\n        \"\"\"Determine if instance has a gt track assignment.\n\n        Returns:\n            True if the gt track id is set, otherwise False.\n        \"\"\"\n        if self._gt_track_id.shape[0] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def pred_track_id(self) -&gt; torch.Tensor:\n        \"\"\"The track id predicted by the tracker using asso_output from model.\n\n        Returns:\n            A tensor containing the predicted track id.\n        \"\"\"\n        return self._pred_track_id\n\n    @pred_track_id.setter\n    def pred_track_id(self, track: int) -&gt; None:\n        \"\"\"Set predicted track id.\n\n        Args:\n            track: an int representing the predicted track id.\n        \"\"\"\n        if track is not None:\n            self._pred_track_id = torch.tensor([track])\n        else:\n            self._pred_track_id = torch.tensor([])\n\n    def has_pred_track_id(self) -&gt; bool:\n        \"\"\"Determine whether instance has predicted track id.\n\n        Returns:\n            True if instance has a pred track id, False otherwise.\n        \"\"\"\n        if self._pred_track_id.item() == -1 or self._pred_track_id.shape[0] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def bbox(self) -&gt; torch.Tensor:\n        \"\"\"The bounding box coordinates of the instance in the original frame.\n\n        Returns:\n            A (1,4) tensor containing the bounding box coordinates.\n        \"\"\"\n        return self._bbox\n\n    @bbox.setter\n    def bbox(self, bbox: ArrayLike) -&gt; None:\n        \"\"\"Set the instance bounding box.\n\n        Args:\n            bbox: an arraylike object containing the bounding box coordinates.\n        \"\"\"\n        if bbox is None or len(bbox) == 0:\n            self._bbox = torch.empty((0, 4))\n        else:\n            if not isinstance(bbox, torch.Tensor):\n                self._bbox = torch.tensor(bbox)\n            else:\n                self._bbox = bbox\n\n        if self._bbox.shape[0] and len(self._bbox.shape) == 1:\n            self._bbox = self._bbox.unsqueeze(0)\n        if self._bbox.shape[1] and len(self._bbox.shape) == 2:\n            self._bbox = self._bbox.unsqueeze(0)\n\n    def has_bbox(self) -&gt; bool:\n        \"\"\"Determine if the instance has a bbox.\n\n        Returns:\n            True if the instance has a bounding box, false otherwise.\n        \"\"\"\n        if self._bbox.shape[1] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def centroid(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"The centroid around which the crop was formed.\n\n        Returns:\n            A dict containing the anchor name and the x, y bbox midpoint.\n        \"\"\"\n        return self._centroid\n\n    @centroid.setter\n    def centroid(self, centroid: dict[str, ArrayLike]) -&gt; None:\n        \"\"\"Set the centroid of the instance.\n\n        Args:\n            centroid: A dict containing the anchor name and points.\n        \"\"\"\n        self._centroid = centroid\n\n    @property\n    def anchor(self) -&gt; list[str]:\n        \"\"\"The anchor node name around which the crop was formed.\n\n        Returns:\n            the list of anchors around which each crop was formed\n            the list of anchors around which each crop was formed\n        \"\"\"\n        if self.centroid:\n            return list(self.centroid.keys())\n        return \"\"\n\n    @property\n    def mask(self) -&gt; torch.Tensor:\n        \"\"\"The mask of the instance.\n\n        Returns:\n            A (h, w) tensor containing the mask of the instance.\n        \"\"\"\n        return self._mask\n\n    @mask.setter\n    def mask(self, mask: ArrayLike) -&gt; None:\n        \"\"\"Set the mask of the instance.\n\n        Args:\n            mask: an arraylike object containing the mask of the instance.\n        \"\"\"\n        if mask is None or len(mask) == 0:\n            self._mask = torch.tensor([])\n        else:\n            if not isinstance(mask, torch.Tensor):\n                self._mask = torch.tensor(mask)\n            else:\n                self._mask = mask\n\n    @property\n    def crop(self) -&gt; torch.Tensor:\n        \"\"\"The crop of the instance.\n\n        Returns:\n            A (1, c, h , w) tensor containing the cropped image centered around the instance.\n        \"\"\"\n        return self._crop\n\n    @crop.setter\n    def crop(self, crop: ArrayLike) -&gt; None:\n        \"\"\"Set the crop of the instance.\n\n        Args:\n            crop: an arraylike object containing the cropped image of the centered instance.\n        \"\"\"\n        if crop is None or len(crop) == 0:\n            self._crop = torch.tensor([])\n        else:\n            if not isinstance(crop, torch.Tensor):\n                self._crop = torch.tensor(crop)\n            else:\n                self._crop = crop\n\n        if len(self._crop.shape) == 2:\n            self._crop = self._crop.unsqueeze(0)\n        if len(self._crop.shape) == 3:\n            self._crop = self._crop.unsqueeze(0)\n\n    def has_crop(self) -&gt; bool:\n        \"\"\"Determine if the instance has a crop.\n\n        Returns:\n            True if the instance has an image otherwise False.\n        \"\"\"\n        if self._crop.shape[-1] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def features(self) -&gt; torch.Tensor:\n        \"\"\"Re-ID feature vector from backbone model to be used as input to transformer.\n\n        Returns:\n            a (1, d) tensor containing the reid feature vector.\n        \"\"\"\n        return self._features\n\n    @features.setter\n    def features(self, features: ArrayLike) -&gt; None:\n        \"\"\"Set the reid feature vector of the instance.\n\n        Args:\n            features: a (1,d) array like object containing the reid features for the instance.\n        \"\"\"\n        if features is None or len(features) == 0:\n            self._features = torch.tensor([])\n\n        elif not isinstance(features, torch.Tensor):\n            self._features = torch.tensor(features)\n        else:\n            self._features = features\n\n        if self._features.shape[0] and len(self._features.shape) == 1:\n            self._features = self._features.unsqueeze(0)\n\n    def has_features(self) -&gt; bool:\n        \"\"\"Determine if the instance has computed reid features.\n\n        Returns:\n            True if the instance has reid features, False otherwise.\n        \"\"\"\n        if self._features.shape[-1] == 0:\n            return False\n        else:\n            return True\n\n    def has_embedding(self, emb_type: str | None = None) -&gt; bool:\n        \"\"\"Determine if the instance has embedding type requested.\n\n        Args:\n            emb_type: The key to check in the embedding dictionary.\n\n        Returns:\n            True if `emb_type` in embedding_dict else false\n        \"\"\"\n        return emb_type in self._embeddings\n\n    def get_embedding(\n        self, emb_type: str = \"all\"\n    ) -&gt; dict[str, torch.Tensor] | torch.Tensor | None:\n        \"\"\"Retrieve instance's spatial/temporal embedding.\n\n        Args:\n            emb_type: The string key of the embedding to retrieve. Should be \"pos\", \"temp\"\n\n        Returns:\n            * A torch tensor representing the spatial/temporal location of the instance.\n            * None if the embedding is not stored\n        \"\"\"\n        if emb_type.lower() == \"all\":\n            return self._embeddings\n        else:\n            try:\n                return self._embeddings[emb_type]\n            except KeyError:\n                logger.exception(\n                    f\"{emb_type} not saved! Only {list(self._embeddings.keys())} are available\"\n                )\n        return None\n\n    def add_embedding(self, emb_type: str, embedding: torch.Tensor) -&gt; None:\n        \"\"\"Save embedding to instance embedding dictionary.\n\n        Args:\n            emb_type: Key/embedding type to be saved to dictionary\n            embedding: The actual torch tensor embedding.\n        \"\"\"\n        embedding = _expand_to_rank(embedding, 2)\n        self._embeddings[emb_type] = embedding\n\n    @property\n    def frame(self) -&gt; \"Frame\":\n        \"\"\"Get the frame the instance belongs to.\n\n        Returns:\n            The back reference to the `Frame` that this `Instance` belongs to.\n        \"\"\"\n        return self._frame\n\n    @frame.setter\n    def frame(self, frame: \"Frame\") -&gt; None:\n        \"\"\"Set the back reference to the `Frame` that this `Instance` belongs to.\n\n        This field is set when instances are added to `Frame` object.\n\n        Args:\n            frame: A `Frame` object containing the metadata for the frame that the instance belongs to\n        \"\"\"\n        self._frame = frame\n\n    @property\n    def pose(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"Get the pose of the instance.\n\n        Returns:\n            A dictionary containing the node and corresponding x,y points\n        \"\"\"\n        return self._pose\n\n    @pose.setter\n    def pose(self, pose: dict[str, ArrayLike]) -&gt; None:\n        \"\"\"Set the pose of the instance.\n\n        Args:\n            pose: A nodes x 2 array containing the pose coordinates.\n        \"\"\"\n        if pose is not None:\n            self._pose = pose\n\n        elif self.bbox.shape[0]:\n            y1, x1, y2, x2 = self.bbox.squeeze()\n            self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        else:\n            self._pose = {}\n\n    def has_pose(self) -&gt; bool:\n        \"\"\"Check if the instance has a pose.\n\n        Returns True if the instance has a pose.\n        \"\"\"\n        if len(self.pose):\n            return True\n        return False\n\n    @property\n    def shown_pose(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"Get the pose with shown nodes only.\n\n        Returns: A dictionary filtered by nodes that are shown (points are not nan).\n        \"\"\"\n        pose = self.pose\n        return {node: point for node, point in pose.items() if not np.isna(point).any()}\n\n    @property\n    def skeleton(self) -&gt; sio.Skeleton:\n        \"\"\"Get the skeleton associated with the instance.\n\n        Returns: The sio.Skeleton associated with the instance.\n        \"\"\"\n        return self._skeleton\n\n    @skeleton.setter\n    def skeleton(self, skeleton: sio.Skeleton) -&gt; None:\n        \"\"\"Set the skeleton associated with the instance.\n\n        Args:\n            skeleton: The sio.Skeleton associated with the instance.\n        \"\"\"\n        self._skeleton = skeleton\n\n    @property\n    def point_scores(self) -&gt; ArrayLike:\n        \"\"\"Get the point scores associated with the pose prediction.\n\n        Returns: a vector of shape n containing the point scores outputted from sleap associated with pose predictions.\n        \"\"\"\n        return self._point_scores\n\n    @point_scores.setter\n    def point_scores(self, point_scores: ArrayLike) -&gt; None:\n        \"\"\"Set the point scores associated with the pose prediction.\n\n        Args:\n            point_scores: a vector of shape n containing the point scores\n            outputted from sleap associated with pose predictions.\n        \"\"\"\n        self._point_scores = point_scores\n\n    @property\n    def instance_score(self) -&gt; float:\n        \"\"\"Get the pose prediction score associated with the instance.\n\n        Returns: a float from 0-1 representing an instance_score.\n        \"\"\"\n        return self._instance_score\n\n    @instance_score.setter\n    def instance_score(self, instance_score: float) -&gt; None:\n        \"\"\"Set the pose prediction score associated with the instance.\n\n        Args:\n            instance_score: a float from 0-1 representing an instance_score.\n        \"\"\"\n        self._instance_score = instance_score\n\n    @property\n    def track_score(self) -&gt; float:\n        \"\"\"Get the track_score of the instance.\n\n        Returns: A float from 0-1 representing the output used in the tracker for assignment.\n        \"\"\"\n        return self._track_score\n\n    @track_score.setter\n    def track_score(self, track_score: float) -&gt; None:\n        \"\"\"Set the track_score of the instance.\n\n        Args:\n            track_score: A float from 0-1 representing the output used in the tracker for assignment.\n        \"\"\"\n        self._track_score = track_score\n</code></pre>"},{"location":"reference/dreem/#dreem.Instance.anchor","title":"<code>anchor</code>  <code>property</code>","text":"<p>The anchor node name around which the crop was formed.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>the list of anchors around which each crop was formed the list of anchors around which each crop was formed</p>"},{"location":"reference/dreem/#dreem.Instance.bbox","title":"<code>bbox</code>  <code>property</code> <code>writable</code>","text":"<p>The bounding box coordinates of the instance in the original frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A (1,4) tensor containing the bounding box coordinates.</p>"},{"location":"reference/dreem/#dreem.Instance.centroid","title":"<code>centroid</code>  <code>property</code> <code>writable</code>","text":"<p>The centroid around which the crop was formed.</p> <p>Returns:</p> Type Description <code>dict[str, ArrayLike]</code> <p>A dict containing the anchor name and the x, y bbox midpoint.</p>"},{"location":"reference/dreem/#dreem.Instance.crop","title":"<code>crop</code>  <code>property</code> <code>writable</code>","text":"<p>The crop of the instance.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A (1, c, h , w) tensor containing the cropped image centered around the instance.</p>"},{"location":"reference/dreem/#dreem.Instance.device","title":"<code>device</code>  <code>property</code> <code>writable</code>","text":"<p>The device the instance is on.</p> <p>Returns:</p> Type Description <code>str</code> <p>The str representation of the device the gpu is on.</p>"},{"location":"reference/dreem/#dreem.Instance.features","title":"<code>features</code>  <code>property</code> <code>writable</code>","text":"<p>Re-ID feature vector from backbone model to be used as input to transformer.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>a (1, d) tensor containing the reid feature vector.</p>"},{"location":"reference/dreem/#dreem.Instance.frame","title":"<code>frame</code>  <code>property</code> <code>writable</code>","text":"<p>Get the frame the instance belongs to.</p> <p>Returns:</p> Type Description <code>Frame</code> <p>The back reference to the <code>Frame</code> that this <code>Instance</code> belongs to.</p>"},{"location":"reference/dreem/#dreem.Instance.gt_track_id","title":"<code>gt_track_id</code>  <code>property</code> <code>writable</code>","text":"<p>The ground truth track id of the instance.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the ground truth track id</p>"},{"location":"reference/dreem/#dreem.Instance.instance_score","title":"<code>instance_score</code>  <code>property</code> <code>writable</code>","text":"<p>Get the pose prediction score associated with the instance.</p> <p>Returns: a float from 0-1 representing an instance_score.</p>"},{"location":"reference/dreem/#dreem.Instance.mask","title":"<code>mask</code>  <code>property</code> <code>writable</code>","text":"<p>The mask of the instance.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A (h, w) tensor containing the mask of the instance.</p>"},{"location":"reference/dreem/#dreem.Instance.point_scores","title":"<code>point_scores</code>  <code>property</code> <code>writable</code>","text":"<p>Get the point scores associated with the pose prediction.</p> <p>Returns: a vector of shape n containing the point scores outputted from sleap associated with pose predictions.</p>"},{"location":"reference/dreem/#dreem.Instance.pose","title":"<code>pose</code>  <code>property</code> <code>writable</code>","text":"<p>Get the pose of the instance.</p> <p>Returns:</p> Type Description <code>dict[str, ArrayLike]</code> <p>A dictionary containing the node and corresponding x,y points</p>"},{"location":"reference/dreem/#dreem.Instance.pred_track_id","title":"<code>pred_track_id</code>  <code>property</code> <code>writable</code>","text":"<p>The track id predicted by the tracker using asso_output from model.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the predicted track id.</p>"},{"location":"reference/dreem/#dreem.Instance.shown_pose","title":"<code>shown_pose</code>  <code>property</code>","text":"<p>Get the pose with shown nodes only.</p> <p>Returns: A dictionary filtered by nodes that are shown (points are not nan).</p>"},{"location":"reference/dreem/#dreem.Instance.skeleton","title":"<code>skeleton</code>  <code>property</code> <code>writable</code>","text":"<p>Get the skeleton associated with the instance.</p> <p>Returns: The sio.Skeleton associated with the instance.</p>"},{"location":"reference/dreem/#dreem.Instance.track_score","title":"<code>track_score</code>  <code>property</code> <code>writable</code>","text":"<p>Get the track_score of the instance.</p> <p>Returns: A float from 0-1 representing the output used in the tracker for assignment.</p>"},{"location":"reference/dreem/#dreem.Instance.__attrs_post_init__","title":"<code>__attrs_post_init__()</code>","text":"<p>Handle dimensionality and more intricate default initializations post-init.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def __attrs_post_init__(self) -&gt; None:\n    \"\"\"Handle dimensionality and more intricate default initializations post-init.\"\"\"\n    self.bbox = _expand_to_rank(self.bbox, 3)\n    self.crop = _expand_to_rank(self.crop, 4)\n    self.features = _expand_to_rank(self.features, 2)\n\n    if self.skeleton is None:\n        self.skeleton = sio.Skeleton([\"centroid\"])\n\n    if self.bbox.shape[-1] == 0:\n        self.bbox = torch.empty([1, 0, 4])\n\n    if self.crop.shape[-1] == 0 and self.bbox.shape[1] != 0:\n        y1, x1, y2, x2 = self.bbox.squeeze(dim=0).nanmean(dim=0)\n        self.centroid = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n    if len(self.pose) == 0 and self.bbox.shape[1]:\n        y1, x1, y2, x2 = self.bbox.squeeze(dim=0).mean(dim=0)\n        self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n    if self.point_scores is None and len(self.pose) != 0:\n        self._point_scores = np.zeros((len(self.pose), 2))\n\n    self.to(self.device)\n</code></pre>"},{"location":"reference/dreem/#dreem.Instance.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of the Instance.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation of the Instance.\"\"\"\n    return (\n        \"Instance(\"\n        f\"gt_track_id={self._gt_track_id.item()}, \"\n        f\"pred_track_id={self._pred_track_id.item()}, \"\n        f\"bbox={self._bbox}, \"\n        f\"centroid={self._centroid}, \"\n        f\"crop={self._crop.shape}, \"\n        f\"features={self._features.shape}, \"\n        f\"device={self._device}\"\n        \")\"\n    )\n</code></pre>"},{"location":"reference/dreem/#dreem.Instance.add_embedding","title":"<code>add_embedding(emb_type, embedding)</code>","text":"<p>Save embedding to instance embedding dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>Key/embedding type to be saved to dictionary</p> required <code>embedding</code> <code>Tensor</code> <p>The actual torch tensor embedding.</p> required Source code in <code>dreem/io/instance.py</code> <pre><code>def add_embedding(self, emb_type: str, embedding: torch.Tensor) -&gt; None:\n    \"\"\"Save embedding to instance embedding dictionary.\n\n    Args:\n        emb_type: Key/embedding type to be saved to dictionary\n        embedding: The actual torch tensor embedding.\n    \"\"\"\n    embedding = _expand_to_rank(embedding, 2)\n    self._embeddings[emb_type] = embedding\n</code></pre>"},{"location":"reference/dreem/#dreem.Instance.from_slp","title":"<code>from_slp(slp_instance, bbox_size=64, crop=None, device=None)</code>  <code>classmethod</code>","text":"<p>Convert a slp instance to a dreem instance.</p> <p>Parameters:</p> Name Type Description Default <code>slp_instance</code> <code>PredictedInstance | Instance</code> <p>A <code>sleap_io.Instance</code> object representing a detection</p> required <code>bbox_size</code> <code>int | tuple[int, int]</code> <p>size of the pose-centered bbox to form.</p> <code>64</code> <code>crop</code> <code>ArrayLike | None</code> <p>The corresponding crop of the bbox</p> <code>None</code> <code>device</code> <code>str | None</code> <p>which device to keep the instance on</p> <code>None</code> <p>Returns:     A dreem.Instance object with a pose-centered bbox and no crop.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>@classmethod\ndef from_slp(\n    cls,\n    slp_instance: sio.PredictedInstance | sio.Instance,\n    bbox_size: int | tuple[int, int] = 64,\n    crop: ArrayLike | None = None,\n    device: str | None = None,\n) -&gt; Self:\n    \"\"\"Convert a slp instance to a dreem instance.\n\n    Args:\n        slp_instance: A `sleap_io.Instance` object representing a detection\n        bbox_size: size of the pose-centered bbox to form.\n        crop: The corresponding crop of the bbox\n        device: which device to keep the instance on\n    Returns:\n        A dreem.Instance object with a pose-centered bbox and no crop.\n    \"\"\"\n    try:\n        track_id = int(slp_instance.track.name)\n    except ValueError:\n        track_id = int(\n            \"\".join([str(ord(c)) for c in slp_instance.track.name])\n        )  # better way to handle this?\n    if isinstance(bbox_size, int):\n        bbox_size = (bbox_size, bbox_size)\n\n    track_score = -1.0\n    point_scores = np.full(len(slp_instance.points), -1)\n    instance_score = -1\n    if isinstance(slp_instance, sio.PredictedInstance):\n        track_score = slp_instance.tracking_score\n        point_scores = slp_instance.numpy()[:, -1]\n        instance_score = slp_instance.score\n\n    centroid = np.nanmean(slp_instance.numpy(), axis=1)\n    bbox = [\n        centroid[1] - bbox_size[1],\n        centroid[0] - bbox_size[0],\n        centroid[1] + bbox_size[1],\n        centroid[0] + bbox_size[0],\n    ]\n    return cls(\n        gt_track_id=track_id,\n        bbox=bbox,\n        crop=crop,\n        centroid={\"centroid\": centroid},\n        track_score=track_score,\n        point_scores=point_scores,\n        instance_score=instance_score,\n        skeleton=slp_instance.skeleton,\n        pose={\n            node.name: point.numpy() for node, point in slp_instance.points.items()\n        },\n        device=device,\n    )\n</code></pre>"},{"location":"reference/dreem/#dreem.Instance.get_embedding","title":"<code>get_embedding(emb_type='all')</code>","text":"<p>Retrieve instance's spatial/temporal embedding.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>The string key of the embedding to retrieve. Should be \"pos\", \"temp\"</p> <code>'all'</code> <p>Returns:</p> Type Description <code>dict[str, Tensor] | Tensor | None</code> <ul> <li>A torch tensor representing the spatial/temporal location of the instance.</li> <li>None if the embedding is not stored</li> </ul> Source code in <code>dreem/io/instance.py</code> <pre><code>def get_embedding(\n    self, emb_type: str = \"all\"\n) -&gt; dict[str, torch.Tensor] | torch.Tensor | None:\n    \"\"\"Retrieve instance's spatial/temporal embedding.\n\n    Args:\n        emb_type: The string key of the embedding to retrieve. Should be \"pos\", \"temp\"\n\n    Returns:\n        * A torch tensor representing the spatial/temporal location of the instance.\n        * None if the embedding is not stored\n    \"\"\"\n    if emb_type.lower() == \"all\":\n        return self._embeddings\n    else:\n        try:\n            return self._embeddings[emb_type]\n        except KeyError:\n            logger.exception(\n                f\"{emb_type} not saved! Only {list(self._embeddings.keys())} are available\"\n            )\n    return None\n</code></pre>"},{"location":"reference/dreem/#dreem.Instance.has_bbox","title":"<code>has_bbox()</code>","text":"<p>Determine if the instance has a bbox.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has a bounding box, false otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_bbox(self) -&gt; bool:\n    \"\"\"Determine if the instance has a bbox.\n\n    Returns:\n        True if the instance has a bounding box, false otherwise.\n    \"\"\"\n    if self._bbox.shape[1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/#dreem.Instance.has_crop","title":"<code>has_crop()</code>","text":"<p>Determine if the instance has a crop.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has an image otherwise False.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_crop(self) -&gt; bool:\n    \"\"\"Determine if the instance has a crop.\n\n    Returns:\n        True if the instance has an image otherwise False.\n    \"\"\"\n    if self._crop.shape[-1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/#dreem.Instance.has_embedding","title":"<code>has_embedding(emb_type=None)</code>","text":"<p>Determine if the instance has embedding type requested.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str | None</code> <p>The key to check in the embedding dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if <code>emb_type</code> in embedding_dict else false</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_embedding(self, emb_type: str | None = None) -&gt; bool:\n    \"\"\"Determine if the instance has embedding type requested.\n\n    Args:\n        emb_type: The key to check in the embedding dictionary.\n\n    Returns:\n        True if `emb_type` in embedding_dict else false\n    \"\"\"\n    return emb_type in self._embeddings\n</code></pre>"},{"location":"reference/dreem/#dreem.Instance.has_features","title":"<code>has_features()</code>","text":"<p>Determine if the instance has computed reid features.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has reid features, False otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_features(self) -&gt; bool:\n    \"\"\"Determine if the instance has computed reid features.\n\n    Returns:\n        True if the instance has reid features, False otherwise.\n    \"\"\"\n    if self._features.shape[-1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/#dreem.Instance.has_gt_track_id","title":"<code>has_gt_track_id()</code>","text":"<p>Determine if instance has a gt track assignment.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the gt track id is set, otherwise False.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_gt_track_id(self) -&gt; bool:\n    \"\"\"Determine if instance has a gt track assignment.\n\n    Returns:\n        True if the gt track id is set, otherwise False.\n    \"\"\"\n    if self._gt_track_id.shape[0] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/#dreem.Instance.has_pose","title":"<code>has_pose()</code>","text":"<p>Check if the instance has a pose.</p> <p>Returns True if the instance has a pose.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_pose(self) -&gt; bool:\n    \"\"\"Check if the instance has a pose.\n\n    Returns True if the instance has a pose.\n    \"\"\"\n    if len(self.pose):\n        return True\n    return False\n</code></pre>"},{"location":"reference/dreem/#dreem.Instance.has_pred_track_id","title":"<code>has_pred_track_id()</code>","text":"<p>Determine whether instance has predicted track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if instance has a pred track id, False otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_pred_track_id(self) -&gt; bool:\n    \"\"\"Determine whether instance has predicted track id.\n\n    Returns:\n        True if instance has a pred track id, False otherwise.\n    \"\"\"\n    if self._pred_track_id.item() == -1 or self._pred_track_id.shape[0] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/#dreem.Instance.to","title":"<code>to(map_location)</code>","text":"<p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <p>Parameters:</p> Name Type Description Default <code>map_location</code> <code>str | device</code> <p>Either the device or dtype for the instance to be moved.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>Self</code> <p>reference to the instance moved to correct device/dtype.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to(self, map_location: str | torch.device) -&gt; Self:\n    \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n    Args:\n        map_location: Either the device or dtype for the instance to be moved.\n\n    Returns:\n        self: reference to the instance moved to correct device/dtype.\n    \"\"\"\n    if map_location is not None and map_location != \"\":\n        self._gt_track_id = self._gt_track_id.to(map_location)\n        self._pred_track_id = self._pred_track_id.to(map_location)\n        self._bbox = self._bbox.to(map_location)\n        self._crop = self._crop.to(map_location)\n        self._features = self._features.to(map_location)\n        if isinstance(map_location, (str, torch.device)):\n            self.device = map_location\n\n    return self\n</code></pre>"},{"location":"reference/dreem/#dreem.Instance.to_h5","title":"<code>to_h5(frame_group, label=None, **kwargs)</code>","text":"<p>Convert instance to an h5 group\".</p> By default we always save <ul> <li>the gt/pred track id</li> <li>bbox</li> <li>centroid</li> <li>pose</li> <li>instance/traj/points score</li> </ul> <p>Larger arrays (crops/features/embeddings) can be saved by passing as kwargs</p> <p>Parameters:</p> Name Type Description Default <code>frame_group</code> <code>Group</code> <p>the h5py group representing the frame the instance appears on</p> required <code>label</code> <code>Any</code> <p>the name of the instance group that will be created</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>additional key:value pairs to be saved as datasets.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Group</code> <p>The h5 group representing this instance.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to_h5(\n    self, frame_group: h5py.Group, label: Any = None, **kwargs: dict\n) -&gt; h5py.Group:\n    \"\"\"Convert instance to an h5 group\".\n\n    By default we always save:\n        - the gt/pred track id\n        - bbox\n        - centroid\n        - pose\n        - instance/traj/points score\n    Larger arrays (crops/features/embeddings) can be saved by passing as kwargs\n\n    Args:\n        frame_group: the h5py group representing the frame the instance appears on\n        label: the name of the instance group that will be created\n        **kwargs: additional key:value pairs to be saved as datasets.\n\n    Returns:\n        The h5 group representing this instance.\n    \"\"\"\n    if label is None:\n        if pred_track_id != -1:\n            label = f\"instance_{self.pred_track_id.item()}\"\n        else:\n            label = f\"instance_{self.gt_track_id.item()}\"\n    instance_group = frame_group.create_group(label)\n    instance_group.attrs.create(\"gt_track_id\", self.gt_track_id.item())\n    instance_group.attrs.create(\"pred_track_id\", self.pred_track_id.item())\n    instance_group.attrs.create(\"track_score\", self.track_score)\n    instance_group.attrs.create(\"instance_score\", self.instance_score)\n\n    instance_group.create_dataset(\"bbox\", data=self.bbox.cpu().numpy())\n\n    pose_group = instance_group.create_group(\"pose\")\n    pose_group.create_dataset(\"points\", data=np.array(list(self.pose.values())))\n    pose_group.attrs.create(\"nodes\", list(self.pose.keys()))\n    pose_group.create_dataset(\"scores\", data=self.point_scores)\n\n    for key, value in kwargs.items():\n        if \"emb\" in key:\n            emb_group = instance_group.require_group(\"emb\")\n            emb_group.create_dataset(key, data=value)\n        else:\n            instance_group.create_dataset(key, data=value)\n\n    return instance_group\n</code></pre>"},{"location":"reference/dreem/#dreem.Instance.to_slp","title":"<code>to_slp(track_lookup={})</code>","text":"<p>Convert instance to sleap_io.PredictedInstance object.</p> <p>Parameters:</p> Name Type Description Default <code>track_lookup</code> <code>dict[int, Track]</code> <p>A track look up dictionary containing track_id:sio.Track.</p> <code>{}</code> <p>Returns: A sleap_io.PredictedInstance with necessary metadata     and a track_lookup dictionary to persist tracks.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to_slp(\n    self, track_lookup: dict[int, sio.Track] = {}\n) -&gt; tuple[sio.PredictedInstance, dict[int, sio.Track]]:\n    \"\"\"Convert instance to sleap_io.PredictedInstance object.\n\n    Args:\n        track_lookup: A track look up dictionary containing track_id:sio.Track.\n    Returns: A sleap_io.PredictedInstance with necessary metadata\n        and a track_lookup dictionary to persist tracks.\n    \"\"\"\n    try:\n        track_id = self.pred_track_id.item()\n        if track_id not in track_lookup:\n            track_lookup[track_id] = sio.Track(name=self.pred_track_id.item())\n\n        track = track_lookup[track_id]\n\n        return (\n            sio.PredictedInstance.from_numpy(\n                points_data=np.array(list(self.pose.values())),\n                skeleton=self.skeleton,\n                point_scores=self.point_scores,\n                score=self.instance_score,\n                tracking_score=self.track_score,\n                track=track,\n            ),\n            track_lookup,\n        )\n    except Exception as e:\n        logger.exception(\n            f\"Pose: {np.array(list(self.pose.values())).shape}, Pose score shape {self.point_scores.shape}\"\n        )\n        raise RuntimeError(f\"Failed to convert to sio.PredictedInstance: {e}\")\n</code></pre>"},{"location":"reference/dreem/#dreem.Tracker","title":"<code>Tracker</code>","text":"<p>Tracker class used for assignment based on sliding inference from GTR.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Wrap around <code>track</code> to enable <code>tracker()</code> instead of <code>tracker.track()</code>.</p> <code>__init__</code> <p>Initialize a tracker to run inference.</p> <code>__repr__</code> <p>Get string representation of tracker.</p> <code>sliding_inference</code> <p>Perform sliding inference on the input video (instances) with a given window size.</p> <code>track</code> <p>Run tracker and get predicted trajectories.</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>class Tracker:\n    \"\"\"Tracker class used for assignment based on sliding inference from GTR.\"\"\"\n\n    def __init__(\n        self,\n        window_size: int = 8,\n        use_vis_feats: bool = True,\n        overlap_thresh: float = 0.01,\n        mult_thresh: bool = True,\n        decay_time: float | None = None,\n        iou: str | None = None,\n        max_center_dist: float | None = None,\n        persistent_tracking: bool = False,\n        max_gap: int = inf,\n        max_tracks: int = inf,\n        verbose: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize a tracker to run inference.\n\n        Args:\n            window_size: the size of the window used during sliding inference.\n            use_vis_feats: Whether or not to use visual feature extractor.\n            overlap_thresh: the trajectory overlap threshold to be used for assignment.\n            mult_thresh: Whether or not to use weight threshold.\n            decay_time: weight for `decay_time` postprocessing.\n            iou: Either [None, '', \"mult\" or \"max\"]\n                 Whether to use multiplicative or max iou reweighting.\n            max_center_dist: distance threshold for filtering trajectory score matrix.\n            persistent_tracking: whether to keep a buffer across chunks or not.\n            max_gap: the max number of frames a trajectory can be missing before termination.\n            max_tracks: the maximum number of tracks that can be created while tracking.\n                We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.\n            verbose: Whether or not to turn on debug printing after each operation.\n        \"\"\"\n        self.track_queue = TrackQueue(\n            window_size=window_size, max_gap=max_gap, verbose=verbose\n        )\n        self.use_vis_feats = use_vis_feats\n        self.overlap_thresh = overlap_thresh\n        self.mult_thresh = mult_thresh\n        self.decay_time = decay_time\n        self.iou = iou\n        self.max_center_dist = max_center_dist\n        self.persistent_tracking = persistent_tracking\n        self.verbose = verbose\n        self.max_tracks = max_tracks\n\n    def __call__(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; list[Frame]:\n        \"\"\"Wrap around `track` to enable `tracker()` instead of `tracker.track()`.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: list of Frames to run inference on\n\n        Returns:\n            List of frames containing association matrix scores and instances populated with pred track ids.\n        \"\"\"\n        return self.track(model, frames)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get string representation of tracker.\n\n        Returns: the string representation of the tracker\n        \"\"\"\n        return (\n            \"Tracker(\"\n            f\"persistent_tracking={self.persistent_tracking}, \"\n            f\"max_tracks={self.max_tracks}, \"\n            f\"use_vis_feats={self.use_vis_feats}, \"\n            f\"overlap_thresh={self.overlap_thresh}, \"\n            f\"mult_thresh={self.mult_thresh}, \"\n            f\"decay_time={self.decay_time}, \"\n            f\"max_center_dist={self.max_center_dist}, \"\n            f\"verbose={self.verbose}, \"\n            f\"queue={self.track_queue}\"\n        )\n\n    def track(\n        self, model: GlobalTrackingTransformer, frames: list[dict]\n    ) -&gt; list[Frame]:\n        \"\"\"Run tracker and get predicted trajectories.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: data dict to run inference on\n\n        Returns:\n            List of Frames populated with pred track ids and association matrix scores\n        \"\"\"\n        # Extract feature representations with pre-trained encoder.\n\n        _ = model.eval()\n\n        for frame in frames:\n            if frame.has_instances():\n                if not self.use_vis_feats:\n                    for instance in frame.instances:\n                        instance.features = torch.zeros(1, model.d_model)\n                    # frame[\"features\"] = torch.randn(\n                    #     num_frame_instances, self.model.d_model\n                    # )\n\n                # comment out to turn encoder off\n\n                # Assuming the encoder is already trained or train encoder jointly.\n                elif not frame.has_features():\n                    with torch.no_grad():\n                        crops = frame.get_crops()\n                        z = model.visual_encoder(crops)\n\n                        for i, z_i in enumerate(z):\n                            frame.instances[i].features = z_i\n\n        # I feel like this chunk is unnecessary:\n        # reid_features = torch.cat(\n        #     [frame[\"features\"] for frame in instances], dim=0\n        # ).unsqueeze(0)\n\n        # asso_preds, pred_boxes, pred_time, embeddings = self.model(\n        #     instances, reid_features\n        # )\n        instances_pred = self.sliding_inference(model, frames)\n\n        if not self.persistent_tracking:\n            logger.debug(\"Clearing Queue after tracking\")\n            self.track_queue.end_tracks()\n\n        return instances_pred\n\n    def sliding_inference(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; list[Frame]:\n        \"\"\"Perform sliding inference on the input video (instances) with a given window size.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n        Returns:\n            frames: A list of Frames populated with pred_track_ids and asso_matrices\n        \"\"\"\n        # B: batch size.\n        # D: embedding dimension.\n        # nc: number of channels.\n        # H: height.\n        # W: width.\n\n        for batch_idx, frame_to_track in enumerate(frames):\n            tracked_frames = self.track_queue.collate_tracks(\n                device=frame_to_track.frame_id.device\n            )\n            logger.debug(f\"Current number of tracks is {self.track_queue.n_tracks}\")\n\n            if (\n                self.persistent_tracking and frame_to_track.frame_id == 0\n            ):  # check for new video and clear queue\n                logger.debug(\"New Video! Resetting Track Queue.\")\n                self.track_queue.end_tracks()\n\n            \"\"\"\n            Initialize tracks on first frame where detections appear.\n            \"\"\"\n            if len(self.track_queue) == 0:\n                if frame_to_track.has_instances():\n                    logger.debug(\n                        f\"Initializing track on clip ind {batch_idx} frame {frame_to_track.frame_id.item()}\"\n                    )\n\n                    curr_track_id = 0\n                    for i, instance in enumerate(frames[batch_idx].instances):\n                        instance.pred_track_id = instance.gt_track_id\n                        curr_track_id = max(curr_track_id, instance.pred_track_id)\n\n                    for i, instance in enumerate(frames[batch_idx].instances):\n                        if instance.pred_track_id == -1:\n                            curr_track_id += 1\n                            instance.pred_track_id = curr_track_id\n\n            else:\n                if frame_to_track.has_instances():  # Check if there are detections. If there are skip and increment gap count\n                    frames_to_track = tracked_frames + [\n                        frame_to_track\n                    ]  # better var name?\n\n                    query_ind = len(frames_to_track) - 1\n\n                    frame_to_track = self._run_global_tracker(\n                        model,\n                        frames_to_track,\n                        query_ind=query_ind,\n                    )\n\n            if frame_to_track.has_instances():\n                self.track_queue.add_frame(frame_to_track)\n            else:\n                self.track_queue.increment_gaps([])\n\n            frames[batch_idx] = frame_to_track\n        return frames\n\n    def _run_global_tracker(\n        self, model: GlobalTrackingTransformer, frames: list[Frame], query_ind: int\n    ) -&gt; Frame:\n        \"\"\"Run global tracker performs the actual tracking.\n\n        Uses Hungarian algorithm to do track assigning.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: A list of Frames containing reid features. See `dreem.io.data_structures` for more info.\n            query_ind: An integer for the query frame within the window of instances.\n\n        Returns:\n            query_frame: The query frame now populated with the pred_track_ids.\n        \"\"\"\n        # *: each item in frames is a frame in the window. So it follows\n        #    that each frame in the window has * detected instances.\n        # D: embedding dimension.\n        # total_instances: number of instances in the window.\n        # N_i: number of detected instances in i-th frame of window.\n        # instances_per_frame: a list of number of instances in each frame of the window.\n        # n_query: number of instances in current/query frame (rightmost frame of the window).\n        # n_nonquery: number of instances in the window excluding the current/query frame.\n        # window_size: length of window.\n        # L: number of decoder blocks.\n        # n_traj: number of existing tracks within the window so far.\n\n        # Number of instances in each frame of the window.\n        # E.g.: instances_per_frame: [4, 5, 6, 7]; window of length 4 with 4 detected instances in the first frame of the window.\n\n        _ = model.eval()\n\n        query_frame = frames[query_ind]\n\n        query_instances = query_frame.instances\n        all_instances = [instance for frame in frames for instance in frame.instances]\n\n        logger.debug(f\"Frame {query_frame.frame_id.item()}\")\n\n        instances_per_frame = [frame.num_detected for frame in frames]\n\n        total_instances, window_size = (\n            sum(instances_per_frame),\n            len(instances_per_frame),\n        )  # Number of instances in window; length of window.\n\n        logger.debug(f\"total_instances: {total_instances}\")\n\n        overlap_thresh = self.overlap_thresh\n        mult_thresh = self.mult_thresh\n        n_traj = self.track_queue.n_tracks\n        curr_track = self.track_queue.curr_track\n\n        reid_features = torch.cat([frame.get_features() for frame in frames], dim=0)[\n            None\n        ]  # (1, total_instances, D=512)\n\n        # (L=1, n_query, total_instances)\n        with torch.no_grad():\n            asso_matrix = model(all_instances, query_instances)\n\n        asso_output = asso_matrix[-1].matrix.split(\n            instances_per_frame, dim=1\n        )  # (window_size, n_query, N_i)\n        asso_output = model_utils.softmax_asso(\n            asso_output\n        )  # (window_size, n_query, N_i)\n        asso_output = torch.cat(asso_output, dim=1).cpu()  # (n_query, total_instances)\n\n        asso_output_df = pd.DataFrame(\n            asso_output.clone().numpy(),\n            columns=[f\"Instance {i}\" for i in range(asso_output.shape[-1])],\n        )\n\n        asso_output_df.index.name = \"Instances\"\n        asso_output_df.columns.name = \"Instances\"\n\n        query_frame.add_traj_score(\"asso_output\", asso_output_df)\n        query_frame.asso_output = asso_matrix[-1]\n\n        n_query = (\n            query_frame.num_detected\n        )  # Number of instances in the current/query frame.\n\n        n_nonquery = (\n            total_instances - n_query\n        )  # Number of instances in the window not including the current/query frame.\n\n        logger.debug(f\"n_nonquery: {n_nonquery}\")\n        logger.debug(f\"n_query: {n_query}\")\n\n        instance_ids = torch.cat(\n            [\n                x.get_pred_track_ids()\n                for batch_idx, x in enumerate(frames)\n                if batch_idx != query_ind\n            ],\n            dim=0,\n        ).view(n_nonquery)  # (n_nonquery,)\n\n        query_inds = [\n            x\n            for x in range(\n                sum(instances_per_frame[:query_ind]),\n                sum(instances_per_frame[: query_ind + 1]),\n            )\n        ]\n\n        nonquery_inds = [i for i in range(total_instances) if i not in query_inds]\n\n        # instead should we do model(nonquery_instances, query_instances)?\n        asso_nonquery = asso_output[:, nonquery_inds]  # (n_query, n_nonquery)\n\n        asso_nonquery_df = pd.DataFrame(\n            asso_nonquery.clone().numpy(), columns=nonquery_inds\n        )\n\n        asso_nonquery_df.index.name = \"Current Frame Instances\"\n        asso_nonquery_df.columns.name = \"Nonquery Instances\"\n\n        query_frame.add_traj_score(\"asso_nonquery\", asso_nonquery_df)\n\n        # get raw bbox coords of prev frame instances from frame.instances_per_frame\n        query_boxes_px = torch.cat(\n            [instance.bbox for instance in query_frame.instances], dim=0\n        )\n        nonquery_boxes_px = torch.cat(\n            [\n                instance.bbox\n                for nonquery_frame in frames\n                if nonquery_frame.frame_id != query_frame.frame_id\n                for instance in nonquery_frame.instances\n            ],\n            dim=0,\n        )\n\n        pred_boxes = model_utils.get_boxes(all_instances)\n        query_boxes = pred_boxes[query_inds]  # n_k x 4\n        nonquery_boxes = pred_boxes[nonquery_inds]  # n_nonquery x 4\n\n        unique_ids = torch.unique(instance_ids)  # (n_nonquery,)\n\n        logger.debug(f\"Instance IDs: {instance_ids}\")\n        logger.debug(f\"unique ids: {unique_ids}\")\n\n        id_inds = (\n            unique_ids[None, :] == instance_ids[:, None]\n        ).float()  # (n_nonquery, n_traj)\n\n        ################################################################################\n\n        # reweighting hyper-parameters for association -&gt; they use 0.9\n\n        traj_score = post_processing.weight_decay_time(\n            asso_nonquery, self.decay_time, reid_features, window_size, query_ind\n        )\n\n        if self.decay_time is not None and self.decay_time &gt; 0:\n            decay_time_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=nonquery_inds\n            )\n\n            decay_time_traj_score.index.name = \"Query Instances\"\n            decay_time_traj_score.columns.name = \"Nonquery Instances\"\n\n            query_frame.add_traj_score(\"decay_time\", decay_time_traj_score)\n        ################################################################################\n\n        # (n_query x n_nonquery) x (n_nonquery x n_traj) --&gt; n_query x n_traj\n        traj_score = torch.mm(traj_score, id_inds.cpu())  # (n_query, n_traj)\n\n        traj_score_df = pd.DataFrame(\n            traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n        )\n\n        traj_score_df.index.name = \"Current Frame Instances\"\n        traj_score_df.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"traj_score\", traj_score_df)\n        ################################################################################\n\n        # with iou -&gt; combining with location in tracker, they set to True\n        # todo -&gt; should also work without pos_embed\n\n        if id_inds.numel() &gt; 0:\n            # this throws error, think we need to slice?\n            # last_inds = (id_inds * torch.arange(\n            #    n_nonquery, device=id_inds.device)[:, None]).max(dim=0)[1] # n_traj\n\n            last_inds = (\n                id_inds * torch.arange(n_nonquery, device=id_inds.device)[:, None]\n            ).max(dim=0)[1]  # M\n\n            last_boxes = nonquery_boxes[last_inds]  # n_traj x 4\n            last_ious = post_processing._pairwise_iou(\n                Boxes(query_boxes), Boxes(last_boxes)\n            )  # n_k x M\n        else:\n            last_ious = traj_score.new_zeros(traj_score.shape)\n\n        traj_score = post_processing.weight_iou(traj_score, self.iou, last_ious.cpu())\n\n        if self.iou is not None and self.iou != \"\":\n            iou_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n            )\n\n            iou_traj_score.index.name = \"Current Frame Instances\"\n            iou_traj_score.columns.name = \"Unique IDs\"\n\n            query_frame.add_traj_score(\"weight_iou\", iou_traj_score)\n        ################################################################################\n\n        # threshold for continuing a tracking or starting a new track -&gt; they use 1.0\n        # todo -&gt; should also work without pos_embed\n        traj_score = post_processing.filter_max_center_dist(\n            traj_score,\n            self.max_center_dist,\n            id_inds,\n            query_boxes_px,\n            nonquery_boxes_px,\n        )\n\n        if self.max_center_dist is not None and self.max_center_dist &gt; 0:\n            max_center_dist_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n            )\n\n            max_center_dist_traj_score.index.name = \"Current Frame Instances\"\n            max_center_dist_traj_score.columns.name = \"Unique IDs\"\n\n            query_frame.add_traj_score(\"max_center_dist\", max_center_dist_traj_score)\n\n        ################################################################################\n        scaled_traj_score = torch.softmax(traj_score, dim=1)\n        scaled_traj_score_df = pd.DataFrame(\n            scaled_traj_score.numpy(), columns=unique_ids.cpu().numpy()\n        )\n        scaled_traj_score_df.index.name = \"Current Frame Instances\"\n        scaled_traj_score_df.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"scaled\", scaled_traj_score_df)\n        ################################################################################\n\n        match_i, match_j = linear_sum_assignment((-traj_score))\n\n        track_ids = instance_ids.new_full((n_query,), -1)\n        for i, j in zip(match_i, match_j):\n            # The overlap threshold is multiplied by the number of times the unique track j is matched to an\n            # instance out of all instances in the window excluding the current frame.\n            #\n            # So if this is correct, the threshold is higher for matching an instance from the current frame\n            # to an existing track if that track has already been matched several times.\n            # So if an existing track in the window has been matched a lot, it gets harder to match to that track.\n            thresh = (\n                overlap_thresh * id_inds[:, j].sum() if mult_thresh else overlap_thresh\n            )\n            if n_traj &gt;= self.max_tracks or traj_score[i, j] &gt; thresh:\n                logger.debug(\n                    f\"Assigning instance {i} to track {j} with id {unique_ids[j]}\"\n                )\n                track_ids[i] = unique_ids[j]\n                query_frame.instances[i].track_score = scaled_traj_score[i, j].item()\n        logger.debug(f\"track_ids: {track_ids}\")\n        for i in range(n_query):\n            if track_ids[i] &lt; 0:\n                logger.debug(f\"Creating new track {curr_track}\")\n                curr_track += 1\n                track_ids[i] = curr_track\n\n        query_frame.matches = (match_i, match_j)\n\n        for instance, track_id in zip(query_frame.instances, track_ids):\n            instance.pred_track_id = track_id\n\n        final_traj_score = pd.DataFrame(\n            traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n        )\n        final_traj_score.index.name = \"Current Frame Instances\"\n        final_traj_score.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"final\", final_traj_score)\n        return query_frame\n</code></pre>"},{"location":"reference/dreem/#dreem.Tracker.__call__","title":"<code>__call__(model, frames)</code>","text":"<p>Wrap around <code>track</code> to enable <code>tracker()</code> instead of <code>tracker.track()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[Frame]</code> <p>list of Frames to run inference on</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>List of frames containing association matrix scores and instances populated with pred track ids.</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def __call__(\n    self, model: GlobalTrackingTransformer, frames: list[Frame]\n) -&gt; list[Frame]:\n    \"\"\"Wrap around `track` to enable `tracker()` instead of `tracker.track()`.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: list of Frames to run inference on\n\n    Returns:\n        List of frames containing association matrix scores and instances populated with pred track ids.\n    \"\"\"\n    return self.track(model, frames)\n</code></pre>"},{"location":"reference/dreem/#dreem.Tracker.__init__","title":"<code>__init__(window_size=8, use_vis_feats=True, overlap_thresh=0.01, mult_thresh=True, decay_time=None, iou=None, max_center_dist=None, persistent_tracking=False, max_gap=inf, max_tracks=inf, verbose=False, **kwargs)</code>","text":"<p>Initialize a tracker to run inference.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>the size of the window used during sliding inference.</p> <code>8</code> <code>use_vis_feats</code> <code>bool</code> <p>Whether or not to use visual feature extractor.</p> <code>True</code> <code>overlap_thresh</code> <code>float</code> <p>the trajectory overlap threshold to be used for assignment.</p> <code>0.01</code> <code>mult_thresh</code> <code>bool</code> <p>Whether or not to use weight threshold.</p> <code>True</code> <code>decay_time</code> <code>float | None</code> <p>weight for <code>decay_time</code> postprocessing.</p> <code>None</code> <code>iou</code> <code>str | None</code> <p>Either [None, '', \"mult\" or \"max\"]  Whether to use multiplicative or max iou reweighting.</p> <code>None</code> <code>max_center_dist</code> <code>float | None</code> <p>distance threshold for filtering trajectory score matrix.</p> <code>None</code> <code>persistent_tracking</code> <code>bool</code> <p>whether to keep a buffer across chunks or not.</p> <code>False</code> <code>max_gap</code> <code>int</code> <p>the max number of frames a trajectory can be missing before termination.</p> <code>inf</code> <code>max_tracks</code> <code>int</code> <p>the maximum number of tracks that can be created while tracking. We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.</p> <code>inf</code> <code>verbose</code> <code>bool</code> <p>Whether or not to turn on debug printing after each operation.</p> <code>False</code> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def __init__(\n    self,\n    window_size: int = 8,\n    use_vis_feats: bool = True,\n    overlap_thresh: float = 0.01,\n    mult_thresh: bool = True,\n    decay_time: float | None = None,\n    iou: str | None = None,\n    max_center_dist: float | None = None,\n    persistent_tracking: bool = False,\n    max_gap: int = inf,\n    max_tracks: int = inf,\n    verbose: bool = False,\n    **kwargs,\n):\n    \"\"\"Initialize a tracker to run inference.\n\n    Args:\n        window_size: the size of the window used during sliding inference.\n        use_vis_feats: Whether or not to use visual feature extractor.\n        overlap_thresh: the trajectory overlap threshold to be used for assignment.\n        mult_thresh: Whether or not to use weight threshold.\n        decay_time: weight for `decay_time` postprocessing.\n        iou: Either [None, '', \"mult\" or \"max\"]\n             Whether to use multiplicative or max iou reweighting.\n        max_center_dist: distance threshold for filtering trajectory score matrix.\n        persistent_tracking: whether to keep a buffer across chunks or not.\n        max_gap: the max number of frames a trajectory can be missing before termination.\n        max_tracks: the maximum number of tracks that can be created while tracking.\n            We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.\n        verbose: Whether or not to turn on debug printing after each operation.\n    \"\"\"\n    self.track_queue = TrackQueue(\n        window_size=window_size, max_gap=max_gap, verbose=verbose\n    )\n    self.use_vis_feats = use_vis_feats\n    self.overlap_thresh = overlap_thresh\n    self.mult_thresh = mult_thresh\n    self.decay_time = decay_time\n    self.iou = iou\n    self.max_center_dist = max_center_dist\n    self.persistent_tracking = persistent_tracking\n    self.verbose = verbose\n    self.max_tracks = max_tracks\n</code></pre>"},{"location":"reference/dreem/#dreem.Tracker.__repr__","title":"<code>__repr__()</code>","text":"<p>Get string representation of tracker.</p> <p>Returns: the string representation of the tracker</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get string representation of tracker.\n\n    Returns: the string representation of the tracker\n    \"\"\"\n    return (\n        \"Tracker(\"\n        f\"persistent_tracking={self.persistent_tracking}, \"\n        f\"max_tracks={self.max_tracks}, \"\n        f\"use_vis_feats={self.use_vis_feats}, \"\n        f\"overlap_thresh={self.overlap_thresh}, \"\n        f\"mult_thresh={self.mult_thresh}, \"\n        f\"decay_time={self.decay_time}, \"\n        f\"max_center_dist={self.max_center_dist}, \"\n        f\"verbose={self.verbose}, \"\n        f\"queue={self.track_queue}\"\n    )\n</code></pre>"},{"location":"reference/dreem/#dreem.Tracker.sliding_inference","title":"<code>sliding_inference(model, frames)</code>","text":"<p>Perform sliding inference on the input video (instances) with a given window size.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[Frame]</code> <p>A list of Frames (See <code>dreem.io.Frame</code> for more info).</p> required <p>Returns:</p> Name Type Description <code>frames</code> <code>list[Frame]</code> <p>A list of Frames populated with pred_track_ids and asso_matrices</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def sliding_inference(\n    self, model: GlobalTrackingTransformer, frames: list[Frame]\n) -&gt; list[Frame]:\n    \"\"\"Perform sliding inference on the input video (instances) with a given window size.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n    Returns:\n        frames: A list of Frames populated with pred_track_ids and asso_matrices\n    \"\"\"\n    # B: batch size.\n    # D: embedding dimension.\n    # nc: number of channels.\n    # H: height.\n    # W: width.\n\n    for batch_idx, frame_to_track in enumerate(frames):\n        tracked_frames = self.track_queue.collate_tracks(\n            device=frame_to_track.frame_id.device\n        )\n        logger.debug(f\"Current number of tracks is {self.track_queue.n_tracks}\")\n\n        if (\n            self.persistent_tracking and frame_to_track.frame_id == 0\n        ):  # check for new video and clear queue\n            logger.debug(\"New Video! Resetting Track Queue.\")\n            self.track_queue.end_tracks()\n\n        \"\"\"\n        Initialize tracks on first frame where detections appear.\n        \"\"\"\n        if len(self.track_queue) == 0:\n            if frame_to_track.has_instances():\n                logger.debug(\n                    f\"Initializing track on clip ind {batch_idx} frame {frame_to_track.frame_id.item()}\"\n                )\n\n                curr_track_id = 0\n                for i, instance in enumerate(frames[batch_idx].instances):\n                    instance.pred_track_id = instance.gt_track_id\n                    curr_track_id = max(curr_track_id, instance.pred_track_id)\n\n                for i, instance in enumerate(frames[batch_idx].instances):\n                    if instance.pred_track_id == -1:\n                        curr_track_id += 1\n                        instance.pred_track_id = curr_track_id\n\n        else:\n            if frame_to_track.has_instances():  # Check if there are detections. If there are skip and increment gap count\n                frames_to_track = tracked_frames + [\n                    frame_to_track\n                ]  # better var name?\n\n                query_ind = len(frames_to_track) - 1\n\n                frame_to_track = self._run_global_tracker(\n                    model,\n                    frames_to_track,\n                    query_ind=query_ind,\n                )\n\n        if frame_to_track.has_instances():\n            self.track_queue.add_frame(frame_to_track)\n        else:\n            self.track_queue.increment_gaps([])\n\n        frames[batch_idx] = frame_to_track\n    return frames\n</code></pre>"},{"location":"reference/dreem/#dreem.Tracker.track","title":"<code>track(model, frames)</code>","text":"<p>Run tracker and get predicted trajectories.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[dict]</code> <p>data dict to run inference on</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>List of Frames populated with pred track ids and association matrix scores</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def track(\n    self, model: GlobalTrackingTransformer, frames: list[dict]\n) -&gt; list[Frame]:\n    \"\"\"Run tracker and get predicted trajectories.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: data dict to run inference on\n\n    Returns:\n        List of Frames populated with pred track ids and association matrix scores\n    \"\"\"\n    # Extract feature representations with pre-trained encoder.\n\n    _ = model.eval()\n\n    for frame in frames:\n        if frame.has_instances():\n            if not self.use_vis_feats:\n                for instance in frame.instances:\n                    instance.features = torch.zeros(1, model.d_model)\n                # frame[\"features\"] = torch.randn(\n                #     num_frame_instances, self.model.d_model\n                # )\n\n            # comment out to turn encoder off\n\n            # Assuming the encoder is already trained or train encoder jointly.\n            elif not frame.has_features():\n                with torch.no_grad():\n                    crops = frame.get_crops()\n                    z = model.visual_encoder(crops)\n\n                    for i, z_i in enumerate(z):\n                        frame.instances[i].features = z_i\n\n    # I feel like this chunk is unnecessary:\n    # reid_features = torch.cat(\n    #     [frame[\"features\"] for frame in instances], dim=0\n    # ).unsqueeze(0)\n\n    # asso_preds, pred_boxes, pred_time, embeddings = self.model(\n    #     instances, reid_features\n    # )\n    instances_pred = self.sliding_inference(model, frames)\n\n    if not self.persistent_tracking:\n        logger.debug(\"Clearing Queue after tracking\")\n        self.track_queue.end_tracks()\n\n    return instances_pred\n</code></pre>"},{"location":"reference/dreem/#dreem.Transformer","title":"<code>Transformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer class.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize Transformer.</p> <code>forward</code> <p>Execute a forward pass through the transformer and attention head.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class Transformer(torch.nn.Module):\n    \"\"\"Transformer class.\"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 1024,\n        nhead: int = 8,\n        num_encoder_layers: int = 6,\n        num_decoder_layers: int = 6,\n        dropout: float = 0.1,\n        activation: str = \"relu\",\n        return_intermediate_dec: bool = False,\n        norm: bool = False,\n        num_layers_attn_head: int = 2,\n        dropout_attn_head: float = 0.1,\n        embedding_meta: dict | None = None,\n        return_embedding: bool = False,\n        decoder_self_attn: bool = False,\n        encoder_cfg: dict | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize Transformer.\n\n        Args:\n            d_model: The number of features in the encoder/decoder inputs.\n            nhead: The number of heads in the transformer encoder/decoder.\n            num_encoder_layers: The number of encoder-layers in the encoder.\n            num_decoder_layers: The number of decoder-layers in the decoder.\n            dropout: Dropout value applied to the output of transformer layers.\n            activation: Activation function to use.\n            return_intermediate_dec: Return intermediate layers from decoder.\n            norm: If True, normalize output of encoder and decoder.\n            num_layers_attn_head: The number of layers in the attention head.\n            dropout_attn_head: Dropout value for the attention_head.\n            embedding_meta: Metadata for positional embeddings. See below.\n            return_embedding: Whether to return the positional embeddings\n            decoder_self_attn: If True, use decoder self attention.\n            encoder_cfg: Encoder configuration.\n\n                More details on `embedding_meta`:\n                    By default this will be an empty dict and indicate\n                    that no positional embeddings should be used. To use the positional embeddings\n                    pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                    {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},\n                    \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                    and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n        \"\"\"\n        super().__init__()\n\n        self.d_model = dim_feedforward = feature_dim_attn_head = d_model\n\n        self.embedding_meta = embedding_meta\n        self.return_embedding = return_embedding\n        self.encoder_cfg = encoder_cfg\n\n        self.pos_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n        self.temp_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n\n        if self.embedding_meta:\n            if \"pos\" in self.embedding_meta:\n                pos_emb_cfg = self.embedding_meta[\"pos\"]\n                if pos_emb_cfg:\n                    self.pos_emb = Embedding(\n                        emb_type=\"pos\", features=self.d_model, **pos_emb_cfg\n                    )\n            if \"temp\" in self.embedding_meta:\n                temp_emb_cfg = self.embedding_meta[\"temp\"]\n                if temp_emb_cfg:\n                    self.temp_emb = Embedding(\n                        emb_type=\"temp\", features=self.d_model, **temp_emb_cfg\n                    )\n\n        self.fourier_embeddings = FourierPositionalEmbeddings(\n            n_components=8, d_model=d_model\n        )\n\n        # Transformer Encoder\n        encoder_layer = TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, activation, norm\n        )\n\n        encoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n        # only used if using descriptor visual encoder; default resnet encoder uses d_model directly\n        if self.encoder_cfg and \"encoder_type\" in self.encoder_cfg:\n            self.visual_feat_dim = (\n                self.encoder_cfg[\"ndim\"] if \"ndim\" in self.encoder_cfg else 5\n            )  # 5 is default for descriptor\n            self.fourier_proj = nn.Linear(self.d_model + self.visual_feat_dim, d_model)\n            self.fourier_norm = nn.LayerNorm(self.d_model)\n\n        self.encoder = TransformerEncoder(\n            encoder_layer, num_encoder_layers, encoder_norm\n        )\n\n        # Transformer Decoder\n        decoder_layer = TransformerDecoderLayer(\n            d_model,\n            nhead,\n            dim_feedforward,\n            dropout,\n            activation,\n            norm,\n            decoder_self_attn,\n        )\n\n        decoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n        self.decoder = TransformerDecoder(\n            decoder_layer, num_decoder_layers, return_intermediate_dec, decoder_norm\n        )\n\n        # Transformer attention head\n        self.attn_head = ATTWeightHead(\n            feature_dim=feature_dim_attn_head,\n            num_layers=num_layers_attn_head,\n            dropout=dropout_attn_head,\n        )\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        \"\"\"Initialize model weights from xavier distribution.\"\"\"\n        for p in self.parameters():\n            if not torch.nn.parameter.is_lazy(p) and p.dim() &gt; 1:\n                try:\n                    nn.init.xavier_uniform_(p)\n                except ValueError as e:\n                    print(f\"Failed Trying to initialize {p}\")\n                    raise (e)\n\n    def forward(\n        self,\n        ref_instances: list[\"dreem.io.Instance\"],\n        query_instances: list[\"dreem.io.Instance\"] | None = None,\n    ) -&gt; list[AssociationMatrix]:\n        \"\"\"Execute a forward pass through the transformer and attention head.\n\n        Args:\n            ref_instances: A list of instance objects (See `dreem.io.Instance` for more info.)\n            query_instances: An set of instances to be used as decoder queries.\n\n        Returns:\n            asso_output: A list of torch.Tensors of shape (L, n_query, total_instances) where:\n                L: number of decoder blocks\n                n_query: number of instances in current query/frame\n                total_instances: number of instances in window\n        \"\"\"\n        ref_features = torch.cat(\n            [instance.features for instance in ref_instances], dim=0\n        ).unsqueeze(0)\n\n        # window_length = len(frames)\n        # instances_per_frame = [frame.num_detected for frame in frames]\n        total_instances = len(ref_instances)\n        embed_dim = self.d_model\n        # print(f'T: {window_length}; N: {total_instances}; N_t: {instances_per_frame} n_reid: {reid_features.shape}')\n        ref_boxes = get_boxes(ref_instances)  # total_instances, 4\n        ref_boxes = torch.nan_to_num(ref_boxes, -1.0)\n        ref_times, query_times = get_times(ref_instances, query_instances)\n\n        # window_length = len(ref_times.unique())  # Currently unused but may be useful for debugging\n\n        ref_temp_emb = self.temp_emb(ref_times)\n\n        ref_pos_emb = self.pos_emb(ref_boxes)\n\n        if self.return_embedding:\n            for i, instance in enumerate(ref_instances):\n                instance.add_embedding(\"pos\", ref_pos_emb[i])\n                instance.add_embedding(\"temp\", ref_temp_emb[i])\n\n        ref_emb = (ref_pos_emb + ref_temp_emb) / 2.0\n\n        ref_emb = ref_emb.view(1, total_instances, embed_dim)\n\n        ref_emb = ref_emb.permute(1, 0, 2)  # (total_instances, batch_size, embed_dim)\n\n        batch_size, total_instances = ref_features.shape[:-1]\n\n        ref_features = ref_features.permute(\n            1, 0, 2\n        )  # (total_instances, batch_size, embed_dim)\n\n        encoder_queries = ref_features\n\n        # apply fourier embeddings if using fourier rope, OR if using descriptor (compact) visual encoder\n        if (\n            self.embedding_meta\n            and \"use_fourier\" in self.embedding_meta\n            and self.embedding_meta[\"use_fourier\"]\n        ) or (\n            self.encoder_cfg\n            and \"encoder_type\" in self.encoder_cfg\n            and self.encoder_cfg[\"encoder_type\"] == \"descriptor\"\n        ):\n            encoder_queries = apply_fourier_embeddings(\n                encoder_queries,\n                ref_times,\n                self.d_model,\n                self.fourier_embeddings,\n                self.fourier_proj,\n                self.fourier_norm,\n            )\n\n        encoder_features = self.encoder(\n            encoder_queries, pos_emb=ref_emb\n        )  # (total_instances, batch_size, embed_dim)\n\n        n_query = total_instances\n\n        query_features = ref_features\n        query_pos_emb = ref_pos_emb\n        query_temp_emb = ref_temp_emb\n        query_emb = ref_emb\n\n        if query_instances is not None:\n            n_query = len(query_instances)\n\n            query_features = torch.cat(\n                [instance.features for instance in query_instances], dim=0\n            ).unsqueeze(0)\n\n            query_features = query_features.permute(\n                1, 0, 2\n            )  # (n_query, batch_size, embed_dim)\n\n            query_boxes = get_boxes(query_instances)\n            query_boxes = torch.nan_to_num(query_boxes, -1.0)\n            query_temp_emb = self.temp_emb(query_times)\n\n            query_pos_emb = self.pos_emb(query_boxes)\n\n            query_emb = (query_pos_emb + query_temp_emb) / 2.0\n            query_emb = query_emb.view(1, n_query, embed_dim)\n            query_emb = query_emb.permute(1, 0, 2)  # (n_query, batch_size, embed_dim)\n\n        else:\n            query_instances = ref_instances\n            query_times = ref_times\n\n        if self.return_embedding:\n            for i, instance in enumerate(query_instances):\n                instance.add_embedding(\"pos\", query_pos_emb[i])\n                instance.add_embedding(\"temp\", query_temp_emb[i])\n\n        # apply fourier embeddings if using fourier rope, OR if using descriptor (compact) visual encoder\n        if (\n            self.embedding_meta\n            and \"use_fourier\" in self.embedding_meta\n            and self.embedding_meta[\"use_fourier\"]\n        ) or (\n            self.encoder_cfg\n            and \"encoder_type\" in self.encoder_cfg\n            and self.encoder_cfg[\"encoder_type\"] == \"descriptor\"\n        ):\n            query_features = apply_fourier_embeddings(\n                query_features,\n                query_times,\n                self.d_model,\n                self.fourier_embeddings,\n                self.fourier_proj,\n                self.fourier_norm,\n            )\n\n        decoder_features = self.decoder(\n            query_features,\n            encoder_features,\n            ref_pos_emb=ref_emb,\n            query_pos_emb=query_emb,\n        )  # (L, n_query, batch_size, embed_dim)\n\n        decoder_features = decoder_features.transpose(\n            1, 2\n        )  # # (L, batch_size, n_query, embed_dim)\n        encoder_features = encoder_features.permute(1, 0, 2).view(\n            batch_size, total_instances, embed_dim\n        )  # (batch_size, total_instances, embed_dim)\n\n        asso_output = []\n        for frame_features in decoder_features:\n            asso_matrix = self.attn_head(frame_features, encoder_features).view(\n                n_query, total_instances\n            )\n            asso_matrix = AssociationMatrix(asso_matrix, ref_instances, query_instances)\n\n            asso_output.append(asso_matrix)\n\n        # (L=1, n_query, total_instances)\n        return asso_output\n</code></pre>"},{"location":"reference/dreem/#dreem.Transformer.__init__","title":"<code>__init__(d_model=1024, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dropout=0.1, activation='relu', return_intermediate_dec=False, norm=False, num_layers_attn_head=2, dropout_attn_head=0.1, embedding_meta=None, return_embedding=False, decoder_self_attn=False, encoder_cfg=None)</code>","text":"<p>Initialize Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of features in the encoder/decoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads in the transformer encoder/decoder.</p> <code>8</code> <code>num_encoder_layers</code> <code>int</code> <p>The number of encoder-layers in the encoder.</p> <code>6</code> <code>num_decoder_layers</code> <code>int</code> <p>The number of decoder-layers in the decoder.</p> <code>6</code> <code>dropout</code> <code>float</code> <p>Dropout value applied to the output of transformer layers.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>return_intermediate_dec</code> <code>bool</code> <p>Return intermediate layers from decoder.</p> <code>False</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of encoder and decoder.</p> <code>False</code> <code>num_layers_attn_head</code> <code>int</code> <p>The number of layers in the attention head.</p> <code>2</code> <code>dropout_attn_head</code> <code>float</code> <p>Dropout value for the attention_head.</p> <code>0.1</code> <code>embedding_meta</code> <code>dict | None</code> <p>Metadata for positional embeddings. See below.</p> <code>None</code> <code>return_embedding</code> <code>bool</code> <p>Whether to return the positional embeddings</p> <code>False</code> <code>decoder_self_attn</code> <code>bool</code> <p>If True, use decoder self attention.</p> <code>False</code> <code>encoder_cfg</code> <code>dict | None</code> <p>Encoder configuration.</p> <p>More details on <code>embedding_meta</code>:     By default this will be an empty dict and indicate     that no positional embeddings should be used. To use the positional embeddings     pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:     {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},     \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see <code>dreem.models.embeddings.Embedding.EMB_TYPES</code>     and <code>dreem.models.embeddings.Embedding.EMB_MODES</code> for embedding parameters).</p> <code>None</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    d_model: int = 1024,\n    nhead: int = 8,\n    num_encoder_layers: int = 6,\n    num_decoder_layers: int = 6,\n    dropout: float = 0.1,\n    activation: str = \"relu\",\n    return_intermediate_dec: bool = False,\n    norm: bool = False,\n    num_layers_attn_head: int = 2,\n    dropout_attn_head: float = 0.1,\n    embedding_meta: dict | None = None,\n    return_embedding: bool = False,\n    decoder_self_attn: bool = False,\n    encoder_cfg: dict | None = None,\n) -&gt; None:\n    \"\"\"Initialize Transformer.\n\n    Args:\n        d_model: The number of features in the encoder/decoder inputs.\n        nhead: The number of heads in the transformer encoder/decoder.\n        num_encoder_layers: The number of encoder-layers in the encoder.\n        num_decoder_layers: The number of decoder-layers in the decoder.\n        dropout: Dropout value applied to the output of transformer layers.\n        activation: Activation function to use.\n        return_intermediate_dec: Return intermediate layers from decoder.\n        norm: If True, normalize output of encoder and decoder.\n        num_layers_attn_head: The number of layers in the attention head.\n        dropout_attn_head: Dropout value for the attention_head.\n        embedding_meta: Metadata for positional embeddings. See below.\n        return_embedding: Whether to return the positional embeddings\n        decoder_self_attn: If True, use decoder self attention.\n        encoder_cfg: Encoder configuration.\n\n            More details on `embedding_meta`:\n                By default this will be an empty dict and indicate\n                that no positional embeddings should be used. To use the positional embeddings\n                pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},\n                \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n    \"\"\"\n    super().__init__()\n\n    self.d_model = dim_feedforward = feature_dim_attn_head = d_model\n\n    self.embedding_meta = embedding_meta\n    self.return_embedding = return_embedding\n    self.encoder_cfg = encoder_cfg\n\n    self.pos_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n    self.temp_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n\n    if self.embedding_meta:\n        if \"pos\" in self.embedding_meta:\n            pos_emb_cfg = self.embedding_meta[\"pos\"]\n            if pos_emb_cfg:\n                self.pos_emb = Embedding(\n                    emb_type=\"pos\", features=self.d_model, **pos_emb_cfg\n                )\n        if \"temp\" in self.embedding_meta:\n            temp_emb_cfg = self.embedding_meta[\"temp\"]\n            if temp_emb_cfg:\n                self.temp_emb = Embedding(\n                    emb_type=\"temp\", features=self.d_model, **temp_emb_cfg\n                )\n\n    self.fourier_embeddings = FourierPositionalEmbeddings(\n        n_components=8, d_model=d_model\n    )\n\n    # Transformer Encoder\n    encoder_layer = TransformerEncoderLayer(\n        d_model, nhead, dim_feedforward, dropout, activation, norm\n    )\n\n    encoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n    # only used if using descriptor visual encoder; default resnet encoder uses d_model directly\n    if self.encoder_cfg and \"encoder_type\" in self.encoder_cfg:\n        self.visual_feat_dim = (\n            self.encoder_cfg[\"ndim\"] if \"ndim\" in self.encoder_cfg else 5\n        )  # 5 is default for descriptor\n        self.fourier_proj = nn.Linear(self.d_model + self.visual_feat_dim, d_model)\n        self.fourier_norm = nn.LayerNorm(self.d_model)\n\n    self.encoder = TransformerEncoder(\n        encoder_layer, num_encoder_layers, encoder_norm\n    )\n\n    # Transformer Decoder\n    decoder_layer = TransformerDecoderLayer(\n        d_model,\n        nhead,\n        dim_feedforward,\n        dropout,\n        activation,\n        norm,\n        decoder_self_attn,\n    )\n\n    decoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n    self.decoder = TransformerDecoder(\n        decoder_layer, num_decoder_layers, return_intermediate_dec, decoder_norm\n    )\n\n    # Transformer attention head\n    self.attn_head = ATTWeightHead(\n        feature_dim=feature_dim_attn_head,\n        num_layers=num_layers_attn_head,\n        dropout=dropout_attn_head,\n    )\n\n    self._reset_parameters()\n</code></pre>"},{"location":"reference/dreem/#dreem.Transformer.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute a forward pass through the transformer and attention head.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>A list of instance objects (See <code>dreem.io.Instance</code> for more info.)</p> required <code>query_instances</code> <code>list[Instance] | None</code> <p>An set of instances to be used as decoder queries.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>asso_output</code> <code>list[AssociationMatrix]</code> <p>A list of torch.Tensors of shape (L, n_query, total_instances) where:     L: number of decoder blocks     n_query: number of instances in current query/frame     total_instances: number of instances in window</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self,\n    ref_instances: list[\"dreem.io.Instance\"],\n    query_instances: list[\"dreem.io.Instance\"] | None = None,\n) -&gt; list[AssociationMatrix]:\n    \"\"\"Execute a forward pass through the transformer and attention head.\n\n    Args:\n        ref_instances: A list of instance objects (See `dreem.io.Instance` for more info.)\n        query_instances: An set of instances to be used as decoder queries.\n\n    Returns:\n        asso_output: A list of torch.Tensors of shape (L, n_query, total_instances) where:\n            L: number of decoder blocks\n            n_query: number of instances in current query/frame\n            total_instances: number of instances in window\n    \"\"\"\n    ref_features = torch.cat(\n        [instance.features for instance in ref_instances], dim=0\n    ).unsqueeze(0)\n\n    # window_length = len(frames)\n    # instances_per_frame = [frame.num_detected for frame in frames]\n    total_instances = len(ref_instances)\n    embed_dim = self.d_model\n    # print(f'T: {window_length}; N: {total_instances}; N_t: {instances_per_frame} n_reid: {reid_features.shape}')\n    ref_boxes = get_boxes(ref_instances)  # total_instances, 4\n    ref_boxes = torch.nan_to_num(ref_boxes, -1.0)\n    ref_times, query_times = get_times(ref_instances, query_instances)\n\n    # window_length = len(ref_times.unique())  # Currently unused but may be useful for debugging\n\n    ref_temp_emb = self.temp_emb(ref_times)\n\n    ref_pos_emb = self.pos_emb(ref_boxes)\n\n    if self.return_embedding:\n        for i, instance in enumerate(ref_instances):\n            instance.add_embedding(\"pos\", ref_pos_emb[i])\n            instance.add_embedding(\"temp\", ref_temp_emb[i])\n\n    ref_emb = (ref_pos_emb + ref_temp_emb) / 2.0\n\n    ref_emb = ref_emb.view(1, total_instances, embed_dim)\n\n    ref_emb = ref_emb.permute(1, 0, 2)  # (total_instances, batch_size, embed_dim)\n\n    batch_size, total_instances = ref_features.shape[:-1]\n\n    ref_features = ref_features.permute(\n        1, 0, 2\n    )  # (total_instances, batch_size, embed_dim)\n\n    encoder_queries = ref_features\n\n    # apply fourier embeddings if using fourier rope, OR if using descriptor (compact) visual encoder\n    if (\n        self.embedding_meta\n        and \"use_fourier\" in self.embedding_meta\n        and self.embedding_meta[\"use_fourier\"]\n    ) or (\n        self.encoder_cfg\n        and \"encoder_type\" in self.encoder_cfg\n        and self.encoder_cfg[\"encoder_type\"] == \"descriptor\"\n    ):\n        encoder_queries = apply_fourier_embeddings(\n            encoder_queries,\n            ref_times,\n            self.d_model,\n            self.fourier_embeddings,\n            self.fourier_proj,\n            self.fourier_norm,\n        )\n\n    encoder_features = self.encoder(\n        encoder_queries, pos_emb=ref_emb\n    )  # (total_instances, batch_size, embed_dim)\n\n    n_query = total_instances\n\n    query_features = ref_features\n    query_pos_emb = ref_pos_emb\n    query_temp_emb = ref_temp_emb\n    query_emb = ref_emb\n\n    if query_instances is not None:\n        n_query = len(query_instances)\n\n        query_features = torch.cat(\n            [instance.features for instance in query_instances], dim=0\n        ).unsqueeze(0)\n\n        query_features = query_features.permute(\n            1, 0, 2\n        )  # (n_query, batch_size, embed_dim)\n\n        query_boxes = get_boxes(query_instances)\n        query_boxes = torch.nan_to_num(query_boxes, -1.0)\n        query_temp_emb = self.temp_emb(query_times)\n\n        query_pos_emb = self.pos_emb(query_boxes)\n\n        query_emb = (query_pos_emb + query_temp_emb) / 2.0\n        query_emb = query_emb.view(1, n_query, embed_dim)\n        query_emb = query_emb.permute(1, 0, 2)  # (n_query, batch_size, embed_dim)\n\n    else:\n        query_instances = ref_instances\n        query_times = ref_times\n\n    if self.return_embedding:\n        for i, instance in enumerate(query_instances):\n            instance.add_embedding(\"pos\", query_pos_emb[i])\n            instance.add_embedding(\"temp\", query_temp_emb[i])\n\n    # apply fourier embeddings if using fourier rope, OR if using descriptor (compact) visual encoder\n    if (\n        self.embedding_meta\n        and \"use_fourier\" in self.embedding_meta\n        and self.embedding_meta[\"use_fourier\"]\n    ) or (\n        self.encoder_cfg\n        and \"encoder_type\" in self.encoder_cfg\n        and self.encoder_cfg[\"encoder_type\"] == \"descriptor\"\n    ):\n        query_features = apply_fourier_embeddings(\n            query_features,\n            query_times,\n            self.d_model,\n            self.fourier_embeddings,\n            self.fourier_proj,\n            self.fourier_norm,\n        )\n\n    decoder_features = self.decoder(\n        query_features,\n        encoder_features,\n        ref_pos_emb=ref_emb,\n        query_pos_emb=query_emb,\n    )  # (L, n_query, batch_size, embed_dim)\n\n    decoder_features = decoder_features.transpose(\n        1, 2\n    )  # # (L, batch_size, n_query, embed_dim)\n    encoder_features = encoder_features.permute(1, 0, 2).view(\n        batch_size, total_instances, embed_dim\n    )  # (batch_size, total_instances, embed_dim)\n\n    asso_output = []\n    for frame_features in decoder_features:\n        asso_matrix = self.attn_head(frame_features, encoder_features).view(\n            n_query, total_instances\n        )\n        asso_matrix = AssociationMatrix(asso_matrix, ref_instances, query_instances)\n\n        asso_output.append(asso_matrix)\n\n    # (L=1, n_query, total_instances)\n    return asso_output\n</code></pre>"},{"location":"reference/dreem/#dreem.VisualEncoder","title":"<code>VisualEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Class wrapping around a visual feature extractor backbone.</p> <p>Currently CNN only.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize Visual Encoder.</p> <code>encoder_dim</code> <p>Compute dummy forward pass of encoder model and get embedding dimension.</p> <code>forward</code> <p>Forward pass of feature extractor to get feature vector.</p> <code>select_feature_extractor</code> <p>Select the appropriate feature extractor based on config.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>class VisualEncoder(torch.nn.Module):\n    \"\"\"Class wrapping around a visual feature extractor backbone.\n\n    Currently CNN only.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"resnet18\",\n        d_model: int = 512,\n        in_chans: int = 3,\n        backend: int = \"timm\",\n        **kwargs: Any | None,\n    ):\n        \"\"\"Initialize Visual Encoder.\n\n        Args:\n            model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n            d_model (int): Output embedding dimension.\n            in_chans: the number of input channels of the image.\n            backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n            kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n        \"\"\"\n        super().__init__()\n\n        self.model_name = model_name.lower()\n        self.d_model = d_model\n        self.backend = backend\n        if in_chans == 1:\n            self.in_chans = 3\n        else:\n            self.in_chans = in_chans\n\n        self.feature_extractor = self.select_feature_extractor(\n            model_name=self.model_name,\n            in_chans=self.in_chans,\n            backend=self.backend,\n            **kwargs,\n        )\n\n        self.out_layer = torch.nn.Linear(\n            self.encoder_dim(self.feature_extractor), self.d_model\n        )\n\n    def select_feature_extractor(\n        self, model_name: str, in_chans: int, backend: str, **kwargs: Any\n    ) -&gt; torch.nn.Module:\n        \"\"\"Select the appropriate feature extractor based on config.\n\n        Args:\n            model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n            in_chans: the number of input channels of the image.\n            backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n            kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n\n        Returns:\n            a CNN encoder based on the config and backend selected.\n        \"\"\"\n        if \"timm\" in backend.lower():\n            feature_extractor = timm.create_model(\n                model_name=self.model_name,\n                in_chans=self.in_chans,\n                num_classes=0,\n                **kwargs,\n            )\n        elif \"torch\" in backend.lower():\n            if model_name.lower() == \"resnet18\":\n                feature_extractor = torchvision.models.resnet18(**kwargs)\n\n            elif model_name.lower() == \"resnet50\":\n                feature_extractor = torchvision.models.resnet50(**kwargs)\n\n            else:\n                raise ValueError(\n                    f\"Only `[resnet18, resnet50]` are available when backend is {backend}. Found {model_name}\"\n                )\n            feature_extractor = torch.nn.Sequential(\n                *list(feature_extractor.children())[:-1]\n            )\n            input_layer = feature_extractor[0]\n            if in_chans != 3:\n                feature_extractor[0] = torch.nn.Conv2d(\n                    in_channels=in_chans,\n                    out_channels=input_layer.out_channels,\n                    kernel_size=input_layer.kernel_size,\n                    stride=input_layer.stride,\n                    padding=input_layer.padding,\n                    dilation=input_layer.dilation,\n                    groups=input_layer.groups,\n                    bias=input_layer.bias,\n                    padding_mode=input_layer.padding_mode,\n                )\n\n        else:\n            raise ValueError(\n                f\"Only ['timm', 'torch'] backends are available! Found {backend}.\"\n            )\n        return feature_extractor\n\n    def encoder_dim(self, model: torch.nn.Module) -&gt; int:\n        \"\"\"Compute dummy forward pass of encoder model and get embedding dimension.\n\n        Args:\n            model: a vision encoder model.\n\n        Returns:\n            The embedding dimension size.\n        \"\"\"\n        _ = model.eval()\n        dummy_output = model(torch.randn(1, self.in_chans, 224, 224)).squeeze()\n        _ = model.train()  # to be safe\n        return dummy_output.shape[-1]\n\n    def forward(self, img: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of feature extractor to get feature vector.\n\n        Args:\n            img: Input image tensor of shape (B, C, H, W).\n\n        Returns:\n            feats: Normalized output tensor of shape (B, d_model).\n        \"\"\"\n        # If grayscale, tile the image to 3 channels.\n        if img.shape[1] == 1:\n            img = img.repeat([1, 3, 1, 1])  # (B, nc=3, H, W)\n\n        b, c, h, w = img.shape\n\n        if c != self.in_chans:\n            raise ValueError(\n                f\"\"\"Found {c} channels in image but model was configured for {self.in_chans} channels! \\n\n                    Hint: have you set the number of anchors in your dataset &gt; 1? \\n\n                    If so, make sure to set `in_chans=3 * n_anchors`\"\"\"\n            )\n        feats = self.feature_extractor(\n            img\n        )  # (B, out_dim, 1, 1) if using resnet18 backbone.\n\n        # Reshape feature vectors\n        feats = feats.reshape([img.shape[0], -1])  # (B, out_dim)\n        # Map feature vectors to output dimension using linear layer.\n        feats = self.out_layer(feats)  # (B, d_model)\n        # Normalize output feature vectors.\n        feats = F.normalize(feats)  # (B, d_model)\n        return feats\n</code></pre>"},{"location":"reference/dreem/#dreem.VisualEncoder.__init__","title":"<code>__init__(model_name='resnet18', d_model=512, in_chans=3, backend='timm', **kwargs)</code>","text":"<p>Initialize Visual Encoder.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").</p> <code>'resnet18'</code> <code>d_model</code> <code>int</code> <p>Output embedding dimension.</p> <code>512</code> <code>in_chans</code> <code>int</code> <p>the number of input channels of the image.</p> <code>3</code> <code>backend</code> <code>int</code> <p>Which model backend to use. One of {\"timm\", \"torchvision\"}</p> <code>'timm'</code> <code>kwargs</code> <code>Any | None</code> <p>see <code>timm.create_model</code> and <code>torchvision.models.resnetX</code> for kwargs.</p> <code>{}</code> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"resnet18\",\n    d_model: int = 512,\n    in_chans: int = 3,\n    backend: int = \"timm\",\n    **kwargs: Any | None,\n):\n    \"\"\"Initialize Visual Encoder.\n\n    Args:\n        model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n        d_model (int): Output embedding dimension.\n        in_chans: the number of input channels of the image.\n        backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n        kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n    \"\"\"\n    super().__init__()\n\n    self.model_name = model_name.lower()\n    self.d_model = d_model\n    self.backend = backend\n    if in_chans == 1:\n        self.in_chans = 3\n    else:\n        self.in_chans = in_chans\n\n    self.feature_extractor = self.select_feature_extractor(\n        model_name=self.model_name,\n        in_chans=self.in_chans,\n        backend=self.backend,\n        **kwargs,\n    )\n\n    self.out_layer = torch.nn.Linear(\n        self.encoder_dim(self.feature_extractor), self.d_model\n    )\n</code></pre>"},{"location":"reference/dreem/#dreem.VisualEncoder.encoder_dim","title":"<code>encoder_dim(model)</code>","text":"<p>Compute dummy forward pass of encoder model and get embedding dimension.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>a vision encoder model.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The embedding dimension size.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def encoder_dim(self, model: torch.nn.Module) -&gt; int:\n    \"\"\"Compute dummy forward pass of encoder model and get embedding dimension.\n\n    Args:\n        model: a vision encoder model.\n\n    Returns:\n        The embedding dimension size.\n    \"\"\"\n    _ = model.eval()\n    dummy_output = model(torch.randn(1, self.in_chans, 224, 224)).squeeze()\n    _ = model.train()  # to be safe\n    return dummy_output.shape[-1]\n</code></pre>"},{"location":"reference/dreem/#dreem.VisualEncoder.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of feature extractor to get feature vector.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor</code> <p>Input image tensor of shape (B, C, H, W).</p> required <p>Returns:</p> Name Type Description <code>feats</code> <code>Tensor</code> <p>Normalized output tensor of shape (B, d_model).</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def forward(self, img: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of feature extractor to get feature vector.\n\n    Args:\n        img: Input image tensor of shape (B, C, H, W).\n\n    Returns:\n        feats: Normalized output tensor of shape (B, d_model).\n    \"\"\"\n    # If grayscale, tile the image to 3 channels.\n    if img.shape[1] == 1:\n        img = img.repeat([1, 3, 1, 1])  # (B, nc=3, H, W)\n\n    b, c, h, w = img.shape\n\n    if c != self.in_chans:\n        raise ValueError(\n            f\"\"\"Found {c} channels in image but model was configured for {self.in_chans} channels! \\n\n                Hint: have you set the number of anchors in your dataset &gt; 1? \\n\n                If so, make sure to set `in_chans=3 * n_anchors`\"\"\"\n        )\n    feats = self.feature_extractor(\n        img\n    )  # (B, out_dim, 1, 1) if using resnet18 backbone.\n\n    # Reshape feature vectors\n    feats = feats.reshape([img.shape[0], -1])  # (B, out_dim)\n    # Map feature vectors to output dimension using linear layer.\n    feats = self.out_layer(feats)  # (B, d_model)\n    # Normalize output feature vectors.\n    feats = F.normalize(feats)  # (B, d_model)\n    return feats\n</code></pre>"},{"location":"reference/dreem/#dreem.VisualEncoder.select_feature_extractor","title":"<code>select_feature_extractor(model_name, in_chans, backend, **kwargs)</code>","text":"<p>Select the appropriate feature extractor based on config.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").</p> required <code>in_chans</code> <code>int</code> <p>the number of input channels of the image.</p> required <code>backend</code> <code>str</code> <p>Which model backend to use. One of {\"timm\", \"torchvision\"}</p> required <code>kwargs</code> <code>Any</code> <p>see <code>timm.create_model</code> and <code>torchvision.models.resnetX</code> for kwargs.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Module</code> <p>a CNN encoder based on the config and backend selected.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def select_feature_extractor(\n    self, model_name: str, in_chans: int, backend: str, **kwargs: Any\n) -&gt; torch.nn.Module:\n    \"\"\"Select the appropriate feature extractor based on config.\n\n    Args:\n        model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n        in_chans: the number of input channels of the image.\n        backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n        kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n\n    Returns:\n        a CNN encoder based on the config and backend selected.\n    \"\"\"\n    if \"timm\" in backend.lower():\n        feature_extractor = timm.create_model(\n            model_name=self.model_name,\n            in_chans=self.in_chans,\n            num_classes=0,\n            **kwargs,\n        )\n    elif \"torch\" in backend.lower():\n        if model_name.lower() == \"resnet18\":\n            feature_extractor = torchvision.models.resnet18(**kwargs)\n\n        elif model_name.lower() == \"resnet50\":\n            feature_extractor = torchvision.models.resnet50(**kwargs)\n\n        else:\n            raise ValueError(\n                f\"Only `[resnet18, resnet50]` are available when backend is {backend}. Found {model_name}\"\n            )\n        feature_extractor = torch.nn.Sequential(\n            *list(feature_extractor.children())[:-1]\n        )\n        input_layer = feature_extractor[0]\n        if in_chans != 3:\n            feature_extractor[0] = torch.nn.Conv2d(\n                in_channels=in_chans,\n                out_channels=input_layer.out_channels,\n                kernel_size=input_layer.kernel_size,\n                stride=input_layer.stride,\n                padding=input_layer.padding,\n                dilation=input_layer.dilation,\n                groups=input_layer.groups,\n                bias=input_layer.bias,\n                padding_mode=input_layer.padding_mode,\n            )\n\n    else:\n        raise ValueError(\n            f\"Only ['timm', 'torch'] backends are available! Found {backend}.\"\n        )\n    return feature_extractor\n</code></pre>"},{"location":"reference/dreem/#dreem.annotate_video","title":"<code>annotate_video(video, labels, key, color_palette=palette, trails=2, boxes=(64, 64), names=True, track_scores=0.5, centroids=4, poses=False, save_path='debug_animal.mp4', fps=30, alpha=0.2)</code>","text":"<p>Annotate video frames with labels.</p> <p>Labels video with bboxes, centroids, trajectory trails, and/or poses.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>Reader</code> <p>The video to be annotated in an ndarray</p> required <code>labels</code> <code>DataFrame</code> <p>The pandas dataframe containing the centroid and/or pose locations of the instances</p> required <code>key</code> <code>str</code> <p>The key where labels are stored in the dataframe - mostly used for choosing whether to annotate based on pred or gt labels</p> required <code>color_palette</code> <code>list | str</code> <p>The matplotlib colorpalette to use for annotating the video. Defaults to <code>tab10</code></p> <code>palette</code> <code>trails</code> <code>int</code> <p>The size of the trajectory trail. If trails size &lt;= 0 or None then it is not added</p> <code>2</code> <code>boxes</code> <code>int</code> <p>The size of the bbox. If bbox size &lt;= 0 or None then it is not added</p> <code>(64, 64)</code> <code>names</code> <code>bool</code> <p>Whether or not to annotate with name</p> <code>True</code> <code>centroids</code> <code>int</code> <p>The size of the centroid. If centroid size &lt;= 0 or None then it is not added</p> <code>4</code> <code>poses</code> <code>bool</code> <p>Whether or not to annotate with poses</p> <code>False</code> <code>fps</code> <code>int</code> <p>The frame rate of the generated video</p> <code>30</code> <code>alpha</code> <code>float</code> <p>The opacity of the annotations.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of annotated video frames</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def annotate_video(\n    video: \"imageio.core.format.Reader\",\n    labels: pd.DataFrame,\n    key: str,\n    color_palette: list | str = palette,\n    trails: int = 2,\n    boxes: int = (64, 64),\n    names: bool = True,\n    track_scores=0.5,\n    centroids: int = 4,\n    poses: bool = False,\n    save_path: str = \"debug_animal.mp4\",\n    fps: int = 30,\n    alpha: float = 0.2,\n) -&gt; list:\n    \"\"\"Annotate video frames with labels.\n\n    Labels video with bboxes, centroids, trajectory trails, and/or poses.\n\n    Args:\n        video: The video to be annotated in an ndarray\n        labels: The pandas dataframe containing the centroid and/or pose locations of the instances\n        key: The key where labels are stored in the dataframe - mostly used for choosing whether to annotate based on pred or gt labels\n        color_palette: The matplotlib colorpalette to use for annotating the video. Defaults to `tab10`\n        trails: The size of the trajectory trail. If trails size &lt;= 0 or None then it is not added\n        boxes: The size of the bbox. If bbox size &lt;= 0 or None then it is not added\n        names: Whether or not to annotate with name\n        centroids: The size of the centroid. If centroid size &lt;= 0 or None then it is not added\n        poses: Whether or not to annotate with poses\n        fps: The frame rate of the generated video\n        alpha: The opacity of the annotations.\n\n    Returns:\n        A list of annotated video frames\n    \"\"\"\n    writer = imageio.get_writer(save_path, fps=fps)\n    color_palette = (\n        sns.color_palette(color_palette)\n        if isinstance(color_palette, str)\n        else deepcopy(color_palette)\n    )\n\n    if trails:\n        track_trails = {}\n    try:\n        for i in tqdm(sorted(labels[\"Frame\"].unique()), desc=\"Frame\", unit=\"Frame\"):\n            frame = video.get_data(i)\n            if frame.shape[0] == 1 or frame.shape[-1] == 1:\n                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n            # else:\n            #     frame = frame.copy()\n\n            lf = labels[labels[\"Frame\"] == i]\n            for idx, instance in lf.iterrows():\n                if not trails:\n                    track_trails = {}\n\n                if poses:\n                    # TODO figure out best way to store poses (maybe pass a slp labels file too?)\n                    trails = False\n                    centroids = False\n                    for idx, (pose, edge) in enumerate(\n                        zip(instance[\"poses\"], instance[\"edges\"])\n                    ):\n                        pose = fill_missing(pose.numpy())\n\n                        pred_track_id = instance[key][idx].numpy().tolist()\n\n                        # Add midpt to track trail.\n                        if pred_track_id not in list(track_trails.keys()):\n                            track_trails[pred_track_id] = []\n\n                        # Select a color based on track_id.\n                        track_color_idx = pred_track_id % len(color_palette)\n                        track_color = (\n                            (np.array(color_palette[track_color_idx]) * 255)\n                            .astype(np.uint8)\n                            .tolist()[::-1]\n                        )\n\n                        for p in pose:\n                            # try:\n                            #    p = tuple([int(i) for i in p.numpy()][::-1])\n                            # except:\n                            #    continue\n\n                            p = tuple(int(i) for i in p)[::-1]\n\n                            track_trails[pred_track_id].append(p)\n\n                            frame = cv2.circle(\n                                frame, p, radius=2, color=track_color, thickness=-1\n                            )\n\n                        for e in edge:\n                            source = tuple(int(i) for i in pose[int(e[0])])[::-1]\n                            target = tuple(int(i) for i in pose[int(e[1])])[::-1]\n\n                            frame = cv2.line(frame, source, target, track_color, 1)\n\n                if (boxes) or centroids:\n                    # Get coordinates for detected objects in the current frame.\n                    if isinstance(boxes, int):\n                        boxes = (boxes, boxes)\n\n                    box_w, box_h = boxes\n                    x = instance[\"X\"]\n                    y = instance[\"Y\"]\n                    min_x, min_y, max_x, max_y = (\n                        int(x - box_w / 2),\n                        int(y - box_h / 2),\n                        int(x + box_w / 2),\n                        int(y + box_h / 2),\n                    )\n                    midpt = (int(x), int(y))\n\n                    pred_track_id = instance[key]\n\n                    if \"Track_score\" in instance.index:\n                        track_score = instance[\"Track_score\"]\n                    else:\n                        track_scores = 0\n\n                    # Add midpt to track trail.\n                    if pred_track_id not in list(track_trails.keys()):\n                        track_trails[pred_track_id] = []\n                    track_trails[pred_track_id].append(midpt)\n\n                    # Select a color based on track_id.\n                    track_color_idx = int(pred_track_id) % len(color_palette)\n                    track_color = (\n                        (np.array(color_palette[track_color_idx]) * 255)\n                        .astype(np.uint8)\n                        .tolist()[::-1]\n                    )\n\n                    # Bbox.\n                    if boxes is not None:\n                        frame = cv2.rectangle(\n                            frame,\n                            (min_x, min_y),\n                            (max_x, max_y),\n                            color=track_color,\n                            thickness=2,\n                        )\n\n                    # Track trail.\n                    if centroids:\n                        frame = cv2.circle(\n                            frame,\n                            midpt,\n                            radius=centroids,\n                            color=track_color,\n                            thickness=-1,\n                        )\n                        for i in range(0, len(track_trails[pred_track_id]) - 1):\n                            frame = cv2.addWeighted(\n                                cv2.circle(\n                                    frame,  # .copy(),\n                                    track_trails[pred_track_id][i],\n                                    radius=4,\n                                    color=track_color,\n                                    thickness=-1,\n                                ),\n                                alpha,\n                                frame,\n                                1 - alpha,\n                                0,\n                            )\n                            if trails:\n                                frame = cv2.line(\n                                    frame,\n                                    track_trails[pred_track_id][i],\n                                    track_trails[pred_track_id][i + 1],\n                                    color=track_color,\n                                    thickness=trails,\n                                )\n\n                # Track name.\n                name_str = \"\"\n\n                if names:\n                    name_str += f\"track_{pred_track_id}\"\n                if names and track_scores:\n                    name_str += \" | \"\n                if track_scores:\n                    name_str += f\"score: {track_score:0.3f}\"\n\n                if len(name_str) &gt; 0:\n                    frame = cv2.putText(\n                        frame,\n                        # f\"idx:{idx} | track_{pred_track_id}\",\n                        name_str,\n                        org=(int(min_x), max(0, int(min_y) - 10)),\n                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n                        fontScale=0.9,\n                        color=track_color,\n                        thickness=2,\n                    )\n            writer.append_data(frame)\n            # if i % fps == 0:\n            #     gc.collect()\n\n    except Exception as e:\n        writer.close()\n        logger.exception(e)\n        return False\n\n    writer.close()\n    return True\n</code></pre>"},{"location":"reference/dreem/#dreem.setup_logging","title":"<code>setup_logging()</code>","text":"<p>Setup logging based on <code>logging.yaml</code>.</p> Source code in <code>dreem/__init__.py</code> <pre><code>def setup_logging():\n    \"\"\"Setup logging based on `logging.yaml`.\"\"\"\n    import logging\n    import logging.config\n    import os\n\n    import yaml\n\n    package_directory = os.path.dirname(os.path.abspath(__file__))\n\n    with open(os.path.join(package_directory, \"..\", \"logging.yaml\"), \"r\") as stream:\n        logging_cfg = yaml.load(stream, Loader=yaml.FullLoader)\n\n    logging.config.dictConfig(logging_cfg)\n</code></pre>"},{"location":"reference/dreem/cli/","title":"cli","text":""},{"location":"reference/dreem/cli/#dreem.cli","title":"<code>dreem.cli</code>","text":"<p>This module contains the command line interfaces for the dreem package.</p>"},{"location":"reference/dreem/version/","title":"version","text":""},{"location":"reference/dreem/version/#dreem.version","title":"<code>dreem.version</code>","text":"<p>Central location for version information.</p>"},{"location":"reference/dreem/datasets/","title":"datasets","text":""},{"location":"reference/dreem/datasets/#dreem.datasets","title":"<code>dreem.datasets</code>","text":"<p>Data loading and preprocessing.</p> <p>Modules:</p> Name Description <code>base_dataset</code> <p>Module containing logic for loading datasets.</p> <code>cell_tracking_dataset</code> <p>Module containing cell tracking challenge dataset.</p> <code>data_utils</code> <p>Module containing helper functions for datasets.</p> <code>microscopy_dataset</code> <p>Module containing microscopy dataset.</p> <code>sleap_dataset</code> <p>Module containing logic for loading sleap datasets.</p> <code>tracking_dataset</code> <p>Module containing Lightning module wrapper around all other datasets.</p> <p>Classes:</p> Name Description <code>BaseDataset</code> <p>Base Dataset for microscopy and sleap datasets to override.</p> <code>CellTrackingDataset</code> <p>Dataset for loading cell tracking challenge data.</p> <code>MicroscopyDataset</code> <p>Dataset for loading Microscopy Data.</p> <code>SleapDataset</code> <p>Dataset for loading animal behavior data from sleap.</p> <code>TrackingDataset</code> <p>Lightning dataset used to load dataloaders for train, test and validation.</p>"},{"location":"reference/dreem/datasets/#dreem.datasets.BaseDataset","title":"<code>BaseDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Base Dataset for microscopy and sleap datasets to override.</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Get an element of the dataset.</p> <code>__init__</code> <p>Initialize Dataset.</p> <code>__len__</code> <p>Get the size of the dataset.</p> <code>create_chunks_other</code> <p>Legacy chunking logic. Does not support unannotated segments.</p> <code>create_chunks_slp</code> <p>Get indexing for data.</p> <code>get_indices</code> <p>Retrieve label and frame indices given batch index.</p> <code>get_instances</code> <p>Build chunk of frames.</p> <code>no_batching_fn</code> <p>Collate function used to overwrite dataloader batching function.</p> <code>process_segments</code> <p>Process segments to stitch.</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>class BaseDataset(Dataset):\n    \"\"\"Base Dataset for microscopy and sleap datasets to override.\"\"\"\n\n    def __init__(\n        self,\n        label_files: list[str],\n        vid_files: list[str],\n        padding: int,\n        crop_size: Union[int, list[int]],\n        chunk: bool,\n        clip_length: int,\n        mode: str,\n        augmentations: dict | None = None,\n        n_chunks: int | float = 1.0,\n        seed: int | None = None,\n        gt_list: str | None = None,\n    ):\n        \"\"\"Initialize Dataset.\n\n        Args:\n            label_files: a list of paths to label files. Should at least contain\n                detections for inference, detections + tracks for training.\n            vid_files: list of paths to video files.\n            padding: amount of padding around object crops\n            crop_size: the size of the object crops\n            chunk: whether or not to chunk the dataset into batches\n            clip_length: the number of frames in each chunk\n            mode: `train` or `val`. Determines whether this dataset is used for\n                training or validation. Currently doesn't affect dataset logic\n            augmentations: An optional dict mapping augmentations to parameters.\n                See subclasses for details.\n            n_chunks: Number of chunks to subsample from.\n                Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n            seed: set a seed for reproducibility\n            gt_list: An optional path to .txt file containing ground truth for\n                cell tracking challenge datasets.\n        \"\"\"\n        self.vid_files = vid_files\n        self.label_files = label_files\n        self.padding = padding\n        self.crop_size = crop_size\n        self.chunk = chunk\n        self.clip_length = clip_length\n        self.mode = mode\n        self.n_chunks = n_chunks\n        self.seed = seed\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n\n        if augmentations and self.mode == \"train\":\n            self.instance_dropout = augmentations.pop(\n                \"InstanceDropout\", {\"p\": 0.0, \"n\": 0}\n            )\n            self.node_dropout = data_utils.NodeDropout(\n                **augmentations.pop(\"NodeDropout\", {\"p\": 0.0, \"n\": 0})\n            )\n            self.augmentations = data_utils.build_augmentations(augmentations)\n        else:\n            self.instance_dropout = {\"p\": 0.0, \"n\": 0}\n            self.node_dropout = data_utils.NodeDropout(p=0.0, n=0)\n            self.augmentations = None\n\n        # Initialize in subclasses\n        self.frame_idx = None\n        self.labels = None\n        self.gt_list = None\n\n    def process_segments(\n        self, i: int, segments_to_stitch: list[torch.Tensor], clip_length: int\n    ) -&gt; None:\n        \"\"\"Process segments to stitch.\n\n        Modifies state variables chunked_frame_idx and label_idx.\n\n        Args:\n            segments_to_stitch: list of segments to stitch\n            i: index of the video\n            clip_length: the number of frames in each chunk\n        Returns: None\n        \"\"\"\n        stitched_segment = torch.cat(segments_to_stitch)\n        frame_idx_split = torch.split(stitched_segment, clip_length)\n        self.chunked_frame_idx.extend(frame_idx_split)\n        self.label_idx.extend(len(frame_idx_split) * [i])\n\n    def create_chunks_slp(self) -&gt; None:\n        \"\"\"Get indexing for data.\n\n        Creates both indexes for selecting dataset (label_idx) and frame in\n        dataset (chunked_frame_idx). If chunking is false, we index directly\n        using the frame ids. Setting chunking to true creates a list of lists\n        containing chunk frames for indexing. This is useful for computational\n        efficiency and data shuffling. To be called by subclass __init__()\n        \"\"\"\n        self.chunked_frame_idx, self.label_idx = [], []\n        # go through each slp file and create chunks that respect max_batching_gap\n        for i, slp_file in enumerate(self.label_files):\n            annotated_segments = self.annotated_segments[slp_file]\n            segments_to_stitch = []\n            prev_end = annotated_segments[0][1]  # end of first segment\n            for start, end in annotated_segments:\n                # check if the start of current segment is within\n                # batching_max_gap of end of previous\n                if (\n                    (int(start) - int(prev_end) &lt; self.max_batching_gap)\n                    or not self.chunk\n                ):  # also takes care of first segment as start &lt; prev_end\n                    segments_to_stitch.append(torch.arange(start, end + 1))\n                    prev_end = end\n                else:\n                    # stitch previous set of segments before creating a new chunk\n                    self.process_segments(i, segments_to_stitch, self.clip_length)\n                    # reset segments_to_stitch as we are starting a new chunk\n                    segments_to_stitch = [torch.arange(start, end + 1)]\n                    prev_end = end\n\n            if not self.chunk:\n                self.process_segments(\n                    i, segments_to_stitch, self.labels[i].video.shape[0]\n                )\n            else:\n                # add last chunk after the loop\n                if segments_to_stitch:\n                    self.process_segments(i, segments_to_stitch, self.clip_length)\n\n        if self.n_chunks &gt; 0 and self.n_chunks &lt;= 1.0:\n            n_chunks = int(self.n_chunks * len(self.chunked_frame_idx))\n\n        elif self.n_chunks &lt;= len(self.chunked_frame_idx):\n            n_chunks = int(self.n_chunks)\n\n        else:\n            n_chunks = len(self.chunked_frame_idx)\n\n        if n_chunks &gt; 0 and n_chunks &lt; len(self.chunked_frame_idx):\n            sample_idx = np.random.choice(\n                np.arange(len(self.chunked_frame_idx)), n_chunks, replace=False\n            )\n\n            self.chunked_frame_idx = [self.chunked_frame_idx[i] for i in sample_idx]\n\n            self.label_idx = [self.label_idx[i] for i in sample_idx]\n\n        # workaround for empty batch bug (needs to be changed).\n        # Check for batch with with only 1/10 size of clip length.\n        # Arbitrary thresholds\n        remove_idx = []\n        for i, frame_chunk in enumerate(self.chunked_frame_idx):\n            if (\n                len(frame_chunk) &lt;= min(int(self.clip_length / 10), 5)\n                # and frame_chunk[-1] % self.clip_length == 0\n            ):\n                logger.warning(\n                    f\"Warning: Batch containing frames {frame_chunk} from video \"\n                    f\"{self.vid_files[self.label_idx[i]]} has {len(frame_chunk)} frames. \"\n                    f\"Removing to avoid empty batch possibility with failed frame loading\"\n                )\n                remove_idx.append(i)\n        if len(remove_idx) &gt; 0:\n            for i in sorted(remove_idx, reverse=True):\n                self.chunked_frame_idx.pop(i)\n                self.label_idx.pop(i)\n\n    def create_chunks_other(self) -&gt; None:\n        \"\"\"Legacy chunking logic. Does not support unannotated segments.\n\n        Creates both indexes for selecting dataset (label_idx) and frame in\n        dataset (chunked_frame_idx). If chunking is false, we index directly\n        using the frame ids. Setting chunking to true creates a list of lists\n        containing chunk frames for indexing. This is useful for computational\n        efficiency and data shuffling. To be called by subclass __init__()\n        \"\"\"\n        if self.chunk:\n            self.chunked_frame_idx, self.label_idx = [], []\n            for i, frame_idx in enumerate(self.frame_idx):\n                frame_idx_split = torch.split(frame_idx, self.clip_length)\n                self.chunked_frame_idx.extend(frame_idx_split)\n                self.label_idx.extend(len(frame_idx_split) * [i])\n\n            if self.n_chunks &gt; 0 and self.n_chunks &lt;= 1.0:\n                n_chunks = int(self.n_chunks * len(self.chunked_frame_idx))\n\n            elif self.n_chunks &lt;= len(self.chunked_frame_idx):\n                n_chunks = int(self.n_chunks)\n\n            else:\n                n_chunks = len(self.chunked_frame_idx)\n\n            if n_chunks &gt; 0 and n_chunks &lt; len(self.chunked_frame_idx):\n                sample_idx = np.random.choice(\n                    np.arange(len(self.chunked_frame_idx)), n_chunks, replace=False\n                )\n\n                self.chunked_frame_idx = [self.chunked_frame_idx[i] for i in sample_idx]\n\n                self.label_idx = [self.label_idx[i] for i in sample_idx]\n\n            # workaround for empty batch bug (needs to be changed).\n            # Check for batch with with only 1/10 size of clip length.\n            # Arbitrary thresholds\n            remove_idx = []\n            for i, frame_chunk in enumerate(self.chunked_frame_idx):\n                if (\n                    len(frame_chunk) &lt;= min(int(self.clip_length / 10), 5)\n                    # and frame_chunk[-1] % self.clip_length == 0\n                ):\n                    logger.warning(\n                        f\"Warning: Batch containing frames {frame_chunk} from video {self.vid_files[self.label_idx[i]]} has {len(frame_chunk)} frames. Removing to avoid empty batch possibility with failed frame loading\"\n                    )\n                    remove_idx.append(i)\n            if len(remove_idx) &gt; 0:\n                for i in sorted(remove_idx, reverse=True):\n                    self.chunked_frame_idx.pop(i)\n                    self.label_idx.pop(i)\n\n        else:\n            self.chunked_frame_idx = self.frame_idx\n            self.label_idx = [i for i in range(len(self.labels))]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the size of the dataset.\n\n        Returns:\n            the size or the number of chunks in the dataset\n        \"\"\"\n        return len(self.chunked_frame_idx)\n\n    def no_batching_fn(self, batch: list[Frame]) -&gt; list[Frame]:\n        \"\"\"Collate function used to overwrite dataloader batching function.\n\n        Args:\n            batch: the chunk of frames to be returned\n\n        Returns:\n            The batch\n        \"\"\"\n        return batch\n\n    def __getitem__(self, idx: int) -&gt; list[Frame]:\n        \"\"\"Get an element of the dataset.\n\n        Args:\n            idx: the index of the batch. Note this is not the index of the video\n                or the frame.\n\n        Returns:\n            A list of `Frame`s in the chunk containing the metadata + instance features.\n        \"\"\"\n        label_idx, frame_idx = self.get_indices(idx)\n\n        return self.get_instances(label_idx, frame_idx)\n\n    def get_indices(self, idx: int):\n        \"\"\"Retrieve label and frame indices given batch index.\n\n        This method should be implemented in any subclass of the BaseDataset.\n\n        Args:\n            idx: the index of the batch.\n\n        Raises:\n            NotImplementedError: If this method is not overridden in a subclass.\n        \"\"\"\n        raise NotImplementedError(\"Must be implemented in subclass\")\n\n    def get_instances(self, label_idx: list[int], frame_idx: list[int]):\n        \"\"\"Build chunk of frames.\n\n        This method should be implemented in any subclass of the BaseDataset.\n\n        Args:\n            label_idx: The index of the labels.\n            frame_idx: The index of the frames.\n\n        Raises:\n            NotImplementedError: If this method is not overridden in a subclass.\n        \"\"\"\n        raise NotImplementedError(\"Must be implemented in subclass\")\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.BaseDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get an element of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the batch. Note this is not the index of the video or the frame.</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A list of <code>Frame</code>s in the chunk containing the metadata + instance features.</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; list[Frame]:\n    \"\"\"Get an element of the dataset.\n\n    Args:\n        idx: the index of the batch. Note this is not the index of the video\n            or the frame.\n\n    Returns:\n        A list of `Frame`s in the chunk containing the metadata + instance features.\n    \"\"\"\n    label_idx, frame_idx = self.get_indices(idx)\n\n    return self.get_instances(label_idx, frame_idx)\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.BaseDataset.__init__","title":"<code>__init__(label_files, vid_files, padding, crop_size, chunk, clip_length, mode, augmentations=None, n_chunks=1.0, seed=None, gt_list=None)</code>","text":"<p>Initialize Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>label_files</code> <code>list[str]</code> <p>a list of paths to label files. Should at least contain detections for inference, detections + tracks for training.</p> required <code>vid_files</code> <code>list[str]</code> <p>list of paths to video files.</p> required <code>padding</code> <code>int</code> <p>amount of padding around object crops</p> required <code>crop_size</code> <code>Union[int, list[int]]</code> <p>the size of the object crops</p> required <code>chunk</code> <code>bool</code> <p>whether or not to chunk the dataset into batches</p> required <code>clip_length</code> <code>int</code> <p>the number of frames in each chunk</p> required <code>mode</code> <code>str</code> <p><code>train</code> or <code>val</code>. Determines whether this dataset is used for training or validation. Currently doesn't affect dataset logic</p> required <code>augmentations</code> <code>dict | None</code> <p>An optional dict mapping augmentations to parameters. See subclasses for details.</p> <code>None</code> <code>n_chunks</code> <code>int | float</code> <p>Number of chunks to subsample from. Can either a fraction of the dataset (ie (0,1.0]) or number of chunks</p> <code>1.0</code> <code>seed</code> <code>int | None</code> <p>set a seed for reproducibility</p> <code>None</code> <code>gt_list</code> <code>str | None</code> <p>An optional path to .txt file containing ground truth for cell tracking challenge datasets.</p> <code>None</code> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def __init__(\n    self,\n    label_files: list[str],\n    vid_files: list[str],\n    padding: int,\n    crop_size: Union[int, list[int]],\n    chunk: bool,\n    clip_length: int,\n    mode: str,\n    augmentations: dict | None = None,\n    n_chunks: int | float = 1.0,\n    seed: int | None = None,\n    gt_list: str | None = None,\n):\n    \"\"\"Initialize Dataset.\n\n    Args:\n        label_files: a list of paths to label files. Should at least contain\n            detections for inference, detections + tracks for training.\n        vid_files: list of paths to video files.\n        padding: amount of padding around object crops\n        crop_size: the size of the object crops\n        chunk: whether or not to chunk the dataset into batches\n        clip_length: the number of frames in each chunk\n        mode: `train` or `val`. Determines whether this dataset is used for\n            training or validation. Currently doesn't affect dataset logic\n        augmentations: An optional dict mapping augmentations to parameters.\n            See subclasses for details.\n        n_chunks: Number of chunks to subsample from.\n            Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n        seed: set a seed for reproducibility\n        gt_list: An optional path to .txt file containing ground truth for\n            cell tracking challenge datasets.\n    \"\"\"\n    self.vid_files = vid_files\n    self.label_files = label_files\n    self.padding = padding\n    self.crop_size = crop_size\n    self.chunk = chunk\n    self.clip_length = clip_length\n    self.mode = mode\n    self.n_chunks = n_chunks\n    self.seed = seed\n\n    if self.seed is not None:\n        np.random.seed(self.seed)\n\n    if augmentations and self.mode == \"train\":\n        self.instance_dropout = augmentations.pop(\n            \"InstanceDropout\", {\"p\": 0.0, \"n\": 0}\n        )\n        self.node_dropout = data_utils.NodeDropout(\n            **augmentations.pop(\"NodeDropout\", {\"p\": 0.0, \"n\": 0})\n        )\n        self.augmentations = data_utils.build_augmentations(augmentations)\n    else:\n        self.instance_dropout = {\"p\": 0.0, \"n\": 0}\n        self.node_dropout = data_utils.NodeDropout(p=0.0, n=0)\n        self.augmentations = None\n\n    # Initialize in subclasses\n    self.frame_idx = None\n    self.labels = None\n    self.gt_list = None\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.BaseDataset.__len__","title":"<code>__len__()</code>","text":"<p>Get the size of the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>the size or the number of chunks in the dataset</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the size of the dataset.\n\n    Returns:\n        the size or the number of chunks in the dataset\n    \"\"\"\n    return len(self.chunked_frame_idx)\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.BaseDataset.create_chunks_other","title":"<code>create_chunks_other()</code>","text":"<p>Legacy chunking logic. Does not support unannotated segments.</p> <p>Creates both indexes for selecting dataset (label_idx) and frame in dataset (chunked_frame_idx). If chunking is false, we index directly using the frame ids. Setting chunking to true creates a list of lists containing chunk frames for indexing. This is useful for computational efficiency and data shuffling. To be called by subclass init()</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def create_chunks_other(self) -&gt; None:\n    \"\"\"Legacy chunking logic. Does not support unannotated segments.\n\n    Creates both indexes for selecting dataset (label_idx) and frame in\n    dataset (chunked_frame_idx). If chunking is false, we index directly\n    using the frame ids. Setting chunking to true creates a list of lists\n    containing chunk frames for indexing. This is useful for computational\n    efficiency and data shuffling. To be called by subclass __init__()\n    \"\"\"\n    if self.chunk:\n        self.chunked_frame_idx, self.label_idx = [], []\n        for i, frame_idx in enumerate(self.frame_idx):\n            frame_idx_split = torch.split(frame_idx, self.clip_length)\n            self.chunked_frame_idx.extend(frame_idx_split)\n            self.label_idx.extend(len(frame_idx_split) * [i])\n\n        if self.n_chunks &gt; 0 and self.n_chunks &lt;= 1.0:\n            n_chunks = int(self.n_chunks * len(self.chunked_frame_idx))\n\n        elif self.n_chunks &lt;= len(self.chunked_frame_idx):\n            n_chunks = int(self.n_chunks)\n\n        else:\n            n_chunks = len(self.chunked_frame_idx)\n\n        if n_chunks &gt; 0 and n_chunks &lt; len(self.chunked_frame_idx):\n            sample_idx = np.random.choice(\n                np.arange(len(self.chunked_frame_idx)), n_chunks, replace=False\n            )\n\n            self.chunked_frame_idx = [self.chunked_frame_idx[i] for i in sample_idx]\n\n            self.label_idx = [self.label_idx[i] for i in sample_idx]\n\n        # workaround for empty batch bug (needs to be changed).\n        # Check for batch with with only 1/10 size of clip length.\n        # Arbitrary thresholds\n        remove_idx = []\n        for i, frame_chunk in enumerate(self.chunked_frame_idx):\n            if (\n                len(frame_chunk) &lt;= min(int(self.clip_length / 10), 5)\n                # and frame_chunk[-1] % self.clip_length == 0\n            ):\n                logger.warning(\n                    f\"Warning: Batch containing frames {frame_chunk} from video {self.vid_files[self.label_idx[i]]} has {len(frame_chunk)} frames. Removing to avoid empty batch possibility with failed frame loading\"\n                )\n                remove_idx.append(i)\n        if len(remove_idx) &gt; 0:\n            for i in sorted(remove_idx, reverse=True):\n                self.chunked_frame_idx.pop(i)\n                self.label_idx.pop(i)\n\n    else:\n        self.chunked_frame_idx = self.frame_idx\n        self.label_idx = [i for i in range(len(self.labels))]\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.BaseDataset.create_chunks_slp","title":"<code>create_chunks_slp()</code>","text":"<p>Get indexing for data.</p> <p>Creates both indexes for selecting dataset (label_idx) and frame in dataset (chunked_frame_idx). If chunking is false, we index directly using the frame ids. Setting chunking to true creates a list of lists containing chunk frames for indexing. This is useful for computational efficiency and data shuffling. To be called by subclass init()</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def create_chunks_slp(self) -&gt; None:\n    \"\"\"Get indexing for data.\n\n    Creates both indexes for selecting dataset (label_idx) and frame in\n    dataset (chunked_frame_idx). If chunking is false, we index directly\n    using the frame ids. Setting chunking to true creates a list of lists\n    containing chunk frames for indexing. This is useful for computational\n    efficiency and data shuffling. To be called by subclass __init__()\n    \"\"\"\n    self.chunked_frame_idx, self.label_idx = [], []\n    # go through each slp file and create chunks that respect max_batching_gap\n    for i, slp_file in enumerate(self.label_files):\n        annotated_segments = self.annotated_segments[slp_file]\n        segments_to_stitch = []\n        prev_end = annotated_segments[0][1]  # end of first segment\n        for start, end in annotated_segments:\n            # check if the start of current segment is within\n            # batching_max_gap of end of previous\n            if (\n                (int(start) - int(prev_end) &lt; self.max_batching_gap)\n                or not self.chunk\n            ):  # also takes care of first segment as start &lt; prev_end\n                segments_to_stitch.append(torch.arange(start, end + 1))\n                prev_end = end\n            else:\n                # stitch previous set of segments before creating a new chunk\n                self.process_segments(i, segments_to_stitch, self.clip_length)\n                # reset segments_to_stitch as we are starting a new chunk\n                segments_to_stitch = [torch.arange(start, end + 1)]\n                prev_end = end\n\n        if not self.chunk:\n            self.process_segments(\n                i, segments_to_stitch, self.labels[i].video.shape[0]\n            )\n        else:\n            # add last chunk after the loop\n            if segments_to_stitch:\n                self.process_segments(i, segments_to_stitch, self.clip_length)\n\n    if self.n_chunks &gt; 0 and self.n_chunks &lt;= 1.0:\n        n_chunks = int(self.n_chunks * len(self.chunked_frame_idx))\n\n    elif self.n_chunks &lt;= len(self.chunked_frame_idx):\n        n_chunks = int(self.n_chunks)\n\n    else:\n        n_chunks = len(self.chunked_frame_idx)\n\n    if n_chunks &gt; 0 and n_chunks &lt; len(self.chunked_frame_idx):\n        sample_idx = np.random.choice(\n            np.arange(len(self.chunked_frame_idx)), n_chunks, replace=False\n        )\n\n        self.chunked_frame_idx = [self.chunked_frame_idx[i] for i in sample_idx]\n\n        self.label_idx = [self.label_idx[i] for i in sample_idx]\n\n    # workaround for empty batch bug (needs to be changed).\n    # Check for batch with with only 1/10 size of clip length.\n    # Arbitrary thresholds\n    remove_idx = []\n    for i, frame_chunk in enumerate(self.chunked_frame_idx):\n        if (\n            len(frame_chunk) &lt;= min(int(self.clip_length / 10), 5)\n            # and frame_chunk[-1] % self.clip_length == 0\n        ):\n            logger.warning(\n                f\"Warning: Batch containing frames {frame_chunk} from video \"\n                f\"{self.vid_files[self.label_idx[i]]} has {len(frame_chunk)} frames. \"\n                f\"Removing to avoid empty batch possibility with failed frame loading\"\n            )\n            remove_idx.append(i)\n    if len(remove_idx) &gt; 0:\n        for i in sorted(remove_idx, reverse=True):\n            self.chunked_frame_idx.pop(i)\n            self.label_idx.pop(i)\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.BaseDataset.get_indices","title":"<code>get_indices(idx)</code>","text":"<p>Retrieve label and frame indices given batch index.</p> <p>This method should be implemented in any subclass of the BaseDataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the batch.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If this method is not overridden in a subclass.</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def get_indices(self, idx: int):\n    \"\"\"Retrieve label and frame indices given batch index.\n\n    This method should be implemented in any subclass of the BaseDataset.\n\n    Args:\n        idx: the index of the batch.\n\n    Raises:\n        NotImplementedError: If this method is not overridden in a subclass.\n    \"\"\"\n    raise NotImplementedError(\"Must be implemented in subclass\")\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.BaseDataset.get_instances","title":"<code>get_instances(label_idx, frame_idx)</code>","text":"<p>Build chunk of frames.</p> <p>This method should be implemented in any subclass of the BaseDataset.</p> <p>Parameters:</p> Name Type Description Default <code>label_idx</code> <code>list[int]</code> <p>The index of the labels.</p> required <code>frame_idx</code> <code>list[int]</code> <p>The index of the frames.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If this method is not overridden in a subclass.</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def get_instances(self, label_idx: list[int], frame_idx: list[int]):\n    \"\"\"Build chunk of frames.\n\n    This method should be implemented in any subclass of the BaseDataset.\n\n    Args:\n        label_idx: The index of the labels.\n        frame_idx: The index of the frames.\n\n    Raises:\n        NotImplementedError: If this method is not overridden in a subclass.\n    \"\"\"\n    raise NotImplementedError(\"Must be implemented in subclass\")\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.BaseDataset.no_batching_fn","title":"<code>no_batching_fn(batch)</code>","text":"<p>Collate function used to overwrite dataloader batching function.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[Frame]</code> <p>the chunk of frames to be returned</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>The batch</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def no_batching_fn(self, batch: list[Frame]) -&gt; list[Frame]:\n    \"\"\"Collate function used to overwrite dataloader batching function.\n\n    Args:\n        batch: the chunk of frames to be returned\n\n    Returns:\n        The batch\n    \"\"\"\n    return batch\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.BaseDataset.process_segments","title":"<code>process_segments(i, segments_to_stitch, clip_length)</code>","text":"<p>Process segments to stitch.</p> <p>Modifies state variables chunked_frame_idx and label_idx.</p> <p>Parameters:</p> Name Type Description Default <code>segments_to_stitch</code> <code>list[Tensor]</code> <p>list of segments to stitch</p> required <code>i</code> <code>int</code> <p>index of the video</p> required <code>clip_length</code> <code>int</code> <p>the number of frames in each chunk</p> required <p>Returns: None</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def process_segments(\n    self, i: int, segments_to_stitch: list[torch.Tensor], clip_length: int\n) -&gt; None:\n    \"\"\"Process segments to stitch.\n\n    Modifies state variables chunked_frame_idx and label_idx.\n\n    Args:\n        segments_to_stitch: list of segments to stitch\n        i: index of the video\n        clip_length: the number of frames in each chunk\n    Returns: None\n    \"\"\"\n    stitched_segment = torch.cat(segments_to_stitch)\n    frame_idx_split = torch.split(stitched_segment, clip_length)\n    self.chunked_frame_idx.extend(frame_idx_split)\n    self.label_idx.extend(len(frame_idx_split) * [i])\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.CellTrackingDataset","title":"<code>CellTrackingDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset for loading cell tracking challenge data.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize CellTrackingDataset.</p> <code>get_indices</code> <p>Retrieve label and frame indices given batch index.</p> <code>get_instances</code> <p>Get an element of the dataset.</p> Source code in <code>dreem/datasets/cell_tracking_dataset.py</code> <pre><code>class CellTrackingDataset(BaseDataset):\n    \"\"\"Dataset for loading cell tracking challenge data.\"\"\"\n\n    def __init__(\n        self,\n        gt_list: list[list[str]],\n        raw_img_list: list[list[str]],\n        data_dirs: Optional[list[str]] = None,\n        padding: int = 5,\n        crop_size: int = 20,\n        chunk: bool = False,\n        clip_length: int = 10,\n        mode: str = \"train\",\n        augmentations: dict | None = None,\n        n_chunks: int | float = 1.0,\n        seed: int | None = None,\n        max_batching_gap: int = 15,\n        use_tight_bbox: bool = False,\n        ctc_track_meta: list[str] | None = None,\n        apply_mask_to_crop: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize CellTrackingDataset.\n\n        Args:\n            gt_list: filepaths of gt label images in a list of lists (each list\n                corresponds to a dataset)\n            raw_img_list: filepaths of original tif images in a list of lists\n                (each list corresponds to a dataset)\n            data_dirs: paths to data directories\n            padding: amount of padding around object crops\n            crop_size: the size of the object crops. Can be either:\n                - An integer specifying a single crop size for all objects\n                - A list of integers specifying different crop sizes for\n                  different data directories\n            chunk: whether or not to chunk the dataset into batches\n            clip_length: the number of frames in each chunk\n            mode: `train` or `val`. Determines whether this dataset is used for\n                training or validation. Currently doesn't affect dataset logic\n            augmentations: An optional dict mapping augmentations to parameters.\n                The keys\n                should map directly to augmentation classes in albumentations. Example:\n                    augs = {\n                        'Rotate': {'limit': [-90, 90]},\n                        'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},\n                        'RandomContrast': {'limit': 0.2}\n                    }\n            n_chunks: Number of chunks to subsample from.\n                Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n            seed: set a seed for reproducibility\n            max_batching_gap: the max number of frames that can be unlabelled\n                before starting a new batch\n            use_tight_bbox: whether to use tight bounding box (around keypoints)\n                instead of the default square bounding box\n            ctc_track_meta: filepaths of man_track.txt files in a list of lists\n                (each list corresponds to a dataset)\n            apply_mask_to_crop: whether to apply the mask to the crop\n        \"\"\"\n        super().__init__(\n            gt_list,\n            raw_img_list,\n            padding,\n            crop_size,\n            chunk,\n            clip_length,\n            mode,\n            augmentations,\n            n_chunks,\n            seed,\n            ctc_track_meta,\n        )\n\n        self.raw_img_list = raw_img_list\n        self.gt_list = gt_list\n        self.ctc_track_meta = ctc_track_meta\n        self.data_dirs = data_dirs\n        self.chunk = chunk\n        self.clip_length = clip_length\n        self.crop_size = crop_size\n        self.padding = padding\n        self.mode = mode.lower()\n        self.n_chunks = n_chunks\n        self.seed = seed\n        self.max_batching_gap = max_batching_gap\n        self.use_tight_bbox = use_tight_bbox\n        self.skeleton = sio.Skeleton(nodes=[\"centroid\"])\n        self.apply_mask_to_crop = apply_mask_to_crop\n        if not isinstance(self.data_dirs, list):\n            self.data_dirs = [self.data_dirs]\n\n        if not isinstance(self.crop_size, list):\n            # make a list so its handled consistently if multiple crops are used\n            if len(self.data_dirs) &gt; 0:  # for test mode, data_dirs is []\n                self.crop_size = [self.crop_size] * len(self.data_dirs)\n            else:\n                self.crop_size = [self.crop_size]\n\n        if len(self.data_dirs) &gt; 0 and len(self.crop_size) != len(self.data_dirs):\n            raise ValueError(\n                f\"If a list of crop sizes or data directories are given,\"\n                f\"they must have the same length but got {len(self.crop_size)} \"\n                f\"and {len(self.data_dirs)}\"\n            )\n\n        # if self.seed is not None:\n        #     np.random.seed(self.seed)\n\n        if augmentations and self.mode == \"train\":\n            self.augmentations = data_utils.build_augmentations(augmentations)\n        else:\n            self.augmentations = None\n\n        #\n        if self.ctc_track_meta is not None:\n            self.list_df_track_meta = [\n                pd.read_csv(\n                    gtf,\n                    delimiter=\" \",\n                    header=None,\n                    names=[\"track_id\", \"start_frame\", \"end_frame\", \"parent_id\"],\n                )\n                for gtf in self.ctc_track_meta\n            ]\n        else:\n            self.list_df_track_meta = None\n        # frame indices for each dataset; list of lists (each list corresponds to a dataset)\n        self.frame_idx = [torch.arange(len(gt_dataset)) for gt_dataset in self.gt_list]\n\n        # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n        # used in call to get_instances()\n        self.create_chunks_other()\n\n    def get_indices(self, idx: int) -&gt; tuple:\n        \"\"\"Retrieve label and frame indices given batch index.\n\n        Args:\n            idx: the index of the batch.\n\n        Returns:\n            the label and frame indices corresponding to a batch,\n        \"\"\"\n        return self.label_idx[idx], self.chunked_frame_idx[idx]\n\n    def get_instances(self, label_idx: list[int], frame_idx: list[int]) -&gt; list[Frame]:\n        \"\"\"Get an element of the dataset.\n\n        Args:\n            label_idx: index of the labels\n            frame_idx: index of the frames\n\n        Returns:\n            a list of Frame objects containing frame metadata and Instance Objects.\n            See `dreem.io.data_structures` for more info.\n        \"\"\"\n        image_paths = self.raw_img_list[label_idx]\n        gt_paths = self.gt_list[label_idx]\n\n        # df_track_meta is currently unused but may be needed for future track metadata processing\n        # if self.list_df_track_meta is not None:\n        #     df_track_meta = self.list_df_track_meta[label_idx]\n        # else:\n        #     df_track_meta = None\n\n        # get the correct crop size based on the video\n        video_par_path = Path(image_paths[0]).parent.parent\n        if len(self.data_dirs) &gt; 0:\n            crop_size = self.crop_size[0]\n            for j, data_dir in enumerate(self.data_dirs):\n                if Path(data_dir) == video_par_path:\n                    crop_size = self.crop_size[j]\n                    break\n        else:\n            crop_size = self.crop_size[0]\n\n        frames = []\n        max_crop_h, max_crop_w = 0, 0\n        for i in frame_idx:\n            instances, gt_track_ids, centroids, dict_centroids, bboxes, masks = (\n                [],\n                [],\n                [],\n                {},\n                [],\n                [],\n            )\n\n            i = int(i)\n\n            img = image_paths[i]\n            gt_sec = gt_paths[i]\n\n            img = np.array(Image.open(img))\n            gt_sec = np.array(Image.open(gt_sec))\n\n            if img.dtype == np.uint16:\n                img = ((img - img.min()) * (1 / (img.max() - img.min()) * 255)).astype(\n                    np.uint8\n                )\n            # if df_track_meta is None:\n            unique_instances = np.unique(gt_sec)\n            # else:\n            # unique_instances = df_track_meta[\"track_id\"].unique()\n\n            for instance in unique_instances:\n                # not all instances are in the frame, and they also label the\n                # background instance as zero\n                if instance in gt_sec and instance != 0:\n                    mask = gt_sec == instance\n                    center_of_mass = measurements.center_of_mass(mask)\n\n                    # scipy returns yx\n                    x, y = center_of_mass[::-1]\n\n                    if self.use_tight_bbox:\n                        bbox = data_utils.get_tight_bbox_masks(mask)\n                    else:\n                        bbox = data_utils.pad_bbox(\n                            data_utils.get_bbox([int(x), int(y)], crop_size),\n                            padding=self.padding,\n                        )\n                    mask = torch.as_tensor(mask)\n\n                    gt_track_ids.append(int(instance))\n                    centroids.append([x, y])\n                    dict_centroids[int(instance)] = [x, y]\n                    bboxes.append(bbox)\n                    masks.append(mask)\n\n            # albumentations wants (spatial, channels), ensure correct dims\n            if self.augmentations is not None:\n                for transform in self.augmentations:\n                    # for occlusion simulation, can remove if we don't want\n                    if isinstance(transform, A.CoarseDropout):\n                        transform.fill_value = random.randint(0, 255)\n\n                augmented = self.augmentations(\n                    image=img,\n                    mask=gt_sec,  # albumentations ensures geometric transformations are synced between image and mask\n                    keypoints=np.vstack(centroids),\n                )\n                img, aug_mask, centroids = (\n                    augmented[\"image\"],\n                    augmented[\"mask\"],\n                    augmented[\"keypoints\"],\n                )\n                aug_mask = torch.Tensor(aug_mask).unsqueeze(0)\n\n            img = torch.Tensor(img).unsqueeze(0)\n\n            for j in range(len(gt_track_ids)):\n                # just formatting for compatibility with Instance class\n                instance_centroid = {\n                    \"centroid\": np.array(dict_centroids[gt_track_ids[j]])\n                }\n                pose = {\"centroid\": dict_centroids[gt_track_ids[j]]}  # more formatting\n                crop_raw = data_utils.crop_bbox(img, bboxes[j])\n                if self.apply_mask_to_crop:\n                    if (\n                        self.augmentations is not None\n                    ):  # TODO: change this to a flag that the user passes in apply_mask_to_crop\n                        cropped_mask = data_utils.crop_bbox(aug_mask, bboxes[j])\n                        # filter for the instance of interest\n                        cropped_mask[cropped_mask != gt_track_ids[j]] = 0\n                    else:\n                        # masks[j] is already filtered for the instance of interest\n                        cropped_mask = data_utils.crop_bbox(masks[j], bboxes[j])\n\n                    cropped_mask[cropped_mask != 0] = 1\n                    # apply mask to crop\n                    crop = crop_raw * cropped_mask\n                else:\n                    crop = crop_raw\n\n                c, h, w = crop.shape\n                if h &gt; max_crop_h:\n                    max_crop_h = h\n                if w &gt; max_crop_w:\n                    max_crop_w = w\n\n                instances.append(\n                    Instance(\n                        gt_track_id=gt_track_ids[j],\n                        pred_track_id=-1,\n                        centroid=instance_centroid,\n                        skeleton=self.skeleton,\n                        point_scores=np.array([1.0]),\n                        instance_score=np.array([1.0]),\n                        pose=pose,\n                        bbox=bboxes[j],\n                        crop=crop,\n                        mask=masks[j],\n                    )\n                )\n\n            if self.mode == \"train\":\n                np.random.shuffle(instances)\n\n            frames.append(\n                Frame(\n                    video_id=label_idx,\n                    frame_id=i,\n                    vid_file=Path(image_paths[0]).parent.name,\n                    img_shape=img.shape,\n                    instances=instances,\n                )\n            )\n\n        # pad bbox to max size\n        if self.use_tight_bbox:\n            # bound the max crop size to the user defined crop size\n            max_crop_h = crop_size if max_crop_h == 0 else min(max_crop_h, crop_size)\n            max_crop_w = crop_size if max_crop_w == 0 else min(max_crop_w, crop_size)\n            # gather all the crops\n            for frame in frames:\n                for instance in frame.instances:\n                    data_utils.pad_variable_size_crops(\n                        instance, (max_crop_h, max_crop_w)\n                    )\n\n        return frames\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.CellTrackingDataset.__init__","title":"<code>__init__(gt_list, raw_img_list, data_dirs=None, padding=5, crop_size=20, chunk=False, clip_length=10, mode='train', augmentations=None, n_chunks=1.0, seed=None, max_batching_gap=15, use_tight_bbox=False, ctc_track_meta=None, apply_mask_to_crop=False, **kwargs)</code>","text":"<p>Initialize CellTrackingDataset.</p> <p>Parameters:</p> Name Type Description Default <code>gt_list</code> <code>list[list[str]]</code> <p>filepaths of gt label images in a list of lists (each list corresponds to a dataset)</p> required <code>raw_img_list</code> <code>list[list[str]]</code> <p>filepaths of original tif images in a list of lists (each list corresponds to a dataset)</p> required <code>data_dirs</code> <code>Optional[list[str]]</code> <p>paths to data directories</p> <code>None</code> <code>padding</code> <code>int</code> <p>amount of padding around object crops</p> <code>5</code> <code>crop_size</code> <code>int</code> <p>the size of the object crops. Can be either: - An integer specifying a single crop size for all objects - A list of integers specifying different crop sizes for   different data directories</p> <code>20</code> <code>chunk</code> <code>bool</code> <p>whether or not to chunk the dataset into batches</p> <code>False</code> <code>clip_length</code> <code>int</code> <p>the number of frames in each chunk</p> <code>10</code> <code>mode</code> <code>str</code> <p><code>train</code> or <code>val</code>. Determines whether this dataset is used for training or validation. Currently doesn't affect dataset logic</p> <code>'train'</code> <code>augmentations</code> <code>dict | None</code> <p>An optional dict mapping augmentations to parameters. The keys should map directly to augmentation classes in albumentations. Example:     augs = {         'Rotate': {'limit': [-90, 90]},         'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},         'RandomContrast': {'limit': 0.2}     }</p> <code>None</code> <code>n_chunks</code> <code>int | float</code> <p>Number of chunks to subsample from. Can either a fraction of the dataset (ie (0,1.0]) or number of chunks</p> <code>1.0</code> <code>seed</code> <code>int | None</code> <p>set a seed for reproducibility</p> <code>None</code> <code>max_batching_gap</code> <code>int</code> <p>the max number of frames that can be unlabelled before starting a new batch</p> <code>15</code> <code>use_tight_bbox</code> <code>bool</code> <p>whether to use tight bounding box (around keypoints) instead of the default square bounding box</p> <code>False</code> <code>ctc_track_meta</code> <code>list[str] | None</code> <p>filepaths of man_track.txt files in a list of lists (each list corresponds to a dataset)</p> <code>None</code> <code>apply_mask_to_crop</code> <code>bool</code> <p>whether to apply the mask to the crop</p> <code>False</code> Source code in <code>dreem/datasets/cell_tracking_dataset.py</code> <pre><code>def __init__(\n    self,\n    gt_list: list[list[str]],\n    raw_img_list: list[list[str]],\n    data_dirs: Optional[list[str]] = None,\n    padding: int = 5,\n    crop_size: int = 20,\n    chunk: bool = False,\n    clip_length: int = 10,\n    mode: str = \"train\",\n    augmentations: dict | None = None,\n    n_chunks: int | float = 1.0,\n    seed: int | None = None,\n    max_batching_gap: int = 15,\n    use_tight_bbox: bool = False,\n    ctc_track_meta: list[str] | None = None,\n    apply_mask_to_crop: bool = False,\n    **kwargs,\n):\n    \"\"\"Initialize CellTrackingDataset.\n\n    Args:\n        gt_list: filepaths of gt label images in a list of lists (each list\n            corresponds to a dataset)\n        raw_img_list: filepaths of original tif images in a list of lists\n            (each list corresponds to a dataset)\n        data_dirs: paths to data directories\n        padding: amount of padding around object crops\n        crop_size: the size of the object crops. Can be either:\n            - An integer specifying a single crop size for all objects\n            - A list of integers specifying different crop sizes for\n              different data directories\n        chunk: whether or not to chunk the dataset into batches\n        clip_length: the number of frames in each chunk\n        mode: `train` or `val`. Determines whether this dataset is used for\n            training or validation. Currently doesn't affect dataset logic\n        augmentations: An optional dict mapping augmentations to parameters.\n            The keys\n            should map directly to augmentation classes in albumentations. Example:\n                augs = {\n                    'Rotate': {'limit': [-90, 90]},\n                    'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},\n                    'RandomContrast': {'limit': 0.2}\n                }\n        n_chunks: Number of chunks to subsample from.\n            Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n        seed: set a seed for reproducibility\n        max_batching_gap: the max number of frames that can be unlabelled\n            before starting a new batch\n        use_tight_bbox: whether to use tight bounding box (around keypoints)\n            instead of the default square bounding box\n        ctc_track_meta: filepaths of man_track.txt files in a list of lists\n            (each list corresponds to a dataset)\n        apply_mask_to_crop: whether to apply the mask to the crop\n    \"\"\"\n    super().__init__(\n        gt_list,\n        raw_img_list,\n        padding,\n        crop_size,\n        chunk,\n        clip_length,\n        mode,\n        augmentations,\n        n_chunks,\n        seed,\n        ctc_track_meta,\n    )\n\n    self.raw_img_list = raw_img_list\n    self.gt_list = gt_list\n    self.ctc_track_meta = ctc_track_meta\n    self.data_dirs = data_dirs\n    self.chunk = chunk\n    self.clip_length = clip_length\n    self.crop_size = crop_size\n    self.padding = padding\n    self.mode = mode.lower()\n    self.n_chunks = n_chunks\n    self.seed = seed\n    self.max_batching_gap = max_batching_gap\n    self.use_tight_bbox = use_tight_bbox\n    self.skeleton = sio.Skeleton(nodes=[\"centroid\"])\n    self.apply_mask_to_crop = apply_mask_to_crop\n    if not isinstance(self.data_dirs, list):\n        self.data_dirs = [self.data_dirs]\n\n    if not isinstance(self.crop_size, list):\n        # make a list so its handled consistently if multiple crops are used\n        if len(self.data_dirs) &gt; 0:  # for test mode, data_dirs is []\n            self.crop_size = [self.crop_size] * len(self.data_dirs)\n        else:\n            self.crop_size = [self.crop_size]\n\n    if len(self.data_dirs) &gt; 0 and len(self.crop_size) != len(self.data_dirs):\n        raise ValueError(\n            f\"If a list of crop sizes or data directories are given,\"\n            f\"they must have the same length but got {len(self.crop_size)} \"\n            f\"and {len(self.data_dirs)}\"\n        )\n\n    # if self.seed is not None:\n    #     np.random.seed(self.seed)\n\n    if augmentations and self.mode == \"train\":\n        self.augmentations = data_utils.build_augmentations(augmentations)\n    else:\n        self.augmentations = None\n\n    #\n    if self.ctc_track_meta is not None:\n        self.list_df_track_meta = [\n            pd.read_csv(\n                gtf,\n                delimiter=\" \",\n                header=None,\n                names=[\"track_id\", \"start_frame\", \"end_frame\", \"parent_id\"],\n            )\n            for gtf in self.ctc_track_meta\n        ]\n    else:\n        self.list_df_track_meta = None\n    # frame indices for each dataset; list of lists (each list corresponds to a dataset)\n    self.frame_idx = [torch.arange(len(gt_dataset)) for gt_dataset in self.gt_list]\n\n    # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n    # used in call to get_instances()\n    self.create_chunks_other()\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.CellTrackingDataset.get_indices","title":"<code>get_indices(idx)</code>","text":"<p>Retrieve label and frame indices given batch index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the batch.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>the label and frame indices corresponding to a batch,</p> Source code in <code>dreem/datasets/cell_tracking_dataset.py</code> <pre><code>def get_indices(self, idx: int) -&gt; tuple:\n    \"\"\"Retrieve label and frame indices given batch index.\n\n    Args:\n        idx: the index of the batch.\n\n    Returns:\n        the label and frame indices corresponding to a batch,\n    \"\"\"\n    return self.label_idx[idx], self.chunked_frame_idx[idx]\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.CellTrackingDataset.get_instances","title":"<code>get_instances(label_idx, frame_idx)</code>","text":"<p>Get an element of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>label_idx</code> <code>list[int]</code> <p>index of the labels</p> required <code>frame_idx</code> <code>list[int]</code> <p>index of the frames</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>a list of Frame objects containing frame metadata and Instance Objects. See <code>dreem.io.data_structures</code> for more info.</p> Source code in <code>dreem/datasets/cell_tracking_dataset.py</code> <pre><code>def get_instances(self, label_idx: list[int], frame_idx: list[int]) -&gt; list[Frame]:\n    \"\"\"Get an element of the dataset.\n\n    Args:\n        label_idx: index of the labels\n        frame_idx: index of the frames\n\n    Returns:\n        a list of Frame objects containing frame metadata and Instance Objects.\n        See `dreem.io.data_structures` for more info.\n    \"\"\"\n    image_paths = self.raw_img_list[label_idx]\n    gt_paths = self.gt_list[label_idx]\n\n    # df_track_meta is currently unused but may be needed for future track metadata processing\n    # if self.list_df_track_meta is not None:\n    #     df_track_meta = self.list_df_track_meta[label_idx]\n    # else:\n    #     df_track_meta = None\n\n    # get the correct crop size based on the video\n    video_par_path = Path(image_paths[0]).parent.parent\n    if len(self.data_dirs) &gt; 0:\n        crop_size = self.crop_size[0]\n        for j, data_dir in enumerate(self.data_dirs):\n            if Path(data_dir) == video_par_path:\n                crop_size = self.crop_size[j]\n                break\n    else:\n        crop_size = self.crop_size[0]\n\n    frames = []\n    max_crop_h, max_crop_w = 0, 0\n    for i in frame_idx:\n        instances, gt_track_ids, centroids, dict_centroids, bboxes, masks = (\n            [],\n            [],\n            [],\n            {},\n            [],\n            [],\n        )\n\n        i = int(i)\n\n        img = image_paths[i]\n        gt_sec = gt_paths[i]\n\n        img = np.array(Image.open(img))\n        gt_sec = np.array(Image.open(gt_sec))\n\n        if img.dtype == np.uint16:\n            img = ((img - img.min()) * (1 / (img.max() - img.min()) * 255)).astype(\n                np.uint8\n            )\n        # if df_track_meta is None:\n        unique_instances = np.unique(gt_sec)\n        # else:\n        # unique_instances = df_track_meta[\"track_id\"].unique()\n\n        for instance in unique_instances:\n            # not all instances are in the frame, and they also label the\n            # background instance as zero\n            if instance in gt_sec and instance != 0:\n                mask = gt_sec == instance\n                center_of_mass = measurements.center_of_mass(mask)\n\n                # scipy returns yx\n                x, y = center_of_mass[::-1]\n\n                if self.use_tight_bbox:\n                    bbox = data_utils.get_tight_bbox_masks(mask)\n                else:\n                    bbox = data_utils.pad_bbox(\n                        data_utils.get_bbox([int(x), int(y)], crop_size),\n                        padding=self.padding,\n                    )\n                mask = torch.as_tensor(mask)\n\n                gt_track_ids.append(int(instance))\n                centroids.append([x, y])\n                dict_centroids[int(instance)] = [x, y]\n                bboxes.append(bbox)\n                masks.append(mask)\n\n        # albumentations wants (spatial, channels), ensure correct dims\n        if self.augmentations is not None:\n            for transform in self.augmentations:\n                # for occlusion simulation, can remove if we don't want\n                if isinstance(transform, A.CoarseDropout):\n                    transform.fill_value = random.randint(0, 255)\n\n            augmented = self.augmentations(\n                image=img,\n                mask=gt_sec,  # albumentations ensures geometric transformations are synced between image and mask\n                keypoints=np.vstack(centroids),\n            )\n            img, aug_mask, centroids = (\n                augmented[\"image\"],\n                augmented[\"mask\"],\n                augmented[\"keypoints\"],\n            )\n            aug_mask = torch.Tensor(aug_mask).unsqueeze(0)\n\n        img = torch.Tensor(img).unsqueeze(0)\n\n        for j in range(len(gt_track_ids)):\n            # just formatting for compatibility with Instance class\n            instance_centroid = {\n                \"centroid\": np.array(dict_centroids[gt_track_ids[j]])\n            }\n            pose = {\"centroid\": dict_centroids[gt_track_ids[j]]}  # more formatting\n            crop_raw = data_utils.crop_bbox(img, bboxes[j])\n            if self.apply_mask_to_crop:\n                if (\n                    self.augmentations is not None\n                ):  # TODO: change this to a flag that the user passes in apply_mask_to_crop\n                    cropped_mask = data_utils.crop_bbox(aug_mask, bboxes[j])\n                    # filter for the instance of interest\n                    cropped_mask[cropped_mask != gt_track_ids[j]] = 0\n                else:\n                    # masks[j] is already filtered for the instance of interest\n                    cropped_mask = data_utils.crop_bbox(masks[j], bboxes[j])\n\n                cropped_mask[cropped_mask != 0] = 1\n                # apply mask to crop\n                crop = crop_raw * cropped_mask\n            else:\n                crop = crop_raw\n\n            c, h, w = crop.shape\n            if h &gt; max_crop_h:\n                max_crop_h = h\n            if w &gt; max_crop_w:\n                max_crop_w = w\n\n            instances.append(\n                Instance(\n                    gt_track_id=gt_track_ids[j],\n                    pred_track_id=-1,\n                    centroid=instance_centroid,\n                    skeleton=self.skeleton,\n                    point_scores=np.array([1.0]),\n                    instance_score=np.array([1.0]),\n                    pose=pose,\n                    bbox=bboxes[j],\n                    crop=crop,\n                    mask=masks[j],\n                )\n            )\n\n        if self.mode == \"train\":\n            np.random.shuffle(instances)\n\n        frames.append(\n            Frame(\n                video_id=label_idx,\n                frame_id=i,\n                vid_file=Path(image_paths[0]).parent.name,\n                img_shape=img.shape,\n                instances=instances,\n            )\n        )\n\n    # pad bbox to max size\n    if self.use_tight_bbox:\n        # bound the max crop size to the user defined crop size\n        max_crop_h = crop_size if max_crop_h == 0 else min(max_crop_h, crop_size)\n        max_crop_w = crop_size if max_crop_w == 0 else min(max_crop_w, crop_size)\n        # gather all the crops\n        for frame in frames:\n            for instance in frame.instances:\n                data_utils.pad_variable_size_crops(\n                    instance, (max_crop_h, max_crop_w)\n                )\n\n    return frames\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.MicroscopyDataset","title":"<code>MicroscopyDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset for loading Microscopy Data.</p> <p>Methods:</p> Name Description <code>__del__</code> <p>Handle file closing before deletion.</p> <code>__init__</code> <p>Initialize MicroscopyDataset.</p> <code>get_indices</code> <p>Retrieve label and frame indices given batch index.</p> <code>get_instances</code> <p>Get an element of the dataset.</p> Source code in <code>dreem/datasets/microscopy_dataset.py</code> <pre><code>class MicroscopyDataset(BaseDataset):\n    \"\"\"Dataset for loading Microscopy Data.\"\"\"\n\n    def __init__(\n        self,\n        videos: list[str],\n        tracks: list[str],\n        source: str,\n        padding: int = 5,\n        crop_size: int = 20,\n        chunk: bool = False,\n        clip_length: int = 10,\n        mode: str = \"Train\",\n        augmentations: dict | None = None,\n        n_chunks: int | float = 1.0,\n        seed: int | None = None,\n    ):\n        \"\"\"Initialize MicroscopyDataset.\n\n        Args:\n            videos: paths to raw microscopy videos\n            tracks: paths to trackmate gt labels (either .xml or .csv)\n            source: file format of gt labels based on label generator\n            padding: amount of padding around object crops\n            crop_size: the size of the object crops\n            chunk: whether or not to chunk the dataset into batches\n            clip_length: the number of frames in each chunk\n            mode: `train` or `val`. Determines whether this dataset is used for\n                training or validation. Currently doesn't affect dataset logic\n            augmentations: An optional dict mapping augmentations to parameters. The keys\n                should map directly to augmentation classes in albumentations. Example:\n                    augs = {\n                        'Rotate': {'limit': [-90, 90]},\n                        'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},\n                        'RandomContrast': {'limit': 0.2}\n                    }\n            n_chunks: Number of chunks to subsample from.\n                Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n            seed: set a seed for reproducibility\n        \"\"\"\n        super().__init__(\n            tracks,\n            videos,\n            padding,\n            crop_size,\n            chunk,\n            clip_length,\n            mode,\n            augmentations,\n            n_chunks,\n            seed,\n        )\n\n        self.vid_files = videos\n        self.tracks = tracks\n        self.chunk = chunk\n        self.clip_length = clip_length\n        self.crop_size = crop_size\n        self.padding = padding\n        self.mode = mode.lower()\n        self.n_chunks = n_chunks\n        self.seed = seed\n\n        # if self.seed is not None:\n        #     np.random.seed(self.seed)\n        if augmentations and self.mode == \"train\":\n            self.augmentations = data_utils.build_augmentations(augmentations)\n        else:\n            self.augmentations = None\n\n        if source.lower() == \"trackmate\":\n            parser = data_utils.parse_trackmate\n        elif source.lower() in [\"icy\", \"isbi\"]:\n            parser = lambda x: data_utils.parse_synthetic(x, source=source)\n        else:\n            raise ValueError(\n                f\"{source} is unsupported! Must be one of [trackmate, icy, isbi]\"\n            )\n\n        self.labels = [\n            parser(self.tracks[video_idx]) for video_idx in range(len(self.tracks))\n        ]\n\n        self.videos = []\n        for vid_file in self.vid_files:\n            if not isinstance(vid_file, list):\n                self.videos.append(data_utils.LazyTiffStack(vid_file))\n            else:\n                self.videos.append([Image.open(frame_file) for frame_file in vid_file])\n        self.frame_idx = [\n            (\n                torch.arange(Image.open(video).n_frames)\n                if isinstance(video, str)\n                else torch.arange(len(video))\n            )\n            for video in self.vid_files\n        ]\n\n        # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n        # used in call to get_instances()\n        self.create_chunks_other()\n\n    def get_indices(self, idx: int) -&gt; tuple:\n        \"\"\"Retrieve label and frame indices given batch index.\n\n        Args:\n            idx: the index of the batch.\n        \"\"\"\n        return self.label_idx[idx], self.chunked_frame_idx[idx]\n\n    def get_instances(self, label_idx: list[int], frame_idx: list[int]) -&gt; list[Frame]:\n        \"\"\"Get an element of the dataset.\n\n        Args:\n            label_idx: index of the labels\n            frame_idx: index of the frames\n\n        Returns:\n            A list of Frames containing Instances to be tracked (See `dreem.io.data_structures for more info`)\n        \"\"\"\n        labels = self.labels[label_idx]\n        labels = labels.dropna(how=\"all\")\n\n        video = self.videos[label_idx]\n\n        frames = []\n        for frame_id in frame_idx:\n            instances, gt_track_ids, centroids = [], [], []\n\n            img = (\n                video.get_section(frame_id)\n                if not isinstance(video, list)\n                else np.array(video[frame_id])\n            )\n\n            lf = labels[labels[\"FRAME\"].astype(int) == frame_id.item()]\n\n            for instance in sorted(lf[\"TRACK_ID\"].unique()):\n                gt_track_ids.append(int(instance))\n\n                x = lf[lf[\"TRACK_ID\"] == instance][\"POSITION_X\"].iloc[0]\n                y = lf[lf[\"TRACK_ID\"] == instance][\"POSITION_Y\"].iloc[0]\n                centroids.append([x, y])\n\n            # albumentations wants (spatial, channels), ensure correct dims\n            if self.augmentations is not None:\n                for transform in self.augmentations:\n                    # for occlusion simulation, can remove if we don't want\n                    if isinstance(transform, A.CoarseDropout):\n                        transform.fill_value = random.randint(0, 255)\n\n                augmented = self.augmentations(\n                    image=img,\n                    keypoints=np.vstack(centroids),\n                )\n                img, centroids = augmented[\"image\"], augmented[\"keypoints\"]\n\n            img = torch.Tensor(img)\n\n            # torch wants (channels, spatial) - ensure correct dims\n            if len(img.shape) == 2:\n                img = img.unsqueeze(0)\n            elif len(img.shape) == 3:\n                if img.shape[2] == 3:\n                    img = img.T  # todo: check for edge cases\n\n            for gt_id in range(len(gt_track_ids)):\n                c = centroids[gt_id]\n                bbox = data_utils.pad_bbox(\n                    data_utils.get_bbox([int(c[0]), int(c[1])], self.crop_size),\n                    padding=self.padding,\n                )\n                crop = data_utils.crop_bbox(img, bbox)\n\n                instances.append(\n                    Instance(\n                        gt_track_id=gt_track_ids[gt_id],\n                        pred_track_id=-1,\n                        bbox=bbox,\n                        crop=crop,\n                    )\n                )\n\n            if self.mode == \"train\":\n                np.random.shuffle(instances)\n\n            frames.append(\n                Frame(\n                    video_id=label_idx,\n                    frame_id=frame_id,\n                    img_shape=img.shape,\n                    instances=instances,\n                )\n            )\n\n        return frames\n\n    def __del__(self):\n        \"\"\"Handle file closing before deletion.\"\"\"\n        for vid_reader in self.videos:\n            if not isinstance(vid_reader, list):\n                vid_reader.close()\n            else:\n                for frame_reader in vid_reader:\n                    frame_reader.close()\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.MicroscopyDataset.__del__","title":"<code>__del__()</code>","text":"<p>Handle file closing before deletion.</p> Source code in <code>dreem/datasets/microscopy_dataset.py</code> <pre><code>def __del__(self):\n    \"\"\"Handle file closing before deletion.\"\"\"\n    for vid_reader in self.videos:\n        if not isinstance(vid_reader, list):\n            vid_reader.close()\n        else:\n            for frame_reader in vid_reader:\n                frame_reader.close()\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.MicroscopyDataset.__init__","title":"<code>__init__(videos, tracks, source, padding=5, crop_size=20, chunk=False, clip_length=10, mode='Train', augmentations=None, n_chunks=1.0, seed=None)</code>","text":"<p>Initialize MicroscopyDataset.</p> <p>Parameters:</p> Name Type Description Default <code>videos</code> <code>list[str]</code> <p>paths to raw microscopy videos</p> required <code>tracks</code> <code>list[str]</code> <p>paths to trackmate gt labels (either .xml or .csv)</p> required <code>source</code> <code>str</code> <p>file format of gt labels based on label generator</p> required <code>padding</code> <code>int</code> <p>amount of padding around object crops</p> <code>5</code> <code>crop_size</code> <code>int</code> <p>the size of the object crops</p> <code>20</code> <code>chunk</code> <code>bool</code> <p>whether or not to chunk the dataset into batches</p> <code>False</code> <code>clip_length</code> <code>int</code> <p>the number of frames in each chunk</p> <code>10</code> <code>mode</code> <code>str</code> <p><code>train</code> or <code>val</code>. Determines whether this dataset is used for training or validation. Currently doesn't affect dataset logic</p> <code>'Train'</code> <code>augmentations</code> <code>dict | None</code> <p>An optional dict mapping augmentations to parameters. The keys should map directly to augmentation classes in albumentations. Example:     augs = {         'Rotate': {'limit': [-90, 90]},         'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},         'RandomContrast': {'limit': 0.2}     }</p> <code>None</code> <code>n_chunks</code> <code>int | float</code> <p>Number of chunks to subsample from. Can either a fraction of the dataset (ie (0,1.0]) or number of chunks</p> <code>1.0</code> <code>seed</code> <code>int | None</code> <p>set a seed for reproducibility</p> <code>None</code> Source code in <code>dreem/datasets/microscopy_dataset.py</code> <pre><code>def __init__(\n    self,\n    videos: list[str],\n    tracks: list[str],\n    source: str,\n    padding: int = 5,\n    crop_size: int = 20,\n    chunk: bool = False,\n    clip_length: int = 10,\n    mode: str = \"Train\",\n    augmentations: dict | None = None,\n    n_chunks: int | float = 1.0,\n    seed: int | None = None,\n):\n    \"\"\"Initialize MicroscopyDataset.\n\n    Args:\n        videos: paths to raw microscopy videos\n        tracks: paths to trackmate gt labels (either .xml or .csv)\n        source: file format of gt labels based on label generator\n        padding: amount of padding around object crops\n        crop_size: the size of the object crops\n        chunk: whether or not to chunk the dataset into batches\n        clip_length: the number of frames in each chunk\n        mode: `train` or `val`. Determines whether this dataset is used for\n            training or validation. Currently doesn't affect dataset logic\n        augmentations: An optional dict mapping augmentations to parameters. The keys\n            should map directly to augmentation classes in albumentations. Example:\n                augs = {\n                    'Rotate': {'limit': [-90, 90]},\n                    'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},\n                    'RandomContrast': {'limit': 0.2}\n                }\n        n_chunks: Number of chunks to subsample from.\n            Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n        seed: set a seed for reproducibility\n    \"\"\"\n    super().__init__(\n        tracks,\n        videos,\n        padding,\n        crop_size,\n        chunk,\n        clip_length,\n        mode,\n        augmentations,\n        n_chunks,\n        seed,\n    )\n\n    self.vid_files = videos\n    self.tracks = tracks\n    self.chunk = chunk\n    self.clip_length = clip_length\n    self.crop_size = crop_size\n    self.padding = padding\n    self.mode = mode.lower()\n    self.n_chunks = n_chunks\n    self.seed = seed\n\n    # if self.seed is not None:\n    #     np.random.seed(self.seed)\n    if augmentations and self.mode == \"train\":\n        self.augmentations = data_utils.build_augmentations(augmentations)\n    else:\n        self.augmentations = None\n\n    if source.lower() == \"trackmate\":\n        parser = data_utils.parse_trackmate\n    elif source.lower() in [\"icy\", \"isbi\"]:\n        parser = lambda x: data_utils.parse_synthetic(x, source=source)\n    else:\n        raise ValueError(\n            f\"{source} is unsupported! Must be one of [trackmate, icy, isbi]\"\n        )\n\n    self.labels = [\n        parser(self.tracks[video_idx]) for video_idx in range(len(self.tracks))\n    ]\n\n    self.videos = []\n    for vid_file in self.vid_files:\n        if not isinstance(vid_file, list):\n            self.videos.append(data_utils.LazyTiffStack(vid_file))\n        else:\n            self.videos.append([Image.open(frame_file) for frame_file in vid_file])\n    self.frame_idx = [\n        (\n            torch.arange(Image.open(video).n_frames)\n            if isinstance(video, str)\n            else torch.arange(len(video))\n        )\n        for video in self.vid_files\n    ]\n\n    # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n    # used in call to get_instances()\n    self.create_chunks_other()\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.MicroscopyDataset.get_indices","title":"<code>get_indices(idx)</code>","text":"<p>Retrieve label and frame indices given batch index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the batch.</p> required Source code in <code>dreem/datasets/microscopy_dataset.py</code> <pre><code>def get_indices(self, idx: int) -&gt; tuple:\n    \"\"\"Retrieve label and frame indices given batch index.\n\n    Args:\n        idx: the index of the batch.\n    \"\"\"\n    return self.label_idx[idx], self.chunked_frame_idx[idx]\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.MicroscopyDataset.get_instances","title":"<code>get_instances(label_idx, frame_idx)</code>","text":"<p>Get an element of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>label_idx</code> <code>list[int]</code> <p>index of the labels</p> required <code>frame_idx</code> <code>list[int]</code> <p>index of the frames</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A list of Frames containing Instances to be tracked (See <code>dreem.io.data_structures for more info</code>)</p> Source code in <code>dreem/datasets/microscopy_dataset.py</code> <pre><code>def get_instances(self, label_idx: list[int], frame_idx: list[int]) -&gt; list[Frame]:\n    \"\"\"Get an element of the dataset.\n\n    Args:\n        label_idx: index of the labels\n        frame_idx: index of the frames\n\n    Returns:\n        A list of Frames containing Instances to be tracked (See `dreem.io.data_structures for more info`)\n    \"\"\"\n    labels = self.labels[label_idx]\n    labels = labels.dropna(how=\"all\")\n\n    video = self.videos[label_idx]\n\n    frames = []\n    for frame_id in frame_idx:\n        instances, gt_track_ids, centroids = [], [], []\n\n        img = (\n            video.get_section(frame_id)\n            if not isinstance(video, list)\n            else np.array(video[frame_id])\n        )\n\n        lf = labels[labels[\"FRAME\"].astype(int) == frame_id.item()]\n\n        for instance in sorted(lf[\"TRACK_ID\"].unique()):\n            gt_track_ids.append(int(instance))\n\n            x = lf[lf[\"TRACK_ID\"] == instance][\"POSITION_X\"].iloc[0]\n            y = lf[lf[\"TRACK_ID\"] == instance][\"POSITION_Y\"].iloc[0]\n            centroids.append([x, y])\n\n        # albumentations wants (spatial, channels), ensure correct dims\n        if self.augmentations is not None:\n            for transform in self.augmentations:\n                # for occlusion simulation, can remove if we don't want\n                if isinstance(transform, A.CoarseDropout):\n                    transform.fill_value = random.randint(0, 255)\n\n            augmented = self.augmentations(\n                image=img,\n                keypoints=np.vstack(centroids),\n            )\n            img, centroids = augmented[\"image\"], augmented[\"keypoints\"]\n\n        img = torch.Tensor(img)\n\n        # torch wants (channels, spatial) - ensure correct dims\n        if len(img.shape) == 2:\n            img = img.unsqueeze(0)\n        elif len(img.shape) == 3:\n            if img.shape[2] == 3:\n                img = img.T  # todo: check for edge cases\n\n        for gt_id in range(len(gt_track_ids)):\n            c = centroids[gt_id]\n            bbox = data_utils.pad_bbox(\n                data_utils.get_bbox([int(c[0]), int(c[1])], self.crop_size),\n                padding=self.padding,\n            )\n            crop = data_utils.crop_bbox(img, bbox)\n\n            instances.append(\n                Instance(\n                    gt_track_id=gt_track_ids[gt_id],\n                    pred_track_id=-1,\n                    bbox=bbox,\n                    crop=crop,\n                )\n            )\n\n        if self.mode == \"train\":\n            np.random.shuffle(instances)\n\n        frames.append(\n            Frame(\n                video_id=label_idx,\n                frame_id=frame_id,\n                img_shape=img.shape,\n                instances=instances,\n            )\n        )\n\n    return frames\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.SleapDataset","title":"<code>SleapDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset for loading animal behavior data from sleap.</p> <p>Methods:</p> Name Description <code>__del__</code> <p>Handle file closing before garbage collection.</p> <code>__init__</code> <p>Initialize SleapDataset.</p> <code>get_indices</code> <p>Retrieve label and frame indices given batch index.</p> <code>get_instances</code> <p>Get an element of the dataset.</p> Source code in <code>dreem/datasets/sleap_dataset.py</code> <pre><code>class SleapDataset(BaseDataset):\n    \"\"\"Dataset for loading animal behavior data from sleap.\"\"\"\n\n    def __init__(\n        self,\n        slp_files: list[str],\n        video_files: list[str],\n        data_dirs: Optional[list[str]] = None,\n        padding: int = 5,\n        crop_size: Union[int, list[int]] = 128,\n        anchors: int | list[str] | str = \"\",\n        chunk: bool = True,\n        clip_length: int = 16,\n        mode: str = \"train\",\n        handle_missing: str = \"centroid\",\n        augmentations: dict | None = None,\n        n_chunks: int | float = 1.0,\n        seed: int | None = None,\n        verbose: bool = False,\n        normalize_image: bool = True,\n        max_batching_gap: int = 15,\n        use_tight_bbox: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize SleapDataset.\n\n        Args:\n            slp_files: a list of .slp files storing tracking annotations\n            video_files: a list of paths to video files\n            data_dirs: a path, or a list of paths to data directories. If provided, crop_size should be a list of integers\n                with the same length as data_dirs.\n            padding: amount of padding around object crops\n            crop_size: the size of the object crops. Can be either:\n                - An integer specifying a single crop size for all objects\n                - A list of integers specifying different crop sizes for different data directories\n            anchors: One of:\n                        * a string indicating a single node to center crops around\n                        * a list of skeleton node names to be used as the center of crops\n                        * an int indicating the number of anchors to randomly select\n                    If unavailable then crop around the midpoint between all visible anchors.\n            chunk: whether or not to chunk the dataset into batches\n            clip_length: the number of frames in each chunk\n            mode: `train`, `val`, or `test`. Determines whether this dataset is used for\n                training, validation/testing/inference.\n            handle_missing: how to handle missing single nodes. one of `[\"drop\", \"ignore\", \"centroid\"]`.\n                            if \"drop\" then we dont include instances which are missing the `anchor`.\n                            if \"ignore\" then we use a mask instead of a crop and nan centroids/bboxes.\n                            if \"centroid\" then we default to the pose centroid as the node to crop around.\n            augmentations: An optional dict mapping augmentations to parameters. The keys\n                should map directly to augmentation classes in albumentations. Example:\n                    augmentations = {\n                        'Rotate': {'limit': [-90, 90], 'p': 0.5},\n                        'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0, 'p': 0.2},\n                        'RandomContrast': {'limit': 0.2, 'p': 0.6}\n                    }\n            n_chunks: Number of chunks to subsample from.\n                Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n            seed: set a seed for reproducibility\n            verbose: boolean representing whether to print\n            normalize_image: whether to normalize the image to [0, 1]\n            max_batching_gap: the max number of frames that can be unlabelled before starting a new batch\n            use_tight_bbox: whether to use tight bounding box (around keypoints) instead of the default square bounding box\n        \"\"\"\n        super().__init__(\n            slp_files,\n            video_files,\n            padding,\n            crop_size,\n            chunk,\n            clip_length,\n            mode,\n            augmentations,\n            n_chunks,\n            seed,\n        )\n\n        self.slp_files = slp_files\n        self.data_dirs = data_dirs\n        self.video_files = video_files\n        self.padding = padding\n        self.crop_size = crop_size\n        self.chunk = chunk\n        self.clip_length = clip_length\n        self.mode = mode.lower()\n        self.handle_missing = handle_missing.lower()\n        self.n_chunks = n_chunks\n        self.seed = seed\n        self.normalize_image = normalize_image\n        self.max_batching_gap = max_batching_gap\n        self.use_tight_bbox = use_tight_bbox\n\n        if isinstance(anchors, int):\n            self.anchors = anchors\n        elif isinstance(anchors, str):\n            self.anchors = [anchors]\n        else:\n            self.anchors = anchors\n\n        if not isinstance(self.data_dirs, list):\n            self.data_dirs = [self.data_dirs]\n\n        if not isinstance(self.crop_size, list):\n            # make a list so its handled consistently if multiple crops are used\n            if len(self.data_dirs) &gt; 0:  # for test mode, data_dirs is []\n                self.crop_size = [self.crop_size] * len(self.data_dirs)\n            else:\n                self.crop_size = [self.crop_size]\n\n        if len(self.data_dirs) &gt; 0 and len(self.crop_size) != len(self.data_dirs):\n            raise ValueError(\n                f\"If a list of crop sizes or data directories are given,\"\n                f\"they must have the same length but got {len(self.crop_size)} \"\n                f\"and {len(self.data_dirs)}\"\n            )\n\n        if (\n            isinstance(self.anchors, list) and len(self.anchors) == 0\n        ) or self.anchors == 0:\n            raise ValueError(f\"Must provide at least one anchor but got {self.anchors}\")\n\n        self.verbose = verbose\n\n        # if self.seed is not None:\n        #     np.random.seed(self.seed)\n\n        # load_slp is a wrapper around sio.load_slp for frame gap checks\n        self.labels = []\n        self.annotated_segments = {}\n        for slp_file in self.slp_files:\n            labels, annotated_segments = data_utils.load_slp(slp_file)\n            self.labels.append(labels)\n            self.annotated_segments[slp_file] = annotated_segments\n\n        self.videos = [imageio.get_reader(vid_file) for vid_file in self.vid_files]\n        # do we need this? would need to update with sleap-io\n\n        # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n        # used in call to get_instances()\n        self.create_chunks_slp()\n\n    def get_indices(self, idx: int) -&gt; tuple:\n        \"\"\"Retrieve label and frame indices given batch index.\n\n        Args:\n            idx: the index of the batch.\n        \"\"\"\n        return self.label_idx[idx], self.chunked_frame_idx[idx]\n\n    def get_instances(\n        self, label_idx: list[int], frame_idx: torch.Tensor\n    ) -&gt; list[Frame]:\n        \"\"\"Get an element of the dataset.\n\n        Args:\n            label_idx: index of the labels\n            frame_idx: indices of the frames to load in to the batch\n\n        Returns:\n            A list of `dreem.io.Frame` objects containing metadata and instance data for the batch/clip.\n\n        \"\"\"\n        sleap_labels_obj = self.labels[label_idx]\n        video_name = self.video_files[label_idx]\n\n        # get the correct crop size based on the video\n        video_par_path = Path(video_name).parent\n        if len(self.data_dirs) &gt; 0:\n            crop_size = self.crop_size[0]\n            for j, data_dir in enumerate(self.data_dirs):\n                if Path(data_dir) == video_par_path:\n                    crop_size = self.crop_size[j]\n                    break\n        else:\n            crop_size = self.crop_size[0]\n\n        vid_reader = self.videos[label_idx]\n\n        skeleton = sleap_labels_obj.skeletons[-1]\n\n        frames = []\n        max_crop_h, max_crop_w = 0, 0\n        for i, frame_ind in enumerate(frame_idx):\n            (\n                instances,\n                gt_track_ids,\n                poses,\n                shown_poses,\n                point_scores,\n                instance_score,\n            ) = ([], [], [], [], [], [])\n\n            frame_ind = int(frame_ind)\n\n            # sleap-io method for indexing a Labels() object based on the frame's index\n            lf = sleap_labels_obj[(sleap_labels_obj.video, frame_ind)]\n            if frame_ind != lf.frame_idx:\n                logger.warning(f\"Frame index mismatch: {frame_ind} != {lf.frame_idx}\")\n\n            try:\n                img = vid_reader.get_data(int(frame_ind))\n            except IndexError as e:\n                logger.warning(\n                    f\"Could not read frame {frame_ind} from {video_name} due to {e}\"\n                )\n                continue\n\n            if len(img.shape) == 2:\n                img = img.expand_dims(-1)\n            h, w, c = img.shape\n\n            if c == 1:\n                img = np.concatenate(\n                    [img, img, img], axis=-1\n                )  # convert to grayscale to rgb\n\n            if np.issubdtype(img.dtype, np.integer):  # convert int to float\n                img = img.astype(np.float32)\n                if self.normalize_image:\n                    img = img / 255\n\n            n_instances_dropped = 0\n\n            gt_instances = []\n            # don't load instances that have been 'greyed out' i.e. all nans for keypoints\n            for inst in lf.instances:\n                pts = np.array([p for p in inst.numpy()])\n                if np.isnan(pts).all():\n                    continue\n                else:\n                    gt_instances.append(inst)\n\n            dict_instances = {}\n            no_track_instances = []\n            for instance in gt_instances:\n                if instance.track is not None:\n                    gt_track_id = sleap_labels_obj.tracks.index(instance.track)\n                    if gt_track_id not in dict_instances:\n                        dict_instances[gt_track_id] = instance\n                    else:\n                        existing_instance = dict_instances[gt_track_id]\n                        # if existing is PredictedInstance and current is not, then current is a UserInstance and should be used\n                        if isinstance(\n                            existing_instance, sio.PredictedInstance\n                        ) and not isinstance(instance, sio.PredictedInstance):\n                            dict_instances[gt_track_id] = instance\n                else:\n                    no_track_instances.append(instance)\n\n            gt_instances = list(dict_instances.values()) + no_track_instances\n\n            if self.mode == \"train\":\n                np.random.shuffle(gt_instances)\n\n            for instance in gt_instances:\n                if (\n                    np.random.uniform() &lt; self.instance_dropout[\"p\"]\n                    and n_instances_dropped &lt; self.instance_dropout[\"n\"]\n                ):\n                    n_instances_dropped += 1\n                    continue\n\n                if instance.track is not None:\n                    gt_track_id = sleap_labels_obj.tracks.index(instance.track)\n                else:\n                    gt_track_id = -1\n                gt_track_ids.append(gt_track_id)\n\n                poses.append(\n                    dict(\n                        zip(\n                            [n.name for n in instance.skeleton.nodes],\n                            [p for p in instance.numpy()],\n                        )\n                    )\n                )\n\n                shown_poses = [\n                    {\n                        key: val\n                        for key, val in instance.items()\n                        if not np.isnan(val).any()\n                    }\n                    for instance in poses\n                ]\n\n                point_scores.append(\n                    np.array(\n                        [\n                            (\n                                1.0  # point scores not reliably available in sleap io PredictedPointsArray\n                                # point.score\n                                # if isinstance(point, sio.PredictedPoint)\n                                # else 1.0\n                            )\n                            for point in instance.numpy()\n                        ]\n                    )\n                )\n                if isinstance(instance, sio.PredictedInstance):\n                    instance_score.append(instance.score)\n                else:\n                    instance_score.append(1.0)\n            # augmentations\n            if self.augmentations is not None:\n                for transform in self.augmentations:\n                    if isinstance(transform, A.CoarseDropout):\n                        transform.fill_value = random.randint(0, 255)\n\n                if shown_poses:\n                    keypoints = np.vstack([list(s.values()) for s in shown_poses])\n\n                else:\n                    keypoints = []\n\n                augmented = self.augmentations(image=img, keypoints=keypoints)\n\n                img, aug_poses = augmented[\"image\"], augmented[\"keypoints\"]\n\n                aug_poses = [\n                    arr\n                    for arr in np.split(\n                        np.array(aug_poses),\n                        np.array([len(s) for s in shown_poses]).cumsum(),\n                    )\n                    if arr.size != 0\n                ]\n\n                aug_poses = [\n                    dict(zip(list(pose_dict.keys()), aug_pose_arr.tolist()))\n                    for aug_pose_arr, pose_dict in zip(aug_poses, shown_poses)\n                ]\n\n                _ = [\n                    pose.update(aug_pose)\n                    for pose, aug_pose in zip(shown_poses, aug_poses)\n                ]\n\n            img = tvf.to_tensor(img)\n\n            for j in range(len(gt_track_ids)):\n                pose = shown_poses[j]\n\n                \"\"\"Check for anchor\"\"\"\n                crops = []\n                boxes = []\n                centroids = {}\n\n                if isinstance(self.anchors, int):\n                    anchors_to_choose = list(pose.keys()) + [\"midpoint\"]\n                    anchors = np.random.choice(anchors_to_choose, self.anchors)\n                else:\n                    anchors = self.anchors\n\n                dropped_anchors = self.node_dropout(anchors)\n\n                for anchor in anchors:\n                    if anchor in dropped_anchors:\n                        centroid = np.array([np.nan, np.nan])\n\n                    elif anchor == \"midpoint\" or anchor == \"centroid\":\n                        centroid = np.nanmean(np.array(list(pose.values())), axis=0)\n\n                    elif anchor in pose:\n                        centroid = np.array(pose[anchor])\n                        if np.isnan(centroid).any():\n                            centroid = np.array([np.nan, np.nan])\n\n                    elif (\n                        anchor not in pose\n                        and len(anchors) == 1\n                        and self.handle_missing == \"centroid\"\n                    ):\n                        anchor = \"midpoint\"\n                        centroid = np.nanmean(np.array(list(pose.values())), axis=0)\n\n                    else:\n                        centroid = np.array([np.nan, np.nan])\n\n                    if np.isnan(centroid).all():\n                        bbox = torch.tensor([np.nan, np.nan, np.nan, np.nan])\n\n                    else:\n                        if self.use_tight_bbox and len(pose) &gt; 1:\n                            # tight bbox\n                            # dont allow this for centroid-only poses!\n                            arr_pose = np.array(list(pose.values()))\n                            # note bbox will be a different size for each instance; padded at the end of the loop\n                            bbox = data_utils.get_tight_bbox(arr_pose)\n\n                        else:\n                            bbox = data_utils.pad_bbox(\n                                data_utils.get_bbox(centroid, crop_size),\n                                padding=self.padding,\n                            )\n\n                    if bbox.isnan().all():\n                        crop = torch.zeros(\n                            c,\n                            crop_size + 2 * self.padding,\n                            crop_size + 2 * self.padding,\n                            dtype=img.dtype,\n                        )\n                    else:\n                        crop = data_utils.crop_bbox(img, bbox)\n\n                    crops.append(crop)\n                    # get max h,w for padding for tight bboxes\n                    c, h, w = crop.shape\n                    if h &gt; max_crop_h:\n                        max_crop_h = h\n                    if w &gt; max_crop_w:\n                        max_crop_w = w\n\n                    centroids[anchor] = centroid\n                    boxes.append(bbox)\n\n                if len(crops) &gt; 0:\n                    crops = torch.concat(crops, dim=0)\n\n                if len(boxes) &gt; 0:\n                    boxes = torch.stack(boxes, dim=0)\n\n                if self.handle_missing == \"drop\" and boxes.isnan().any():\n                    continue\n\n                instance = Instance(\n                    gt_track_id=gt_track_ids[j],\n                    pred_track_id=-1,\n                    crop=crops,\n                    centroid=centroids,\n                    bbox=boxes,\n                    skeleton=skeleton,\n                    pose=poses[j],\n                    point_scores=point_scores[j],\n                    instance_score=instance_score[j],\n                )\n\n                instances.append(instance)\n\n            frame = Frame(\n                video_id=label_idx,\n                frame_id=frame_ind,\n                vid_file=video_name,\n                img_shape=img.shape,\n                instances=instances,\n            )\n            frames.append(frame)\n\n        # pad bbox to max size\n        if self.use_tight_bbox:\n            # bound the max crop size to the user defined crop size\n            max_crop_h = crop_size if max_crop_h == 0 else min(max_crop_h, crop_size)\n            max_crop_w = crop_size if max_crop_w == 0 else min(max_crop_w, crop_size)\n            # gather all the crops\n            for frame in frames:\n                for instance in frame.instances:\n                    data_utils.pad_variable_size_crops(\n                        instance, (max_crop_h, max_crop_w)\n                    )\n        return frames\n\n    def __del__(self):\n        \"\"\"Handle file closing before garbage collection.\"\"\"\n        for reader in self.videos:\n            reader.close()\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.SleapDataset.__del__","title":"<code>__del__()</code>","text":"<p>Handle file closing before garbage collection.</p> Source code in <code>dreem/datasets/sleap_dataset.py</code> <pre><code>def __del__(self):\n    \"\"\"Handle file closing before garbage collection.\"\"\"\n    for reader in self.videos:\n        reader.close()\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.SleapDataset.__init__","title":"<code>__init__(slp_files, video_files, data_dirs=None, padding=5, crop_size=128, anchors='', chunk=True, clip_length=16, mode='train', handle_missing='centroid', augmentations=None, n_chunks=1.0, seed=None, verbose=False, normalize_image=True, max_batching_gap=15, use_tight_bbox=False, **kwargs)</code>","text":"<p>Initialize SleapDataset.</p> <p>Parameters:</p> Name Type Description Default <code>slp_files</code> <code>list[str]</code> <p>a list of .slp files storing tracking annotations</p> required <code>video_files</code> <code>list[str]</code> <p>a list of paths to video files</p> required <code>data_dirs</code> <code>Optional[list[str]]</code> <p>a path, or a list of paths to data directories. If provided, crop_size should be a list of integers with the same length as data_dirs.</p> <code>None</code> <code>padding</code> <code>int</code> <p>amount of padding around object crops</p> <code>5</code> <code>crop_size</code> <code>Union[int, list[int]]</code> <p>the size of the object crops. Can be either: - An integer specifying a single crop size for all objects - A list of integers specifying different crop sizes for different data directories</p> <code>128</code> <code>anchors</code> <code>int | list[str] | str</code> <p>One of:         * a string indicating a single node to center crops around         * a list of skeleton node names to be used as the center of crops         * an int indicating the number of anchors to randomly select     If unavailable then crop around the midpoint between all visible anchors.</p> <code>''</code> <code>chunk</code> <code>bool</code> <p>whether or not to chunk the dataset into batches</p> <code>True</code> <code>clip_length</code> <code>int</code> <p>the number of frames in each chunk</p> <code>16</code> <code>mode</code> <code>str</code> <p><code>train</code>, <code>val</code>, or <code>test</code>. Determines whether this dataset is used for training, validation/testing/inference.</p> <code>'train'</code> <code>handle_missing</code> <code>str</code> <p>how to handle missing single nodes. one of <code>[\"drop\", \"ignore\", \"centroid\"]</code>.             if \"drop\" then we dont include instances which are missing the <code>anchor</code>.             if \"ignore\" then we use a mask instead of a crop and nan centroids/bboxes.             if \"centroid\" then we default to the pose centroid as the node to crop around.</p> <code>'centroid'</code> <code>augmentations</code> <code>dict | None</code> <p>An optional dict mapping augmentations to parameters. The keys should map directly to augmentation classes in albumentations. Example:     augmentations = {         'Rotate': {'limit': [-90, 90], 'p': 0.5},         'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0, 'p': 0.2},         'RandomContrast': {'limit': 0.2, 'p': 0.6}     }</p> <code>None</code> <code>n_chunks</code> <code>int | float</code> <p>Number of chunks to subsample from. Can either a fraction of the dataset (ie (0,1.0]) or number of chunks</p> <code>1.0</code> <code>seed</code> <code>int | None</code> <p>set a seed for reproducibility</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>boolean representing whether to print</p> <code>False</code> <code>normalize_image</code> <code>bool</code> <p>whether to normalize the image to [0, 1]</p> <code>True</code> <code>max_batching_gap</code> <code>int</code> <p>the max number of frames that can be unlabelled before starting a new batch</p> <code>15</code> <code>use_tight_bbox</code> <code>bool</code> <p>whether to use tight bounding box (around keypoints) instead of the default square bounding box</p> <code>False</code> Source code in <code>dreem/datasets/sleap_dataset.py</code> <pre><code>def __init__(\n    self,\n    slp_files: list[str],\n    video_files: list[str],\n    data_dirs: Optional[list[str]] = None,\n    padding: int = 5,\n    crop_size: Union[int, list[int]] = 128,\n    anchors: int | list[str] | str = \"\",\n    chunk: bool = True,\n    clip_length: int = 16,\n    mode: str = \"train\",\n    handle_missing: str = \"centroid\",\n    augmentations: dict | None = None,\n    n_chunks: int | float = 1.0,\n    seed: int | None = None,\n    verbose: bool = False,\n    normalize_image: bool = True,\n    max_batching_gap: int = 15,\n    use_tight_bbox: bool = False,\n    **kwargs,\n):\n    \"\"\"Initialize SleapDataset.\n\n    Args:\n        slp_files: a list of .slp files storing tracking annotations\n        video_files: a list of paths to video files\n        data_dirs: a path, or a list of paths to data directories. If provided, crop_size should be a list of integers\n            with the same length as data_dirs.\n        padding: amount of padding around object crops\n        crop_size: the size of the object crops. Can be either:\n            - An integer specifying a single crop size for all objects\n            - A list of integers specifying different crop sizes for different data directories\n        anchors: One of:\n                    * a string indicating a single node to center crops around\n                    * a list of skeleton node names to be used as the center of crops\n                    * an int indicating the number of anchors to randomly select\n                If unavailable then crop around the midpoint between all visible anchors.\n        chunk: whether or not to chunk the dataset into batches\n        clip_length: the number of frames in each chunk\n        mode: `train`, `val`, or `test`. Determines whether this dataset is used for\n            training, validation/testing/inference.\n        handle_missing: how to handle missing single nodes. one of `[\"drop\", \"ignore\", \"centroid\"]`.\n                        if \"drop\" then we dont include instances which are missing the `anchor`.\n                        if \"ignore\" then we use a mask instead of a crop and nan centroids/bboxes.\n                        if \"centroid\" then we default to the pose centroid as the node to crop around.\n        augmentations: An optional dict mapping augmentations to parameters. The keys\n            should map directly to augmentation classes in albumentations. Example:\n                augmentations = {\n                    'Rotate': {'limit': [-90, 90], 'p': 0.5},\n                    'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0, 'p': 0.2},\n                    'RandomContrast': {'limit': 0.2, 'p': 0.6}\n                }\n        n_chunks: Number of chunks to subsample from.\n            Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n        seed: set a seed for reproducibility\n        verbose: boolean representing whether to print\n        normalize_image: whether to normalize the image to [0, 1]\n        max_batching_gap: the max number of frames that can be unlabelled before starting a new batch\n        use_tight_bbox: whether to use tight bounding box (around keypoints) instead of the default square bounding box\n    \"\"\"\n    super().__init__(\n        slp_files,\n        video_files,\n        padding,\n        crop_size,\n        chunk,\n        clip_length,\n        mode,\n        augmentations,\n        n_chunks,\n        seed,\n    )\n\n    self.slp_files = slp_files\n    self.data_dirs = data_dirs\n    self.video_files = video_files\n    self.padding = padding\n    self.crop_size = crop_size\n    self.chunk = chunk\n    self.clip_length = clip_length\n    self.mode = mode.lower()\n    self.handle_missing = handle_missing.lower()\n    self.n_chunks = n_chunks\n    self.seed = seed\n    self.normalize_image = normalize_image\n    self.max_batching_gap = max_batching_gap\n    self.use_tight_bbox = use_tight_bbox\n\n    if isinstance(anchors, int):\n        self.anchors = anchors\n    elif isinstance(anchors, str):\n        self.anchors = [anchors]\n    else:\n        self.anchors = anchors\n\n    if not isinstance(self.data_dirs, list):\n        self.data_dirs = [self.data_dirs]\n\n    if not isinstance(self.crop_size, list):\n        # make a list so its handled consistently if multiple crops are used\n        if len(self.data_dirs) &gt; 0:  # for test mode, data_dirs is []\n            self.crop_size = [self.crop_size] * len(self.data_dirs)\n        else:\n            self.crop_size = [self.crop_size]\n\n    if len(self.data_dirs) &gt; 0 and len(self.crop_size) != len(self.data_dirs):\n        raise ValueError(\n            f\"If a list of crop sizes or data directories are given,\"\n            f\"they must have the same length but got {len(self.crop_size)} \"\n            f\"and {len(self.data_dirs)}\"\n        )\n\n    if (\n        isinstance(self.anchors, list) and len(self.anchors) == 0\n    ) or self.anchors == 0:\n        raise ValueError(f\"Must provide at least one anchor but got {self.anchors}\")\n\n    self.verbose = verbose\n\n    # if self.seed is not None:\n    #     np.random.seed(self.seed)\n\n    # load_slp is a wrapper around sio.load_slp for frame gap checks\n    self.labels = []\n    self.annotated_segments = {}\n    for slp_file in self.slp_files:\n        labels, annotated_segments = data_utils.load_slp(slp_file)\n        self.labels.append(labels)\n        self.annotated_segments[slp_file] = annotated_segments\n\n    self.videos = [imageio.get_reader(vid_file) for vid_file in self.vid_files]\n    # do we need this? would need to update with sleap-io\n\n    # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n    # used in call to get_instances()\n    self.create_chunks_slp()\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.SleapDataset.get_indices","title":"<code>get_indices(idx)</code>","text":"<p>Retrieve label and frame indices given batch index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the batch.</p> required Source code in <code>dreem/datasets/sleap_dataset.py</code> <pre><code>def get_indices(self, idx: int) -&gt; tuple:\n    \"\"\"Retrieve label and frame indices given batch index.\n\n    Args:\n        idx: the index of the batch.\n    \"\"\"\n    return self.label_idx[idx], self.chunked_frame_idx[idx]\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.SleapDataset.get_instances","title":"<code>get_instances(label_idx, frame_idx)</code>","text":"<p>Get an element of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>label_idx</code> <code>list[int]</code> <p>index of the labels</p> required <code>frame_idx</code> <code>Tensor</code> <p>indices of the frames to load in to the batch</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A list of <code>dreem.io.Frame</code> objects containing metadata and instance data for the batch/clip.</p> Source code in <code>dreem/datasets/sleap_dataset.py</code> <pre><code>def get_instances(\n    self, label_idx: list[int], frame_idx: torch.Tensor\n) -&gt; list[Frame]:\n    \"\"\"Get an element of the dataset.\n\n    Args:\n        label_idx: index of the labels\n        frame_idx: indices of the frames to load in to the batch\n\n    Returns:\n        A list of `dreem.io.Frame` objects containing metadata and instance data for the batch/clip.\n\n    \"\"\"\n    sleap_labels_obj = self.labels[label_idx]\n    video_name = self.video_files[label_idx]\n\n    # get the correct crop size based on the video\n    video_par_path = Path(video_name).parent\n    if len(self.data_dirs) &gt; 0:\n        crop_size = self.crop_size[0]\n        for j, data_dir in enumerate(self.data_dirs):\n            if Path(data_dir) == video_par_path:\n                crop_size = self.crop_size[j]\n                break\n    else:\n        crop_size = self.crop_size[0]\n\n    vid_reader = self.videos[label_idx]\n\n    skeleton = sleap_labels_obj.skeletons[-1]\n\n    frames = []\n    max_crop_h, max_crop_w = 0, 0\n    for i, frame_ind in enumerate(frame_idx):\n        (\n            instances,\n            gt_track_ids,\n            poses,\n            shown_poses,\n            point_scores,\n            instance_score,\n        ) = ([], [], [], [], [], [])\n\n        frame_ind = int(frame_ind)\n\n        # sleap-io method for indexing a Labels() object based on the frame's index\n        lf = sleap_labels_obj[(sleap_labels_obj.video, frame_ind)]\n        if frame_ind != lf.frame_idx:\n            logger.warning(f\"Frame index mismatch: {frame_ind} != {lf.frame_idx}\")\n\n        try:\n            img = vid_reader.get_data(int(frame_ind))\n        except IndexError as e:\n            logger.warning(\n                f\"Could not read frame {frame_ind} from {video_name} due to {e}\"\n            )\n            continue\n\n        if len(img.shape) == 2:\n            img = img.expand_dims(-1)\n        h, w, c = img.shape\n\n        if c == 1:\n            img = np.concatenate(\n                [img, img, img], axis=-1\n            )  # convert to grayscale to rgb\n\n        if np.issubdtype(img.dtype, np.integer):  # convert int to float\n            img = img.astype(np.float32)\n            if self.normalize_image:\n                img = img / 255\n\n        n_instances_dropped = 0\n\n        gt_instances = []\n        # don't load instances that have been 'greyed out' i.e. all nans for keypoints\n        for inst in lf.instances:\n            pts = np.array([p for p in inst.numpy()])\n            if np.isnan(pts).all():\n                continue\n            else:\n                gt_instances.append(inst)\n\n        dict_instances = {}\n        no_track_instances = []\n        for instance in gt_instances:\n            if instance.track is not None:\n                gt_track_id = sleap_labels_obj.tracks.index(instance.track)\n                if gt_track_id not in dict_instances:\n                    dict_instances[gt_track_id] = instance\n                else:\n                    existing_instance = dict_instances[gt_track_id]\n                    # if existing is PredictedInstance and current is not, then current is a UserInstance and should be used\n                    if isinstance(\n                        existing_instance, sio.PredictedInstance\n                    ) and not isinstance(instance, sio.PredictedInstance):\n                        dict_instances[gt_track_id] = instance\n            else:\n                no_track_instances.append(instance)\n\n        gt_instances = list(dict_instances.values()) + no_track_instances\n\n        if self.mode == \"train\":\n            np.random.shuffle(gt_instances)\n\n        for instance in gt_instances:\n            if (\n                np.random.uniform() &lt; self.instance_dropout[\"p\"]\n                and n_instances_dropped &lt; self.instance_dropout[\"n\"]\n            ):\n                n_instances_dropped += 1\n                continue\n\n            if instance.track is not None:\n                gt_track_id = sleap_labels_obj.tracks.index(instance.track)\n            else:\n                gt_track_id = -1\n            gt_track_ids.append(gt_track_id)\n\n            poses.append(\n                dict(\n                    zip(\n                        [n.name for n in instance.skeleton.nodes],\n                        [p for p in instance.numpy()],\n                    )\n                )\n            )\n\n            shown_poses = [\n                {\n                    key: val\n                    for key, val in instance.items()\n                    if not np.isnan(val).any()\n                }\n                for instance in poses\n            ]\n\n            point_scores.append(\n                np.array(\n                    [\n                        (\n                            1.0  # point scores not reliably available in sleap io PredictedPointsArray\n                            # point.score\n                            # if isinstance(point, sio.PredictedPoint)\n                            # else 1.0\n                        )\n                        for point in instance.numpy()\n                    ]\n                )\n            )\n            if isinstance(instance, sio.PredictedInstance):\n                instance_score.append(instance.score)\n            else:\n                instance_score.append(1.0)\n        # augmentations\n        if self.augmentations is not None:\n            for transform in self.augmentations:\n                if isinstance(transform, A.CoarseDropout):\n                    transform.fill_value = random.randint(0, 255)\n\n            if shown_poses:\n                keypoints = np.vstack([list(s.values()) for s in shown_poses])\n\n            else:\n                keypoints = []\n\n            augmented = self.augmentations(image=img, keypoints=keypoints)\n\n            img, aug_poses = augmented[\"image\"], augmented[\"keypoints\"]\n\n            aug_poses = [\n                arr\n                for arr in np.split(\n                    np.array(aug_poses),\n                    np.array([len(s) for s in shown_poses]).cumsum(),\n                )\n                if arr.size != 0\n            ]\n\n            aug_poses = [\n                dict(zip(list(pose_dict.keys()), aug_pose_arr.tolist()))\n                for aug_pose_arr, pose_dict in zip(aug_poses, shown_poses)\n            ]\n\n            _ = [\n                pose.update(aug_pose)\n                for pose, aug_pose in zip(shown_poses, aug_poses)\n            ]\n\n        img = tvf.to_tensor(img)\n\n        for j in range(len(gt_track_ids)):\n            pose = shown_poses[j]\n\n            \"\"\"Check for anchor\"\"\"\n            crops = []\n            boxes = []\n            centroids = {}\n\n            if isinstance(self.anchors, int):\n                anchors_to_choose = list(pose.keys()) + [\"midpoint\"]\n                anchors = np.random.choice(anchors_to_choose, self.anchors)\n            else:\n                anchors = self.anchors\n\n            dropped_anchors = self.node_dropout(anchors)\n\n            for anchor in anchors:\n                if anchor in dropped_anchors:\n                    centroid = np.array([np.nan, np.nan])\n\n                elif anchor == \"midpoint\" or anchor == \"centroid\":\n                    centroid = np.nanmean(np.array(list(pose.values())), axis=0)\n\n                elif anchor in pose:\n                    centroid = np.array(pose[anchor])\n                    if np.isnan(centroid).any():\n                        centroid = np.array([np.nan, np.nan])\n\n                elif (\n                    anchor not in pose\n                    and len(anchors) == 1\n                    and self.handle_missing == \"centroid\"\n                ):\n                    anchor = \"midpoint\"\n                    centroid = np.nanmean(np.array(list(pose.values())), axis=0)\n\n                else:\n                    centroid = np.array([np.nan, np.nan])\n\n                if np.isnan(centroid).all():\n                    bbox = torch.tensor([np.nan, np.nan, np.nan, np.nan])\n\n                else:\n                    if self.use_tight_bbox and len(pose) &gt; 1:\n                        # tight bbox\n                        # dont allow this for centroid-only poses!\n                        arr_pose = np.array(list(pose.values()))\n                        # note bbox will be a different size for each instance; padded at the end of the loop\n                        bbox = data_utils.get_tight_bbox(arr_pose)\n\n                    else:\n                        bbox = data_utils.pad_bbox(\n                            data_utils.get_bbox(centroid, crop_size),\n                            padding=self.padding,\n                        )\n\n                if bbox.isnan().all():\n                    crop = torch.zeros(\n                        c,\n                        crop_size + 2 * self.padding,\n                        crop_size + 2 * self.padding,\n                        dtype=img.dtype,\n                    )\n                else:\n                    crop = data_utils.crop_bbox(img, bbox)\n\n                crops.append(crop)\n                # get max h,w for padding for tight bboxes\n                c, h, w = crop.shape\n                if h &gt; max_crop_h:\n                    max_crop_h = h\n                if w &gt; max_crop_w:\n                    max_crop_w = w\n\n                centroids[anchor] = centroid\n                boxes.append(bbox)\n\n            if len(crops) &gt; 0:\n                crops = torch.concat(crops, dim=0)\n\n            if len(boxes) &gt; 0:\n                boxes = torch.stack(boxes, dim=0)\n\n            if self.handle_missing == \"drop\" and boxes.isnan().any():\n                continue\n\n            instance = Instance(\n                gt_track_id=gt_track_ids[j],\n                pred_track_id=-1,\n                crop=crops,\n                centroid=centroids,\n                bbox=boxes,\n                skeleton=skeleton,\n                pose=poses[j],\n                point_scores=point_scores[j],\n                instance_score=instance_score[j],\n            )\n\n            instances.append(instance)\n\n        frame = Frame(\n            video_id=label_idx,\n            frame_id=frame_ind,\n            vid_file=video_name,\n            img_shape=img.shape,\n            instances=instances,\n        )\n        frames.append(frame)\n\n    # pad bbox to max size\n    if self.use_tight_bbox:\n        # bound the max crop size to the user defined crop size\n        max_crop_h = crop_size if max_crop_h == 0 else min(max_crop_h, crop_size)\n        max_crop_w = crop_size if max_crop_w == 0 else min(max_crop_w, crop_size)\n        # gather all the crops\n        for frame in frames:\n            for instance in frame.instances:\n                data_utils.pad_variable_size_crops(\n                    instance, (max_crop_h, max_crop_w)\n                )\n    return frames\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.TrackingDataset","title":"<code>TrackingDataset</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>Lightning dataset used to load dataloaders for train, test and validation.</p> <p>Nice for wrapping around other data formats.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize tracking dataset.</p> <code>setup</code> <p>Set up lightning dataset.</p> <code>test_dataloader</code> <p>Get.</p> <code>train_dataloader</code> <p>Get train_dataloader.</p> <code>val_dataloader</code> <p>Get val dataloader.</p> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>class TrackingDataset(LightningDataModule):\n    \"\"\"Lightning dataset used to load dataloaders for train, test and validation.\n\n    Nice for wrapping around other data formats.\n    \"\"\"\n\n    def __init__(\n        self,\n        train_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n        train_dl: DataLoader | None = None,\n        val_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n        val_dl: DataLoader | None = None,\n        test_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n        test_dl: DataLoader | None = None,\n    ):\n        \"\"\"Initialize tracking dataset.\n\n        Args:\n            train_ds: Sleap or Microscopy training Dataset\n            train_dl: Training dataloader. Only used for overriding `train_dataloader`.\n            val_ds: Sleap or Microscopy Validation set\n            val_dl : Validation dataloader. Only used for overriding `val_dataloader`.\n            test_ds: Sleap or Microscopy test set\n            test_dl : Test dataloader. Only used for overriding `test_dataloader`.\n        \"\"\"\n        super().__init__()\n        self.train_ds = train_ds\n        self.train_dl = train_dl\n        self.val_ds = val_ds\n        self.val_dl = val_dl\n        self.test_ds = test_ds\n        self.test_dl = test_dl\n\n    def setup(self, stage=None):\n        \"\"\"Set up lightning dataset.\n\n        UNUSED.\n        \"\"\"\n        pass\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"Get train_dataloader.\n\n        Returns: The Training Dataloader.\n        \"\"\"\n        if self.train_dl is None and self.train_ds is None:\n            return None\n        elif self.train_dl is None:\n            return DataLoader(\n                self.train_ds,\n                batch_size=1,\n                shuffle=True,\n                pin_memory=False,\n                collate_fn=self.train_ds.no_batching_fn,\n                num_workers=0,\n                generator=(\n                    torch.Generator(device=\"cuda\")\n                    if torch.cuda.is_available()\n                    else torch.Generator()\n                ),\n            )\n        else:\n            return self.train_dl\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"Get val dataloader.\n\n        Returns: The validation dataloader.\n        \"\"\"\n        if self.val_dl is None and self.val_ds is None:\n            return None\n        elif self.val_dl is None:\n            return DataLoader(\n                self.val_ds,\n                batch_size=1,\n                shuffle=False,\n                pin_memory=0,\n                collate_fn=self.train_ds.no_batching_fn,\n                num_workers=False,\n                generator=None,\n            )\n        else:\n            return self.val_dl\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"Get.\n\n        Returns: The test dataloader\n        \"\"\"\n        if self.test_dl is None and self.test_ds is None:\n            return None\n        elif self.test_dl is None:\n            return DataLoader(\n                self.test_ds,\n                batch_size=1,\n                shuffle=False,\n                pin_memory=0,\n                collate_fn=self.train_ds.no_batching_fn,\n                num_workers=False,\n                generator=None,\n            )\n        else:\n            return self.test_dl\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.TrackingDataset.__init__","title":"<code>__init__(train_ds=None, train_dl=None, val_ds=None, val_dl=None, test_ds=None, test_dl=None)</code>","text":"<p>Initialize tracking dataset.</p> <p>Parameters:</p> Name Type Description Default <code>train_ds</code> <code>SleapDataset | MicroscopyDataset | CellTrackingDataset | None</code> <p>Sleap or Microscopy training Dataset</p> <code>None</code> <code>train_dl</code> <code>DataLoader | None</code> <p>Training dataloader. Only used for overriding <code>train_dataloader</code>.</p> <code>None</code> <code>val_ds</code> <code>SleapDataset | MicroscopyDataset | CellTrackingDataset | None</code> <p>Sleap or Microscopy Validation set</p> <code>None</code> <code>val_dl</code> <p>Validation dataloader. Only used for overriding <code>val_dataloader</code>.</p> <code>None</code> <code>test_ds</code> <code>SleapDataset | MicroscopyDataset | CellTrackingDataset | None</code> <p>Sleap or Microscopy test set</p> <code>None</code> <code>test_dl</code> <p>Test dataloader. Only used for overriding <code>test_dataloader</code>.</p> <code>None</code> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>def __init__(\n    self,\n    train_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n    train_dl: DataLoader | None = None,\n    val_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n    val_dl: DataLoader | None = None,\n    test_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n    test_dl: DataLoader | None = None,\n):\n    \"\"\"Initialize tracking dataset.\n\n    Args:\n        train_ds: Sleap or Microscopy training Dataset\n        train_dl: Training dataloader. Only used for overriding `train_dataloader`.\n        val_ds: Sleap or Microscopy Validation set\n        val_dl : Validation dataloader. Only used for overriding `val_dataloader`.\n        test_ds: Sleap or Microscopy test set\n        test_dl : Test dataloader. Only used for overriding `test_dataloader`.\n    \"\"\"\n    super().__init__()\n    self.train_ds = train_ds\n    self.train_dl = train_dl\n    self.val_ds = val_ds\n    self.val_dl = val_dl\n    self.test_ds = test_ds\n    self.test_dl = test_dl\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.TrackingDataset.setup","title":"<code>setup(stage=None)</code>","text":"<p>Set up lightning dataset.</p> <p>UNUSED.</p> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>def setup(self, stage=None):\n    \"\"\"Set up lightning dataset.\n\n    UNUSED.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.TrackingDataset.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Get.</p> <p>Returns: The test dataloader</p> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"Get.\n\n    Returns: The test dataloader\n    \"\"\"\n    if self.test_dl is None and self.test_ds is None:\n        return None\n    elif self.test_dl is None:\n        return DataLoader(\n            self.test_ds,\n            batch_size=1,\n            shuffle=False,\n            pin_memory=0,\n            collate_fn=self.train_ds.no_batching_fn,\n            num_workers=False,\n            generator=None,\n        )\n    else:\n        return self.test_dl\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.TrackingDataset.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Get train_dataloader.</p> <p>Returns: The Training Dataloader.</p> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"Get train_dataloader.\n\n    Returns: The Training Dataloader.\n    \"\"\"\n    if self.train_dl is None and self.train_ds is None:\n        return None\n    elif self.train_dl is None:\n        return DataLoader(\n            self.train_ds,\n            batch_size=1,\n            shuffle=True,\n            pin_memory=False,\n            collate_fn=self.train_ds.no_batching_fn,\n            num_workers=0,\n            generator=(\n                torch.Generator(device=\"cuda\")\n                if torch.cuda.is_available()\n                else torch.Generator()\n            ),\n        )\n    else:\n        return self.train_dl\n</code></pre>"},{"location":"reference/dreem/datasets/#dreem.datasets.TrackingDataset.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Get val dataloader.</p> <p>Returns: The validation dataloader.</p> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"Get val dataloader.\n\n    Returns: The validation dataloader.\n    \"\"\"\n    if self.val_dl is None and self.val_ds is None:\n        return None\n    elif self.val_dl is None:\n        return DataLoader(\n            self.val_ds,\n            batch_size=1,\n            shuffle=False,\n            pin_memory=0,\n            collate_fn=self.train_ds.no_batching_fn,\n            num_workers=False,\n            generator=None,\n        )\n    else:\n        return self.val_dl\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/","title":"base_dataset","text":""},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset","title":"<code>dreem.datasets.base_dataset</code>","text":"<p>Module containing logic for loading datasets.</p> <p>Classes:</p> Name Description <code>BaseDataset</code> <p>Base Dataset for microscopy and sleap datasets to override.</p>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset","title":"<code>BaseDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Base Dataset for microscopy and sleap datasets to override.</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Get an element of the dataset.</p> <code>__init__</code> <p>Initialize Dataset.</p> <code>__len__</code> <p>Get the size of the dataset.</p> <code>create_chunks_other</code> <p>Legacy chunking logic. Does not support unannotated segments.</p> <code>create_chunks_slp</code> <p>Get indexing for data.</p> <code>get_indices</code> <p>Retrieve label and frame indices given batch index.</p> <code>get_instances</code> <p>Build chunk of frames.</p> <code>no_batching_fn</code> <p>Collate function used to overwrite dataloader batching function.</p> <code>process_segments</code> <p>Process segments to stitch.</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>class BaseDataset(Dataset):\n    \"\"\"Base Dataset for microscopy and sleap datasets to override.\"\"\"\n\n    def __init__(\n        self,\n        label_files: list[str],\n        vid_files: list[str],\n        padding: int,\n        crop_size: Union[int, list[int]],\n        chunk: bool,\n        clip_length: int,\n        mode: str,\n        augmentations: dict | None = None,\n        n_chunks: int | float = 1.0,\n        seed: int | None = None,\n        gt_list: str | None = None,\n    ):\n        \"\"\"Initialize Dataset.\n\n        Args:\n            label_files: a list of paths to label files. Should at least contain\n                detections for inference, detections + tracks for training.\n            vid_files: list of paths to video files.\n            padding: amount of padding around object crops\n            crop_size: the size of the object crops\n            chunk: whether or not to chunk the dataset into batches\n            clip_length: the number of frames in each chunk\n            mode: `train` or `val`. Determines whether this dataset is used for\n                training or validation. Currently doesn't affect dataset logic\n            augmentations: An optional dict mapping augmentations to parameters.\n                See subclasses for details.\n            n_chunks: Number of chunks to subsample from.\n                Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n            seed: set a seed for reproducibility\n            gt_list: An optional path to .txt file containing ground truth for\n                cell tracking challenge datasets.\n        \"\"\"\n        self.vid_files = vid_files\n        self.label_files = label_files\n        self.padding = padding\n        self.crop_size = crop_size\n        self.chunk = chunk\n        self.clip_length = clip_length\n        self.mode = mode\n        self.n_chunks = n_chunks\n        self.seed = seed\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n\n        if augmentations and self.mode == \"train\":\n            self.instance_dropout = augmentations.pop(\n                \"InstanceDropout\", {\"p\": 0.0, \"n\": 0}\n            )\n            self.node_dropout = data_utils.NodeDropout(\n                **augmentations.pop(\"NodeDropout\", {\"p\": 0.0, \"n\": 0})\n            )\n            self.augmentations = data_utils.build_augmentations(augmentations)\n        else:\n            self.instance_dropout = {\"p\": 0.0, \"n\": 0}\n            self.node_dropout = data_utils.NodeDropout(p=0.0, n=0)\n            self.augmentations = None\n\n        # Initialize in subclasses\n        self.frame_idx = None\n        self.labels = None\n        self.gt_list = None\n\n    def process_segments(\n        self, i: int, segments_to_stitch: list[torch.Tensor], clip_length: int\n    ) -&gt; None:\n        \"\"\"Process segments to stitch.\n\n        Modifies state variables chunked_frame_idx and label_idx.\n\n        Args:\n            segments_to_stitch: list of segments to stitch\n            i: index of the video\n            clip_length: the number of frames in each chunk\n        Returns: None\n        \"\"\"\n        stitched_segment = torch.cat(segments_to_stitch)\n        frame_idx_split = torch.split(stitched_segment, clip_length)\n        self.chunked_frame_idx.extend(frame_idx_split)\n        self.label_idx.extend(len(frame_idx_split) * [i])\n\n    def create_chunks_slp(self) -&gt; None:\n        \"\"\"Get indexing for data.\n\n        Creates both indexes for selecting dataset (label_idx) and frame in\n        dataset (chunked_frame_idx). If chunking is false, we index directly\n        using the frame ids. Setting chunking to true creates a list of lists\n        containing chunk frames for indexing. This is useful for computational\n        efficiency and data shuffling. To be called by subclass __init__()\n        \"\"\"\n        self.chunked_frame_idx, self.label_idx = [], []\n        # go through each slp file and create chunks that respect max_batching_gap\n        for i, slp_file in enumerate(self.label_files):\n            annotated_segments = self.annotated_segments[slp_file]\n            segments_to_stitch = []\n            prev_end = annotated_segments[0][1]  # end of first segment\n            for start, end in annotated_segments:\n                # check if the start of current segment is within\n                # batching_max_gap of end of previous\n                if (\n                    (int(start) - int(prev_end) &lt; self.max_batching_gap)\n                    or not self.chunk\n                ):  # also takes care of first segment as start &lt; prev_end\n                    segments_to_stitch.append(torch.arange(start, end + 1))\n                    prev_end = end\n                else:\n                    # stitch previous set of segments before creating a new chunk\n                    self.process_segments(i, segments_to_stitch, self.clip_length)\n                    # reset segments_to_stitch as we are starting a new chunk\n                    segments_to_stitch = [torch.arange(start, end + 1)]\n                    prev_end = end\n\n            if not self.chunk:\n                self.process_segments(\n                    i, segments_to_stitch, self.labels[i].video.shape[0]\n                )\n            else:\n                # add last chunk after the loop\n                if segments_to_stitch:\n                    self.process_segments(i, segments_to_stitch, self.clip_length)\n\n        if self.n_chunks &gt; 0 and self.n_chunks &lt;= 1.0:\n            n_chunks = int(self.n_chunks * len(self.chunked_frame_idx))\n\n        elif self.n_chunks &lt;= len(self.chunked_frame_idx):\n            n_chunks = int(self.n_chunks)\n\n        else:\n            n_chunks = len(self.chunked_frame_idx)\n\n        if n_chunks &gt; 0 and n_chunks &lt; len(self.chunked_frame_idx):\n            sample_idx = np.random.choice(\n                np.arange(len(self.chunked_frame_idx)), n_chunks, replace=False\n            )\n\n            self.chunked_frame_idx = [self.chunked_frame_idx[i] for i in sample_idx]\n\n            self.label_idx = [self.label_idx[i] for i in sample_idx]\n\n        # workaround for empty batch bug (needs to be changed).\n        # Check for batch with with only 1/10 size of clip length.\n        # Arbitrary thresholds\n        remove_idx = []\n        for i, frame_chunk in enumerate(self.chunked_frame_idx):\n            if (\n                len(frame_chunk) &lt;= min(int(self.clip_length / 10), 5)\n                # and frame_chunk[-1] % self.clip_length == 0\n            ):\n                logger.warning(\n                    f\"Warning: Batch containing frames {frame_chunk} from video \"\n                    f\"{self.vid_files[self.label_idx[i]]} has {len(frame_chunk)} frames. \"\n                    f\"Removing to avoid empty batch possibility with failed frame loading\"\n                )\n                remove_idx.append(i)\n        if len(remove_idx) &gt; 0:\n            for i in sorted(remove_idx, reverse=True):\n                self.chunked_frame_idx.pop(i)\n                self.label_idx.pop(i)\n\n    def create_chunks_other(self) -&gt; None:\n        \"\"\"Legacy chunking logic. Does not support unannotated segments.\n\n        Creates both indexes for selecting dataset (label_idx) and frame in\n        dataset (chunked_frame_idx). If chunking is false, we index directly\n        using the frame ids. Setting chunking to true creates a list of lists\n        containing chunk frames for indexing. This is useful for computational\n        efficiency and data shuffling. To be called by subclass __init__()\n        \"\"\"\n        if self.chunk:\n            self.chunked_frame_idx, self.label_idx = [], []\n            for i, frame_idx in enumerate(self.frame_idx):\n                frame_idx_split = torch.split(frame_idx, self.clip_length)\n                self.chunked_frame_idx.extend(frame_idx_split)\n                self.label_idx.extend(len(frame_idx_split) * [i])\n\n            if self.n_chunks &gt; 0 and self.n_chunks &lt;= 1.0:\n                n_chunks = int(self.n_chunks * len(self.chunked_frame_idx))\n\n            elif self.n_chunks &lt;= len(self.chunked_frame_idx):\n                n_chunks = int(self.n_chunks)\n\n            else:\n                n_chunks = len(self.chunked_frame_idx)\n\n            if n_chunks &gt; 0 and n_chunks &lt; len(self.chunked_frame_idx):\n                sample_idx = np.random.choice(\n                    np.arange(len(self.chunked_frame_idx)), n_chunks, replace=False\n                )\n\n                self.chunked_frame_idx = [self.chunked_frame_idx[i] for i in sample_idx]\n\n                self.label_idx = [self.label_idx[i] for i in sample_idx]\n\n            # workaround for empty batch bug (needs to be changed).\n            # Check for batch with with only 1/10 size of clip length.\n            # Arbitrary thresholds\n            remove_idx = []\n            for i, frame_chunk in enumerate(self.chunked_frame_idx):\n                if (\n                    len(frame_chunk) &lt;= min(int(self.clip_length / 10), 5)\n                    # and frame_chunk[-1] % self.clip_length == 0\n                ):\n                    logger.warning(\n                        f\"Warning: Batch containing frames {frame_chunk} from video {self.vid_files[self.label_idx[i]]} has {len(frame_chunk)} frames. Removing to avoid empty batch possibility with failed frame loading\"\n                    )\n                    remove_idx.append(i)\n            if len(remove_idx) &gt; 0:\n                for i in sorted(remove_idx, reverse=True):\n                    self.chunked_frame_idx.pop(i)\n                    self.label_idx.pop(i)\n\n        else:\n            self.chunked_frame_idx = self.frame_idx\n            self.label_idx = [i for i in range(len(self.labels))]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the size of the dataset.\n\n        Returns:\n            the size or the number of chunks in the dataset\n        \"\"\"\n        return len(self.chunked_frame_idx)\n\n    def no_batching_fn(self, batch: list[Frame]) -&gt; list[Frame]:\n        \"\"\"Collate function used to overwrite dataloader batching function.\n\n        Args:\n            batch: the chunk of frames to be returned\n\n        Returns:\n            The batch\n        \"\"\"\n        return batch\n\n    def __getitem__(self, idx: int) -&gt; list[Frame]:\n        \"\"\"Get an element of the dataset.\n\n        Args:\n            idx: the index of the batch. Note this is not the index of the video\n                or the frame.\n\n        Returns:\n            A list of `Frame`s in the chunk containing the metadata + instance features.\n        \"\"\"\n        label_idx, frame_idx = self.get_indices(idx)\n\n        return self.get_instances(label_idx, frame_idx)\n\n    def get_indices(self, idx: int):\n        \"\"\"Retrieve label and frame indices given batch index.\n\n        This method should be implemented in any subclass of the BaseDataset.\n\n        Args:\n            idx: the index of the batch.\n\n        Raises:\n            NotImplementedError: If this method is not overridden in a subclass.\n        \"\"\"\n        raise NotImplementedError(\"Must be implemented in subclass\")\n\n    def get_instances(self, label_idx: list[int], frame_idx: list[int]):\n        \"\"\"Build chunk of frames.\n\n        This method should be implemented in any subclass of the BaseDataset.\n\n        Args:\n            label_idx: The index of the labels.\n            frame_idx: The index of the frames.\n\n        Raises:\n            NotImplementedError: If this method is not overridden in a subclass.\n        \"\"\"\n        raise NotImplementedError(\"Must be implemented in subclass\")\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get an element of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the batch. Note this is not the index of the video or the frame.</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A list of <code>Frame</code>s in the chunk containing the metadata + instance features.</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; list[Frame]:\n    \"\"\"Get an element of the dataset.\n\n    Args:\n        idx: the index of the batch. Note this is not the index of the video\n            or the frame.\n\n    Returns:\n        A list of `Frame`s in the chunk containing the metadata + instance features.\n    \"\"\"\n    label_idx, frame_idx = self.get_indices(idx)\n\n    return self.get_instances(label_idx, frame_idx)\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset.__init__","title":"<code>__init__(label_files, vid_files, padding, crop_size, chunk, clip_length, mode, augmentations=None, n_chunks=1.0, seed=None, gt_list=None)</code>","text":"<p>Initialize Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>label_files</code> <code>list[str]</code> <p>a list of paths to label files. Should at least contain detections for inference, detections + tracks for training.</p> required <code>vid_files</code> <code>list[str]</code> <p>list of paths to video files.</p> required <code>padding</code> <code>int</code> <p>amount of padding around object crops</p> required <code>crop_size</code> <code>Union[int, list[int]]</code> <p>the size of the object crops</p> required <code>chunk</code> <code>bool</code> <p>whether or not to chunk the dataset into batches</p> required <code>clip_length</code> <code>int</code> <p>the number of frames in each chunk</p> required <code>mode</code> <code>str</code> <p><code>train</code> or <code>val</code>. Determines whether this dataset is used for training or validation. Currently doesn't affect dataset logic</p> required <code>augmentations</code> <code>dict | None</code> <p>An optional dict mapping augmentations to parameters. See subclasses for details.</p> <code>None</code> <code>n_chunks</code> <code>int | float</code> <p>Number of chunks to subsample from. Can either a fraction of the dataset (ie (0,1.0]) or number of chunks</p> <code>1.0</code> <code>seed</code> <code>int | None</code> <p>set a seed for reproducibility</p> <code>None</code> <code>gt_list</code> <code>str | None</code> <p>An optional path to .txt file containing ground truth for cell tracking challenge datasets.</p> <code>None</code> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def __init__(\n    self,\n    label_files: list[str],\n    vid_files: list[str],\n    padding: int,\n    crop_size: Union[int, list[int]],\n    chunk: bool,\n    clip_length: int,\n    mode: str,\n    augmentations: dict | None = None,\n    n_chunks: int | float = 1.0,\n    seed: int | None = None,\n    gt_list: str | None = None,\n):\n    \"\"\"Initialize Dataset.\n\n    Args:\n        label_files: a list of paths to label files. Should at least contain\n            detections for inference, detections + tracks for training.\n        vid_files: list of paths to video files.\n        padding: amount of padding around object crops\n        crop_size: the size of the object crops\n        chunk: whether or not to chunk the dataset into batches\n        clip_length: the number of frames in each chunk\n        mode: `train` or `val`. Determines whether this dataset is used for\n            training or validation. Currently doesn't affect dataset logic\n        augmentations: An optional dict mapping augmentations to parameters.\n            See subclasses for details.\n        n_chunks: Number of chunks to subsample from.\n            Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n        seed: set a seed for reproducibility\n        gt_list: An optional path to .txt file containing ground truth for\n            cell tracking challenge datasets.\n    \"\"\"\n    self.vid_files = vid_files\n    self.label_files = label_files\n    self.padding = padding\n    self.crop_size = crop_size\n    self.chunk = chunk\n    self.clip_length = clip_length\n    self.mode = mode\n    self.n_chunks = n_chunks\n    self.seed = seed\n\n    if self.seed is not None:\n        np.random.seed(self.seed)\n\n    if augmentations and self.mode == \"train\":\n        self.instance_dropout = augmentations.pop(\n            \"InstanceDropout\", {\"p\": 0.0, \"n\": 0}\n        )\n        self.node_dropout = data_utils.NodeDropout(\n            **augmentations.pop(\"NodeDropout\", {\"p\": 0.0, \"n\": 0})\n        )\n        self.augmentations = data_utils.build_augmentations(augmentations)\n    else:\n        self.instance_dropout = {\"p\": 0.0, \"n\": 0}\n        self.node_dropout = data_utils.NodeDropout(p=0.0, n=0)\n        self.augmentations = None\n\n    # Initialize in subclasses\n    self.frame_idx = None\n    self.labels = None\n    self.gt_list = None\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset.__len__","title":"<code>__len__()</code>","text":"<p>Get the size of the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>the size or the number of chunks in the dataset</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the size of the dataset.\n\n    Returns:\n        the size or the number of chunks in the dataset\n    \"\"\"\n    return len(self.chunked_frame_idx)\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset.create_chunks_other","title":"<code>create_chunks_other()</code>","text":"<p>Legacy chunking logic. Does not support unannotated segments.</p> <p>Creates both indexes for selecting dataset (label_idx) and frame in dataset (chunked_frame_idx). If chunking is false, we index directly using the frame ids. Setting chunking to true creates a list of lists containing chunk frames for indexing. This is useful for computational efficiency and data shuffling. To be called by subclass init()</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def create_chunks_other(self) -&gt; None:\n    \"\"\"Legacy chunking logic. Does not support unannotated segments.\n\n    Creates both indexes for selecting dataset (label_idx) and frame in\n    dataset (chunked_frame_idx). If chunking is false, we index directly\n    using the frame ids. Setting chunking to true creates a list of lists\n    containing chunk frames for indexing. This is useful for computational\n    efficiency and data shuffling. To be called by subclass __init__()\n    \"\"\"\n    if self.chunk:\n        self.chunked_frame_idx, self.label_idx = [], []\n        for i, frame_idx in enumerate(self.frame_idx):\n            frame_idx_split = torch.split(frame_idx, self.clip_length)\n            self.chunked_frame_idx.extend(frame_idx_split)\n            self.label_idx.extend(len(frame_idx_split) * [i])\n\n        if self.n_chunks &gt; 0 and self.n_chunks &lt;= 1.0:\n            n_chunks = int(self.n_chunks * len(self.chunked_frame_idx))\n\n        elif self.n_chunks &lt;= len(self.chunked_frame_idx):\n            n_chunks = int(self.n_chunks)\n\n        else:\n            n_chunks = len(self.chunked_frame_idx)\n\n        if n_chunks &gt; 0 and n_chunks &lt; len(self.chunked_frame_idx):\n            sample_idx = np.random.choice(\n                np.arange(len(self.chunked_frame_idx)), n_chunks, replace=False\n            )\n\n            self.chunked_frame_idx = [self.chunked_frame_idx[i] for i in sample_idx]\n\n            self.label_idx = [self.label_idx[i] for i in sample_idx]\n\n        # workaround for empty batch bug (needs to be changed).\n        # Check for batch with with only 1/10 size of clip length.\n        # Arbitrary thresholds\n        remove_idx = []\n        for i, frame_chunk in enumerate(self.chunked_frame_idx):\n            if (\n                len(frame_chunk) &lt;= min(int(self.clip_length / 10), 5)\n                # and frame_chunk[-1] % self.clip_length == 0\n            ):\n                logger.warning(\n                    f\"Warning: Batch containing frames {frame_chunk} from video {self.vid_files[self.label_idx[i]]} has {len(frame_chunk)} frames. Removing to avoid empty batch possibility with failed frame loading\"\n                )\n                remove_idx.append(i)\n        if len(remove_idx) &gt; 0:\n            for i in sorted(remove_idx, reverse=True):\n                self.chunked_frame_idx.pop(i)\n                self.label_idx.pop(i)\n\n    else:\n        self.chunked_frame_idx = self.frame_idx\n        self.label_idx = [i for i in range(len(self.labels))]\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset.create_chunks_slp","title":"<code>create_chunks_slp()</code>","text":"<p>Get indexing for data.</p> <p>Creates both indexes for selecting dataset (label_idx) and frame in dataset (chunked_frame_idx). If chunking is false, we index directly using the frame ids. Setting chunking to true creates a list of lists containing chunk frames for indexing. This is useful for computational efficiency and data shuffling. To be called by subclass init()</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def create_chunks_slp(self) -&gt; None:\n    \"\"\"Get indexing for data.\n\n    Creates both indexes for selecting dataset (label_idx) and frame in\n    dataset (chunked_frame_idx). If chunking is false, we index directly\n    using the frame ids. Setting chunking to true creates a list of lists\n    containing chunk frames for indexing. This is useful for computational\n    efficiency and data shuffling. To be called by subclass __init__()\n    \"\"\"\n    self.chunked_frame_idx, self.label_idx = [], []\n    # go through each slp file and create chunks that respect max_batching_gap\n    for i, slp_file in enumerate(self.label_files):\n        annotated_segments = self.annotated_segments[slp_file]\n        segments_to_stitch = []\n        prev_end = annotated_segments[0][1]  # end of first segment\n        for start, end in annotated_segments:\n            # check if the start of current segment is within\n            # batching_max_gap of end of previous\n            if (\n                (int(start) - int(prev_end) &lt; self.max_batching_gap)\n                or not self.chunk\n            ):  # also takes care of first segment as start &lt; prev_end\n                segments_to_stitch.append(torch.arange(start, end + 1))\n                prev_end = end\n            else:\n                # stitch previous set of segments before creating a new chunk\n                self.process_segments(i, segments_to_stitch, self.clip_length)\n                # reset segments_to_stitch as we are starting a new chunk\n                segments_to_stitch = [torch.arange(start, end + 1)]\n                prev_end = end\n\n        if not self.chunk:\n            self.process_segments(\n                i, segments_to_stitch, self.labels[i].video.shape[0]\n            )\n        else:\n            # add last chunk after the loop\n            if segments_to_stitch:\n                self.process_segments(i, segments_to_stitch, self.clip_length)\n\n    if self.n_chunks &gt; 0 and self.n_chunks &lt;= 1.0:\n        n_chunks = int(self.n_chunks * len(self.chunked_frame_idx))\n\n    elif self.n_chunks &lt;= len(self.chunked_frame_idx):\n        n_chunks = int(self.n_chunks)\n\n    else:\n        n_chunks = len(self.chunked_frame_idx)\n\n    if n_chunks &gt; 0 and n_chunks &lt; len(self.chunked_frame_idx):\n        sample_idx = np.random.choice(\n            np.arange(len(self.chunked_frame_idx)), n_chunks, replace=False\n        )\n\n        self.chunked_frame_idx = [self.chunked_frame_idx[i] for i in sample_idx]\n\n        self.label_idx = [self.label_idx[i] for i in sample_idx]\n\n    # workaround for empty batch bug (needs to be changed).\n    # Check for batch with with only 1/10 size of clip length.\n    # Arbitrary thresholds\n    remove_idx = []\n    for i, frame_chunk in enumerate(self.chunked_frame_idx):\n        if (\n            len(frame_chunk) &lt;= min(int(self.clip_length / 10), 5)\n            # and frame_chunk[-1] % self.clip_length == 0\n        ):\n            logger.warning(\n                f\"Warning: Batch containing frames {frame_chunk} from video \"\n                f\"{self.vid_files[self.label_idx[i]]} has {len(frame_chunk)} frames. \"\n                f\"Removing to avoid empty batch possibility with failed frame loading\"\n            )\n            remove_idx.append(i)\n    if len(remove_idx) &gt; 0:\n        for i in sorted(remove_idx, reverse=True):\n            self.chunked_frame_idx.pop(i)\n            self.label_idx.pop(i)\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset.get_indices","title":"<code>get_indices(idx)</code>","text":"<p>Retrieve label and frame indices given batch index.</p> <p>This method should be implemented in any subclass of the BaseDataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the batch.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If this method is not overridden in a subclass.</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def get_indices(self, idx: int):\n    \"\"\"Retrieve label and frame indices given batch index.\n\n    This method should be implemented in any subclass of the BaseDataset.\n\n    Args:\n        idx: the index of the batch.\n\n    Raises:\n        NotImplementedError: If this method is not overridden in a subclass.\n    \"\"\"\n    raise NotImplementedError(\"Must be implemented in subclass\")\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset.get_instances","title":"<code>get_instances(label_idx, frame_idx)</code>","text":"<p>Build chunk of frames.</p> <p>This method should be implemented in any subclass of the BaseDataset.</p> <p>Parameters:</p> Name Type Description Default <code>label_idx</code> <code>list[int]</code> <p>The index of the labels.</p> required <code>frame_idx</code> <code>list[int]</code> <p>The index of the frames.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If this method is not overridden in a subclass.</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def get_instances(self, label_idx: list[int], frame_idx: list[int]):\n    \"\"\"Build chunk of frames.\n\n    This method should be implemented in any subclass of the BaseDataset.\n\n    Args:\n        label_idx: The index of the labels.\n        frame_idx: The index of the frames.\n\n    Raises:\n        NotImplementedError: If this method is not overridden in a subclass.\n    \"\"\"\n    raise NotImplementedError(\"Must be implemented in subclass\")\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset.no_batching_fn","title":"<code>no_batching_fn(batch)</code>","text":"<p>Collate function used to overwrite dataloader batching function.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[Frame]</code> <p>the chunk of frames to be returned</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>The batch</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def no_batching_fn(self, batch: list[Frame]) -&gt; list[Frame]:\n    \"\"\"Collate function used to overwrite dataloader batching function.\n\n    Args:\n        batch: the chunk of frames to be returned\n\n    Returns:\n        The batch\n    \"\"\"\n    return batch\n</code></pre>"},{"location":"reference/dreem/datasets/base_dataset/#dreem.datasets.base_dataset.BaseDataset.process_segments","title":"<code>process_segments(i, segments_to_stitch, clip_length)</code>","text":"<p>Process segments to stitch.</p> <p>Modifies state variables chunked_frame_idx and label_idx.</p> <p>Parameters:</p> Name Type Description Default <code>segments_to_stitch</code> <code>list[Tensor]</code> <p>list of segments to stitch</p> required <code>i</code> <code>int</code> <p>index of the video</p> required <code>clip_length</code> <code>int</code> <p>the number of frames in each chunk</p> required <p>Returns: None</p> Source code in <code>dreem/datasets/base_dataset.py</code> <pre><code>def process_segments(\n    self, i: int, segments_to_stitch: list[torch.Tensor], clip_length: int\n) -&gt; None:\n    \"\"\"Process segments to stitch.\n\n    Modifies state variables chunked_frame_idx and label_idx.\n\n    Args:\n        segments_to_stitch: list of segments to stitch\n        i: index of the video\n        clip_length: the number of frames in each chunk\n    Returns: None\n    \"\"\"\n    stitched_segment = torch.cat(segments_to_stitch)\n    frame_idx_split = torch.split(stitched_segment, clip_length)\n    self.chunked_frame_idx.extend(frame_idx_split)\n    self.label_idx.extend(len(frame_idx_split) * [i])\n</code></pre>"},{"location":"reference/dreem/datasets/cell_tracking_dataset/","title":"cell_tracking_dataset","text":""},{"location":"reference/dreem/datasets/cell_tracking_dataset/#dreem.datasets.cell_tracking_dataset","title":"<code>dreem.datasets.cell_tracking_dataset</code>","text":"<p>Module containing cell tracking challenge dataset.</p> <p>Classes:</p> Name Description <code>CellTrackingDataset</code> <p>Dataset for loading cell tracking challenge data.</p>"},{"location":"reference/dreem/datasets/cell_tracking_dataset/#dreem.datasets.cell_tracking_dataset.CellTrackingDataset","title":"<code>CellTrackingDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset for loading cell tracking challenge data.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize CellTrackingDataset.</p> <code>get_indices</code> <p>Retrieve label and frame indices given batch index.</p> <code>get_instances</code> <p>Get an element of the dataset.</p> Source code in <code>dreem/datasets/cell_tracking_dataset.py</code> <pre><code>class CellTrackingDataset(BaseDataset):\n    \"\"\"Dataset for loading cell tracking challenge data.\"\"\"\n\n    def __init__(\n        self,\n        gt_list: list[list[str]],\n        raw_img_list: list[list[str]],\n        data_dirs: Optional[list[str]] = None,\n        padding: int = 5,\n        crop_size: int = 20,\n        chunk: bool = False,\n        clip_length: int = 10,\n        mode: str = \"train\",\n        augmentations: dict | None = None,\n        n_chunks: int | float = 1.0,\n        seed: int | None = None,\n        max_batching_gap: int = 15,\n        use_tight_bbox: bool = False,\n        ctc_track_meta: list[str] | None = None,\n        apply_mask_to_crop: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize CellTrackingDataset.\n\n        Args:\n            gt_list: filepaths of gt label images in a list of lists (each list\n                corresponds to a dataset)\n            raw_img_list: filepaths of original tif images in a list of lists\n                (each list corresponds to a dataset)\n            data_dirs: paths to data directories\n            padding: amount of padding around object crops\n            crop_size: the size of the object crops. Can be either:\n                - An integer specifying a single crop size for all objects\n                - A list of integers specifying different crop sizes for\n                  different data directories\n            chunk: whether or not to chunk the dataset into batches\n            clip_length: the number of frames in each chunk\n            mode: `train` or `val`. Determines whether this dataset is used for\n                training or validation. Currently doesn't affect dataset logic\n            augmentations: An optional dict mapping augmentations to parameters.\n                The keys\n                should map directly to augmentation classes in albumentations. Example:\n                    augs = {\n                        'Rotate': {'limit': [-90, 90]},\n                        'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},\n                        'RandomContrast': {'limit': 0.2}\n                    }\n            n_chunks: Number of chunks to subsample from.\n                Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n            seed: set a seed for reproducibility\n            max_batching_gap: the max number of frames that can be unlabelled\n                before starting a new batch\n            use_tight_bbox: whether to use tight bounding box (around keypoints)\n                instead of the default square bounding box\n            ctc_track_meta: filepaths of man_track.txt files in a list of lists\n                (each list corresponds to a dataset)\n            apply_mask_to_crop: whether to apply the mask to the crop\n        \"\"\"\n        super().__init__(\n            gt_list,\n            raw_img_list,\n            padding,\n            crop_size,\n            chunk,\n            clip_length,\n            mode,\n            augmentations,\n            n_chunks,\n            seed,\n            ctc_track_meta,\n        )\n\n        self.raw_img_list = raw_img_list\n        self.gt_list = gt_list\n        self.ctc_track_meta = ctc_track_meta\n        self.data_dirs = data_dirs\n        self.chunk = chunk\n        self.clip_length = clip_length\n        self.crop_size = crop_size\n        self.padding = padding\n        self.mode = mode.lower()\n        self.n_chunks = n_chunks\n        self.seed = seed\n        self.max_batching_gap = max_batching_gap\n        self.use_tight_bbox = use_tight_bbox\n        self.skeleton = sio.Skeleton(nodes=[\"centroid\"])\n        self.apply_mask_to_crop = apply_mask_to_crop\n        if not isinstance(self.data_dirs, list):\n            self.data_dirs = [self.data_dirs]\n\n        if not isinstance(self.crop_size, list):\n            # make a list so its handled consistently if multiple crops are used\n            if len(self.data_dirs) &gt; 0:  # for test mode, data_dirs is []\n                self.crop_size = [self.crop_size] * len(self.data_dirs)\n            else:\n                self.crop_size = [self.crop_size]\n\n        if len(self.data_dirs) &gt; 0 and len(self.crop_size) != len(self.data_dirs):\n            raise ValueError(\n                f\"If a list of crop sizes or data directories are given,\"\n                f\"they must have the same length but got {len(self.crop_size)} \"\n                f\"and {len(self.data_dirs)}\"\n            )\n\n        # if self.seed is not None:\n        #     np.random.seed(self.seed)\n\n        if augmentations and self.mode == \"train\":\n            self.augmentations = data_utils.build_augmentations(augmentations)\n        else:\n            self.augmentations = None\n\n        #\n        if self.ctc_track_meta is not None:\n            self.list_df_track_meta = [\n                pd.read_csv(\n                    gtf,\n                    delimiter=\" \",\n                    header=None,\n                    names=[\"track_id\", \"start_frame\", \"end_frame\", \"parent_id\"],\n                )\n                for gtf in self.ctc_track_meta\n            ]\n        else:\n            self.list_df_track_meta = None\n        # frame indices for each dataset; list of lists (each list corresponds to a dataset)\n        self.frame_idx = [torch.arange(len(gt_dataset)) for gt_dataset in self.gt_list]\n\n        # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n        # used in call to get_instances()\n        self.create_chunks_other()\n\n    def get_indices(self, idx: int) -&gt; tuple:\n        \"\"\"Retrieve label and frame indices given batch index.\n\n        Args:\n            idx: the index of the batch.\n\n        Returns:\n            the label and frame indices corresponding to a batch,\n        \"\"\"\n        return self.label_idx[idx], self.chunked_frame_idx[idx]\n\n    def get_instances(self, label_idx: list[int], frame_idx: list[int]) -&gt; list[Frame]:\n        \"\"\"Get an element of the dataset.\n\n        Args:\n            label_idx: index of the labels\n            frame_idx: index of the frames\n\n        Returns:\n            a list of Frame objects containing frame metadata and Instance Objects.\n            See `dreem.io.data_structures` for more info.\n        \"\"\"\n        image_paths = self.raw_img_list[label_idx]\n        gt_paths = self.gt_list[label_idx]\n\n        # df_track_meta is currently unused but may be needed for future track metadata processing\n        # if self.list_df_track_meta is not None:\n        #     df_track_meta = self.list_df_track_meta[label_idx]\n        # else:\n        #     df_track_meta = None\n\n        # get the correct crop size based on the video\n        video_par_path = Path(image_paths[0]).parent.parent\n        if len(self.data_dirs) &gt; 0:\n            crop_size = self.crop_size[0]\n            for j, data_dir in enumerate(self.data_dirs):\n                if Path(data_dir) == video_par_path:\n                    crop_size = self.crop_size[j]\n                    break\n        else:\n            crop_size = self.crop_size[0]\n\n        frames = []\n        max_crop_h, max_crop_w = 0, 0\n        for i in frame_idx:\n            instances, gt_track_ids, centroids, dict_centroids, bboxes, masks = (\n                [],\n                [],\n                [],\n                {},\n                [],\n                [],\n            )\n\n            i = int(i)\n\n            img = image_paths[i]\n            gt_sec = gt_paths[i]\n\n            img = np.array(Image.open(img))\n            gt_sec = np.array(Image.open(gt_sec))\n\n            if img.dtype == np.uint16:\n                img = ((img - img.min()) * (1 / (img.max() - img.min()) * 255)).astype(\n                    np.uint8\n                )\n            # if df_track_meta is None:\n            unique_instances = np.unique(gt_sec)\n            # else:\n            # unique_instances = df_track_meta[\"track_id\"].unique()\n\n            for instance in unique_instances:\n                # not all instances are in the frame, and they also label the\n                # background instance as zero\n                if instance in gt_sec and instance != 0:\n                    mask = gt_sec == instance\n                    center_of_mass = measurements.center_of_mass(mask)\n\n                    # scipy returns yx\n                    x, y = center_of_mass[::-1]\n\n                    if self.use_tight_bbox:\n                        bbox = data_utils.get_tight_bbox_masks(mask)\n                    else:\n                        bbox = data_utils.pad_bbox(\n                            data_utils.get_bbox([int(x), int(y)], crop_size),\n                            padding=self.padding,\n                        )\n                    mask = torch.as_tensor(mask)\n\n                    gt_track_ids.append(int(instance))\n                    centroids.append([x, y])\n                    dict_centroids[int(instance)] = [x, y]\n                    bboxes.append(bbox)\n                    masks.append(mask)\n\n            # albumentations wants (spatial, channels), ensure correct dims\n            if self.augmentations is not None:\n                for transform in self.augmentations:\n                    # for occlusion simulation, can remove if we don't want\n                    if isinstance(transform, A.CoarseDropout):\n                        transform.fill_value = random.randint(0, 255)\n\n                augmented = self.augmentations(\n                    image=img,\n                    mask=gt_sec,  # albumentations ensures geometric transformations are synced between image and mask\n                    keypoints=np.vstack(centroids),\n                )\n                img, aug_mask, centroids = (\n                    augmented[\"image\"],\n                    augmented[\"mask\"],\n                    augmented[\"keypoints\"],\n                )\n                aug_mask = torch.Tensor(aug_mask).unsqueeze(0)\n\n            img = torch.Tensor(img).unsqueeze(0)\n\n            for j in range(len(gt_track_ids)):\n                # just formatting for compatibility with Instance class\n                instance_centroid = {\n                    \"centroid\": np.array(dict_centroids[gt_track_ids[j]])\n                }\n                pose = {\"centroid\": dict_centroids[gt_track_ids[j]]}  # more formatting\n                crop_raw = data_utils.crop_bbox(img, bboxes[j])\n                if self.apply_mask_to_crop:\n                    if (\n                        self.augmentations is not None\n                    ):  # TODO: change this to a flag that the user passes in apply_mask_to_crop\n                        cropped_mask = data_utils.crop_bbox(aug_mask, bboxes[j])\n                        # filter for the instance of interest\n                        cropped_mask[cropped_mask != gt_track_ids[j]] = 0\n                    else:\n                        # masks[j] is already filtered for the instance of interest\n                        cropped_mask = data_utils.crop_bbox(masks[j], bboxes[j])\n\n                    cropped_mask[cropped_mask != 0] = 1\n                    # apply mask to crop\n                    crop = crop_raw * cropped_mask\n                else:\n                    crop = crop_raw\n\n                c, h, w = crop.shape\n                if h &gt; max_crop_h:\n                    max_crop_h = h\n                if w &gt; max_crop_w:\n                    max_crop_w = w\n\n                instances.append(\n                    Instance(\n                        gt_track_id=gt_track_ids[j],\n                        pred_track_id=-1,\n                        centroid=instance_centroid,\n                        skeleton=self.skeleton,\n                        point_scores=np.array([1.0]),\n                        instance_score=np.array([1.0]),\n                        pose=pose,\n                        bbox=bboxes[j],\n                        crop=crop,\n                        mask=masks[j],\n                    )\n                )\n\n            if self.mode == \"train\":\n                np.random.shuffle(instances)\n\n            frames.append(\n                Frame(\n                    video_id=label_idx,\n                    frame_id=i,\n                    vid_file=Path(image_paths[0]).parent.name,\n                    img_shape=img.shape,\n                    instances=instances,\n                )\n            )\n\n        # pad bbox to max size\n        if self.use_tight_bbox:\n            # bound the max crop size to the user defined crop size\n            max_crop_h = crop_size if max_crop_h == 0 else min(max_crop_h, crop_size)\n            max_crop_w = crop_size if max_crop_w == 0 else min(max_crop_w, crop_size)\n            # gather all the crops\n            for frame in frames:\n                for instance in frame.instances:\n                    data_utils.pad_variable_size_crops(\n                        instance, (max_crop_h, max_crop_w)\n                    )\n\n        return frames\n</code></pre>"},{"location":"reference/dreem/datasets/cell_tracking_dataset/#dreem.datasets.cell_tracking_dataset.CellTrackingDataset.__init__","title":"<code>__init__(gt_list, raw_img_list, data_dirs=None, padding=5, crop_size=20, chunk=False, clip_length=10, mode='train', augmentations=None, n_chunks=1.0, seed=None, max_batching_gap=15, use_tight_bbox=False, ctc_track_meta=None, apply_mask_to_crop=False, **kwargs)</code>","text":"<p>Initialize CellTrackingDataset.</p> <p>Parameters:</p> Name Type Description Default <code>gt_list</code> <code>list[list[str]]</code> <p>filepaths of gt label images in a list of lists (each list corresponds to a dataset)</p> required <code>raw_img_list</code> <code>list[list[str]]</code> <p>filepaths of original tif images in a list of lists (each list corresponds to a dataset)</p> required <code>data_dirs</code> <code>Optional[list[str]]</code> <p>paths to data directories</p> <code>None</code> <code>padding</code> <code>int</code> <p>amount of padding around object crops</p> <code>5</code> <code>crop_size</code> <code>int</code> <p>the size of the object crops. Can be either: - An integer specifying a single crop size for all objects - A list of integers specifying different crop sizes for   different data directories</p> <code>20</code> <code>chunk</code> <code>bool</code> <p>whether or not to chunk the dataset into batches</p> <code>False</code> <code>clip_length</code> <code>int</code> <p>the number of frames in each chunk</p> <code>10</code> <code>mode</code> <code>str</code> <p><code>train</code> or <code>val</code>. Determines whether this dataset is used for training or validation. Currently doesn't affect dataset logic</p> <code>'train'</code> <code>augmentations</code> <code>dict | None</code> <p>An optional dict mapping augmentations to parameters. The keys should map directly to augmentation classes in albumentations. Example:     augs = {         'Rotate': {'limit': [-90, 90]},         'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},         'RandomContrast': {'limit': 0.2}     }</p> <code>None</code> <code>n_chunks</code> <code>int | float</code> <p>Number of chunks to subsample from. Can either a fraction of the dataset (ie (0,1.0]) or number of chunks</p> <code>1.0</code> <code>seed</code> <code>int | None</code> <p>set a seed for reproducibility</p> <code>None</code> <code>max_batching_gap</code> <code>int</code> <p>the max number of frames that can be unlabelled before starting a new batch</p> <code>15</code> <code>use_tight_bbox</code> <code>bool</code> <p>whether to use tight bounding box (around keypoints) instead of the default square bounding box</p> <code>False</code> <code>ctc_track_meta</code> <code>list[str] | None</code> <p>filepaths of man_track.txt files in a list of lists (each list corresponds to a dataset)</p> <code>None</code> <code>apply_mask_to_crop</code> <code>bool</code> <p>whether to apply the mask to the crop</p> <code>False</code> Source code in <code>dreem/datasets/cell_tracking_dataset.py</code> <pre><code>def __init__(\n    self,\n    gt_list: list[list[str]],\n    raw_img_list: list[list[str]],\n    data_dirs: Optional[list[str]] = None,\n    padding: int = 5,\n    crop_size: int = 20,\n    chunk: bool = False,\n    clip_length: int = 10,\n    mode: str = \"train\",\n    augmentations: dict | None = None,\n    n_chunks: int | float = 1.0,\n    seed: int | None = None,\n    max_batching_gap: int = 15,\n    use_tight_bbox: bool = False,\n    ctc_track_meta: list[str] | None = None,\n    apply_mask_to_crop: bool = False,\n    **kwargs,\n):\n    \"\"\"Initialize CellTrackingDataset.\n\n    Args:\n        gt_list: filepaths of gt label images in a list of lists (each list\n            corresponds to a dataset)\n        raw_img_list: filepaths of original tif images in a list of lists\n            (each list corresponds to a dataset)\n        data_dirs: paths to data directories\n        padding: amount of padding around object crops\n        crop_size: the size of the object crops. Can be either:\n            - An integer specifying a single crop size for all objects\n            - A list of integers specifying different crop sizes for\n              different data directories\n        chunk: whether or not to chunk the dataset into batches\n        clip_length: the number of frames in each chunk\n        mode: `train` or `val`. Determines whether this dataset is used for\n            training or validation. Currently doesn't affect dataset logic\n        augmentations: An optional dict mapping augmentations to parameters.\n            The keys\n            should map directly to augmentation classes in albumentations. Example:\n                augs = {\n                    'Rotate': {'limit': [-90, 90]},\n                    'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},\n                    'RandomContrast': {'limit': 0.2}\n                }\n        n_chunks: Number of chunks to subsample from.\n            Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n        seed: set a seed for reproducibility\n        max_batching_gap: the max number of frames that can be unlabelled\n            before starting a new batch\n        use_tight_bbox: whether to use tight bounding box (around keypoints)\n            instead of the default square bounding box\n        ctc_track_meta: filepaths of man_track.txt files in a list of lists\n            (each list corresponds to a dataset)\n        apply_mask_to_crop: whether to apply the mask to the crop\n    \"\"\"\n    super().__init__(\n        gt_list,\n        raw_img_list,\n        padding,\n        crop_size,\n        chunk,\n        clip_length,\n        mode,\n        augmentations,\n        n_chunks,\n        seed,\n        ctc_track_meta,\n    )\n\n    self.raw_img_list = raw_img_list\n    self.gt_list = gt_list\n    self.ctc_track_meta = ctc_track_meta\n    self.data_dirs = data_dirs\n    self.chunk = chunk\n    self.clip_length = clip_length\n    self.crop_size = crop_size\n    self.padding = padding\n    self.mode = mode.lower()\n    self.n_chunks = n_chunks\n    self.seed = seed\n    self.max_batching_gap = max_batching_gap\n    self.use_tight_bbox = use_tight_bbox\n    self.skeleton = sio.Skeleton(nodes=[\"centroid\"])\n    self.apply_mask_to_crop = apply_mask_to_crop\n    if not isinstance(self.data_dirs, list):\n        self.data_dirs = [self.data_dirs]\n\n    if not isinstance(self.crop_size, list):\n        # make a list so its handled consistently if multiple crops are used\n        if len(self.data_dirs) &gt; 0:  # for test mode, data_dirs is []\n            self.crop_size = [self.crop_size] * len(self.data_dirs)\n        else:\n            self.crop_size = [self.crop_size]\n\n    if len(self.data_dirs) &gt; 0 and len(self.crop_size) != len(self.data_dirs):\n        raise ValueError(\n            f\"If a list of crop sizes or data directories are given,\"\n            f\"they must have the same length but got {len(self.crop_size)} \"\n            f\"and {len(self.data_dirs)}\"\n        )\n\n    # if self.seed is not None:\n    #     np.random.seed(self.seed)\n\n    if augmentations and self.mode == \"train\":\n        self.augmentations = data_utils.build_augmentations(augmentations)\n    else:\n        self.augmentations = None\n\n    #\n    if self.ctc_track_meta is not None:\n        self.list_df_track_meta = [\n            pd.read_csv(\n                gtf,\n                delimiter=\" \",\n                header=None,\n                names=[\"track_id\", \"start_frame\", \"end_frame\", \"parent_id\"],\n            )\n            for gtf in self.ctc_track_meta\n        ]\n    else:\n        self.list_df_track_meta = None\n    # frame indices for each dataset; list of lists (each list corresponds to a dataset)\n    self.frame_idx = [torch.arange(len(gt_dataset)) for gt_dataset in self.gt_list]\n\n    # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n    # used in call to get_instances()\n    self.create_chunks_other()\n</code></pre>"},{"location":"reference/dreem/datasets/cell_tracking_dataset/#dreem.datasets.cell_tracking_dataset.CellTrackingDataset.get_indices","title":"<code>get_indices(idx)</code>","text":"<p>Retrieve label and frame indices given batch index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the batch.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>the label and frame indices corresponding to a batch,</p> Source code in <code>dreem/datasets/cell_tracking_dataset.py</code> <pre><code>def get_indices(self, idx: int) -&gt; tuple:\n    \"\"\"Retrieve label and frame indices given batch index.\n\n    Args:\n        idx: the index of the batch.\n\n    Returns:\n        the label and frame indices corresponding to a batch,\n    \"\"\"\n    return self.label_idx[idx], self.chunked_frame_idx[idx]\n</code></pre>"},{"location":"reference/dreem/datasets/cell_tracking_dataset/#dreem.datasets.cell_tracking_dataset.CellTrackingDataset.get_instances","title":"<code>get_instances(label_idx, frame_idx)</code>","text":"<p>Get an element of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>label_idx</code> <code>list[int]</code> <p>index of the labels</p> required <code>frame_idx</code> <code>list[int]</code> <p>index of the frames</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>a list of Frame objects containing frame metadata and Instance Objects. See <code>dreem.io.data_structures</code> for more info.</p> Source code in <code>dreem/datasets/cell_tracking_dataset.py</code> <pre><code>def get_instances(self, label_idx: list[int], frame_idx: list[int]) -&gt; list[Frame]:\n    \"\"\"Get an element of the dataset.\n\n    Args:\n        label_idx: index of the labels\n        frame_idx: index of the frames\n\n    Returns:\n        a list of Frame objects containing frame metadata and Instance Objects.\n        See `dreem.io.data_structures` for more info.\n    \"\"\"\n    image_paths = self.raw_img_list[label_idx]\n    gt_paths = self.gt_list[label_idx]\n\n    # df_track_meta is currently unused but may be needed for future track metadata processing\n    # if self.list_df_track_meta is not None:\n    #     df_track_meta = self.list_df_track_meta[label_idx]\n    # else:\n    #     df_track_meta = None\n\n    # get the correct crop size based on the video\n    video_par_path = Path(image_paths[0]).parent.parent\n    if len(self.data_dirs) &gt; 0:\n        crop_size = self.crop_size[0]\n        for j, data_dir in enumerate(self.data_dirs):\n            if Path(data_dir) == video_par_path:\n                crop_size = self.crop_size[j]\n                break\n    else:\n        crop_size = self.crop_size[0]\n\n    frames = []\n    max_crop_h, max_crop_w = 0, 0\n    for i in frame_idx:\n        instances, gt_track_ids, centroids, dict_centroids, bboxes, masks = (\n            [],\n            [],\n            [],\n            {},\n            [],\n            [],\n        )\n\n        i = int(i)\n\n        img = image_paths[i]\n        gt_sec = gt_paths[i]\n\n        img = np.array(Image.open(img))\n        gt_sec = np.array(Image.open(gt_sec))\n\n        if img.dtype == np.uint16:\n            img = ((img - img.min()) * (1 / (img.max() - img.min()) * 255)).astype(\n                np.uint8\n            )\n        # if df_track_meta is None:\n        unique_instances = np.unique(gt_sec)\n        # else:\n        # unique_instances = df_track_meta[\"track_id\"].unique()\n\n        for instance in unique_instances:\n            # not all instances are in the frame, and they also label the\n            # background instance as zero\n            if instance in gt_sec and instance != 0:\n                mask = gt_sec == instance\n                center_of_mass = measurements.center_of_mass(mask)\n\n                # scipy returns yx\n                x, y = center_of_mass[::-1]\n\n                if self.use_tight_bbox:\n                    bbox = data_utils.get_tight_bbox_masks(mask)\n                else:\n                    bbox = data_utils.pad_bbox(\n                        data_utils.get_bbox([int(x), int(y)], crop_size),\n                        padding=self.padding,\n                    )\n                mask = torch.as_tensor(mask)\n\n                gt_track_ids.append(int(instance))\n                centroids.append([x, y])\n                dict_centroids[int(instance)] = [x, y]\n                bboxes.append(bbox)\n                masks.append(mask)\n\n        # albumentations wants (spatial, channels), ensure correct dims\n        if self.augmentations is not None:\n            for transform in self.augmentations:\n                # for occlusion simulation, can remove if we don't want\n                if isinstance(transform, A.CoarseDropout):\n                    transform.fill_value = random.randint(0, 255)\n\n            augmented = self.augmentations(\n                image=img,\n                mask=gt_sec,  # albumentations ensures geometric transformations are synced between image and mask\n                keypoints=np.vstack(centroids),\n            )\n            img, aug_mask, centroids = (\n                augmented[\"image\"],\n                augmented[\"mask\"],\n                augmented[\"keypoints\"],\n            )\n            aug_mask = torch.Tensor(aug_mask).unsqueeze(0)\n\n        img = torch.Tensor(img).unsqueeze(0)\n\n        for j in range(len(gt_track_ids)):\n            # just formatting for compatibility with Instance class\n            instance_centroid = {\n                \"centroid\": np.array(dict_centroids[gt_track_ids[j]])\n            }\n            pose = {\"centroid\": dict_centroids[gt_track_ids[j]]}  # more formatting\n            crop_raw = data_utils.crop_bbox(img, bboxes[j])\n            if self.apply_mask_to_crop:\n                if (\n                    self.augmentations is not None\n                ):  # TODO: change this to a flag that the user passes in apply_mask_to_crop\n                    cropped_mask = data_utils.crop_bbox(aug_mask, bboxes[j])\n                    # filter for the instance of interest\n                    cropped_mask[cropped_mask != gt_track_ids[j]] = 0\n                else:\n                    # masks[j] is already filtered for the instance of interest\n                    cropped_mask = data_utils.crop_bbox(masks[j], bboxes[j])\n\n                cropped_mask[cropped_mask != 0] = 1\n                # apply mask to crop\n                crop = crop_raw * cropped_mask\n            else:\n                crop = crop_raw\n\n            c, h, w = crop.shape\n            if h &gt; max_crop_h:\n                max_crop_h = h\n            if w &gt; max_crop_w:\n                max_crop_w = w\n\n            instances.append(\n                Instance(\n                    gt_track_id=gt_track_ids[j],\n                    pred_track_id=-1,\n                    centroid=instance_centroid,\n                    skeleton=self.skeleton,\n                    point_scores=np.array([1.0]),\n                    instance_score=np.array([1.0]),\n                    pose=pose,\n                    bbox=bboxes[j],\n                    crop=crop,\n                    mask=masks[j],\n                )\n            )\n\n        if self.mode == \"train\":\n            np.random.shuffle(instances)\n\n        frames.append(\n            Frame(\n                video_id=label_idx,\n                frame_id=i,\n                vid_file=Path(image_paths[0]).parent.name,\n                img_shape=img.shape,\n                instances=instances,\n            )\n        )\n\n    # pad bbox to max size\n    if self.use_tight_bbox:\n        # bound the max crop size to the user defined crop size\n        max_crop_h = crop_size if max_crop_h == 0 else min(max_crop_h, crop_size)\n        max_crop_w = crop_size if max_crop_w == 0 else min(max_crop_w, crop_size)\n        # gather all the crops\n        for frame in frames:\n            for instance in frame.instances:\n                data_utils.pad_variable_size_crops(\n                    instance, (max_crop_h, max_crop_w)\n                )\n\n    return frames\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/","title":"data_utils","text":""},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils","title":"<code>dreem.datasets.data_utils</code>","text":"<p>Module containing helper functions for datasets.</p> <p>Classes:</p> Name Description <code>LazyTiffStack</code> <p>Class used for loading tiffs without loading into memory.</p> <code>NodeDropout</code> <p>Node dropout augmentation.</p> <p>Functions:</p> Name Description <code>build_augmentations</code> <p>Get augmentations for dataset.</p> <code>centroid_bbox</code> <p>Calculate bbox around instance centroid.</p> <code>crop_bbox</code> <p>Crop an image to a bounding box.</p> <code>get_bbox</code> <p>Get a square bbox around a centroid coordinates.</p> <code>get_max_padding</code> <p>Calculate maximum padding dimensions for a given height and width.</p> <code>get_tight_bbox</code> <p>Get a tight bbox around an instance.</p> <code>get_tight_bbox_masks</code> <p>Get a tight bbox around an instance.</p> <code>load_slp</code> <p>Read a SLEAP labels file.</p> <code>pad_bbox</code> <p>Pad bounding box coordinates.</p> <code>pad_variable_size_crops</code> <p>Pad or crop an instance's crop to the target size.</p> <code>parse_synthetic</code> <p>Parse .xml labels from synthetic data generated by ICY or ISBI tracking challenge.</p> <code>parse_trackmate</code> <p>Parse trackmate xml or csv labels file.</p> <code>pose_bbox</code> <p>Calculate bbox around instance pose.</p> <code>resize_and_pad</code> <p>Resize and pad an image to fit a square output size.</p> <code>sorted_anchors</code> <p>Sort anchor names from most instances with that node to least.</p> <code>view_training_batch</code> <p>Display a grid of images from a batch of training instances.</p>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.LazyTiffStack","title":"<code>LazyTiffStack</code>","text":"<p>Class used for loading tiffs without loading into memory.</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Get frame.</p> <code>__init__</code> <p>Initialize class.</p> <code>close</code> <p>Close tiff stack.</p> <code>get_section</code> <p>Get frame as ndarray.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>class LazyTiffStack:\n    \"\"\"Class used for loading tiffs without loading into memory.\"\"\"\n\n    def __init__(self, filename: str):\n        \"\"\"Initialize class.\n\n        Args:\n            filename: name of tif file to be opened\n        \"\"\"\n        # expects spatial, channels\n        self.image = Image.open(filename)\n\n    def __getitem__(self, section_idx: int) -&gt; Image:\n        \"\"\"Get frame.\n\n        Args:\n            section_idx: index of frame or z-slice to get.\n\n        Returns:\n            a PIL image of that frame/z-slice.\n        \"\"\"\n        self.image.seek(section_idx)\n        return self.image\n\n    def get_section(self, section_idx: int) -&gt; np.array:\n        \"\"\"Get frame as ndarray.\n\n        Args:\n            section_idx: index of frame or z-slice to get.\n\n        Returns:\n            an np.array of that frame/z-slice.\n        \"\"\"\n        section = self.__getitem__(section_idx)\n        return np.array(section)\n\n    def close(self):\n        \"\"\"Close tiff stack.\"\"\"\n        self.file.close()\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.LazyTiffStack.__getitem__","title":"<code>__getitem__(section_idx)</code>","text":"<p>Get frame.</p> <p>Parameters:</p> Name Type Description Default <code>section_idx</code> <code>int</code> <p>index of frame or z-slice to get.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>a PIL image of that frame/z-slice.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def __getitem__(self, section_idx: int) -&gt; Image:\n    \"\"\"Get frame.\n\n    Args:\n        section_idx: index of frame or z-slice to get.\n\n    Returns:\n        a PIL image of that frame/z-slice.\n    \"\"\"\n    self.image.seek(section_idx)\n    return self.image\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.LazyTiffStack.__init__","title":"<code>__init__(filename)</code>","text":"<p>Initialize class.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>name of tif file to be opened</p> required Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def __init__(self, filename: str):\n    \"\"\"Initialize class.\n\n    Args:\n        filename: name of tif file to be opened\n    \"\"\"\n    # expects spatial, channels\n    self.image = Image.open(filename)\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.LazyTiffStack.close","title":"<code>close()</code>","text":"<p>Close tiff stack.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def close(self):\n    \"\"\"Close tiff stack.\"\"\"\n    self.file.close()\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.LazyTiffStack.get_section","title":"<code>get_section(section_idx)</code>","text":"<p>Get frame as ndarray.</p> <p>Parameters:</p> Name Type Description Default <code>section_idx</code> <code>int</code> <p>index of frame or z-slice to get.</p> required <p>Returns:</p> Type Description <code>array</code> <p>an np.array of that frame/z-slice.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def get_section(self, section_idx: int) -&gt; np.array:\n    \"\"\"Get frame as ndarray.\n\n    Args:\n        section_idx: index of frame or z-slice to get.\n\n    Returns:\n        an np.array of that frame/z-slice.\n    \"\"\"\n    section = self.__getitem__(section_idx)\n    return np.array(section)\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.NodeDropout","title":"<code>NodeDropout</code>","text":"<p>Node dropout augmentation.</p> <p>Drop up to <code>n</code> nodes with probability <code>p</code>.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Wrap <code>drop_nodes</code> to enable class call.</p> <code>__init__</code> <p>Initialize Node Dropout Augmentation.</p> <code>forward</code> <p>Drop up to <code>n</code> random nodes with probability p.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>class NodeDropout:\n    \"\"\"Node dropout augmentation.\n\n    Drop up to `n` nodes with probability `p`.\n    \"\"\"\n\n    def __init__(self, p: float, n: int) -&gt; None:\n        \"\"\"Initialize Node Dropout Augmentation.\n\n        Args:\n            p: the probability with which to drop the nodes\n            n: the maximum number of nodes to drop\n        \"\"\"\n        self.n = n\n        self.p = p\n\n    def __call__(self, nodes: list[str]) -&gt; list[str]:\n        \"\"\"Wrap `drop_nodes` to enable class call.\n\n        Args:\n            nodes: A list of available node names to drop.\n\n        Returns:\n            dropped_nodes: A list of up to `self.n` nodes to drop.\n        \"\"\"\n        return self.forward(nodes)\n\n    def forward(self, nodes: list[str]) -&gt; list[str]:\n        \"\"\"Drop up to `n` random nodes with probability p.\n\n        Args:\n            nodes: A list of available node names to drop.\n\n        Returns:\n            dropped_nodes: A list of up to `self.n` nodes to drop.\n        \"\"\"\n        if self.n == 0 or self.p == 0:\n            return []\n\n        nodes_to_drop = np.random.permutation(nodes)\n        node_dropout_p = np.random.uniform(size=len(nodes_to_drop))\n\n        dropped_node_inds = np.where(node_dropout_p &lt; self.p)\n        node_dropout_p = node_dropout_p[dropped_node_inds]\n\n        n_nodes_to_drop = min(self.n, len(node_dropout_p))\n\n        dropped_node_inds = np.argpartition(node_dropout_p, -n_nodes_to_drop)[\n            -n_nodes_to_drop:\n        ]\n\n        dropped_nodes = nodes_to_drop[dropped_node_inds]\n\n        return dropped_nodes\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.NodeDropout.__call__","title":"<code>__call__(nodes)</code>","text":"<p>Wrap <code>drop_nodes</code> to enable class call.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[str]</code> <p>A list of available node names to drop.</p> required <p>Returns:</p> Name Type Description <code>dropped_nodes</code> <code>list[str]</code> <p>A list of up to <code>self.n</code> nodes to drop.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def __call__(self, nodes: list[str]) -&gt; list[str]:\n    \"\"\"Wrap `drop_nodes` to enable class call.\n\n    Args:\n        nodes: A list of available node names to drop.\n\n    Returns:\n        dropped_nodes: A list of up to `self.n` nodes to drop.\n    \"\"\"\n    return self.forward(nodes)\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.NodeDropout.__init__","title":"<code>__init__(p, n)</code>","text":"<p>Initialize Node Dropout Augmentation.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>the probability with which to drop the nodes</p> required <code>n</code> <code>int</code> <p>the maximum number of nodes to drop</p> required Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def __init__(self, p: float, n: int) -&gt; None:\n    \"\"\"Initialize Node Dropout Augmentation.\n\n    Args:\n        p: the probability with which to drop the nodes\n        n: the maximum number of nodes to drop\n    \"\"\"\n    self.n = n\n    self.p = p\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.NodeDropout.forward","title":"<code>forward(nodes)</code>","text":"<p>Drop up to <code>n</code> random nodes with probability p.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[str]</code> <p>A list of available node names to drop.</p> required <p>Returns:</p> Name Type Description <code>dropped_nodes</code> <code>list[str]</code> <p>A list of up to <code>self.n</code> nodes to drop.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def forward(self, nodes: list[str]) -&gt; list[str]:\n    \"\"\"Drop up to `n` random nodes with probability p.\n\n    Args:\n        nodes: A list of available node names to drop.\n\n    Returns:\n        dropped_nodes: A list of up to `self.n` nodes to drop.\n    \"\"\"\n    if self.n == 0 or self.p == 0:\n        return []\n\n    nodes_to_drop = np.random.permutation(nodes)\n    node_dropout_p = np.random.uniform(size=len(nodes_to_drop))\n\n    dropped_node_inds = np.where(node_dropout_p &lt; self.p)\n    node_dropout_p = node_dropout_p[dropped_node_inds]\n\n    n_nodes_to_drop = min(self.n, len(node_dropout_p))\n\n    dropped_node_inds = np.argpartition(node_dropout_p, -n_nodes_to_drop)[\n        -n_nodes_to_drop:\n    ]\n\n    dropped_nodes = nodes_to_drop[dropped_node_inds]\n\n    return dropped_nodes\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.build_augmentations","title":"<code>build_augmentations(augmentations)</code>","text":"<p>Get augmentations for dataset.</p> <p>Parameters:</p> Name Type Description Default <code>augmentations</code> <code>dict</code> <p>a dict containing the name of the augmentations            and their parameters</p> required <p>Returns:</p> Type Description <code>Compose</code> <p>An Albumentations composition of different augmentations.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def build_augmentations(augmentations: dict) -&gt; A.Compose:\n    \"\"\"Get augmentations for dataset.\n\n    Args:\n        augmentations: a dict containing the name of the augmentations\n                       and their parameters\n\n    Returns:\n        An Albumentations composition of different augmentations.\n    \"\"\"\n    aug_list = []\n    for aug_name, aug_args in augmentations.items():\n        aug_class = getattr(A, aug_name)\n        aug = aug_class(**aug_args)\n        aug_list.append(aug)\n\n    augs = A.Compose(\n        aug_list,\n        p=1.0,\n        keypoint_params=A.KeypointParams(format=\"xy\", remove_invisible=False),\n    )\n\n    return augs\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.centroid_bbox","title":"<code>centroid_bbox(points, anchors, crop_size)</code>","text":"<p>Calculate bbox around instance centroid.</p> <p>This is useful for ensuring that crops are centered around each instance in the case of incorrect pose estimates.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ArrayLike</code> <p>2d array of centroid coordinates where each row corresponds to a different anchor point.</p> required <code>anchors</code> <code>list</code> <p>indices of a given anchor point to use as the centroid</p> required <code>crop_size</code> <code>int</code> <p>Integer specifying the crop height and width</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Bounding box in [y1, x1, y2, x2] format.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def centroid_bbox(points: ArrayLike, anchors: list, crop_size: int) -&gt; torch.Tensor:\n    \"\"\"Calculate bbox around instance centroid.\n\n    This is useful for ensuring that crops are centered around each instance\n    in the case of incorrect pose estimates.\n\n    Args:\n        points: 2d array of centroid coordinates where each row corresponds to a\n            different anchor point.\n        anchors: indices of a given anchor point to use as the centroid\n        crop_size: Integer specifying the crop height and width\n\n    Returns:\n        Bounding box in [y1, x1, y2, x2] format.\n    \"\"\"\n    for anchor in anchors:\n        cx, cy = points[anchor][0], points[anchor][1]\n        if not np.isnan(cx):\n            break\n\n    bbox = torch.Tensor(\n        [\n            -crop_size / 2 + cy,\n            -crop_size / 2 + cx,\n            crop_size / 2 + cy,\n            crop_size / 2 + cx,\n        ]\n    )\n\n    return bbox\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.crop_bbox","title":"<code>crop_bbox(img, bbox)</code>","text":"<p>Crop an image to a bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor</code> <p>Image as a tensor of shape (channels, height, width).</p> required <code>bbox</code> <code>ArrayLike</code> <p>Bounding box in [y1, x1, y2, x2] format.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Cropped pixels as tensor of shape (channels, height, width).</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def crop_bbox(img: torch.Tensor, bbox: ArrayLike) -&gt; torch.Tensor:\n    \"\"\"Crop an image to a bounding box.\n\n    Args:\n        img: Image as a tensor of shape (channels, height, width).\n        bbox: Bounding box in [y1, x1, y2, x2] format.\n\n    Returns:\n        Cropped pixels as tensor of shape (channels, height, width).\n    \"\"\"\n    # Crop to the bounding box.\n    y1, x1, y2, x2 = bbox\n    crop = tvf.crop(\n        img,\n        top=int(y1.round()),\n        left=int(x1.round()),\n        height=int((y2 - y1).round()),\n        width=int((x2 - x1).round()),\n    )\n\n    return crop\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.get_bbox","title":"<code>get_bbox(center, size)</code>","text":"<p>Get a square bbox around a centroid coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>center</code> <code>ArrayLike</code> <p>centroid coordinates in (x,y)</p> required <code>size</code> <code>int | tuple[int]</code> <p>size of the bounding box</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor in form y1, x1, y2, x2</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def get_bbox(center: ArrayLike, size: int | tuple[int]) -&gt; torch.Tensor:\n    \"\"\"Get a square bbox around a centroid coordinates.\n\n    Args:\n        center: centroid coordinates in (x,y)\n        size: size of the bounding box\n\n    Returns:\n        A torch tensor in form y1, x1, y2, x2\n    \"\"\"\n    if isinstance(size, int):\n        size = (size, size)\n    cx, cy = center[0], center[1]\n\n    y1 = max(0, -size[-1] // 2 + cy)\n    x1 = max(0, -size[0] // 2 + cx)\n    y2 = size[-1] // 2 + cy if y1 != 0 else size[1]\n    x2 = size[0] // 2 + cx if x1 != 0 else size[0]\n    bbox = torch.Tensor([y1, x1, y2, x2])\n\n    return bbox\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.get_max_padding","title":"<code>get_max_padding(height, width)</code>","text":"<p>Calculate maximum padding dimensions for a given height and width.</p> <p>Useful if padding is required for rotational augmentations, e.g when centroids lie on the borders of an image.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>The original height.</p> required <code>width</code> <code>int</code> <p>The original width.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the padded height and padded width.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def get_max_padding(height: int, width: int) -&gt; tuple:\n    \"\"\"Calculate maximum padding dimensions for a given height and width.\n\n    Useful if padding is required for rotational augmentations, e.g when\n    centroids lie on the borders of an image.\n\n    Args:\n        height: The original height.\n        width: The original width.\n\n    Returns:\n        A tuple containing the padded height and padded width.\n    \"\"\"\n    diagonal = math.ceil(math.sqrt(height**2 + width**2))\n\n    padded_height = height + (diagonal - height)\n    padded_width = width + (diagonal - width)\n\n    return padded_height, padded_width\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.get_tight_bbox","title":"<code>get_tight_bbox(pose)</code>","text":"<p>Get a tight bbox around an instance.</p> <p>Parameters:</p> Name Type Description Default <code>poses</code> <p>array of keypoints around which to create the tight bbox</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor in form y1, x1, y2, x2 representing the tight bbox</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def get_tight_bbox(pose: ArrayLike) -&gt; torch.Tensor:\n    \"\"\"Get a tight bbox around an instance.\n\n    Args:\n        poses: array of keypoints around which to create the tight bbox\n\n    Returns:\n        A torch tensor in form y1, x1, y2, x2 representing the tight bbox\n    \"\"\"\n    x_coords = pose[:, 0]\n    y_coords = pose[:, 1]\n    x1 = np.min(x_coords)\n    x2 = np.max(x_coords)\n    y1 = np.min(y_coords)\n    y2 = np.max(y_coords)\n    bbox = torch.Tensor([y1, x1, y2, x2])\n\n    return bbox\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.get_tight_bbox_masks","title":"<code>get_tight_bbox_masks(mask)</code>","text":"<p>Get a tight bbox around an instance.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ArrayLike</code> <p>mask of the instance</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor in form y1, x1, y2, x2 representing the tight bbox</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def get_tight_bbox_masks(mask: ArrayLike) -&gt; torch.Tensor:\n    \"\"\"Get a tight bbox around an instance.\n\n    Args:\n        mask: mask of the instance\n\n    Returns:\n        A torch tensor in form y1, x1, y2, x2 representing the tight bbox\n    \"\"\"\n    max_x = np.asarray(mask != 0).nonzero()[1].max()\n    max_y = np.asarray(mask != 0).nonzero()[0].max()\n    min_x = np.asarray(mask != 0).nonzero()[1].min()\n    min_y = np.asarray(mask != 0).nonzero()[0].min()\n    bbox = torch.Tensor([min_y, min_x, max_y, max_x])\n\n    return bbox\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.load_slp","title":"<code>load_slp(labels_path, open_videos=True)</code>","text":"<p>Read a SLEAP labels file.</p> <p>Parameters:</p> Name Type Description Default <code>labels_path</code> <code>str</code> <p>A string path to the SLEAP labels file.</p> required <code>open_videos</code> <code>bool</code> <p>If <code>True</code> (the default), attempt to open the video backend for I/O. If <code>False</code>, the backend will not be opened (useful for reading metadata when the video files are not available).</p> <code>True</code> <p>Returns:</p> Type Description <code>Labels</code> <p>The processed <code>Labels</code> object.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def load_slp(labels_path: str, open_videos: bool = True) -&gt; Labels:\n    \"\"\"Read a SLEAP labels file.\n\n    Args:\n        labels_path: A string path to the SLEAP labels file.\n        open_videos: If `True` (the default), attempt to open the video backend for\n            I/O. If `False`, the backend will not be opened (useful for reading metadata\n            when the video files are not available).\n\n    Returns:\n        The processed `Labels` object.\n    \"\"\"\n    tracks = read_tracks(labels_path)\n    videos = read_videos(labels_path, open_backend=open_videos)\n    skeletons = read_skeletons(labels_path)\n    points = read_points(labels_path)\n    pred_points = read_pred_points(labels_path)\n    format_id = read_hdf5_attrs(labels_path, \"metadata\", \"format_id\")\n    instances = read_instances(\n        labels_path, skeletons, tracks, points, pred_points, format_id\n    )\n    metadata = read_metadata(labels_path)\n    provenance = metadata.get(\"provenance\", dict())\n\n    frames = read_hdf5_dataset(labels_path, \"frames\")\n    labeled_frames = []\n    annotated_segments = []\n    curr_segment_start = frames[0][2]\n    curr_frame = curr_segment_start\n    # note that frames only contains frames with labelled instances, not all frames\n    for i, video_id, frame_idx, instance_id_start, instance_id_end in frames:\n        # if no instances, don't add this frame to the labeled frames\n        if len(instances[instance_id_start:instance_id_end]) == 0:\n            continue\n\n        labeled_frames.append(\n            LabeledFrame(\n                video=videos[video_id],\n                frame_idx=int(frame_idx),\n                instances=instances[instance_id_start:instance_id_end],\n            )\n        )\n        if frame_idx == curr_frame:\n            pass\n        elif frame_idx == curr_frame + 1:\n            curr_frame = frame_idx\n        elif frame_idx &gt; curr_frame + 1:\n            annotated_segments.append((curr_segment_start, curr_frame))\n            curr_segment_start = frame_idx\n            curr_frame = frame_idx\n\n    # add last segment\n    annotated_segments.append((curr_segment_start, curr_frame))\n\n    labels = Labels(\n        labeled_frames=labeled_frames,\n        videos=videos,\n        skeletons=skeletons,\n        tracks=tracks,\n        provenance=provenance,\n    )\n    labels.provenance[\"filename\"] = labels_path\n\n    return labels, annotated_segments\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.pad_bbox","title":"<code>pad_bbox(bbox, padding=16)</code>","text":"<p>Pad bounding box coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>ArrayLike</code> <p>Bounding box in [y1, x1, y2, x2] format.</p> required <code>padding</code> <code>int</code> <p>Padding to add to each side in pixels.</p> <code>16</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Padded bounding box in [y1, x1, y2, x2] format.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def pad_bbox(bbox: ArrayLike, padding: int = 16) -&gt; torch.Tensor:\n    \"\"\"Pad bounding box coordinates.\n\n    Args:\n        bbox: Bounding box in [y1, x1, y2, x2] format.\n        padding: Padding to add to each side in pixels.\n\n    Returns:\n        Padded bounding box in [y1, x1, y2, x2] format.\n    \"\"\"\n    y1, x1, y2, x2 = bbox\n    y1, x1 = y1 - padding, x1 - padding\n    y2, x2 = y2 + padding, x2 + padding\n    return torch.Tensor([y1, x1, y2, x2])\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.pad_variable_size_crops","title":"<code>pad_variable_size_crops(instance, target_size)</code>","text":"<p>Pad or crop an instance's crop to the target size.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <p>Instance object with a crop attribute</p> required <code>target_size</code> <p>Tuple of (height, width) for the target size</p> required <p>Returns:</p> Type Description <p>The instance with modified crop</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def pad_variable_size_crops(instance, target_size):\n    \"\"\"Pad or crop an instance's crop to the target size.\n\n    Args:\n        instance: Instance object with a crop attribute\n        target_size: Tuple of (height, width) for the target size\n\n    Returns:\n        The instance with modified crop\n    \"\"\"\n    _, c, h, w = instance.crop.shape\n    target_h, target_w = target_size\n\n    # Crop the image further if target_size is smaller than current crop size\n    if h &gt; target_h or w &gt; target_w:\n        instance.crop = tvf.center_crop(\n            instance.crop, (min(h, target_h), min(w, target_w))\n        )\n\n    _, c, h, w = instance.crop.shape\n\n    if h &lt; target_h or w &lt; target_w:\n        # If height or width is smaller than target size, pad the image to target_size\n        pad_w = max(0, target_w - w)\n        pad_h = max(0, target_h - h)\n\n        pad_w_left = pad_w // 2\n        pad_w_right = pad_w - pad_w_left\n\n        pad_h_top = pad_h // 2\n        pad_h_bottom = pad_h - pad_h_top\n\n        # Apply padding\n        instance.crop = tvf.pad(\n            instance.crop,\n            (pad_w_left, pad_h_top, pad_w_right, pad_h_bottom),\n            0,\n            \"constant\",\n        )\n\n    return instance\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.parse_synthetic","title":"<code>parse_synthetic(xml_path, source='icy')</code>","text":"<p>Parse .xml labels from synthetic data generated by ICY or ISBI tracking challenge.</p> <p>Logic adapted from https://github.com/sylvainprigent/napari-tracks-reader/blob/main/napari_tracks_reader</p> <p>Parameters:</p> Name Type Description Default <code>xml_path</code> <code>str</code> <p>path to .xml file containing ICY or ISBI gt trajectory labels</p> required <code>source</code> <code>str</code> <p>synthetic dataset type. Should be either icy or isbi</p> <code>'icy'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas DataFrame containing frame idx, gt track id and centroid x,y coordinates in pixels</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def parse_synthetic(xml_path: str, source: str = \"icy\") -&gt; pd.DataFrame:\n    \"\"\"Parse .xml labels from synthetic data generated by ICY or ISBI tracking challenge.\n\n    Logic adapted from https://github.com/sylvainprigent/napari-tracks-reader/blob/main/napari_tracks_reader\n\n    Args:\n        xml_path: path to .xml file containing ICY or ISBI gt trajectory labels\n        source: synthetic dataset type. Should be either icy or isbi\n\n    Returns:\n        pandas DataFrame containing frame idx, gt track id\n        and centroid x,y coordinates in pixels\n    \"\"\"\n    if source.lower() == \"icy\":\n        root_tag = \"trackgroup\"\n    elif source.lower() == \"isbi\":\n        root_tag = \"TrackContestISBI2012\"\n    else:\n        raise ValueError(f\"{source} source mode not supported\")\n\n    tree = et.parse(xml_path)\n\n    root = tree.getroot()\n    tracks = np.empty((0, 4))\n\n    # get the trackgroup element\n    idx_trackgroup = 0\n    for i in range(len(root)):\n        if root[i].tag == root_tag:\n            idx_trackgroup = i\n            break\n\n    ids_map = {}\n    track_id = -1\n    for track_element in root[idx_trackgroup]:\n        track_id += 1\n\n        try:\n            ids_map[track_element.attrib[\"id\"]] = track_id\n        except:\n            pass\n\n        for detection_element in track_element:\n            row = [\n                float(track_id),\n                float(detection_element.attrib[\"t\"]),\n                float(detection_element.attrib[\"y\"]),\n                float(detection_element.attrib[\"x\"]),\n            ]\n            tracks = np.concatenate((tracks, [row]), axis=0)\n\n    tracks_df = pd.DataFrame(\n        tracks, columns=[\"TRACK_ID\", \"FRAME\", \"POSITION_Y\", \"POSITION_X\"]\n    )\n\n    tracks_df = tracks_df.apply(pd.to_numeric, errors=\"coerce\", downcast=\"integer\")\n\n    return tracks_df\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.parse_trackmate","title":"<code>parse_trackmate(data_path)</code>","text":"<p>Parse trackmate xml or csv labels file.</p> <p>Logic adapted from https://github.com/hadim/pytrackmate.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>string path to xml or csv file storing trackmate trajectory labels</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>pandas DataFrame</code> containing frame number, track_ids, and centroid x,y coordinates in pixels</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def parse_trackmate(data_path: str) -&gt; pd.DataFrame:\n    \"\"\"Parse trackmate xml or csv labels file.\n\n    Logic adapted from https://github.com/hadim/pytrackmate.\n\n    Args:\n        data_path: string path to xml or csv file storing trackmate trajectory labels\n\n    Returns:\n        `pandas DataFrame` containing frame number, track_ids,\n        and centroid x,y coordinates in pixels\n    \"\"\"\n    if data_path.endswith(\".xml\"):\n        root = et.fromstring(open(data_path).read())\n\n        objects = []\n        features = root.find(\"Model\").find(\"FeatureDeclarations\").find(\"SpotFeatures\")\n        features = [c.get(\"feature\") for c in list(features)] + [\"ID\"]\n\n        spots = root.find(\"Model\").find(\"AllSpots\")\n\n        objects = []\n\n        for frame in spots.findall(\"SpotsInFrame\"):\n            for spot in frame.findall(\"Spot\"):\n                single_object = []\n                for label in features:\n                    single_object.append(spot.get(label))\n                objects.append(single_object)\n\n        tracks_df = pd.DataFrame(objects, columns=features)\n        tracks_df = tracks_df.astype(np.float)\n\n        filtered_track_ids = [\n            int(track.get(\"TRACK_ID\"))\n            for track in root.find(\"Model\").find(\"FilteredTracks\").findall(\"TrackID\")\n        ]\n\n        label_id = 0\n        tracks_df[\"label\"] = np.nan\n\n        tracks = root.find(\"Model\").find(\"AllTracks\")\n        for track in tracks.findall(\"Track\"):\n            track_id = int(track.get(\"TRACK_ID\"))\n            if track_id in filtered_track_ids:\n                spot_ids = [\n                    (\n                        edge.get(\"SPOT_SOURCE_ID\"),\n                        edge.get(\"SPOT_TARGET_ID\"),\n                        edge.get(\"EDGE_TIME\"),\n                    )\n                    for edge in track.findall(\"Edge\")\n                ]\n                spot_ids = np.array(spot_ids).astype(\"float\")[:, :2]\n                spot_ids = set(spot_ids.flatten())\n\n                tracks_df.loc[tracks_df[\"ID\"].isin(spot_ids), \"TRACK_ID\"] = label_id\n                label_id += 1\n\n    elif data_path.endswith(\".csv\"):\n        tracks_df = pd.read_csv(data_path, encoding=\"ISO-8859-1\")\n\n    else:\n        raise ValueError(f\"Unsupported trackmate file extension: {data_path}\")\n\n    tracks_df = tracks_df.apply(pd.to_numeric, errors=\"coerce\", downcast=\"integer\")\n\n    posx_key = \"POSITION_X\"\n    posy_key = \"POSITION_Y\"\n    frame_key = \"FRAME\"\n    track_key = \"TRACK_ID\"\n\n    mapper = {\n        \"X\": posx_key,\n        \"Y\": posy_key,\n        \"x\": posx_key,\n        \"y\": posy_key,\n        \"Slice n\u00b0\": frame_key,\n        \"Track n\u00b0\": track_key,\n    }\n\n    if \"t\" in tracks_df:\n        mapper.update({\"t\": frame_key})\n\n    tracks_df = tracks_df.rename(mapper=mapper, axis=1)\n\n    if data_path.endswith(\".csv\"):\n        # 0 index track and frame ids\n        if min(tracks_df[frame_key]) == 1:\n            tracks_df[frame_key] = tracks_df[frame_key] - 1\n\n        if min(tracks_df[track_key] == 1):\n            tracks_df[track_key] = tracks_df[track_key] - 1\n\n    return tracks_df\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.pose_bbox","title":"<code>pose_bbox(points, bbox_size)</code>","text":"<p>Calculate bbox around instance pose.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>an np array of shape nodes x 2,</p> required <code>bbox_size</code> <code>tuple[int] | int</code> <p>size of bbox either an int indicating square bbox or in (x,y)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Bounding box in [y1, x1, y2, x2] format.</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def pose_bbox(points: np.ndarray, bbox_size: tuple[int] | int) -&gt; torch.Tensor:\n    \"\"\"Calculate bbox around instance pose.\n\n    Args:\n        points: an np array of shape nodes x 2,\n        bbox_size: size of bbox either an int indicating square bbox or in (x,y)\n\n    Returns:\n        Bounding box in [y1, x1, y2, x2] format.\n    \"\"\"\n    if isinstance(bbox_size, int):\n        bbox_size = (bbox_size, bbox_size)\n\n    c = np.nanmean(points, axis=0)\n    bbox = torch.Tensor(\n        [\n            c[-1] - bbox_size[-1] / 2,\n            c[0] - bbox_size[0] / 2,\n            c[-1] + bbox_size[-1] / 2,\n            c[0] + bbox_size[0] / 2,\n        ]\n    )\n    return bbox\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.resize_and_pad","title":"<code>resize_and_pad(img, output_size)</code>","text":"<p>Resize and pad an image to fit a square output size.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor</code> <p>Image as a tensor of shape (channels, height, width).</p> required <code>output_size</code> <code>int</code> <p>Integer size of height and width of output.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The image zero padded to be of shape (channels, output_size, output_size).</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def resize_and_pad(img: torch.Tensor, output_size: int) -&gt; torch.Tensor:\n    \"\"\"Resize and pad an image to fit a square output size.\n\n    Args:\n        img: Image as a tensor of shape (channels, height, width).\n        output_size: Integer size of height and width of output.\n\n    Returns:\n        The image zero padded to be of shape (channels, output_size, output_size).\n    \"\"\"\n    # Figure out how to scale without breaking aspect ratio.\n    img_height, img_width = img.shape[-2:]\n    if img_width &lt; img_height:  # taller\n        crop_height = output_size\n        scale = crop_height / img_height\n        crop_width = int(img_width * scale)\n    else:  # wider\n        crop_width = output_size\n        scale = crop_width / img_width\n        crop_height = int(img_height * scale)\n\n    # Scale without breaking aspect ratio.\n    img = tvf.resize(img, size=[crop_height, crop_width])\n\n    # Pad to square.\n    img_height, img_width = img.shape[-2:]\n    hp1 = int((output_size - img_width) / 2)\n    vp1 = int((output_size - img_height) / 2)\n    hp2 = output_size - (img_width + hp1)\n    vp2 = output_size - (img_height + vp1)\n    padding = (hp1, vp1, hp2, vp2)\n    return tvf.pad(img, padding, 0, \"constant\")\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.sorted_anchors","title":"<code>sorted_anchors(labels)</code>","text":"<p>Sort anchor names from most instances with that node to least.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Labels</code> <p>a sleap_io.labels object containing all the labels for that video</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of anchor names sorted by most nodes to least nodes</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def sorted_anchors(labels: sio.Labels) -&gt; list[str]:\n    \"\"\"Sort anchor names from most instances with that node to least.\n\n    Args:\n        labels: a sleap_io.labels object containing all the labels for that video\n\n    Returns:\n        A list of anchor names sorted by most nodes to least nodes\n    \"\"\"\n    all_anchors = labels.skeletons[0].node_names\n\n    anchor_counts = {anchor: 0 for anchor in all_anchors}\n\n    for i in range(len(labels)):\n        lf = labels[i]\n        for instance in lf:\n            for anchor in all_anchors:\n                x, y = instance[anchor].x, instance[anchor].y\n                if np.isnan(x) or np.isnan(y):\n                    anchor_counts[anchor] += 1\n\n    sorted_anchors = sorted(anchor_counts.keys(), key=lambda k: anchor_counts[k])\n\n    return sorted_anchors\n</code></pre>"},{"location":"reference/dreem/datasets/data_utils/#dreem.datasets.data_utils.view_training_batch","title":"<code>view_training_batch(instances, num_frames=1, cmap=None)</code>","text":"<p>Display a grid of images from a batch of training instances.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[dict[str, list[ndarray]]]</code> <p>A list of training instances, where each instance is a dictionary containing the object crops.</p> required <code>num_frames</code> <code>int</code> <p>The number of frames to display per instance.</p> <code>1</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>dreem/datasets/data_utils.py</code> <pre><code>def view_training_batch(\n    instances: list[dict[str, list[np.ndarray]]], num_frames: int = 1, cmap=None\n) -&gt; None:\n    \"\"\"Display a grid of images from a batch of training instances.\n\n    Args:\n        instances: A list of training instances, where each instance is a\n            dictionary containing the object crops.\n        num_frames: The number of frames to display per instance.\n\n    Returns:\n        None\n    \"\"\"\n    num_crops = len(instances[0][\"crops\"])\n    num_columns = num_crops\n    num_rows = num_frames\n\n    base_size = 2\n    fig_size = (base_size * num_columns, base_size * num_rows)\n\n    fig, axes = plt.subplots(num_rows, num_columns, figsize=fig_size)\n\n    for i in range(num_frames):\n        for j, data in enumerate(instances[i][\"crops\"]):\n            try:\n                ax = (\n                    axes[j]\n                    if num_frames == 1\n                    else (axes[i] if num_crops == 1 else axes[i, j])\n                )\n\n                (ax.imshow(data.T) if cmap is None else ax.imshow(data.T, cmap=cmap))\n                ax.axis(\"off\")\n\n            except Exception as e:\n                print(e)\n                pass\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/dreem/datasets/microscopy_dataset/","title":"microscopy_dataset","text":""},{"location":"reference/dreem/datasets/microscopy_dataset/#dreem.datasets.microscopy_dataset","title":"<code>dreem.datasets.microscopy_dataset</code>","text":"<p>Module containing microscopy dataset.</p> <p>Classes:</p> Name Description <code>MicroscopyDataset</code> <p>Dataset for loading Microscopy Data.</p>"},{"location":"reference/dreem/datasets/microscopy_dataset/#dreem.datasets.microscopy_dataset.MicroscopyDataset","title":"<code>MicroscopyDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset for loading Microscopy Data.</p> <p>Methods:</p> Name Description <code>__del__</code> <p>Handle file closing before deletion.</p> <code>__init__</code> <p>Initialize MicroscopyDataset.</p> <code>get_indices</code> <p>Retrieve label and frame indices given batch index.</p> <code>get_instances</code> <p>Get an element of the dataset.</p> Source code in <code>dreem/datasets/microscopy_dataset.py</code> <pre><code>class MicroscopyDataset(BaseDataset):\n    \"\"\"Dataset for loading Microscopy Data.\"\"\"\n\n    def __init__(\n        self,\n        videos: list[str],\n        tracks: list[str],\n        source: str,\n        padding: int = 5,\n        crop_size: int = 20,\n        chunk: bool = False,\n        clip_length: int = 10,\n        mode: str = \"Train\",\n        augmentations: dict | None = None,\n        n_chunks: int | float = 1.0,\n        seed: int | None = None,\n    ):\n        \"\"\"Initialize MicroscopyDataset.\n\n        Args:\n            videos: paths to raw microscopy videos\n            tracks: paths to trackmate gt labels (either .xml or .csv)\n            source: file format of gt labels based on label generator\n            padding: amount of padding around object crops\n            crop_size: the size of the object crops\n            chunk: whether or not to chunk the dataset into batches\n            clip_length: the number of frames in each chunk\n            mode: `train` or `val`. Determines whether this dataset is used for\n                training or validation. Currently doesn't affect dataset logic\n            augmentations: An optional dict mapping augmentations to parameters. The keys\n                should map directly to augmentation classes in albumentations. Example:\n                    augs = {\n                        'Rotate': {'limit': [-90, 90]},\n                        'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},\n                        'RandomContrast': {'limit': 0.2}\n                    }\n            n_chunks: Number of chunks to subsample from.\n                Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n            seed: set a seed for reproducibility\n        \"\"\"\n        super().__init__(\n            tracks,\n            videos,\n            padding,\n            crop_size,\n            chunk,\n            clip_length,\n            mode,\n            augmentations,\n            n_chunks,\n            seed,\n        )\n\n        self.vid_files = videos\n        self.tracks = tracks\n        self.chunk = chunk\n        self.clip_length = clip_length\n        self.crop_size = crop_size\n        self.padding = padding\n        self.mode = mode.lower()\n        self.n_chunks = n_chunks\n        self.seed = seed\n\n        # if self.seed is not None:\n        #     np.random.seed(self.seed)\n        if augmentations and self.mode == \"train\":\n            self.augmentations = data_utils.build_augmentations(augmentations)\n        else:\n            self.augmentations = None\n\n        if source.lower() == \"trackmate\":\n            parser = data_utils.parse_trackmate\n        elif source.lower() in [\"icy\", \"isbi\"]:\n            parser = lambda x: data_utils.parse_synthetic(x, source=source)\n        else:\n            raise ValueError(\n                f\"{source} is unsupported! Must be one of [trackmate, icy, isbi]\"\n            )\n\n        self.labels = [\n            parser(self.tracks[video_idx]) for video_idx in range(len(self.tracks))\n        ]\n\n        self.videos = []\n        for vid_file in self.vid_files:\n            if not isinstance(vid_file, list):\n                self.videos.append(data_utils.LazyTiffStack(vid_file))\n            else:\n                self.videos.append([Image.open(frame_file) for frame_file in vid_file])\n        self.frame_idx = [\n            (\n                torch.arange(Image.open(video).n_frames)\n                if isinstance(video, str)\n                else torch.arange(len(video))\n            )\n            for video in self.vid_files\n        ]\n\n        # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n        # used in call to get_instances()\n        self.create_chunks_other()\n\n    def get_indices(self, idx: int) -&gt; tuple:\n        \"\"\"Retrieve label and frame indices given batch index.\n\n        Args:\n            idx: the index of the batch.\n        \"\"\"\n        return self.label_idx[idx], self.chunked_frame_idx[idx]\n\n    def get_instances(self, label_idx: list[int], frame_idx: list[int]) -&gt; list[Frame]:\n        \"\"\"Get an element of the dataset.\n\n        Args:\n            label_idx: index of the labels\n            frame_idx: index of the frames\n\n        Returns:\n            A list of Frames containing Instances to be tracked (See `dreem.io.data_structures for more info`)\n        \"\"\"\n        labels = self.labels[label_idx]\n        labels = labels.dropna(how=\"all\")\n\n        video = self.videos[label_idx]\n\n        frames = []\n        for frame_id in frame_idx:\n            instances, gt_track_ids, centroids = [], [], []\n\n            img = (\n                video.get_section(frame_id)\n                if not isinstance(video, list)\n                else np.array(video[frame_id])\n            )\n\n            lf = labels[labels[\"FRAME\"].astype(int) == frame_id.item()]\n\n            for instance in sorted(lf[\"TRACK_ID\"].unique()):\n                gt_track_ids.append(int(instance))\n\n                x = lf[lf[\"TRACK_ID\"] == instance][\"POSITION_X\"].iloc[0]\n                y = lf[lf[\"TRACK_ID\"] == instance][\"POSITION_Y\"].iloc[0]\n                centroids.append([x, y])\n\n            # albumentations wants (spatial, channels), ensure correct dims\n            if self.augmentations is not None:\n                for transform in self.augmentations:\n                    # for occlusion simulation, can remove if we don't want\n                    if isinstance(transform, A.CoarseDropout):\n                        transform.fill_value = random.randint(0, 255)\n\n                augmented = self.augmentations(\n                    image=img,\n                    keypoints=np.vstack(centroids),\n                )\n                img, centroids = augmented[\"image\"], augmented[\"keypoints\"]\n\n            img = torch.Tensor(img)\n\n            # torch wants (channels, spatial) - ensure correct dims\n            if len(img.shape) == 2:\n                img = img.unsqueeze(0)\n            elif len(img.shape) == 3:\n                if img.shape[2] == 3:\n                    img = img.T  # todo: check for edge cases\n\n            for gt_id in range(len(gt_track_ids)):\n                c = centroids[gt_id]\n                bbox = data_utils.pad_bbox(\n                    data_utils.get_bbox([int(c[0]), int(c[1])], self.crop_size),\n                    padding=self.padding,\n                )\n                crop = data_utils.crop_bbox(img, bbox)\n\n                instances.append(\n                    Instance(\n                        gt_track_id=gt_track_ids[gt_id],\n                        pred_track_id=-1,\n                        bbox=bbox,\n                        crop=crop,\n                    )\n                )\n\n            if self.mode == \"train\":\n                np.random.shuffle(instances)\n\n            frames.append(\n                Frame(\n                    video_id=label_idx,\n                    frame_id=frame_id,\n                    img_shape=img.shape,\n                    instances=instances,\n                )\n            )\n\n        return frames\n\n    def __del__(self):\n        \"\"\"Handle file closing before deletion.\"\"\"\n        for vid_reader in self.videos:\n            if not isinstance(vid_reader, list):\n                vid_reader.close()\n            else:\n                for frame_reader in vid_reader:\n                    frame_reader.close()\n</code></pre>"},{"location":"reference/dreem/datasets/microscopy_dataset/#dreem.datasets.microscopy_dataset.MicroscopyDataset.__del__","title":"<code>__del__()</code>","text":"<p>Handle file closing before deletion.</p> Source code in <code>dreem/datasets/microscopy_dataset.py</code> <pre><code>def __del__(self):\n    \"\"\"Handle file closing before deletion.\"\"\"\n    for vid_reader in self.videos:\n        if not isinstance(vid_reader, list):\n            vid_reader.close()\n        else:\n            for frame_reader in vid_reader:\n                frame_reader.close()\n</code></pre>"},{"location":"reference/dreem/datasets/microscopy_dataset/#dreem.datasets.microscopy_dataset.MicroscopyDataset.__init__","title":"<code>__init__(videos, tracks, source, padding=5, crop_size=20, chunk=False, clip_length=10, mode='Train', augmentations=None, n_chunks=1.0, seed=None)</code>","text":"<p>Initialize MicroscopyDataset.</p> <p>Parameters:</p> Name Type Description Default <code>videos</code> <code>list[str]</code> <p>paths to raw microscopy videos</p> required <code>tracks</code> <code>list[str]</code> <p>paths to trackmate gt labels (either .xml or .csv)</p> required <code>source</code> <code>str</code> <p>file format of gt labels based on label generator</p> required <code>padding</code> <code>int</code> <p>amount of padding around object crops</p> <code>5</code> <code>crop_size</code> <code>int</code> <p>the size of the object crops</p> <code>20</code> <code>chunk</code> <code>bool</code> <p>whether or not to chunk the dataset into batches</p> <code>False</code> <code>clip_length</code> <code>int</code> <p>the number of frames in each chunk</p> <code>10</code> <code>mode</code> <code>str</code> <p><code>train</code> or <code>val</code>. Determines whether this dataset is used for training or validation. Currently doesn't affect dataset logic</p> <code>'Train'</code> <code>augmentations</code> <code>dict | None</code> <p>An optional dict mapping augmentations to parameters. The keys should map directly to augmentation classes in albumentations. Example:     augs = {         'Rotate': {'limit': [-90, 90]},         'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},         'RandomContrast': {'limit': 0.2}     }</p> <code>None</code> <code>n_chunks</code> <code>int | float</code> <p>Number of chunks to subsample from. Can either a fraction of the dataset (ie (0,1.0]) or number of chunks</p> <code>1.0</code> <code>seed</code> <code>int | None</code> <p>set a seed for reproducibility</p> <code>None</code> Source code in <code>dreem/datasets/microscopy_dataset.py</code> <pre><code>def __init__(\n    self,\n    videos: list[str],\n    tracks: list[str],\n    source: str,\n    padding: int = 5,\n    crop_size: int = 20,\n    chunk: bool = False,\n    clip_length: int = 10,\n    mode: str = \"Train\",\n    augmentations: dict | None = None,\n    n_chunks: int | float = 1.0,\n    seed: int | None = None,\n):\n    \"\"\"Initialize MicroscopyDataset.\n\n    Args:\n        videos: paths to raw microscopy videos\n        tracks: paths to trackmate gt labels (either .xml or .csv)\n        source: file format of gt labels based on label generator\n        padding: amount of padding around object crops\n        crop_size: the size of the object crops\n        chunk: whether or not to chunk the dataset into batches\n        clip_length: the number of frames in each chunk\n        mode: `train` or `val`. Determines whether this dataset is used for\n            training or validation. Currently doesn't affect dataset logic\n        augmentations: An optional dict mapping augmentations to parameters. The keys\n            should map directly to augmentation classes in albumentations. Example:\n                augs = {\n                    'Rotate': {'limit': [-90, 90]},\n                    'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0},\n                    'RandomContrast': {'limit': 0.2}\n                }\n        n_chunks: Number of chunks to subsample from.\n            Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n        seed: set a seed for reproducibility\n    \"\"\"\n    super().__init__(\n        tracks,\n        videos,\n        padding,\n        crop_size,\n        chunk,\n        clip_length,\n        mode,\n        augmentations,\n        n_chunks,\n        seed,\n    )\n\n    self.vid_files = videos\n    self.tracks = tracks\n    self.chunk = chunk\n    self.clip_length = clip_length\n    self.crop_size = crop_size\n    self.padding = padding\n    self.mode = mode.lower()\n    self.n_chunks = n_chunks\n    self.seed = seed\n\n    # if self.seed is not None:\n    #     np.random.seed(self.seed)\n    if augmentations and self.mode == \"train\":\n        self.augmentations = data_utils.build_augmentations(augmentations)\n    else:\n        self.augmentations = None\n\n    if source.lower() == \"trackmate\":\n        parser = data_utils.parse_trackmate\n    elif source.lower() in [\"icy\", \"isbi\"]:\n        parser = lambda x: data_utils.parse_synthetic(x, source=source)\n    else:\n        raise ValueError(\n            f\"{source} is unsupported! Must be one of [trackmate, icy, isbi]\"\n        )\n\n    self.labels = [\n        parser(self.tracks[video_idx]) for video_idx in range(len(self.tracks))\n    ]\n\n    self.videos = []\n    for vid_file in self.vid_files:\n        if not isinstance(vid_file, list):\n            self.videos.append(data_utils.LazyTiffStack(vid_file))\n        else:\n            self.videos.append([Image.open(frame_file) for frame_file in vid_file])\n    self.frame_idx = [\n        (\n            torch.arange(Image.open(video).n_frames)\n            if isinstance(video, str)\n            else torch.arange(len(video))\n        )\n        for video in self.vid_files\n    ]\n\n    # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n    # used in call to get_instances()\n    self.create_chunks_other()\n</code></pre>"},{"location":"reference/dreem/datasets/microscopy_dataset/#dreem.datasets.microscopy_dataset.MicroscopyDataset.get_indices","title":"<code>get_indices(idx)</code>","text":"<p>Retrieve label and frame indices given batch index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the batch.</p> required Source code in <code>dreem/datasets/microscopy_dataset.py</code> <pre><code>def get_indices(self, idx: int) -&gt; tuple:\n    \"\"\"Retrieve label and frame indices given batch index.\n\n    Args:\n        idx: the index of the batch.\n    \"\"\"\n    return self.label_idx[idx], self.chunked_frame_idx[idx]\n</code></pre>"},{"location":"reference/dreem/datasets/microscopy_dataset/#dreem.datasets.microscopy_dataset.MicroscopyDataset.get_instances","title":"<code>get_instances(label_idx, frame_idx)</code>","text":"<p>Get an element of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>label_idx</code> <code>list[int]</code> <p>index of the labels</p> required <code>frame_idx</code> <code>list[int]</code> <p>index of the frames</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A list of Frames containing Instances to be tracked (See <code>dreem.io.data_structures for more info</code>)</p> Source code in <code>dreem/datasets/microscopy_dataset.py</code> <pre><code>def get_instances(self, label_idx: list[int], frame_idx: list[int]) -&gt; list[Frame]:\n    \"\"\"Get an element of the dataset.\n\n    Args:\n        label_idx: index of the labels\n        frame_idx: index of the frames\n\n    Returns:\n        A list of Frames containing Instances to be tracked (See `dreem.io.data_structures for more info`)\n    \"\"\"\n    labels = self.labels[label_idx]\n    labels = labels.dropna(how=\"all\")\n\n    video = self.videos[label_idx]\n\n    frames = []\n    for frame_id in frame_idx:\n        instances, gt_track_ids, centroids = [], [], []\n\n        img = (\n            video.get_section(frame_id)\n            if not isinstance(video, list)\n            else np.array(video[frame_id])\n        )\n\n        lf = labels[labels[\"FRAME\"].astype(int) == frame_id.item()]\n\n        for instance in sorted(lf[\"TRACK_ID\"].unique()):\n            gt_track_ids.append(int(instance))\n\n            x = lf[lf[\"TRACK_ID\"] == instance][\"POSITION_X\"].iloc[0]\n            y = lf[lf[\"TRACK_ID\"] == instance][\"POSITION_Y\"].iloc[0]\n            centroids.append([x, y])\n\n        # albumentations wants (spatial, channels), ensure correct dims\n        if self.augmentations is not None:\n            for transform in self.augmentations:\n                # for occlusion simulation, can remove if we don't want\n                if isinstance(transform, A.CoarseDropout):\n                    transform.fill_value = random.randint(0, 255)\n\n            augmented = self.augmentations(\n                image=img,\n                keypoints=np.vstack(centroids),\n            )\n            img, centroids = augmented[\"image\"], augmented[\"keypoints\"]\n\n        img = torch.Tensor(img)\n\n        # torch wants (channels, spatial) - ensure correct dims\n        if len(img.shape) == 2:\n            img = img.unsqueeze(0)\n        elif len(img.shape) == 3:\n            if img.shape[2] == 3:\n                img = img.T  # todo: check for edge cases\n\n        for gt_id in range(len(gt_track_ids)):\n            c = centroids[gt_id]\n            bbox = data_utils.pad_bbox(\n                data_utils.get_bbox([int(c[0]), int(c[1])], self.crop_size),\n                padding=self.padding,\n            )\n            crop = data_utils.crop_bbox(img, bbox)\n\n            instances.append(\n                Instance(\n                    gt_track_id=gt_track_ids[gt_id],\n                    pred_track_id=-1,\n                    bbox=bbox,\n                    crop=crop,\n                )\n            )\n\n        if self.mode == \"train\":\n            np.random.shuffle(instances)\n\n        frames.append(\n            Frame(\n                video_id=label_idx,\n                frame_id=frame_id,\n                img_shape=img.shape,\n                instances=instances,\n            )\n        )\n\n    return frames\n</code></pre>"},{"location":"reference/dreem/datasets/sleap_dataset/","title":"sleap_dataset","text":""},{"location":"reference/dreem/datasets/sleap_dataset/#dreem.datasets.sleap_dataset","title":"<code>dreem.datasets.sleap_dataset</code>","text":"<p>Module containing logic for loading sleap datasets.</p> <p>Classes:</p> Name Description <code>SleapDataset</code> <p>Dataset for loading animal behavior data from sleap.</p>"},{"location":"reference/dreem/datasets/sleap_dataset/#dreem.datasets.sleap_dataset.SleapDataset","title":"<code>SleapDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset for loading animal behavior data from sleap.</p> <p>Methods:</p> Name Description <code>__del__</code> <p>Handle file closing before garbage collection.</p> <code>__init__</code> <p>Initialize SleapDataset.</p> <code>get_indices</code> <p>Retrieve label and frame indices given batch index.</p> <code>get_instances</code> <p>Get an element of the dataset.</p> Source code in <code>dreem/datasets/sleap_dataset.py</code> <pre><code>class SleapDataset(BaseDataset):\n    \"\"\"Dataset for loading animal behavior data from sleap.\"\"\"\n\n    def __init__(\n        self,\n        slp_files: list[str],\n        video_files: list[str],\n        data_dirs: Optional[list[str]] = None,\n        padding: int = 5,\n        crop_size: Union[int, list[int]] = 128,\n        anchors: int | list[str] | str = \"\",\n        chunk: bool = True,\n        clip_length: int = 16,\n        mode: str = \"train\",\n        handle_missing: str = \"centroid\",\n        augmentations: dict | None = None,\n        n_chunks: int | float = 1.0,\n        seed: int | None = None,\n        verbose: bool = False,\n        normalize_image: bool = True,\n        max_batching_gap: int = 15,\n        use_tight_bbox: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize SleapDataset.\n\n        Args:\n            slp_files: a list of .slp files storing tracking annotations\n            video_files: a list of paths to video files\n            data_dirs: a path, or a list of paths to data directories. If provided, crop_size should be a list of integers\n                with the same length as data_dirs.\n            padding: amount of padding around object crops\n            crop_size: the size of the object crops. Can be either:\n                - An integer specifying a single crop size for all objects\n                - A list of integers specifying different crop sizes for different data directories\n            anchors: One of:\n                        * a string indicating a single node to center crops around\n                        * a list of skeleton node names to be used as the center of crops\n                        * an int indicating the number of anchors to randomly select\n                    If unavailable then crop around the midpoint between all visible anchors.\n            chunk: whether or not to chunk the dataset into batches\n            clip_length: the number of frames in each chunk\n            mode: `train`, `val`, or `test`. Determines whether this dataset is used for\n                training, validation/testing/inference.\n            handle_missing: how to handle missing single nodes. one of `[\"drop\", \"ignore\", \"centroid\"]`.\n                            if \"drop\" then we dont include instances which are missing the `anchor`.\n                            if \"ignore\" then we use a mask instead of a crop and nan centroids/bboxes.\n                            if \"centroid\" then we default to the pose centroid as the node to crop around.\n            augmentations: An optional dict mapping augmentations to parameters. The keys\n                should map directly to augmentation classes in albumentations. Example:\n                    augmentations = {\n                        'Rotate': {'limit': [-90, 90], 'p': 0.5},\n                        'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0, 'p': 0.2},\n                        'RandomContrast': {'limit': 0.2, 'p': 0.6}\n                    }\n            n_chunks: Number of chunks to subsample from.\n                Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n            seed: set a seed for reproducibility\n            verbose: boolean representing whether to print\n            normalize_image: whether to normalize the image to [0, 1]\n            max_batching_gap: the max number of frames that can be unlabelled before starting a new batch\n            use_tight_bbox: whether to use tight bounding box (around keypoints) instead of the default square bounding box\n        \"\"\"\n        super().__init__(\n            slp_files,\n            video_files,\n            padding,\n            crop_size,\n            chunk,\n            clip_length,\n            mode,\n            augmentations,\n            n_chunks,\n            seed,\n        )\n\n        self.slp_files = slp_files\n        self.data_dirs = data_dirs\n        self.video_files = video_files\n        self.padding = padding\n        self.crop_size = crop_size\n        self.chunk = chunk\n        self.clip_length = clip_length\n        self.mode = mode.lower()\n        self.handle_missing = handle_missing.lower()\n        self.n_chunks = n_chunks\n        self.seed = seed\n        self.normalize_image = normalize_image\n        self.max_batching_gap = max_batching_gap\n        self.use_tight_bbox = use_tight_bbox\n\n        if isinstance(anchors, int):\n            self.anchors = anchors\n        elif isinstance(anchors, str):\n            self.anchors = [anchors]\n        else:\n            self.anchors = anchors\n\n        if not isinstance(self.data_dirs, list):\n            self.data_dirs = [self.data_dirs]\n\n        if not isinstance(self.crop_size, list):\n            # make a list so its handled consistently if multiple crops are used\n            if len(self.data_dirs) &gt; 0:  # for test mode, data_dirs is []\n                self.crop_size = [self.crop_size] * len(self.data_dirs)\n            else:\n                self.crop_size = [self.crop_size]\n\n        if len(self.data_dirs) &gt; 0 and len(self.crop_size) != len(self.data_dirs):\n            raise ValueError(\n                f\"If a list of crop sizes or data directories are given,\"\n                f\"they must have the same length but got {len(self.crop_size)} \"\n                f\"and {len(self.data_dirs)}\"\n            )\n\n        if (\n            isinstance(self.anchors, list) and len(self.anchors) == 0\n        ) or self.anchors == 0:\n            raise ValueError(f\"Must provide at least one anchor but got {self.anchors}\")\n\n        self.verbose = verbose\n\n        # if self.seed is not None:\n        #     np.random.seed(self.seed)\n\n        # load_slp is a wrapper around sio.load_slp for frame gap checks\n        self.labels = []\n        self.annotated_segments = {}\n        for slp_file in self.slp_files:\n            labels, annotated_segments = data_utils.load_slp(slp_file)\n            self.labels.append(labels)\n            self.annotated_segments[slp_file] = annotated_segments\n\n        self.videos = [imageio.get_reader(vid_file) for vid_file in self.vid_files]\n        # do we need this? would need to update with sleap-io\n\n        # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n        # used in call to get_instances()\n        self.create_chunks_slp()\n\n    def get_indices(self, idx: int) -&gt; tuple:\n        \"\"\"Retrieve label and frame indices given batch index.\n\n        Args:\n            idx: the index of the batch.\n        \"\"\"\n        return self.label_idx[idx], self.chunked_frame_idx[idx]\n\n    def get_instances(\n        self, label_idx: list[int], frame_idx: torch.Tensor\n    ) -&gt; list[Frame]:\n        \"\"\"Get an element of the dataset.\n\n        Args:\n            label_idx: index of the labels\n            frame_idx: indices of the frames to load in to the batch\n\n        Returns:\n            A list of `dreem.io.Frame` objects containing metadata and instance data for the batch/clip.\n\n        \"\"\"\n        sleap_labels_obj = self.labels[label_idx]\n        video_name = self.video_files[label_idx]\n\n        # get the correct crop size based on the video\n        video_par_path = Path(video_name).parent\n        if len(self.data_dirs) &gt; 0:\n            crop_size = self.crop_size[0]\n            for j, data_dir in enumerate(self.data_dirs):\n                if Path(data_dir) == video_par_path:\n                    crop_size = self.crop_size[j]\n                    break\n        else:\n            crop_size = self.crop_size[0]\n\n        vid_reader = self.videos[label_idx]\n\n        skeleton = sleap_labels_obj.skeletons[-1]\n\n        frames = []\n        max_crop_h, max_crop_w = 0, 0\n        for i, frame_ind in enumerate(frame_idx):\n            (\n                instances,\n                gt_track_ids,\n                poses,\n                shown_poses,\n                point_scores,\n                instance_score,\n            ) = ([], [], [], [], [], [])\n\n            frame_ind = int(frame_ind)\n\n            # sleap-io method for indexing a Labels() object based on the frame's index\n            lf = sleap_labels_obj[(sleap_labels_obj.video, frame_ind)]\n            if frame_ind != lf.frame_idx:\n                logger.warning(f\"Frame index mismatch: {frame_ind} != {lf.frame_idx}\")\n\n            try:\n                img = vid_reader.get_data(int(frame_ind))\n            except IndexError as e:\n                logger.warning(\n                    f\"Could not read frame {frame_ind} from {video_name} due to {e}\"\n                )\n                continue\n\n            if len(img.shape) == 2:\n                img = img.expand_dims(-1)\n            h, w, c = img.shape\n\n            if c == 1:\n                img = np.concatenate(\n                    [img, img, img], axis=-1\n                )  # convert to grayscale to rgb\n\n            if np.issubdtype(img.dtype, np.integer):  # convert int to float\n                img = img.astype(np.float32)\n                if self.normalize_image:\n                    img = img / 255\n\n            n_instances_dropped = 0\n\n            gt_instances = []\n            # don't load instances that have been 'greyed out' i.e. all nans for keypoints\n            for inst in lf.instances:\n                pts = np.array([p for p in inst.numpy()])\n                if np.isnan(pts).all():\n                    continue\n                else:\n                    gt_instances.append(inst)\n\n            dict_instances = {}\n            no_track_instances = []\n            for instance in gt_instances:\n                if instance.track is not None:\n                    gt_track_id = sleap_labels_obj.tracks.index(instance.track)\n                    if gt_track_id not in dict_instances:\n                        dict_instances[gt_track_id] = instance\n                    else:\n                        existing_instance = dict_instances[gt_track_id]\n                        # if existing is PredictedInstance and current is not, then current is a UserInstance and should be used\n                        if isinstance(\n                            existing_instance, sio.PredictedInstance\n                        ) and not isinstance(instance, sio.PredictedInstance):\n                            dict_instances[gt_track_id] = instance\n                else:\n                    no_track_instances.append(instance)\n\n            gt_instances = list(dict_instances.values()) + no_track_instances\n\n            if self.mode == \"train\":\n                np.random.shuffle(gt_instances)\n\n            for instance in gt_instances:\n                if (\n                    np.random.uniform() &lt; self.instance_dropout[\"p\"]\n                    and n_instances_dropped &lt; self.instance_dropout[\"n\"]\n                ):\n                    n_instances_dropped += 1\n                    continue\n\n                if instance.track is not None:\n                    gt_track_id = sleap_labels_obj.tracks.index(instance.track)\n                else:\n                    gt_track_id = -1\n                gt_track_ids.append(gt_track_id)\n\n                poses.append(\n                    dict(\n                        zip(\n                            [n.name for n in instance.skeleton.nodes],\n                            [p for p in instance.numpy()],\n                        )\n                    )\n                )\n\n                shown_poses = [\n                    {\n                        key: val\n                        for key, val in instance.items()\n                        if not np.isnan(val).any()\n                    }\n                    for instance in poses\n                ]\n\n                point_scores.append(\n                    np.array(\n                        [\n                            (\n                                1.0  # point scores not reliably available in sleap io PredictedPointsArray\n                                # point.score\n                                # if isinstance(point, sio.PredictedPoint)\n                                # else 1.0\n                            )\n                            for point in instance.numpy()\n                        ]\n                    )\n                )\n                if isinstance(instance, sio.PredictedInstance):\n                    instance_score.append(instance.score)\n                else:\n                    instance_score.append(1.0)\n            # augmentations\n            if self.augmentations is not None:\n                for transform in self.augmentations:\n                    if isinstance(transform, A.CoarseDropout):\n                        transform.fill_value = random.randint(0, 255)\n\n                if shown_poses:\n                    keypoints = np.vstack([list(s.values()) for s in shown_poses])\n\n                else:\n                    keypoints = []\n\n                augmented = self.augmentations(image=img, keypoints=keypoints)\n\n                img, aug_poses = augmented[\"image\"], augmented[\"keypoints\"]\n\n                aug_poses = [\n                    arr\n                    for arr in np.split(\n                        np.array(aug_poses),\n                        np.array([len(s) for s in shown_poses]).cumsum(),\n                    )\n                    if arr.size != 0\n                ]\n\n                aug_poses = [\n                    dict(zip(list(pose_dict.keys()), aug_pose_arr.tolist()))\n                    for aug_pose_arr, pose_dict in zip(aug_poses, shown_poses)\n                ]\n\n                _ = [\n                    pose.update(aug_pose)\n                    for pose, aug_pose in zip(shown_poses, aug_poses)\n                ]\n\n            img = tvf.to_tensor(img)\n\n            for j in range(len(gt_track_ids)):\n                pose = shown_poses[j]\n\n                \"\"\"Check for anchor\"\"\"\n                crops = []\n                boxes = []\n                centroids = {}\n\n                if isinstance(self.anchors, int):\n                    anchors_to_choose = list(pose.keys()) + [\"midpoint\"]\n                    anchors = np.random.choice(anchors_to_choose, self.anchors)\n                else:\n                    anchors = self.anchors\n\n                dropped_anchors = self.node_dropout(anchors)\n\n                for anchor in anchors:\n                    if anchor in dropped_anchors:\n                        centroid = np.array([np.nan, np.nan])\n\n                    elif anchor == \"midpoint\" or anchor == \"centroid\":\n                        centroid = np.nanmean(np.array(list(pose.values())), axis=0)\n\n                    elif anchor in pose:\n                        centroid = np.array(pose[anchor])\n                        if np.isnan(centroid).any():\n                            centroid = np.array([np.nan, np.nan])\n\n                    elif (\n                        anchor not in pose\n                        and len(anchors) == 1\n                        and self.handle_missing == \"centroid\"\n                    ):\n                        anchor = \"midpoint\"\n                        centroid = np.nanmean(np.array(list(pose.values())), axis=0)\n\n                    else:\n                        centroid = np.array([np.nan, np.nan])\n\n                    if np.isnan(centroid).all():\n                        bbox = torch.tensor([np.nan, np.nan, np.nan, np.nan])\n\n                    else:\n                        if self.use_tight_bbox and len(pose) &gt; 1:\n                            # tight bbox\n                            # dont allow this for centroid-only poses!\n                            arr_pose = np.array(list(pose.values()))\n                            # note bbox will be a different size for each instance; padded at the end of the loop\n                            bbox = data_utils.get_tight_bbox(arr_pose)\n\n                        else:\n                            bbox = data_utils.pad_bbox(\n                                data_utils.get_bbox(centroid, crop_size),\n                                padding=self.padding,\n                            )\n\n                    if bbox.isnan().all():\n                        crop = torch.zeros(\n                            c,\n                            crop_size + 2 * self.padding,\n                            crop_size + 2 * self.padding,\n                            dtype=img.dtype,\n                        )\n                    else:\n                        crop = data_utils.crop_bbox(img, bbox)\n\n                    crops.append(crop)\n                    # get max h,w for padding for tight bboxes\n                    c, h, w = crop.shape\n                    if h &gt; max_crop_h:\n                        max_crop_h = h\n                    if w &gt; max_crop_w:\n                        max_crop_w = w\n\n                    centroids[anchor] = centroid\n                    boxes.append(bbox)\n\n                if len(crops) &gt; 0:\n                    crops = torch.concat(crops, dim=0)\n\n                if len(boxes) &gt; 0:\n                    boxes = torch.stack(boxes, dim=0)\n\n                if self.handle_missing == \"drop\" and boxes.isnan().any():\n                    continue\n\n                instance = Instance(\n                    gt_track_id=gt_track_ids[j],\n                    pred_track_id=-1,\n                    crop=crops,\n                    centroid=centroids,\n                    bbox=boxes,\n                    skeleton=skeleton,\n                    pose=poses[j],\n                    point_scores=point_scores[j],\n                    instance_score=instance_score[j],\n                )\n\n                instances.append(instance)\n\n            frame = Frame(\n                video_id=label_idx,\n                frame_id=frame_ind,\n                vid_file=video_name,\n                img_shape=img.shape,\n                instances=instances,\n            )\n            frames.append(frame)\n\n        # pad bbox to max size\n        if self.use_tight_bbox:\n            # bound the max crop size to the user defined crop size\n            max_crop_h = crop_size if max_crop_h == 0 else min(max_crop_h, crop_size)\n            max_crop_w = crop_size if max_crop_w == 0 else min(max_crop_w, crop_size)\n            # gather all the crops\n            for frame in frames:\n                for instance in frame.instances:\n                    data_utils.pad_variable_size_crops(\n                        instance, (max_crop_h, max_crop_w)\n                    )\n        return frames\n\n    def __del__(self):\n        \"\"\"Handle file closing before garbage collection.\"\"\"\n        for reader in self.videos:\n            reader.close()\n</code></pre>"},{"location":"reference/dreem/datasets/sleap_dataset/#dreem.datasets.sleap_dataset.SleapDataset.__del__","title":"<code>__del__()</code>","text":"<p>Handle file closing before garbage collection.</p> Source code in <code>dreem/datasets/sleap_dataset.py</code> <pre><code>def __del__(self):\n    \"\"\"Handle file closing before garbage collection.\"\"\"\n    for reader in self.videos:\n        reader.close()\n</code></pre>"},{"location":"reference/dreem/datasets/sleap_dataset/#dreem.datasets.sleap_dataset.SleapDataset.__init__","title":"<code>__init__(slp_files, video_files, data_dirs=None, padding=5, crop_size=128, anchors='', chunk=True, clip_length=16, mode='train', handle_missing='centroid', augmentations=None, n_chunks=1.0, seed=None, verbose=False, normalize_image=True, max_batching_gap=15, use_tight_bbox=False, **kwargs)</code>","text":"<p>Initialize SleapDataset.</p> <p>Parameters:</p> Name Type Description Default <code>slp_files</code> <code>list[str]</code> <p>a list of .slp files storing tracking annotations</p> required <code>video_files</code> <code>list[str]</code> <p>a list of paths to video files</p> required <code>data_dirs</code> <code>Optional[list[str]]</code> <p>a path, or a list of paths to data directories. If provided, crop_size should be a list of integers with the same length as data_dirs.</p> <code>None</code> <code>padding</code> <code>int</code> <p>amount of padding around object crops</p> <code>5</code> <code>crop_size</code> <code>Union[int, list[int]]</code> <p>the size of the object crops. Can be either: - An integer specifying a single crop size for all objects - A list of integers specifying different crop sizes for different data directories</p> <code>128</code> <code>anchors</code> <code>int | list[str] | str</code> <p>One of:         * a string indicating a single node to center crops around         * a list of skeleton node names to be used as the center of crops         * an int indicating the number of anchors to randomly select     If unavailable then crop around the midpoint between all visible anchors.</p> <code>''</code> <code>chunk</code> <code>bool</code> <p>whether or not to chunk the dataset into batches</p> <code>True</code> <code>clip_length</code> <code>int</code> <p>the number of frames in each chunk</p> <code>16</code> <code>mode</code> <code>str</code> <p><code>train</code>, <code>val</code>, or <code>test</code>. Determines whether this dataset is used for training, validation/testing/inference.</p> <code>'train'</code> <code>handle_missing</code> <code>str</code> <p>how to handle missing single nodes. one of <code>[\"drop\", \"ignore\", \"centroid\"]</code>.             if \"drop\" then we dont include instances which are missing the <code>anchor</code>.             if \"ignore\" then we use a mask instead of a crop and nan centroids/bboxes.             if \"centroid\" then we default to the pose centroid as the node to crop around.</p> <code>'centroid'</code> <code>augmentations</code> <code>dict | None</code> <p>An optional dict mapping augmentations to parameters. The keys should map directly to augmentation classes in albumentations. Example:     augmentations = {         'Rotate': {'limit': [-90, 90], 'p': 0.5},         'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0, 'p': 0.2},         'RandomContrast': {'limit': 0.2, 'p': 0.6}     }</p> <code>None</code> <code>n_chunks</code> <code>int | float</code> <p>Number of chunks to subsample from. Can either a fraction of the dataset (ie (0,1.0]) or number of chunks</p> <code>1.0</code> <code>seed</code> <code>int | None</code> <p>set a seed for reproducibility</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>boolean representing whether to print</p> <code>False</code> <code>normalize_image</code> <code>bool</code> <p>whether to normalize the image to [0, 1]</p> <code>True</code> <code>max_batching_gap</code> <code>int</code> <p>the max number of frames that can be unlabelled before starting a new batch</p> <code>15</code> <code>use_tight_bbox</code> <code>bool</code> <p>whether to use tight bounding box (around keypoints) instead of the default square bounding box</p> <code>False</code> Source code in <code>dreem/datasets/sleap_dataset.py</code> <pre><code>def __init__(\n    self,\n    slp_files: list[str],\n    video_files: list[str],\n    data_dirs: Optional[list[str]] = None,\n    padding: int = 5,\n    crop_size: Union[int, list[int]] = 128,\n    anchors: int | list[str] | str = \"\",\n    chunk: bool = True,\n    clip_length: int = 16,\n    mode: str = \"train\",\n    handle_missing: str = \"centroid\",\n    augmentations: dict | None = None,\n    n_chunks: int | float = 1.0,\n    seed: int | None = None,\n    verbose: bool = False,\n    normalize_image: bool = True,\n    max_batching_gap: int = 15,\n    use_tight_bbox: bool = False,\n    **kwargs,\n):\n    \"\"\"Initialize SleapDataset.\n\n    Args:\n        slp_files: a list of .slp files storing tracking annotations\n        video_files: a list of paths to video files\n        data_dirs: a path, or a list of paths to data directories. If provided, crop_size should be a list of integers\n            with the same length as data_dirs.\n        padding: amount of padding around object crops\n        crop_size: the size of the object crops. Can be either:\n            - An integer specifying a single crop size for all objects\n            - A list of integers specifying different crop sizes for different data directories\n        anchors: One of:\n                    * a string indicating a single node to center crops around\n                    * a list of skeleton node names to be used as the center of crops\n                    * an int indicating the number of anchors to randomly select\n                If unavailable then crop around the midpoint between all visible anchors.\n        chunk: whether or not to chunk the dataset into batches\n        clip_length: the number of frames in each chunk\n        mode: `train`, `val`, or `test`. Determines whether this dataset is used for\n            training, validation/testing/inference.\n        handle_missing: how to handle missing single nodes. one of `[\"drop\", \"ignore\", \"centroid\"]`.\n                        if \"drop\" then we dont include instances which are missing the `anchor`.\n                        if \"ignore\" then we use a mask instead of a crop and nan centroids/bboxes.\n                        if \"centroid\" then we default to the pose centroid as the node to crop around.\n        augmentations: An optional dict mapping augmentations to parameters. The keys\n            should map directly to augmentation classes in albumentations. Example:\n                augmentations = {\n                    'Rotate': {'limit': [-90, 90], 'p': 0.5},\n                    'GaussianBlur': {'blur_limit': (3, 7), 'sigma_limit': 0, 'p': 0.2},\n                    'RandomContrast': {'limit': 0.2, 'p': 0.6}\n                }\n        n_chunks: Number of chunks to subsample from.\n            Can either a fraction of the dataset (ie (0,1.0]) or number of chunks\n        seed: set a seed for reproducibility\n        verbose: boolean representing whether to print\n        normalize_image: whether to normalize the image to [0, 1]\n        max_batching_gap: the max number of frames that can be unlabelled before starting a new batch\n        use_tight_bbox: whether to use tight bounding box (around keypoints) instead of the default square bounding box\n    \"\"\"\n    super().__init__(\n        slp_files,\n        video_files,\n        padding,\n        crop_size,\n        chunk,\n        clip_length,\n        mode,\n        augmentations,\n        n_chunks,\n        seed,\n    )\n\n    self.slp_files = slp_files\n    self.data_dirs = data_dirs\n    self.video_files = video_files\n    self.padding = padding\n    self.crop_size = crop_size\n    self.chunk = chunk\n    self.clip_length = clip_length\n    self.mode = mode.lower()\n    self.handle_missing = handle_missing.lower()\n    self.n_chunks = n_chunks\n    self.seed = seed\n    self.normalize_image = normalize_image\n    self.max_batching_gap = max_batching_gap\n    self.use_tight_bbox = use_tight_bbox\n\n    if isinstance(anchors, int):\n        self.anchors = anchors\n    elif isinstance(anchors, str):\n        self.anchors = [anchors]\n    else:\n        self.anchors = anchors\n\n    if not isinstance(self.data_dirs, list):\n        self.data_dirs = [self.data_dirs]\n\n    if not isinstance(self.crop_size, list):\n        # make a list so its handled consistently if multiple crops are used\n        if len(self.data_dirs) &gt; 0:  # for test mode, data_dirs is []\n            self.crop_size = [self.crop_size] * len(self.data_dirs)\n        else:\n            self.crop_size = [self.crop_size]\n\n    if len(self.data_dirs) &gt; 0 and len(self.crop_size) != len(self.data_dirs):\n        raise ValueError(\n            f\"If a list of crop sizes or data directories are given,\"\n            f\"they must have the same length but got {len(self.crop_size)} \"\n            f\"and {len(self.data_dirs)}\"\n        )\n\n    if (\n        isinstance(self.anchors, list) and len(self.anchors) == 0\n    ) or self.anchors == 0:\n        raise ValueError(f\"Must provide at least one anchor but got {self.anchors}\")\n\n    self.verbose = verbose\n\n    # if self.seed is not None:\n    #     np.random.seed(self.seed)\n\n    # load_slp is a wrapper around sio.load_slp for frame gap checks\n    self.labels = []\n    self.annotated_segments = {}\n    for slp_file in self.slp_files:\n        labels, annotated_segments = data_utils.load_slp(slp_file)\n        self.labels.append(labels)\n        self.annotated_segments[slp_file] = annotated_segments\n\n    self.videos = [imageio.get_reader(vid_file) for vid_file in self.vid_files]\n    # do we need this? would need to update with sleap-io\n\n    # Method in BaseDataset. Creates label_idx and chunked_frame_idx to be\n    # used in call to get_instances()\n    self.create_chunks_slp()\n</code></pre>"},{"location":"reference/dreem/datasets/sleap_dataset/#dreem.datasets.sleap_dataset.SleapDataset.get_indices","title":"<code>get_indices(idx)</code>","text":"<p>Retrieve label and frame indices given batch index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the batch.</p> required Source code in <code>dreem/datasets/sleap_dataset.py</code> <pre><code>def get_indices(self, idx: int) -&gt; tuple:\n    \"\"\"Retrieve label and frame indices given batch index.\n\n    Args:\n        idx: the index of the batch.\n    \"\"\"\n    return self.label_idx[idx], self.chunked_frame_idx[idx]\n</code></pre>"},{"location":"reference/dreem/datasets/sleap_dataset/#dreem.datasets.sleap_dataset.SleapDataset.get_instances","title":"<code>get_instances(label_idx, frame_idx)</code>","text":"<p>Get an element of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>label_idx</code> <code>list[int]</code> <p>index of the labels</p> required <code>frame_idx</code> <code>Tensor</code> <p>indices of the frames to load in to the batch</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A list of <code>dreem.io.Frame</code> objects containing metadata and instance data for the batch/clip.</p> Source code in <code>dreem/datasets/sleap_dataset.py</code> <pre><code>def get_instances(\n    self, label_idx: list[int], frame_idx: torch.Tensor\n) -&gt; list[Frame]:\n    \"\"\"Get an element of the dataset.\n\n    Args:\n        label_idx: index of the labels\n        frame_idx: indices of the frames to load in to the batch\n\n    Returns:\n        A list of `dreem.io.Frame` objects containing metadata and instance data for the batch/clip.\n\n    \"\"\"\n    sleap_labels_obj = self.labels[label_idx]\n    video_name = self.video_files[label_idx]\n\n    # get the correct crop size based on the video\n    video_par_path = Path(video_name).parent\n    if len(self.data_dirs) &gt; 0:\n        crop_size = self.crop_size[0]\n        for j, data_dir in enumerate(self.data_dirs):\n            if Path(data_dir) == video_par_path:\n                crop_size = self.crop_size[j]\n                break\n    else:\n        crop_size = self.crop_size[0]\n\n    vid_reader = self.videos[label_idx]\n\n    skeleton = sleap_labels_obj.skeletons[-1]\n\n    frames = []\n    max_crop_h, max_crop_w = 0, 0\n    for i, frame_ind in enumerate(frame_idx):\n        (\n            instances,\n            gt_track_ids,\n            poses,\n            shown_poses,\n            point_scores,\n            instance_score,\n        ) = ([], [], [], [], [], [])\n\n        frame_ind = int(frame_ind)\n\n        # sleap-io method for indexing a Labels() object based on the frame's index\n        lf = sleap_labels_obj[(sleap_labels_obj.video, frame_ind)]\n        if frame_ind != lf.frame_idx:\n            logger.warning(f\"Frame index mismatch: {frame_ind} != {lf.frame_idx}\")\n\n        try:\n            img = vid_reader.get_data(int(frame_ind))\n        except IndexError as e:\n            logger.warning(\n                f\"Could not read frame {frame_ind} from {video_name} due to {e}\"\n            )\n            continue\n\n        if len(img.shape) == 2:\n            img = img.expand_dims(-1)\n        h, w, c = img.shape\n\n        if c == 1:\n            img = np.concatenate(\n                [img, img, img], axis=-1\n            )  # convert to grayscale to rgb\n\n        if np.issubdtype(img.dtype, np.integer):  # convert int to float\n            img = img.astype(np.float32)\n            if self.normalize_image:\n                img = img / 255\n\n        n_instances_dropped = 0\n\n        gt_instances = []\n        # don't load instances that have been 'greyed out' i.e. all nans for keypoints\n        for inst in lf.instances:\n            pts = np.array([p for p in inst.numpy()])\n            if np.isnan(pts).all():\n                continue\n            else:\n                gt_instances.append(inst)\n\n        dict_instances = {}\n        no_track_instances = []\n        for instance in gt_instances:\n            if instance.track is not None:\n                gt_track_id = sleap_labels_obj.tracks.index(instance.track)\n                if gt_track_id not in dict_instances:\n                    dict_instances[gt_track_id] = instance\n                else:\n                    existing_instance = dict_instances[gt_track_id]\n                    # if existing is PredictedInstance and current is not, then current is a UserInstance and should be used\n                    if isinstance(\n                        existing_instance, sio.PredictedInstance\n                    ) and not isinstance(instance, sio.PredictedInstance):\n                        dict_instances[gt_track_id] = instance\n            else:\n                no_track_instances.append(instance)\n\n        gt_instances = list(dict_instances.values()) + no_track_instances\n\n        if self.mode == \"train\":\n            np.random.shuffle(gt_instances)\n\n        for instance in gt_instances:\n            if (\n                np.random.uniform() &lt; self.instance_dropout[\"p\"]\n                and n_instances_dropped &lt; self.instance_dropout[\"n\"]\n            ):\n                n_instances_dropped += 1\n                continue\n\n            if instance.track is not None:\n                gt_track_id = sleap_labels_obj.tracks.index(instance.track)\n            else:\n                gt_track_id = -1\n            gt_track_ids.append(gt_track_id)\n\n            poses.append(\n                dict(\n                    zip(\n                        [n.name for n in instance.skeleton.nodes],\n                        [p for p in instance.numpy()],\n                    )\n                )\n            )\n\n            shown_poses = [\n                {\n                    key: val\n                    for key, val in instance.items()\n                    if not np.isnan(val).any()\n                }\n                for instance in poses\n            ]\n\n            point_scores.append(\n                np.array(\n                    [\n                        (\n                            1.0  # point scores not reliably available in sleap io PredictedPointsArray\n                            # point.score\n                            # if isinstance(point, sio.PredictedPoint)\n                            # else 1.0\n                        )\n                        for point in instance.numpy()\n                    ]\n                )\n            )\n            if isinstance(instance, sio.PredictedInstance):\n                instance_score.append(instance.score)\n            else:\n                instance_score.append(1.0)\n        # augmentations\n        if self.augmentations is not None:\n            for transform in self.augmentations:\n                if isinstance(transform, A.CoarseDropout):\n                    transform.fill_value = random.randint(0, 255)\n\n            if shown_poses:\n                keypoints = np.vstack([list(s.values()) for s in shown_poses])\n\n            else:\n                keypoints = []\n\n            augmented = self.augmentations(image=img, keypoints=keypoints)\n\n            img, aug_poses = augmented[\"image\"], augmented[\"keypoints\"]\n\n            aug_poses = [\n                arr\n                for arr in np.split(\n                    np.array(aug_poses),\n                    np.array([len(s) for s in shown_poses]).cumsum(),\n                )\n                if arr.size != 0\n            ]\n\n            aug_poses = [\n                dict(zip(list(pose_dict.keys()), aug_pose_arr.tolist()))\n                for aug_pose_arr, pose_dict in zip(aug_poses, shown_poses)\n            ]\n\n            _ = [\n                pose.update(aug_pose)\n                for pose, aug_pose in zip(shown_poses, aug_poses)\n            ]\n\n        img = tvf.to_tensor(img)\n\n        for j in range(len(gt_track_ids)):\n            pose = shown_poses[j]\n\n            \"\"\"Check for anchor\"\"\"\n            crops = []\n            boxes = []\n            centroids = {}\n\n            if isinstance(self.anchors, int):\n                anchors_to_choose = list(pose.keys()) + [\"midpoint\"]\n                anchors = np.random.choice(anchors_to_choose, self.anchors)\n            else:\n                anchors = self.anchors\n\n            dropped_anchors = self.node_dropout(anchors)\n\n            for anchor in anchors:\n                if anchor in dropped_anchors:\n                    centroid = np.array([np.nan, np.nan])\n\n                elif anchor == \"midpoint\" or anchor == \"centroid\":\n                    centroid = np.nanmean(np.array(list(pose.values())), axis=0)\n\n                elif anchor in pose:\n                    centroid = np.array(pose[anchor])\n                    if np.isnan(centroid).any():\n                        centroid = np.array([np.nan, np.nan])\n\n                elif (\n                    anchor not in pose\n                    and len(anchors) == 1\n                    and self.handle_missing == \"centroid\"\n                ):\n                    anchor = \"midpoint\"\n                    centroid = np.nanmean(np.array(list(pose.values())), axis=0)\n\n                else:\n                    centroid = np.array([np.nan, np.nan])\n\n                if np.isnan(centroid).all():\n                    bbox = torch.tensor([np.nan, np.nan, np.nan, np.nan])\n\n                else:\n                    if self.use_tight_bbox and len(pose) &gt; 1:\n                        # tight bbox\n                        # dont allow this for centroid-only poses!\n                        arr_pose = np.array(list(pose.values()))\n                        # note bbox will be a different size for each instance; padded at the end of the loop\n                        bbox = data_utils.get_tight_bbox(arr_pose)\n\n                    else:\n                        bbox = data_utils.pad_bbox(\n                            data_utils.get_bbox(centroid, crop_size),\n                            padding=self.padding,\n                        )\n\n                if bbox.isnan().all():\n                    crop = torch.zeros(\n                        c,\n                        crop_size + 2 * self.padding,\n                        crop_size + 2 * self.padding,\n                        dtype=img.dtype,\n                    )\n                else:\n                    crop = data_utils.crop_bbox(img, bbox)\n\n                crops.append(crop)\n                # get max h,w for padding for tight bboxes\n                c, h, w = crop.shape\n                if h &gt; max_crop_h:\n                    max_crop_h = h\n                if w &gt; max_crop_w:\n                    max_crop_w = w\n\n                centroids[anchor] = centroid\n                boxes.append(bbox)\n\n            if len(crops) &gt; 0:\n                crops = torch.concat(crops, dim=0)\n\n            if len(boxes) &gt; 0:\n                boxes = torch.stack(boxes, dim=0)\n\n            if self.handle_missing == \"drop\" and boxes.isnan().any():\n                continue\n\n            instance = Instance(\n                gt_track_id=gt_track_ids[j],\n                pred_track_id=-1,\n                crop=crops,\n                centroid=centroids,\n                bbox=boxes,\n                skeleton=skeleton,\n                pose=poses[j],\n                point_scores=point_scores[j],\n                instance_score=instance_score[j],\n            )\n\n            instances.append(instance)\n\n        frame = Frame(\n            video_id=label_idx,\n            frame_id=frame_ind,\n            vid_file=video_name,\n            img_shape=img.shape,\n            instances=instances,\n        )\n        frames.append(frame)\n\n    # pad bbox to max size\n    if self.use_tight_bbox:\n        # bound the max crop size to the user defined crop size\n        max_crop_h = crop_size if max_crop_h == 0 else min(max_crop_h, crop_size)\n        max_crop_w = crop_size if max_crop_w == 0 else min(max_crop_w, crop_size)\n        # gather all the crops\n        for frame in frames:\n            for instance in frame.instances:\n                data_utils.pad_variable_size_crops(\n                    instance, (max_crop_h, max_crop_w)\n                )\n    return frames\n</code></pre>"},{"location":"reference/dreem/datasets/tracking_dataset/","title":"tracking_dataset","text":""},{"location":"reference/dreem/datasets/tracking_dataset/#dreem.datasets.tracking_dataset","title":"<code>dreem.datasets.tracking_dataset</code>","text":"<p>Module containing Lightning module wrapper around all other datasets.</p> <p>Classes:</p> Name Description <code>TrackingDataset</code> <p>Lightning dataset used to load dataloaders for train, test and validation.</p>"},{"location":"reference/dreem/datasets/tracking_dataset/#dreem.datasets.tracking_dataset.TrackingDataset","title":"<code>TrackingDataset</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>Lightning dataset used to load dataloaders for train, test and validation.</p> <p>Nice for wrapping around other data formats.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize tracking dataset.</p> <code>setup</code> <p>Set up lightning dataset.</p> <code>test_dataloader</code> <p>Get.</p> <code>train_dataloader</code> <p>Get train_dataloader.</p> <code>val_dataloader</code> <p>Get val dataloader.</p> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>class TrackingDataset(LightningDataModule):\n    \"\"\"Lightning dataset used to load dataloaders for train, test and validation.\n\n    Nice for wrapping around other data formats.\n    \"\"\"\n\n    def __init__(\n        self,\n        train_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n        train_dl: DataLoader | None = None,\n        val_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n        val_dl: DataLoader | None = None,\n        test_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n        test_dl: DataLoader | None = None,\n    ):\n        \"\"\"Initialize tracking dataset.\n\n        Args:\n            train_ds: Sleap or Microscopy training Dataset\n            train_dl: Training dataloader. Only used for overriding `train_dataloader`.\n            val_ds: Sleap or Microscopy Validation set\n            val_dl : Validation dataloader. Only used for overriding `val_dataloader`.\n            test_ds: Sleap or Microscopy test set\n            test_dl : Test dataloader. Only used for overriding `test_dataloader`.\n        \"\"\"\n        super().__init__()\n        self.train_ds = train_ds\n        self.train_dl = train_dl\n        self.val_ds = val_ds\n        self.val_dl = val_dl\n        self.test_ds = test_ds\n        self.test_dl = test_dl\n\n    def setup(self, stage=None):\n        \"\"\"Set up lightning dataset.\n\n        UNUSED.\n        \"\"\"\n        pass\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"Get train_dataloader.\n\n        Returns: The Training Dataloader.\n        \"\"\"\n        if self.train_dl is None and self.train_ds is None:\n            return None\n        elif self.train_dl is None:\n            return DataLoader(\n                self.train_ds,\n                batch_size=1,\n                shuffle=True,\n                pin_memory=False,\n                collate_fn=self.train_ds.no_batching_fn,\n                num_workers=0,\n                generator=(\n                    torch.Generator(device=\"cuda\")\n                    if torch.cuda.is_available()\n                    else torch.Generator()\n                ),\n            )\n        else:\n            return self.train_dl\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"Get val dataloader.\n\n        Returns: The validation dataloader.\n        \"\"\"\n        if self.val_dl is None and self.val_ds is None:\n            return None\n        elif self.val_dl is None:\n            return DataLoader(\n                self.val_ds,\n                batch_size=1,\n                shuffle=False,\n                pin_memory=0,\n                collate_fn=self.train_ds.no_batching_fn,\n                num_workers=False,\n                generator=None,\n            )\n        else:\n            return self.val_dl\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"Get.\n\n        Returns: The test dataloader\n        \"\"\"\n        if self.test_dl is None and self.test_ds is None:\n            return None\n        elif self.test_dl is None:\n            return DataLoader(\n                self.test_ds,\n                batch_size=1,\n                shuffle=False,\n                pin_memory=0,\n                collate_fn=self.train_ds.no_batching_fn,\n                num_workers=False,\n                generator=None,\n            )\n        else:\n            return self.test_dl\n</code></pre>"},{"location":"reference/dreem/datasets/tracking_dataset/#dreem.datasets.tracking_dataset.TrackingDataset.__init__","title":"<code>__init__(train_ds=None, train_dl=None, val_ds=None, val_dl=None, test_ds=None, test_dl=None)</code>","text":"<p>Initialize tracking dataset.</p> <p>Parameters:</p> Name Type Description Default <code>train_ds</code> <code>SleapDataset | MicroscopyDataset | CellTrackingDataset | None</code> <p>Sleap or Microscopy training Dataset</p> <code>None</code> <code>train_dl</code> <code>DataLoader | None</code> <p>Training dataloader. Only used for overriding <code>train_dataloader</code>.</p> <code>None</code> <code>val_ds</code> <code>SleapDataset | MicroscopyDataset | CellTrackingDataset | None</code> <p>Sleap or Microscopy Validation set</p> <code>None</code> <code>val_dl</code> <p>Validation dataloader. Only used for overriding <code>val_dataloader</code>.</p> <code>None</code> <code>test_ds</code> <code>SleapDataset | MicroscopyDataset | CellTrackingDataset | None</code> <p>Sleap or Microscopy test set</p> <code>None</code> <code>test_dl</code> <p>Test dataloader. Only used for overriding <code>test_dataloader</code>.</p> <code>None</code> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>def __init__(\n    self,\n    train_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n    train_dl: DataLoader | None = None,\n    val_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n    val_dl: DataLoader | None = None,\n    test_ds: SleapDataset | MicroscopyDataset | CellTrackingDataset | None = None,\n    test_dl: DataLoader | None = None,\n):\n    \"\"\"Initialize tracking dataset.\n\n    Args:\n        train_ds: Sleap or Microscopy training Dataset\n        train_dl: Training dataloader. Only used for overriding `train_dataloader`.\n        val_ds: Sleap or Microscopy Validation set\n        val_dl : Validation dataloader. Only used for overriding `val_dataloader`.\n        test_ds: Sleap or Microscopy test set\n        test_dl : Test dataloader. Only used for overriding `test_dataloader`.\n    \"\"\"\n    super().__init__()\n    self.train_ds = train_ds\n    self.train_dl = train_dl\n    self.val_ds = val_ds\n    self.val_dl = val_dl\n    self.test_ds = test_ds\n    self.test_dl = test_dl\n</code></pre>"},{"location":"reference/dreem/datasets/tracking_dataset/#dreem.datasets.tracking_dataset.TrackingDataset.setup","title":"<code>setup(stage=None)</code>","text":"<p>Set up lightning dataset.</p> <p>UNUSED.</p> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>def setup(self, stage=None):\n    \"\"\"Set up lightning dataset.\n\n    UNUSED.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/dreem/datasets/tracking_dataset/#dreem.datasets.tracking_dataset.TrackingDataset.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Get.</p> <p>Returns: The test dataloader</p> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"Get.\n\n    Returns: The test dataloader\n    \"\"\"\n    if self.test_dl is None and self.test_ds is None:\n        return None\n    elif self.test_dl is None:\n        return DataLoader(\n            self.test_ds,\n            batch_size=1,\n            shuffle=False,\n            pin_memory=0,\n            collate_fn=self.train_ds.no_batching_fn,\n            num_workers=False,\n            generator=None,\n        )\n    else:\n        return self.test_dl\n</code></pre>"},{"location":"reference/dreem/datasets/tracking_dataset/#dreem.datasets.tracking_dataset.TrackingDataset.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Get train_dataloader.</p> <p>Returns: The Training Dataloader.</p> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"Get train_dataloader.\n\n    Returns: The Training Dataloader.\n    \"\"\"\n    if self.train_dl is None and self.train_ds is None:\n        return None\n    elif self.train_dl is None:\n        return DataLoader(\n            self.train_ds,\n            batch_size=1,\n            shuffle=True,\n            pin_memory=False,\n            collate_fn=self.train_ds.no_batching_fn,\n            num_workers=0,\n            generator=(\n                torch.Generator(device=\"cuda\")\n                if torch.cuda.is_available()\n                else torch.Generator()\n            ),\n        )\n    else:\n        return self.train_dl\n</code></pre>"},{"location":"reference/dreem/datasets/tracking_dataset/#dreem.datasets.tracking_dataset.TrackingDataset.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Get val dataloader.</p> <p>Returns: The validation dataloader.</p> Source code in <code>dreem/datasets/tracking_dataset.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"Get val dataloader.\n\n    Returns: The validation dataloader.\n    \"\"\"\n    if self.val_dl is None and self.val_ds is None:\n        return None\n    elif self.val_dl is None:\n        return DataLoader(\n            self.val_ds,\n            batch_size=1,\n            shuffle=False,\n            pin_memory=0,\n            collate_fn=self.train_ds.no_batching_fn,\n            num_workers=False,\n            generator=None,\n        )\n    else:\n        return self.val_dl\n</code></pre>"},{"location":"reference/dreem/inference/","title":"inference","text":""},{"location":"reference/dreem/inference/#dreem.inference","title":"<code>dreem.inference</code>","text":"<p>Tracking Inference using GTR Model.</p> <p>Modules:</p> Name Description <code>batch_tracker</code> <p>Module containing logic for going from association -&gt; assignment.</p> <code>boxes</code> <p>Module containing Boxes class.</p> <code>eval</code> <p>Script to evaluate model.</p> <code>metrics</code> <p>Helper functions for calculating mot metrics.</p> <code>post_processing</code> <p>Helper functions for post-processing association matrix pre-tracking.</p> <code>track</code> <p>Script to run inference and get out tracks.</p> <code>track_queue</code> <p>Module handling sliding window tracking.</p> <code>tracker</code> <p>Module containing logic for going from association -&gt; assignment.</p> <p>Classes:</p> Name Description <code>BatchTracker</code> <p>Tracker class used for assignment based on sliding inference from GTR.</p> <code>Tracker</code> <p>Tracker class used for assignment based on sliding inference from GTR.</p>"},{"location":"reference/dreem/inference/#dreem.inference.BatchTracker","title":"<code>BatchTracker</code>","text":"<p>Tracker class used for assignment based on sliding inference from GTR.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Wrap around <code>track</code> to enable <code>tracker()</code> instead of <code>tracker.track()</code>.</p> <code>__init__</code> <p>Initialize a tracker to run inference.</p> <code>__repr__</code> <p>Get string representation of tracker.</p> <code>track</code> <p>Perform sliding inference on the input video (instances) with a given window size. This method is called once per batch.</p> <code>track_by_batch</code> <p>Perform sliding inference, on an entire batch of frames, on the input video (instances) with a given context length (window size).</p> <code>track_by_frame</code> <p>Perform sliding inference on the input video (instances) with a given window size.</p> Source code in <code>dreem/inference/batch_tracker.py</code> <pre><code>class BatchTracker:\n    \"\"\"Tracker class used for assignment based on sliding inference from GTR.\"\"\"\n\n    def __init__(\n        self,\n        window_size: int = 8,\n        use_vis_feats: bool = True,\n        overlap_thresh: float = 0.01,\n        mult_thresh: bool = True,\n        decay_time: float | None = None,\n        iou: str | None = None,\n        max_center_dist: float | None = None,\n        persistent_tracking: bool = True,\n        max_gap: int = inf,\n        max_tracks: int = inf,\n        verbose: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize a tracker to run inference.\n\n        Args:\n            window_size: the size of the window used during sliding inference.\n            use_vis_feats: Whether or not to use visual feature extractor.\n            overlap_thresh: the trajectory overlap threshold to be used for assignment.\n            mult_thresh: Whether or not to use weight threshold.\n            decay_time: weight for `decay_time` postprocessing.\n            iou: Either [None, '', \"mult\" or \"max\"]\n                 Whether to use multiplicative or max iou reweighting.\n            max_center_dist: distance threshold for filtering trajectory score matrix.\n            persistent_tracking: whether to keep a buffer across chunks or not.\n            max_gap: the max number of frames a trajectory can be missing before termination.\n            max_tracks: the maximum number of tracks that can be created while tracking.\n                We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.\n            verbose: Whether or not to turn on debug printing after each operation.\n        \"\"\"\n        self.track_queue = TrackQueue(\n            window_size=window_size, max_gap=max_gap, verbose=verbose\n        )\n        self.num_frames_tracked = 0\n        self.window_size = window_size\n        self.use_vis_feats = use_vis_feats\n        self.overlap_thresh = overlap_thresh\n        self.mult_thresh = mult_thresh\n        self.decay_time = decay_time\n        self.iou = iou\n        self.max_center_dist = max_center_dist\n        self.persistent_tracking = persistent_tracking\n        self.verbose = verbose\n        self.max_tracks = max_tracks\n\n    def __call__(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; list[Frame]:\n        \"\"\"Wrap around `track` to enable `tracker()` instead of `tracker.track()`.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: list of Frames to run inference on\n\n        Returns:\n            List of frames containing association matrix scores and instances populated with pred track ids.\n        \"\"\"\n        _ = model.eval()\n\n        for frame in frames:\n            if frame.has_instances():\n                if not self.use_vis_feats:\n                    for instance in frame.instances:\n                        instance.features = torch.zeros(1, model.d_model)\n                    # frame[\"features\"] = torch.randn(\n                    #     num_frame_instances, self.model.d_model\n                    # )\n\n                # comment out to turn encoder off\n\n                # Assuming the encoder is already trained or train encoder jointly.\n                elif not frame.has_features():\n                    with torch.no_grad():\n                        crops = frame.get_crops()\n                        z = model.visual_encoder(crops)\n\n                        for i, z_i in enumerate(z):\n                            frame.instances[i].features = z_i\n\n        instances_pred = self.track(model, frames)\n        # no more persistent tracking. It is on by default\n        # if not self.persistent_tracking:\n        #     logger.debug(f\"Clearing Queue after tracking\")\n        #     self.track_queue.end_tracks()\n\n        return instances_pred\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get string representation of tracker.\n\n        Returns: the string representation of the tracker\n        \"\"\"\n        return (\n            \"Tracker(\"\n            f\"persistent_tracking={self.persistent_tracking}, \"\n            f\"max_tracks={self.max_tracks}, \"\n            f\"use_vis_feats={self.use_vis_feats}, \"\n            f\"overlap_thresh={self.overlap_thresh}, \"\n            f\"mult_thresh={self.mult_thresh}, \"\n            f\"decay_time={self.decay_time}, \"\n            f\"max_center_dist={self.max_center_dist}, \"\n            f\"verbose={self.verbose}, \"\n            f\"queue={self.track_queue}\"\n        )\n\n    def track(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; list[Frame]:\n        \"\"\"Perform sliding inference on the input video (instances) with a given window size. This method is called once per batch.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n        Returns:\n            frames: A list of Frames populated with pred_track_ids and asso_matrices\n        \"\"\"\n        # all batches up until context_length number of frames have been tracked, will be tracked frame-by-frame\n        if self.num_frames_tracked &lt; self.window_size:\n            frames = self.track_by_frame(model, frames)\n        else:\n            frames = self.track_by_batch(model, frames)\n\n        return frames\n\n    def track_by_batch(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; list[Frame]:\n        \"\"\"Perform sliding inference, on an entire batch of frames, on the input video (instances) with a given context length (window size).\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n        Returns:\n            frames: A list of Frames populated with pred_track_ids and asso_matrices\n        \"\"\"\n        # context window starts from last frame just before start of current batch, to window_size frames preceding it\n        # note; can't use last frame of previous batch, because there could be empty frames in between batches that must\n        # be part of the context window for consistency\n        context_window_frames = self.track_queue.collate_tracks(\n            context_start_frame_id=frames[0].frame_id.item()\n            - 1,  # switched off in collate_tracks; there is no cutoff for context, only until the deque gets filled\n            device=frames[0].frame_id.device,\n        )\n\n        context_window_instances = []\n        context_window_instance_frame_ids = []\n        for frame in context_window_frames:\n            context_window_instances.extend(frame.instances)\n            context_window_instance_frame_ids.extend(\n                [frame.frame_id] * len(frame.instances)\n            )\n\n        current_batch_instances = []\n        current_batch_instance_frame_ids = []\n        for frame in frames:\n            current_batch_instances.extend(frame.instances)\n            current_batch_instance_frame_ids.extend(\n                [frame.frame_id] * len(frame.instances)\n            )\n\n        # query is current batch instances, key is context window and current batch instances\n        association_matrix = model(\n            context_window_instances + current_batch_instances, current_batch_instances\n        )\n\n        # take association matrix and all frames off GPU (frames include instances)\n        association_matrix = association_matrix[-1].to(\"cpu\")\n        context_window_frames = [frame.to(\"cpu\") for frame in context_window_frames]\n        frames = [frame.to(\"cpu\") for frame in frames]\n\n        # keep current batch instances in assoc matrix, and remove them after softmax (mirrors the training scheme)\n        pred_frames = self._run_batch_tracker(\n            association_matrix.matrix,\n            context_window_frames,\n            frames,\n            compute_probs_by_frame=True,\n        )\n\n        return pred_frames\n\n    def track_by_frame(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; list[Frame]:\n        \"\"\"Perform sliding inference on the input video (instances) with a given window size.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n        Returns:\n            frames: A list of Frames populated with pred_track_ids and asso_matrices\n        \"\"\"\n        # B: batch size.\n        # D: embedding dimension.\n        # nc: number of channels.\n        # H: height.\n        # W: width.\n\n        for batch_idx, frame_to_track in enumerate(frames):\n            # if we're tracking by frame, it means context length of frames hasn't been reached yet, so context start frame id is 0\n            context_window_frames = self.track_queue.collate_tracks(\n                context_start_frame_id=0, device=frame_to_track.frame_id.device\n            )\n            logger.debug(f\"Current number of tracks is {self.track_queue.n_tracks}\")\n\n            if (\n                self.persistent_tracking and frame_to_track.frame_id == 0\n            ):  # check for new video and clear queue\n                logger.debug(\"New Video! Resetting Track Queue.\")\n                self.track_queue.end_tracks()\n\n            \"\"\"\n            Initialize tracks on first frame where detections appear. This is the first frame of the first batch\n            \"\"\"\n            if len(self.track_queue) == 0:\n                if frame_to_track.has_instances():\n                    logger.debug(\n                        f\"Initializing track on clip ind {batch_idx} frame {frame_to_track.frame_id.item()}\"\n                    )\n\n                    curr_track_id = 0\n                    for i, instance in enumerate(frames[batch_idx].instances):\n                        instance.pred_track_id = instance.gt_track_id\n                        curr_track_id = max(curr_track_id, instance.pred_track_id)\n\n                    for i, instance in enumerate(frames[batch_idx].instances):\n                        if instance.pred_track_id == -1:\n                            curr_track_id += 1\n                            instance.pred_track_id = curr_track_id\n\n            else:\n                if frame_to_track.has_instances():  # Check if there are detections. If there are skip and increment gap count\n                    frames_to_track = context_window_frames + [\n                        frame_to_track\n                    ]  # better var name?\n\n                    frame_to_track = self._run_frame_by_frame_tracker(\n                        model,\n                        frames_to_track,\n                    )\n\n            if frame_to_track.has_instances():\n                self.track_queue.add_frame(frame_to_track)\n                self.num_frames_tracked += 1\n            else:\n                self.track_queue.increment_gaps([])\n\n            frames[batch_idx] = frame_to_track\n\n        return frames\n\n    def _run_batch_tracker(\n        self,\n        association_matrix: torch.Tensor,\n        context_window_frames: list[Frame],\n        frames: list[Frame],\n        compute_probs_by_frame: bool = True,\n    ) -&gt; Frame:\n        \"\"\"Run batch tracker performs track assignment for each frame in the current batch.\n\n        Supports 2 methods for computing association probabilities. First is to softmax each query instance in each query frame in the batch, with only 1 frame at a time from the context window.\n        This is the default method and only supports local track linking.\n        Second is to softmax the entire context + curr batch, then index. This enables global track linking via e.g. ILP. In this case, prob values will be smaller and the overlap thresh should be decreased\n\n        Args:\n            association_matrix: the association matrix to be used for tracking\n            context_window_frames: list of frames in the context window\n            frames: list of frames in the current batch\n            compute_probs_by_frame: Whether to softmax the association matrix logits for each frame in context separately, or globally for the entire context window + current batch\n\n        Returns:\n            List of frames populated with pred_track_ids and asso_matrices\n        \"\"\"\n        tracked_frames = []\n        num_instances_per_frame = [\n            frame.num_detected for frame in context_window_frames + frames\n        ]\n        all_frames = context_window_frames + frames\n        batch_start_ind = len(num_instances_per_frame) - len(frames)\n        overlap_thresh = self.overlap_thresh\n        mult_thresh = self.mult_thresh\n        n_traj = self.track_queue.n_tracks\n        curr_track = self.track_queue.curr_track\n\n        for query_frame_idx, frame in enumerate(\n            frames\n        ):  # only track frames in current batch, not in context window\n            all_prev_instances = [\n                instance\n                for frame in context_window_frames + frames[:query_frame_idx]\n                for instance in frame.instances\n            ]\n            # indices that will be used to index the rows of the association matrix corresponding to the query frame instances\n            query_inds = [\n                x\n                for x in range(\n                    sum(\n                        num_instances_per_frame[\n                            batch_start_ind : batch_start_ind + query_frame_idx\n                        ]\n                    ),\n                    sum(\n                        num_instances_per_frame[\n                            batch_start_ind : batch_start_ind + query_frame_idx + 1\n                        ]\n                    ),\n                )\n            ]\n            # first, slice the association matrix to only include the query frame instances along the rows; these are the 'detections' to be matched to tracks\n            # recall incoming association_matrix is (num_instances_in_batch, num_instances_in_context_window + num_instances_in_batch)\n            assoc_curr_frame = association_matrix[query_inds, :]\n            # discard the columns (ref instances) corresponding to frames including and after the current frame; this means each frame will see previous frames in the batch as well as the context window when linking to tracks\n            # importantly, this means that tracks will be aggregated over a much longer time period than the context window size, making many more tracks visible to each frame to link detections to\n            assoc_curr_frame_by_previous_frames = assoc_curr_frame[\n                :, : sum(num_instances_per_frame[: batch_start_ind + query_frame_idx])\n            ]  # (num_query_instances, num instances in context window + num instances in current batch up till current frame)\n\n            # method 1\n            if compute_probs_by_frame:\n                # for each frame in the context window, split the assoc matrix columns by frame\n                split_assos = assoc_curr_frame_by_previous_frames.split(\n                    num_instances_per_frame[: batch_start_ind + query_frame_idx], dim=1\n                )\n                # compute softmax per-frame\n                softmaxed_asso = model_utils.softmax_asso(split_assos)\n                # merge the softmaxed assoc matrices back together to get (num_query_instances, num_instances_in_context_window)\n                softmaxed_asso = torch.cat(softmaxed_asso, dim=1)\n            # method 2\n            else:\n                # compute softmax across the entire context window and current batch frames\n                softmaxed_asso = model_utils.softmax_asso(\n                    assoc_curr_frame_by_previous_frames\n                )[0]\n\n            # proceed with post processing, LSA, and track assignment (duplicated code with frame by frame tracker)\n\n            # get raw bbox coords of prev frame instances from frame.instances_per_frame\n            prev_frame = all_frames[batch_start_ind + query_frame_idx - 1]\n            prev_frame_instance_ids = torch.cat(\n                [instance.pred_track_id for instance in prev_frame.instances], dim=0\n            )\n            prev_frame_boxes = torch.cat(\n                [instance.bbox for instance in prev_frame.instances], dim=0\n            )\n            all_prev_frames_boxes = torch.cat(\n                [instance.bbox for instance in all_prev_instances], dim=0\n            )\n            curr_frame_boxes = torch.cat(\n                [instance.bbox for instance in frame.instances], dim=0\n            )\n            # get the pred track ids for all instances up until the current frame\n            instance_ids = torch.cat(\n                [\n                    x.get_pred_track_ids()\n                    for x in all_frames[: batch_start_ind + query_frame_idx]\n                ],\n                dim=0,\n            ).view(\n                sum(num_instances_per_frame[: batch_start_ind + query_frame_idx])\n            )  # (n_nonquery,)\n\n            unique_ids = torch.unique(instance_ids)  # (n_nonquery,)\n\n            _, h, w = frame.img_shape.flatten()\n            bbox_scaler = torch.tensor([w, h, w, h])\n            query_boxes = curr_frame_boxes / bbox_scaler  # n_k x 4\n            nonquery_boxes = all_prev_frames_boxes / bbox_scaler  # n_nonquery x 4\n\n            logger.debug(f\"Instance IDs: {instance_ids}\")\n            logger.debug(f\"unique ids: {unique_ids}\")\n\n            id_inds = (\n                unique_ids[None, :] == instance_ids[:, None]\n            ).float()  # (n_nonquery, n_traj)\n\n            prev_frame_id_inds = (\n                unique_ids[None, :] == prev_frame_instance_ids[:, None]\n            ).float()  # (n_prev_frame_instances, n_traj)\n\n            # aggregate the association matrix by tracks; output is shape (num_query_instances, num_tracks)\n            # note the reduce operation is over the context window instances as well as current batch instances up until the current frame\n            # (n_query x n_nonquery) x (n_nonquery x n_traj) --&gt; n_query x n_traj\n            traj_score = torch.mm(softmaxed_asso, id_inds.cpu())  # (n_query, n_traj)\n\n            # post processing\n            if id_inds.numel() &gt; 0:\n                last_inds = (\n                    id_inds\n                    * torch.arange(len(all_prev_instances), device=id_inds.device)[\n                        :, None\n                    ]\n                ).max(dim=0)[1]  # M\n\n                last_boxes = nonquery_boxes[last_inds]  # n_traj x 4\n                last_ious = post_processing._pairwise_iou(\n                    Boxes(query_boxes), Boxes(last_boxes)\n                )  # n_k x M\n            else:\n                last_ious = traj_score.new_zeros(traj_score.shape)\n\n            traj_score = post_processing.weight_iou(traj_score, self.iou, last_ious)\n\n            # threshold for continuing a tracking or starting a new track -&gt; they use 1.0\n            traj_score = post_processing.filter_max_center_dist(\n                traj_score,\n                self.max_center_dist,\n                prev_frame_id_inds,\n                curr_frame_boxes,\n                prev_frame_boxes,\n            )\n\n            match_i, match_j = linear_sum_assignment((-traj_score))\n\n            track_ids = instance_ids.new_full((frame.num_detected,), -1)\n            for i, j in zip(match_i, match_j):\n                # The overlap threshold is multiplied by the number of times the unique track j is matched to an\n                # instance out of all instances in the window excluding the current frame.\n                #\n                # So if this is correct, the threshold is higher for matching an instance from the current frame\n                # to an existing track if that track has already been matched several times.\n                # So if an existing track in the window has been matched a lot, it gets harder to match to that track.\n                thresh = (\n                    overlap_thresh * id_inds[:, j].sum()\n                    if mult_thresh\n                    else overlap_thresh\n                )\n                if n_traj &gt;= self.max_tracks or traj_score[i, j] &gt; thresh:\n                    logger.debug(\n                        f\"Assigning instance {i} to track {j} with id {unique_ids[j]}\"\n                    )\n                    track_ids[i] = unique_ids[j]\n                    frame.instances[i].track_score = traj_score[i, j].item()\n            logger.debug(f\"track_ids: {track_ids}\")\n            for i in range(frame.num_detected):\n                if track_ids[i] &lt; 0:\n                    logger.debug(f\"Creating new track {curr_track}\")\n                    curr_track += 1\n                    track_ids[i] = curr_track\n\n            frame.matches = (match_i, match_j)\n\n            for instance, track_id in zip(frame.instances, track_ids):\n                instance.pred_track_id = track_id\n\n            tracked_frames.append(frame)\n\n            if frame.has_instances():\n                self.track_queue.add_frame(frame)\n                self.num_frames_tracked += 1\n            else:\n                self.track_queue.increment_gaps([])\n\n        return tracked_frames\n\n    def _run_frame_by_frame_tracker(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; Frame:\n        \"\"\"Run global tracker performs the actual tracking.\n\n        Uses Hungarian algorithm to do track assigning.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: A list of Frames containing reid features. See `dreem.io.data_structures` for more info.\n\n        Returns:\n            query_frame: The query frame now populated with the pred_track_ids.\n        \"\"\"\n        # *: each item in frames is a frame in the window. So it follows\n        #    that each frame in the window has * detected instances.\n        # D: embedding dimension.\n        # total_instances: number of instances in the window.\n        # N_i: number of detected instances in i-th frame of window.\n        # instances_per_frame: a list of number of instances in each frame of the window.\n        # n_query: number of instances in current/query frame (rightmost frame of the window).\n        # n_nonquery: number of instances in the window excluding the current/query frame.\n        # window_size: length of window.\n        # L: number of decoder blocks.\n        # n_traj: number of existing tracks within the window so far.\n\n        # Number of instances in each frame of the window.\n        # E.g.: instances_per_frame: [4, 5, 6, 7]; window of length 4 with 4 detected instances in the first frame of the window.\n\n        _ = model.eval()\n\n        query_ind = len(frames) - 1\n        query_frame = frames[query_ind]\n\n        query_instances = query_frame.instances\n        all_instances = [instance for frame in frames for instance in frame.instances]\n\n        logger.debug(f\"Frame {query_frame.frame_id.item()}\")\n\n        instances_per_frame = [frame.num_detected for frame in frames]\n\n        total_instances, window_size = (\n            sum(instances_per_frame),\n            len(instances_per_frame),\n        )  # Number of instances in window; length of window.\n\n        logger.debug(f\"total_instances: {total_instances}\")\n\n        overlap_thresh = self.overlap_thresh\n        mult_thresh = self.mult_thresh\n        n_traj = self.track_queue.n_tracks\n        curr_track = self.track_queue.curr_track\n\n        reid_features = torch.cat([frame.get_features() for frame in frames], dim=0)[\n            None\n        ]  # (1, total_instances, D=512)\n\n        # (L=1, n_query, total_instances)\n        with torch.no_grad():\n            asso_matrix = model(all_instances, query_instances)\n\n        asso_output = asso_matrix[-1].matrix.split(\n            instances_per_frame, dim=1\n        )  # (window_size, n_query, N_i)\n        asso_output = model_utils.softmax_asso(\n            asso_output\n        )  # (window_size, n_query, N_i)\n        asso_output = torch.cat(asso_output, dim=1).cpu()  # (n_query, total_instances)\n\n        asso_output_df = pd.DataFrame(\n            asso_output.clone().numpy(),\n            columns=[f\"Instance {i}\" for i in range(asso_output.shape[-1])],\n        )\n\n        asso_output_df.index.name = \"Instances\"\n        asso_output_df.columns.name = \"Instances\"\n\n        query_frame.add_traj_score(\"asso_output\", asso_output_df)\n        query_frame.asso_output = asso_matrix[-1]\n\n        n_query = (\n            query_frame.num_detected\n        )  # Number of instances in the current/query frame.\n\n        n_nonquery = (\n            total_instances - n_query\n        )  # Number of instances in the window not including the current/query frame.\n\n        logger.debug(f\"n_nonquery: {n_nonquery}\")\n        logger.debug(f\"n_query: {n_query}\")\n\n        instance_ids = torch.cat(\n            [\n                x.get_pred_track_ids()\n                for batch_idx, x in enumerate(frames)\n                if batch_idx != query_ind\n            ],\n            dim=0,\n        ).view(n_nonquery)  # (n_nonquery,)\n\n        query_inds = [\n            x\n            for x in range(\n                sum(instances_per_frame[:query_ind]),\n                sum(instances_per_frame[: query_ind + 1]),\n            )\n        ]\n\n        nonquery_inds = [i for i in range(total_instances) if i not in query_inds]\n\n        # instead should we do model(nonquery_instances, query_instances)?\n        asso_nonquery = asso_output[:, nonquery_inds]  # (n_query, n_nonquery)\n\n        asso_nonquery_df = pd.DataFrame(\n            asso_nonquery.clone().numpy(), columns=nonquery_inds\n        )\n\n        asso_nonquery_df.index.name = \"Current Frame Instances\"\n        asso_nonquery_df.columns.name = \"Nonquery Instances\"\n\n        query_frame.add_traj_score(\"asso_nonquery\", asso_nonquery_df)\n\n        # get raw bbox coords of prev frame instances from frame.instances_per_frame\n        query_boxes_px = torch.cat(\n            [instance.bbox for instance in query_frame.instances], dim=0\n        )\n        nonquery_boxes_px = torch.cat(\n            [\n                instance.bbox\n                for nonquery_frame in frames\n                if nonquery_frame.frame_id != query_frame.frame_id\n                for instance in nonquery_frame.instances\n            ],\n            dim=0,\n        )\n\n        pred_boxes = model_utils.get_boxes(all_instances)\n        query_boxes = pred_boxes[query_inds]  # n_k x 4\n        nonquery_boxes = pred_boxes[nonquery_inds]  # n_nonquery x 4\n\n        unique_ids = torch.unique(instance_ids)  # (n_nonquery,)\n\n        logger.debug(f\"Instance IDs: {instance_ids}\")\n        logger.debug(f\"unique ids: {unique_ids}\")\n\n        id_inds = (\n            unique_ids[None, :] == instance_ids[:, None]\n        ).float()  # (n_nonquery, n_traj)\n\n        ################################################################################\n\n        # reweighting hyper-parameters for association -&gt; they use 0.9\n\n        traj_score = post_processing.weight_decay_time(\n            asso_nonquery, self.decay_time, reid_features, window_size, query_ind\n        )\n\n        if self.decay_time is not None and self.decay_time &gt; 0:\n            decay_time_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=nonquery_inds\n            )\n\n            decay_time_traj_score.index.name = \"Query Instances\"\n            decay_time_traj_score.columns.name = \"Nonquery Instances\"\n\n            query_frame.add_traj_score(\"decay_time\", decay_time_traj_score)\n        ################################################################################\n\n        # (n_query x n_nonquery) x (n_nonquery x n_traj) --&gt; n_query x n_traj\n        traj_score = torch.mm(traj_score, id_inds.cpu())  # (n_query, n_traj)\n\n        traj_score_df = pd.DataFrame(\n            traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n        )\n\n        traj_score_df.index.name = \"Current Frame Instances\"\n        traj_score_df.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"traj_score\", traj_score_df)\n        ################################################################################\n\n        # with iou -&gt; combining with location in tracker, they set to True\n        # todo -&gt; should also work without pos_embed\n\n        if id_inds.numel() &gt; 0:\n            # this throws error, think we need to slice?\n            # last_inds = (id_inds * torch.arange(\n            #    n_nonquery, device=id_inds.device)[:, None]).max(dim=0)[1] # n_traj\n\n            last_inds = (\n                id_inds * torch.arange(n_nonquery, device=id_inds.device)[:, None]\n            ).max(dim=0)[1]  # M\n\n            last_boxes = nonquery_boxes[last_inds]  # n_traj x 4\n            last_ious = post_processing._pairwise_iou(\n                Boxes(query_boxes), Boxes(last_boxes)\n            )  # n_k x M\n        else:\n            last_ious = traj_score.new_zeros(traj_score.shape)\n\n        traj_score = post_processing.weight_iou(traj_score, self.iou, last_ious.cpu())\n\n        if self.iou is not None and self.iou != \"\":\n            iou_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n            )\n\n            iou_traj_score.index.name = \"Current Frame Instances\"\n            iou_traj_score.columns.name = \"Unique IDs\"\n\n            query_frame.add_traj_score(\"weight_iou\", iou_traj_score)\n        ################################################################################\n\n        # threshold for continuing a tracking or starting a new track -&gt; they use 1.0\n        # todo -&gt; should also work without pos_embed\n        traj_score = post_processing.filter_max_center_dist(\n            traj_score,\n            self.max_center_dist,\n            id_inds,\n            query_boxes_px,\n            nonquery_boxes_px,\n        )\n\n        if self.max_center_dist is not None and self.max_center_dist &gt; 0:\n            max_center_dist_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n            )\n\n            max_center_dist_traj_score.index.name = \"Current Frame Instances\"\n            max_center_dist_traj_score.columns.name = \"Unique IDs\"\n\n            query_frame.add_traj_score(\"max_center_dist\", max_center_dist_traj_score)\n\n        ################################################################################\n        scaled_traj_score = torch.softmax(traj_score, dim=1)\n        scaled_traj_score_df = pd.DataFrame(\n            scaled_traj_score.numpy(), columns=unique_ids.cpu().numpy()\n        )\n        scaled_traj_score_df.index.name = \"Current Frame Instances\"\n        scaled_traj_score_df.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"scaled\", scaled_traj_score_df)\n        ################################################################################\n\n        match_i, match_j = linear_sum_assignment((-traj_score))\n\n        track_ids = instance_ids.new_full((n_query,), -1)\n        for i, j in zip(match_i, match_j):\n            # The overlap threshold is multiplied by the number of times the unique track j is matched to an\n            # instance out of all instances in the window excluding the current frame.\n            #\n            # So if this is correct, the threshold is higher for matching an instance from the current frame\n            # to an existing track if that track has already been matched several times.\n            # So if an existing track in the window has been matched a lot, it gets harder to match to that track.\n            thresh = (\n                overlap_thresh * id_inds[:, j].sum() if mult_thresh else overlap_thresh\n            )\n            if n_traj &gt;= self.max_tracks or traj_score[i, j] &gt; thresh:\n                logger.debug(\n                    f\"Assigning instance {i} to track {j} with id {unique_ids[j]}\"\n                )\n                track_ids[i] = unique_ids[j]\n                query_frame.instances[i].track_score = scaled_traj_score[i, j].item()\n        logger.debug(f\"track_ids: {track_ids}\")\n        for i in range(n_query):\n            if track_ids[i] &lt; 0:\n                logger.debug(f\"Creating new track {curr_track}\")\n                curr_track += 1\n                track_ids[i] = curr_track\n\n        query_frame.matches = (match_i, match_j)\n\n        for instance, track_id in zip(query_frame.instances, track_ids):\n            instance.pred_track_id = track_id\n\n        final_traj_score = pd.DataFrame(\n            traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n        )\n        final_traj_score.index.name = \"Current Frame Instances\"\n        final_traj_score.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"final\", final_traj_score)\n        return query_frame\n</code></pre>"},{"location":"reference/dreem/inference/#dreem.inference.BatchTracker.__call__","title":"<code>__call__(model, frames)</code>","text":"<p>Wrap around <code>track</code> to enable <code>tracker()</code> instead of <code>tracker.track()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[Frame]</code> <p>list of Frames to run inference on</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>List of frames containing association matrix scores and instances populated with pred track ids.</p> Source code in <code>dreem/inference/batch_tracker.py</code> <pre><code>def __call__(\n    self, model: GlobalTrackingTransformer, frames: list[Frame]\n) -&gt; list[Frame]:\n    \"\"\"Wrap around `track` to enable `tracker()` instead of `tracker.track()`.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: list of Frames to run inference on\n\n    Returns:\n        List of frames containing association matrix scores and instances populated with pred track ids.\n    \"\"\"\n    _ = model.eval()\n\n    for frame in frames:\n        if frame.has_instances():\n            if not self.use_vis_feats:\n                for instance in frame.instances:\n                    instance.features = torch.zeros(1, model.d_model)\n                # frame[\"features\"] = torch.randn(\n                #     num_frame_instances, self.model.d_model\n                # )\n\n            # comment out to turn encoder off\n\n            # Assuming the encoder is already trained or train encoder jointly.\n            elif not frame.has_features():\n                with torch.no_grad():\n                    crops = frame.get_crops()\n                    z = model.visual_encoder(crops)\n\n                    for i, z_i in enumerate(z):\n                        frame.instances[i].features = z_i\n\n    instances_pred = self.track(model, frames)\n    # no more persistent tracking. It is on by default\n    # if not self.persistent_tracking:\n    #     logger.debug(f\"Clearing Queue after tracking\")\n    #     self.track_queue.end_tracks()\n\n    return instances_pred\n</code></pre>"},{"location":"reference/dreem/inference/#dreem.inference.BatchTracker.__init__","title":"<code>__init__(window_size=8, use_vis_feats=True, overlap_thresh=0.01, mult_thresh=True, decay_time=None, iou=None, max_center_dist=None, persistent_tracking=True, max_gap=inf, max_tracks=inf, verbose=False, **kwargs)</code>","text":"<p>Initialize a tracker to run inference.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>the size of the window used during sliding inference.</p> <code>8</code> <code>use_vis_feats</code> <code>bool</code> <p>Whether or not to use visual feature extractor.</p> <code>True</code> <code>overlap_thresh</code> <code>float</code> <p>the trajectory overlap threshold to be used for assignment.</p> <code>0.01</code> <code>mult_thresh</code> <code>bool</code> <p>Whether or not to use weight threshold.</p> <code>True</code> <code>decay_time</code> <code>float | None</code> <p>weight for <code>decay_time</code> postprocessing.</p> <code>None</code> <code>iou</code> <code>str | None</code> <p>Either [None, '', \"mult\" or \"max\"]  Whether to use multiplicative or max iou reweighting.</p> <code>None</code> <code>max_center_dist</code> <code>float | None</code> <p>distance threshold for filtering trajectory score matrix.</p> <code>None</code> <code>persistent_tracking</code> <code>bool</code> <p>whether to keep a buffer across chunks or not.</p> <code>True</code> <code>max_gap</code> <code>int</code> <p>the max number of frames a trajectory can be missing before termination.</p> <code>inf</code> <code>max_tracks</code> <code>int</code> <p>the maximum number of tracks that can be created while tracking. We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.</p> <code>inf</code> <code>verbose</code> <code>bool</code> <p>Whether or not to turn on debug printing after each operation.</p> <code>False</code> Source code in <code>dreem/inference/batch_tracker.py</code> <pre><code>def __init__(\n    self,\n    window_size: int = 8,\n    use_vis_feats: bool = True,\n    overlap_thresh: float = 0.01,\n    mult_thresh: bool = True,\n    decay_time: float | None = None,\n    iou: str | None = None,\n    max_center_dist: float | None = None,\n    persistent_tracking: bool = True,\n    max_gap: int = inf,\n    max_tracks: int = inf,\n    verbose: bool = False,\n    **kwargs,\n):\n    \"\"\"Initialize a tracker to run inference.\n\n    Args:\n        window_size: the size of the window used during sliding inference.\n        use_vis_feats: Whether or not to use visual feature extractor.\n        overlap_thresh: the trajectory overlap threshold to be used for assignment.\n        mult_thresh: Whether or not to use weight threshold.\n        decay_time: weight for `decay_time` postprocessing.\n        iou: Either [None, '', \"mult\" or \"max\"]\n             Whether to use multiplicative or max iou reweighting.\n        max_center_dist: distance threshold for filtering trajectory score matrix.\n        persistent_tracking: whether to keep a buffer across chunks or not.\n        max_gap: the max number of frames a trajectory can be missing before termination.\n        max_tracks: the maximum number of tracks that can be created while tracking.\n            We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.\n        verbose: Whether or not to turn on debug printing after each operation.\n    \"\"\"\n    self.track_queue = TrackQueue(\n        window_size=window_size, max_gap=max_gap, verbose=verbose\n    )\n    self.num_frames_tracked = 0\n    self.window_size = window_size\n    self.use_vis_feats = use_vis_feats\n    self.overlap_thresh = overlap_thresh\n    self.mult_thresh = mult_thresh\n    self.decay_time = decay_time\n    self.iou = iou\n    self.max_center_dist = max_center_dist\n    self.persistent_tracking = persistent_tracking\n    self.verbose = verbose\n    self.max_tracks = max_tracks\n</code></pre>"},{"location":"reference/dreem/inference/#dreem.inference.BatchTracker.__repr__","title":"<code>__repr__()</code>","text":"<p>Get string representation of tracker.</p> <p>Returns: the string representation of the tracker</p> Source code in <code>dreem/inference/batch_tracker.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get string representation of tracker.\n\n    Returns: the string representation of the tracker\n    \"\"\"\n    return (\n        \"Tracker(\"\n        f\"persistent_tracking={self.persistent_tracking}, \"\n        f\"max_tracks={self.max_tracks}, \"\n        f\"use_vis_feats={self.use_vis_feats}, \"\n        f\"overlap_thresh={self.overlap_thresh}, \"\n        f\"mult_thresh={self.mult_thresh}, \"\n        f\"decay_time={self.decay_time}, \"\n        f\"max_center_dist={self.max_center_dist}, \"\n        f\"verbose={self.verbose}, \"\n        f\"queue={self.track_queue}\"\n    )\n</code></pre>"},{"location":"reference/dreem/inference/#dreem.inference.BatchTracker.track","title":"<code>track(model, frames)</code>","text":"<p>Perform sliding inference on the input video (instances) with a given window size. This method is called once per batch.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[Frame]</code> <p>A list of Frames (See <code>dreem.io.Frame</code> for more info).</p> required <p>Returns:</p> Name Type Description <code>frames</code> <code>list[Frame]</code> <p>A list of Frames populated with pred_track_ids and asso_matrices</p> Source code in <code>dreem/inference/batch_tracker.py</code> <pre><code>def track(\n    self, model: GlobalTrackingTransformer, frames: list[Frame]\n) -&gt; list[Frame]:\n    \"\"\"Perform sliding inference on the input video (instances) with a given window size. This method is called once per batch.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n    Returns:\n        frames: A list of Frames populated with pred_track_ids and asso_matrices\n    \"\"\"\n    # all batches up until context_length number of frames have been tracked, will be tracked frame-by-frame\n    if self.num_frames_tracked &lt; self.window_size:\n        frames = self.track_by_frame(model, frames)\n    else:\n        frames = self.track_by_batch(model, frames)\n\n    return frames\n</code></pre>"},{"location":"reference/dreem/inference/#dreem.inference.BatchTracker.track_by_batch","title":"<code>track_by_batch(model, frames)</code>","text":"<p>Perform sliding inference, on an entire batch of frames, on the input video (instances) with a given context length (window size).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[Frame]</code> <p>A list of Frames (See <code>dreem.io.Frame</code> for more info).</p> required <p>Returns:</p> Name Type Description <code>frames</code> <code>list[Frame]</code> <p>A list of Frames populated with pred_track_ids and asso_matrices</p> Source code in <code>dreem/inference/batch_tracker.py</code> <pre><code>def track_by_batch(\n    self, model: GlobalTrackingTransformer, frames: list[Frame]\n) -&gt; list[Frame]:\n    \"\"\"Perform sliding inference, on an entire batch of frames, on the input video (instances) with a given context length (window size).\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n    Returns:\n        frames: A list of Frames populated with pred_track_ids and asso_matrices\n    \"\"\"\n    # context window starts from last frame just before start of current batch, to window_size frames preceding it\n    # note; can't use last frame of previous batch, because there could be empty frames in between batches that must\n    # be part of the context window for consistency\n    context_window_frames = self.track_queue.collate_tracks(\n        context_start_frame_id=frames[0].frame_id.item()\n        - 1,  # switched off in collate_tracks; there is no cutoff for context, only until the deque gets filled\n        device=frames[0].frame_id.device,\n    )\n\n    context_window_instances = []\n    context_window_instance_frame_ids = []\n    for frame in context_window_frames:\n        context_window_instances.extend(frame.instances)\n        context_window_instance_frame_ids.extend(\n            [frame.frame_id] * len(frame.instances)\n        )\n\n    current_batch_instances = []\n    current_batch_instance_frame_ids = []\n    for frame in frames:\n        current_batch_instances.extend(frame.instances)\n        current_batch_instance_frame_ids.extend(\n            [frame.frame_id] * len(frame.instances)\n        )\n\n    # query is current batch instances, key is context window and current batch instances\n    association_matrix = model(\n        context_window_instances + current_batch_instances, current_batch_instances\n    )\n\n    # take association matrix and all frames off GPU (frames include instances)\n    association_matrix = association_matrix[-1].to(\"cpu\")\n    context_window_frames = [frame.to(\"cpu\") for frame in context_window_frames]\n    frames = [frame.to(\"cpu\") for frame in frames]\n\n    # keep current batch instances in assoc matrix, and remove them after softmax (mirrors the training scheme)\n    pred_frames = self._run_batch_tracker(\n        association_matrix.matrix,\n        context_window_frames,\n        frames,\n        compute_probs_by_frame=True,\n    )\n\n    return pred_frames\n</code></pre>"},{"location":"reference/dreem/inference/#dreem.inference.BatchTracker.track_by_frame","title":"<code>track_by_frame(model, frames)</code>","text":"<p>Perform sliding inference on the input video (instances) with a given window size.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[Frame]</code> <p>A list of Frames (See <code>dreem.io.Frame</code> for more info).</p> required <p>Returns:</p> Name Type Description <code>frames</code> <code>list[Frame]</code> <p>A list of Frames populated with pred_track_ids and asso_matrices</p> Source code in <code>dreem/inference/batch_tracker.py</code> <pre><code>def track_by_frame(\n    self, model: GlobalTrackingTransformer, frames: list[Frame]\n) -&gt; list[Frame]:\n    \"\"\"Perform sliding inference on the input video (instances) with a given window size.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n    Returns:\n        frames: A list of Frames populated with pred_track_ids and asso_matrices\n    \"\"\"\n    # B: batch size.\n    # D: embedding dimension.\n    # nc: number of channels.\n    # H: height.\n    # W: width.\n\n    for batch_idx, frame_to_track in enumerate(frames):\n        # if we're tracking by frame, it means context length of frames hasn't been reached yet, so context start frame id is 0\n        context_window_frames = self.track_queue.collate_tracks(\n            context_start_frame_id=0, device=frame_to_track.frame_id.device\n        )\n        logger.debug(f\"Current number of tracks is {self.track_queue.n_tracks}\")\n\n        if (\n            self.persistent_tracking and frame_to_track.frame_id == 0\n        ):  # check for new video and clear queue\n            logger.debug(\"New Video! Resetting Track Queue.\")\n            self.track_queue.end_tracks()\n\n        \"\"\"\n        Initialize tracks on first frame where detections appear. This is the first frame of the first batch\n        \"\"\"\n        if len(self.track_queue) == 0:\n            if frame_to_track.has_instances():\n                logger.debug(\n                    f\"Initializing track on clip ind {batch_idx} frame {frame_to_track.frame_id.item()}\"\n                )\n\n                curr_track_id = 0\n                for i, instance in enumerate(frames[batch_idx].instances):\n                    instance.pred_track_id = instance.gt_track_id\n                    curr_track_id = max(curr_track_id, instance.pred_track_id)\n\n                for i, instance in enumerate(frames[batch_idx].instances):\n                    if instance.pred_track_id == -1:\n                        curr_track_id += 1\n                        instance.pred_track_id = curr_track_id\n\n        else:\n            if frame_to_track.has_instances():  # Check if there are detections. If there are skip and increment gap count\n                frames_to_track = context_window_frames + [\n                    frame_to_track\n                ]  # better var name?\n\n                frame_to_track = self._run_frame_by_frame_tracker(\n                    model,\n                    frames_to_track,\n                )\n\n        if frame_to_track.has_instances():\n            self.track_queue.add_frame(frame_to_track)\n            self.num_frames_tracked += 1\n        else:\n            self.track_queue.increment_gaps([])\n\n        frames[batch_idx] = frame_to_track\n\n    return frames\n</code></pre>"},{"location":"reference/dreem/inference/#dreem.inference.Tracker","title":"<code>Tracker</code>","text":"<p>Tracker class used for assignment based on sliding inference from GTR.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Wrap around <code>track</code> to enable <code>tracker()</code> instead of <code>tracker.track()</code>.</p> <code>__init__</code> <p>Initialize a tracker to run inference.</p> <code>__repr__</code> <p>Get string representation of tracker.</p> <code>sliding_inference</code> <p>Perform sliding inference on the input video (instances) with a given window size.</p> <code>track</code> <p>Run tracker and get predicted trajectories.</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>class Tracker:\n    \"\"\"Tracker class used for assignment based on sliding inference from GTR.\"\"\"\n\n    def __init__(\n        self,\n        window_size: int = 8,\n        use_vis_feats: bool = True,\n        overlap_thresh: float = 0.01,\n        mult_thresh: bool = True,\n        decay_time: float | None = None,\n        iou: str | None = None,\n        max_center_dist: float | None = None,\n        persistent_tracking: bool = False,\n        max_gap: int = inf,\n        max_tracks: int = inf,\n        verbose: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize a tracker to run inference.\n\n        Args:\n            window_size: the size of the window used during sliding inference.\n            use_vis_feats: Whether or not to use visual feature extractor.\n            overlap_thresh: the trajectory overlap threshold to be used for assignment.\n            mult_thresh: Whether or not to use weight threshold.\n            decay_time: weight for `decay_time` postprocessing.\n            iou: Either [None, '', \"mult\" or \"max\"]\n                 Whether to use multiplicative or max iou reweighting.\n            max_center_dist: distance threshold for filtering trajectory score matrix.\n            persistent_tracking: whether to keep a buffer across chunks or not.\n            max_gap: the max number of frames a trajectory can be missing before termination.\n            max_tracks: the maximum number of tracks that can be created while tracking.\n                We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.\n            verbose: Whether or not to turn on debug printing after each operation.\n        \"\"\"\n        self.track_queue = TrackQueue(\n            window_size=window_size, max_gap=max_gap, verbose=verbose\n        )\n        self.use_vis_feats = use_vis_feats\n        self.overlap_thresh = overlap_thresh\n        self.mult_thresh = mult_thresh\n        self.decay_time = decay_time\n        self.iou = iou\n        self.max_center_dist = max_center_dist\n        self.persistent_tracking = persistent_tracking\n        self.verbose = verbose\n        self.max_tracks = max_tracks\n\n    def __call__(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; list[Frame]:\n        \"\"\"Wrap around `track` to enable `tracker()` instead of `tracker.track()`.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: list of Frames to run inference on\n\n        Returns:\n            List of frames containing association matrix scores and instances populated with pred track ids.\n        \"\"\"\n        return self.track(model, frames)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get string representation of tracker.\n\n        Returns: the string representation of the tracker\n        \"\"\"\n        return (\n            \"Tracker(\"\n            f\"persistent_tracking={self.persistent_tracking}, \"\n            f\"max_tracks={self.max_tracks}, \"\n            f\"use_vis_feats={self.use_vis_feats}, \"\n            f\"overlap_thresh={self.overlap_thresh}, \"\n            f\"mult_thresh={self.mult_thresh}, \"\n            f\"decay_time={self.decay_time}, \"\n            f\"max_center_dist={self.max_center_dist}, \"\n            f\"verbose={self.verbose}, \"\n            f\"queue={self.track_queue}\"\n        )\n\n    def track(\n        self, model: GlobalTrackingTransformer, frames: list[dict]\n    ) -&gt; list[Frame]:\n        \"\"\"Run tracker and get predicted trajectories.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: data dict to run inference on\n\n        Returns:\n            List of Frames populated with pred track ids and association matrix scores\n        \"\"\"\n        # Extract feature representations with pre-trained encoder.\n\n        _ = model.eval()\n\n        for frame in frames:\n            if frame.has_instances():\n                if not self.use_vis_feats:\n                    for instance in frame.instances:\n                        instance.features = torch.zeros(1, model.d_model)\n                    # frame[\"features\"] = torch.randn(\n                    #     num_frame_instances, self.model.d_model\n                    # )\n\n                # comment out to turn encoder off\n\n                # Assuming the encoder is already trained or train encoder jointly.\n                elif not frame.has_features():\n                    with torch.no_grad():\n                        crops = frame.get_crops()\n                        z = model.visual_encoder(crops)\n\n                        for i, z_i in enumerate(z):\n                            frame.instances[i].features = z_i\n\n        # I feel like this chunk is unnecessary:\n        # reid_features = torch.cat(\n        #     [frame[\"features\"] for frame in instances], dim=0\n        # ).unsqueeze(0)\n\n        # asso_preds, pred_boxes, pred_time, embeddings = self.model(\n        #     instances, reid_features\n        # )\n        instances_pred = self.sliding_inference(model, frames)\n\n        if not self.persistent_tracking:\n            logger.debug(\"Clearing Queue after tracking\")\n            self.track_queue.end_tracks()\n\n        return instances_pred\n\n    def sliding_inference(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; list[Frame]:\n        \"\"\"Perform sliding inference on the input video (instances) with a given window size.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n        Returns:\n            frames: A list of Frames populated with pred_track_ids and asso_matrices\n        \"\"\"\n        # B: batch size.\n        # D: embedding dimension.\n        # nc: number of channels.\n        # H: height.\n        # W: width.\n\n        for batch_idx, frame_to_track in enumerate(frames):\n            tracked_frames = self.track_queue.collate_tracks(\n                device=frame_to_track.frame_id.device\n            )\n            logger.debug(f\"Current number of tracks is {self.track_queue.n_tracks}\")\n\n            if (\n                self.persistent_tracking and frame_to_track.frame_id == 0\n            ):  # check for new video and clear queue\n                logger.debug(\"New Video! Resetting Track Queue.\")\n                self.track_queue.end_tracks()\n\n            \"\"\"\n            Initialize tracks on first frame where detections appear.\n            \"\"\"\n            if len(self.track_queue) == 0:\n                if frame_to_track.has_instances():\n                    logger.debug(\n                        f\"Initializing track on clip ind {batch_idx} frame {frame_to_track.frame_id.item()}\"\n                    )\n\n                    curr_track_id = 0\n                    for i, instance in enumerate(frames[batch_idx].instances):\n                        instance.pred_track_id = instance.gt_track_id\n                        curr_track_id = max(curr_track_id, instance.pred_track_id)\n\n                    for i, instance in enumerate(frames[batch_idx].instances):\n                        if instance.pred_track_id == -1:\n                            curr_track_id += 1\n                            instance.pred_track_id = curr_track_id\n\n            else:\n                if frame_to_track.has_instances():  # Check if there are detections. If there are skip and increment gap count\n                    frames_to_track = tracked_frames + [\n                        frame_to_track\n                    ]  # better var name?\n\n                    query_ind = len(frames_to_track) - 1\n\n                    frame_to_track = self._run_global_tracker(\n                        model,\n                        frames_to_track,\n                        query_ind=query_ind,\n                    )\n\n            if frame_to_track.has_instances():\n                self.track_queue.add_frame(frame_to_track)\n            else:\n                self.track_queue.increment_gaps([])\n\n            frames[batch_idx] = frame_to_track\n        return frames\n\n    def _run_global_tracker(\n        self, model: GlobalTrackingTransformer, frames: list[Frame], query_ind: int\n    ) -&gt; Frame:\n        \"\"\"Run global tracker performs the actual tracking.\n\n        Uses Hungarian algorithm to do track assigning.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: A list of Frames containing reid features. See `dreem.io.data_structures` for more info.\n            query_ind: An integer for the query frame within the window of instances.\n\n        Returns:\n            query_frame: The query frame now populated with the pred_track_ids.\n        \"\"\"\n        # *: each item in frames is a frame in the window. So it follows\n        #    that each frame in the window has * detected instances.\n        # D: embedding dimension.\n        # total_instances: number of instances in the window.\n        # N_i: number of detected instances in i-th frame of window.\n        # instances_per_frame: a list of number of instances in each frame of the window.\n        # n_query: number of instances in current/query frame (rightmost frame of the window).\n        # n_nonquery: number of instances in the window excluding the current/query frame.\n        # window_size: length of window.\n        # L: number of decoder blocks.\n        # n_traj: number of existing tracks within the window so far.\n\n        # Number of instances in each frame of the window.\n        # E.g.: instances_per_frame: [4, 5, 6, 7]; window of length 4 with 4 detected instances in the first frame of the window.\n\n        _ = model.eval()\n\n        query_frame = frames[query_ind]\n\n        query_instances = query_frame.instances\n        all_instances = [instance for frame in frames for instance in frame.instances]\n\n        logger.debug(f\"Frame {query_frame.frame_id.item()}\")\n\n        instances_per_frame = [frame.num_detected for frame in frames]\n\n        total_instances, window_size = (\n            sum(instances_per_frame),\n            len(instances_per_frame),\n        )  # Number of instances in window; length of window.\n\n        logger.debug(f\"total_instances: {total_instances}\")\n\n        overlap_thresh = self.overlap_thresh\n        mult_thresh = self.mult_thresh\n        n_traj = self.track_queue.n_tracks\n        curr_track = self.track_queue.curr_track\n\n        reid_features = torch.cat([frame.get_features() for frame in frames], dim=0)[\n            None\n        ]  # (1, total_instances, D=512)\n\n        # (L=1, n_query, total_instances)\n        with torch.no_grad():\n            asso_matrix = model(all_instances, query_instances)\n\n        asso_output = asso_matrix[-1].matrix.split(\n            instances_per_frame, dim=1\n        )  # (window_size, n_query, N_i)\n        asso_output = model_utils.softmax_asso(\n            asso_output\n        )  # (window_size, n_query, N_i)\n        asso_output = torch.cat(asso_output, dim=1).cpu()  # (n_query, total_instances)\n\n        asso_output_df = pd.DataFrame(\n            asso_output.clone().numpy(),\n            columns=[f\"Instance {i}\" for i in range(asso_output.shape[-1])],\n        )\n\n        asso_output_df.index.name = \"Instances\"\n        asso_output_df.columns.name = \"Instances\"\n\n        query_frame.add_traj_score(\"asso_output\", asso_output_df)\n        query_frame.asso_output = asso_matrix[-1]\n\n        n_query = (\n            query_frame.num_detected\n        )  # Number of instances in the current/query frame.\n\n        n_nonquery = (\n            total_instances - n_query\n        )  # Number of instances in the window not including the current/query frame.\n\n        logger.debug(f\"n_nonquery: {n_nonquery}\")\n        logger.debug(f\"n_query: {n_query}\")\n\n        instance_ids = torch.cat(\n            [\n                x.get_pred_track_ids()\n                for batch_idx, x in enumerate(frames)\n                if batch_idx != query_ind\n            ],\n            dim=0,\n        ).view(n_nonquery)  # (n_nonquery,)\n\n        query_inds = [\n            x\n            for x in range(\n                sum(instances_per_frame[:query_ind]),\n                sum(instances_per_frame[: query_ind + 1]),\n            )\n        ]\n\n        nonquery_inds = [i for i in range(total_instances) if i not in query_inds]\n\n        # instead should we do model(nonquery_instances, query_instances)?\n        asso_nonquery = asso_output[:, nonquery_inds]  # (n_query, n_nonquery)\n\n        asso_nonquery_df = pd.DataFrame(\n            asso_nonquery.clone().numpy(), columns=nonquery_inds\n        )\n\n        asso_nonquery_df.index.name = \"Current Frame Instances\"\n        asso_nonquery_df.columns.name = \"Nonquery Instances\"\n\n        query_frame.add_traj_score(\"asso_nonquery\", asso_nonquery_df)\n\n        # get raw bbox coords of prev frame instances from frame.instances_per_frame\n        query_boxes_px = torch.cat(\n            [instance.bbox for instance in query_frame.instances], dim=0\n        )\n        nonquery_boxes_px = torch.cat(\n            [\n                instance.bbox\n                for nonquery_frame in frames\n                if nonquery_frame.frame_id != query_frame.frame_id\n                for instance in nonquery_frame.instances\n            ],\n            dim=0,\n        )\n\n        pred_boxes = model_utils.get_boxes(all_instances)\n        query_boxes = pred_boxes[query_inds]  # n_k x 4\n        nonquery_boxes = pred_boxes[nonquery_inds]  # n_nonquery x 4\n\n        unique_ids = torch.unique(instance_ids)  # (n_nonquery,)\n\n        logger.debug(f\"Instance IDs: {instance_ids}\")\n        logger.debug(f\"unique ids: {unique_ids}\")\n\n        id_inds = (\n            unique_ids[None, :] == instance_ids[:, None]\n        ).float()  # (n_nonquery, n_traj)\n\n        ################################################################################\n\n        # reweighting hyper-parameters for association -&gt; they use 0.9\n\n        traj_score = post_processing.weight_decay_time(\n            asso_nonquery, self.decay_time, reid_features, window_size, query_ind\n        )\n\n        if self.decay_time is not None and self.decay_time &gt; 0:\n            decay_time_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=nonquery_inds\n            )\n\n            decay_time_traj_score.index.name = \"Query Instances\"\n            decay_time_traj_score.columns.name = \"Nonquery Instances\"\n\n            query_frame.add_traj_score(\"decay_time\", decay_time_traj_score)\n        ################################################################################\n\n        # (n_query x n_nonquery) x (n_nonquery x n_traj) --&gt; n_query x n_traj\n        traj_score = torch.mm(traj_score, id_inds.cpu())  # (n_query, n_traj)\n\n        traj_score_df = pd.DataFrame(\n            traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n        )\n\n        traj_score_df.index.name = \"Current Frame Instances\"\n        traj_score_df.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"traj_score\", traj_score_df)\n        ################################################################################\n\n        # with iou -&gt; combining with location in tracker, they set to True\n        # todo -&gt; should also work without pos_embed\n\n        if id_inds.numel() &gt; 0:\n            # this throws error, think we need to slice?\n            # last_inds = (id_inds * torch.arange(\n            #    n_nonquery, device=id_inds.device)[:, None]).max(dim=0)[1] # n_traj\n\n            last_inds = (\n                id_inds * torch.arange(n_nonquery, device=id_inds.device)[:, None]\n            ).max(dim=0)[1]  # M\n\n            last_boxes = nonquery_boxes[last_inds]  # n_traj x 4\n            last_ious = post_processing._pairwise_iou(\n                Boxes(query_boxes), Boxes(last_boxes)\n            )  # n_k x M\n        else:\n            last_ious = traj_score.new_zeros(traj_score.shape)\n\n        traj_score = post_processing.weight_iou(traj_score, self.iou, last_ious.cpu())\n\n        if self.iou is not None and self.iou != \"\":\n            iou_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n            )\n\n            iou_traj_score.index.name = \"Current Frame Instances\"\n            iou_traj_score.columns.name = \"Unique IDs\"\n\n            query_frame.add_traj_score(\"weight_iou\", iou_traj_score)\n        ################################################################################\n\n        # threshold for continuing a tracking or starting a new track -&gt; they use 1.0\n        # todo -&gt; should also work without pos_embed\n        traj_score = post_processing.filter_max_center_dist(\n            traj_score,\n            self.max_center_dist,\n            id_inds,\n            query_boxes_px,\n            nonquery_boxes_px,\n        )\n\n        if self.max_center_dist is not None and self.max_center_dist &gt; 0:\n            max_center_dist_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n            )\n\n            max_center_dist_traj_score.index.name = \"Current Frame Instances\"\n            max_center_dist_traj_score.columns.name = \"Unique IDs\"\n\n            query_frame.add_traj_score(\"max_center_dist\", max_center_dist_traj_score)\n\n        ################################################################################\n        scaled_traj_score = torch.softmax(traj_score, dim=1)\n        scaled_traj_score_df = pd.DataFrame(\n            scaled_traj_score.numpy(), columns=unique_ids.cpu().numpy()\n        )\n        scaled_traj_score_df.index.name = \"Current Frame Instances\"\n        scaled_traj_score_df.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"scaled\", scaled_traj_score_df)\n        ################################################################################\n\n        match_i, match_j = linear_sum_assignment((-traj_score))\n\n        track_ids = instance_ids.new_full((n_query,), -1)\n        for i, j in zip(match_i, match_j):\n            # The overlap threshold is multiplied by the number of times the unique track j is matched to an\n            # instance out of all instances in the window excluding the current frame.\n            #\n            # So if this is correct, the threshold is higher for matching an instance from the current frame\n            # to an existing track if that track has already been matched several times.\n            # So if an existing track in the window has been matched a lot, it gets harder to match to that track.\n            thresh = (\n                overlap_thresh * id_inds[:, j].sum() if mult_thresh else overlap_thresh\n            )\n            if n_traj &gt;= self.max_tracks or traj_score[i, j] &gt; thresh:\n                logger.debug(\n                    f\"Assigning instance {i} to track {j} with id {unique_ids[j]}\"\n                )\n                track_ids[i] = unique_ids[j]\n                query_frame.instances[i].track_score = scaled_traj_score[i, j].item()\n        logger.debug(f\"track_ids: {track_ids}\")\n        for i in range(n_query):\n            if track_ids[i] &lt; 0:\n                logger.debug(f\"Creating new track {curr_track}\")\n                curr_track += 1\n                track_ids[i] = curr_track\n\n        query_frame.matches = (match_i, match_j)\n\n        for instance, track_id in zip(query_frame.instances, track_ids):\n            instance.pred_track_id = track_id\n\n        final_traj_score = pd.DataFrame(\n            traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n        )\n        final_traj_score.index.name = \"Current Frame Instances\"\n        final_traj_score.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"final\", final_traj_score)\n        return query_frame\n</code></pre>"},{"location":"reference/dreem/inference/#dreem.inference.Tracker.__call__","title":"<code>__call__(model, frames)</code>","text":"<p>Wrap around <code>track</code> to enable <code>tracker()</code> instead of <code>tracker.track()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[Frame]</code> <p>list of Frames to run inference on</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>List of frames containing association matrix scores and instances populated with pred track ids.</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def __call__(\n    self, model: GlobalTrackingTransformer, frames: list[Frame]\n) -&gt; list[Frame]:\n    \"\"\"Wrap around `track` to enable `tracker()` instead of `tracker.track()`.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: list of Frames to run inference on\n\n    Returns:\n        List of frames containing association matrix scores and instances populated with pred track ids.\n    \"\"\"\n    return self.track(model, frames)\n</code></pre>"},{"location":"reference/dreem/inference/#dreem.inference.Tracker.__init__","title":"<code>__init__(window_size=8, use_vis_feats=True, overlap_thresh=0.01, mult_thresh=True, decay_time=None, iou=None, max_center_dist=None, persistent_tracking=False, max_gap=inf, max_tracks=inf, verbose=False, **kwargs)</code>","text":"<p>Initialize a tracker to run inference.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>the size of the window used during sliding inference.</p> <code>8</code> <code>use_vis_feats</code> <code>bool</code> <p>Whether or not to use visual feature extractor.</p> <code>True</code> <code>overlap_thresh</code> <code>float</code> <p>the trajectory overlap threshold to be used for assignment.</p> <code>0.01</code> <code>mult_thresh</code> <code>bool</code> <p>Whether or not to use weight threshold.</p> <code>True</code> <code>decay_time</code> <code>float | None</code> <p>weight for <code>decay_time</code> postprocessing.</p> <code>None</code> <code>iou</code> <code>str | None</code> <p>Either [None, '', \"mult\" or \"max\"]  Whether to use multiplicative or max iou reweighting.</p> <code>None</code> <code>max_center_dist</code> <code>float | None</code> <p>distance threshold for filtering trajectory score matrix.</p> <code>None</code> <code>persistent_tracking</code> <code>bool</code> <p>whether to keep a buffer across chunks or not.</p> <code>False</code> <code>max_gap</code> <code>int</code> <p>the max number of frames a trajectory can be missing before termination.</p> <code>inf</code> <code>max_tracks</code> <code>int</code> <p>the maximum number of tracks that can be created while tracking. We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.</p> <code>inf</code> <code>verbose</code> <code>bool</code> <p>Whether or not to turn on debug printing after each operation.</p> <code>False</code> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def __init__(\n    self,\n    window_size: int = 8,\n    use_vis_feats: bool = True,\n    overlap_thresh: float = 0.01,\n    mult_thresh: bool = True,\n    decay_time: float | None = None,\n    iou: str | None = None,\n    max_center_dist: float | None = None,\n    persistent_tracking: bool = False,\n    max_gap: int = inf,\n    max_tracks: int = inf,\n    verbose: bool = False,\n    **kwargs,\n):\n    \"\"\"Initialize a tracker to run inference.\n\n    Args:\n        window_size: the size of the window used during sliding inference.\n        use_vis_feats: Whether or not to use visual feature extractor.\n        overlap_thresh: the trajectory overlap threshold to be used for assignment.\n        mult_thresh: Whether or not to use weight threshold.\n        decay_time: weight for `decay_time` postprocessing.\n        iou: Either [None, '', \"mult\" or \"max\"]\n             Whether to use multiplicative or max iou reweighting.\n        max_center_dist: distance threshold for filtering trajectory score matrix.\n        persistent_tracking: whether to keep a buffer across chunks or not.\n        max_gap: the max number of frames a trajectory can be missing before termination.\n        max_tracks: the maximum number of tracks that can be created while tracking.\n            We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.\n        verbose: Whether or not to turn on debug printing after each operation.\n    \"\"\"\n    self.track_queue = TrackQueue(\n        window_size=window_size, max_gap=max_gap, verbose=verbose\n    )\n    self.use_vis_feats = use_vis_feats\n    self.overlap_thresh = overlap_thresh\n    self.mult_thresh = mult_thresh\n    self.decay_time = decay_time\n    self.iou = iou\n    self.max_center_dist = max_center_dist\n    self.persistent_tracking = persistent_tracking\n    self.verbose = verbose\n    self.max_tracks = max_tracks\n</code></pre>"},{"location":"reference/dreem/inference/#dreem.inference.Tracker.__repr__","title":"<code>__repr__()</code>","text":"<p>Get string representation of tracker.</p> <p>Returns: the string representation of the tracker</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get string representation of tracker.\n\n    Returns: the string representation of the tracker\n    \"\"\"\n    return (\n        \"Tracker(\"\n        f\"persistent_tracking={self.persistent_tracking}, \"\n        f\"max_tracks={self.max_tracks}, \"\n        f\"use_vis_feats={self.use_vis_feats}, \"\n        f\"overlap_thresh={self.overlap_thresh}, \"\n        f\"mult_thresh={self.mult_thresh}, \"\n        f\"decay_time={self.decay_time}, \"\n        f\"max_center_dist={self.max_center_dist}, \"\n        f\"verbose={self.verbose}, \"\n        f\"queue={self.track_queue}\"\n    )\n</code></pre>"},{"location":"reference/dreem/inference/#dreem.inference.Tracker.sliding_inference","title":"<code>sliding_inference(model, frames)</code>","text":"<p>Perform sliding inference on the input video (instances) with a given window size.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[Frame]</code> <p>A list of Frames (See <code>dreem.io.Frame</code> for more info).</p> required <p>Returns:</p> Name Type Description <code>frames</code> <code>list[Frame]</code> <p>A list of Frames populated with pred_track_ids and asso_matrices</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def sliding_inference(\n    self, model: GlobalTrackingTransformer, frames: list[Frame]\n) -&gt; list[Frame]:\n    \"\"\"Perform sliding inference on the input video (instances) with a given window size.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n    Returns:\n        frames: A list of Frames populated with pred_track_ids and asso_matrices\n    \"\"\"\n    # B: batch size.\n    # D: embedding dimension.\n    # nc: number of channels.\n    # H: height.\n    # W: width.\n\n    for batch_idx, frame_to_track in enumerate(frames):\n        tracked_frames = self.track_queue.collate_tracks(\n            device=frame_to_track.frame_id.device\n        )\n        logger.debug(f\"Current number of tracks is {self.track_queue.n_tracks}\")\n\n        if (\n            self.persistent_tracking and frame_to_track.frame_id == 0\n        ):  # check for new video and clear queue\n            logger.debug(\"New Video! Resetting Track Queue.\")\n            self.track_queue.end_tracks()\n\n        \"\"\"\n        Initialize tracks on first frame where detections appear.\n        \"\"\"\n        if len(self.track_queue) == 0:\n            if frame_to_track.has_instances():\n                logger.debug(\n                    f\"Initializing track on clip ind {batch_idx} frame {frame_to_track.frame_id.item()}\"\n                )\n\n                curr_track_id = 0\n                for i, instance in enumerate(frames[batch_idx].instances):\n                    instance.pred_track_id = instance.gt_track_id\n                    curr_track_id = max(curr_track_id, instance.pred_track_id)\n\n                for i, instance in enumerate(frames[batch_idx].instances):\n                    if instance.pred_track_id == -1:\n                        curr_track_id += 1\n                        instance.pred_track_id = curr_track_id\n\n        else:\n            if frame_to_track.has_instances():  # Check if there are detections. If there are skip and increment gap count\n                frames_to_track = tracked_frames + [\n                    frame_to_track\n                ]  # better var name?\n\n                query_ind = len(frames_to_track) - 1\n\n                frame_to_track = self._run_global_tracker(\n                    model,\n                    frames_to_track,\n                    query_ind=query_ind,\n                )\n\n        if frame_to_track.has_instances():\n            self.track_queue.add_frame(frame_to_track)\n        else:\n            self.track_queue.increment_gaps([])\n\n        frames[batch_idx] = frame_to_track\n    return frames\n</code></pre>"},{"location":"reference/dreem/inference/#dreem.inference.Tracker.track","title":"<code>track(model, frames)</code>","text":"<p>Run tracker and get predicted trajectories.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[dict]</code> <p>data dict to run inference on</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>List of Frames populated with pred track ids and association matrix scores</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def track(\n    self, model: GlobalTrackingTransformer, frames: list[dict]\n) -&gt; list[Frame]:\n    \"\"\"Run tracker and get predicted trajectories.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: data dict to run inference on\n\n    Returns:\n        List of Frames populated with pred track ids and association matrix scores\n    \"\"\"\n    # Extract feature representations with pre-trained encoder.\n\n    _ = model.eval()\n\n    for frame in frames:\n        if frame.has_instances():\n            if not self.use_vis_feats:\n                for instance in frame.instances:\n                    instance.features = torch.zeros(1, model.d_model)\n                # frame[\"features\"] = torch.randn(\n                #     num_frame_instances, self.model.d_model\n                # )\n\n            # comment out to turn encoder off\n\n            # Assuming the encoder is already trained or train encoder jointly.\n            elif not frame.has_features():\n                with torch.no_grad():\n                    crops = frame.get_crops()\n                    z = model.visual_encoder(crops)\n\n                    for i, z_i in enumerate(z):\n                        frame.instances[i].features = z_i\n\n    # I feel like this chunk is unnecessary:\n    # reid_features = torch.cat(\n    #     [frame[\"features\"] for frame in instances], dim=0\n    # ).unsqueeze(0)\n\n    # asso_preds, pred_boxes, pred_time, embeddings = self.model(\n    #     instances, reid_features\n    # )\n    instances_pred = self.sliding_inference(model, frames)\n\n    if not self.persistent_tracking:\n        logger.debug(\"Clearing Queue after tracking\")\n        self.track_queue.end_tracks()\n\n    return instances_pred\n</code></pre>"},{"location":"reference/dreem/inference/batch_tracker/","title":"batch_tracker","text":""},{"location":"reference/dreem/inference/batch_tracker/#dreem.inference.batch_tracker","title":"<code>dreem.inference.batch_tracker</code>","text":"<p>Module containing logic for going from association -&gt; assignment.</p> <p>Classes:</p> Name Description <code>BatchTracker</code> <p>Tracker class used for assignment based on sliding inference from GTR.</p>"},{"location":"reference/dreem/inference/batch_tracker/#dreem.inference.batch_tracker.BatchTracker","title":"<code>BatchTracker</code>","text":"<p>Tracker class used for assignment based on sliding inference from GTR.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Wrap around <code>track</code> to enable <code>tracker()</code> instead of <code>tracker.track()</code>.</p> <code>__init__</code> <p>Initialize a tracker to run inference.</p> <code>__repr__</code> <p>Get string representation of tracker.</p> <code>track</code> <p>Perform sliding inference on the input video (instances) with a given window size. This method is called once per batch.</p> <code>track_by_batch</code> <p>Perform sliding inference, on an entire batch of frames, on the input video (instances) with a given context length (window size).</p> <code>track_by_frame</code> <p>Perform sliding inference on the input video (instances) with a given window size.</p> Source code in <code>dreem/inference/batch_tracker.py</code> <pre><code>class BatchTracker:\n    \"\"\"Tracker class used for assignment based on sliding inference from GTR.\"\"\"\n\n    def __init__(\n        self,\n        window_size: int = 8,\n        use_vis_feats: bool = True,\n        overlap_thresh: float = 0.01,\n        mult_thresh: bool = True,\n        decay_time: float | None = None,\n        iou: str | None = None,\n        max_center_dist: float | None = None,\n        persistent_tracking: bool = True,\n        max_gap: int = inf,\n        max_tracks: int = inf,\n        verbose: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize a tracker to run inference.\n\n        Args:\n            window_size: the size of the window used during sliding inference.\n            use_vis_feats: Whether or not to use visual feature extractor.\n            overlap_thresh: the trajectory overlap threshold to be used for assignment.\n            mult_thresh: Whether or not to use weight threshold.\n            decay_time: weight for `decay_time` postprocessing.\n            iou: Either [None, '', \"mult\" or \"max\"]\n                 Whether to use multiplicative or max iou reweighting.\n            max_center_dist: distance threshold for filtering trajectory score matrix.\n            persistent_tracking: whether to keep a buffer across chunks or not.\n            max_gap: the max number of frames a trajectory can be missing before termination.\n            max_tracks: the maximum number of tracks that can be created while tracking.\n                We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.\n            verbose: Whether or not to turn on debug printing after each operation.\n        \"\"\"\n        self.track_queue = TrackQueue(\n            window_size=window_size, max_gap=max_gap, verbose=verbose\n        )\n        self.num_frames_tracked = 0\n        self.window_size = window_size\n        self.use_vis_feats = use_vis_feats\n        self.overlap_thresh = overlap_thresh\n        self.mult_thresh = mult_thresh\n        self.decay_time = decay_time\n        self.iou = iou\n        self.max_center_dist = max_center_dist\n        self.persistent_tracking = persistent_tracking\n        self.verbose = verbose\n        self.max_tracks = max_tracks\n\n    def __call__(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; list[Frame]:\n        \"\"\"Wrap around `track` to enable `tracker()` instead of `tracker.track()`.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: list of Frames to run inference on\n\n        Returns:\n            List of frames containing association matrix scores and instances populated with pred track ids.\n        \"\"\"\n        _ = model.eval()\n\n        for frame in frames:\n            if frame.has_instances():\n                if not self.use_vis_feats:\n                    for instance in frame.instances:\n                        instance.features = torch.zeros(1, model.d_model)\n                    # frame[\"features\"] = torch.randn(\n                    #     num_frame_instances, self.model.d_model\n                    # )\n\n                # comment out to turn encoder off\n\n                # Assuming the encoder is already trained or train encoder jointly.\n                elif not frame.has_features():\n                    with torch.no_grad():\n                        crops = frame.get_crops()\n                        z = model.visual_encoder(crops)\n\n                        for i, z_i in enumerate(z):\n                            frame.instances[i].features = z_i\n\n        instances_pred = self.track(model, frames)\n        # no more persistent tracking. It is on by default\n        # if not self.persistent_tracking:\n        #     logger.debug(f\"Clearing Queue after tracking\")\n        #     self.track_queue.end_tracks()\n\n        return instances_pred\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get string representation of tracker.\n\n        Returns: the string representation of the tracker\n        \"\"\"\n        return (\n            \"Tracker(\"\n            f\"persistent_tracking={self.persistent_tracking}, \"\n            f\"max_tracks={self.max_tracks}, \"\n            f\"use_vis_feats={self.use_vis_feats}, \"\n            f\"overlap_thresh={self.overlap_thresh}, \"\n            f\"mult_thresh={self.mult_thresh}, \"\n            f\"decay_time={self.decay_time}, \"\n            f\"max_center_dist={self.max_center_dist}, \"\n            f\"verbose={self.verbose}, \"\n            f\"queue={self.track_queue}\"\n        )\n\n    def track(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; list[Frame]:\n        \"\"\"Perform sliding inference on the input video (instances) with a given window size. This method is called once per batch.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n        Returns:\n            frames: A list of Frames populated with pred_track_ids and asso_matrices\n        \"\"\"\n        # all batches up until context_length number of frames have been tracked, will be tracked frame-by-frame\n        if self.num_frames_tracked &lt; self.window_size:\n            frames = self.track_by_frame(model, frames)\n        else:\n            frames = self.track_by_batch(model, frames)\n\n        return frames\n\n    def track_by_batch(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; list[Frame]:\n        \"\"\"Perform sliding inference, on an entire batch of frames, on the input video (instances) with a given context length (window size).\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n        Returns:\n            frames: A list of Frames populated with pred_track_ids and asso_matrices\n        \"\"\"\n        # context window starts from last frame just before start of current batch, to window_size frames preceding it\n        # note; can't use last frame of previous batch, because there could be empty frames in between batches that must\n        # be part of the context window for consistency\n        context_window_frames = self.track_queue.collate_tracks(\n            context_start_frame_id=frames[0].frame_id.item()\n            - 1,  # switched off in collate_tracks; there is no cutoff for context, only until the deque gets filled\n            device=frames[0].frame_id.device,\n        )\n\n        context_window_instances = []\n        context_window_instance_frame_ids = []\n        for frame in context_window_frames:\n            context_window_instances.extend(frame.instances)\n            context_window_instance_frame_ids.extend(\n                [frame.frame_id] * len(frame.instances)\n            )\n\n        current_batch_instances = []\n        current_batch_instance_frame_ids = []\n        for frame in frames:\n            current_batch_instances.extend(frame.instances)\n            current_batch_instance_frame_ids.extend(\n                [frame.frame_id] * len(frame.instances)\n            )\n\n        # query is current batch instances, key is context window and current batch instances\n        association_matrix = model(\n            context_window_instances + current_batch_instances, current_batch_instances\n        )\n\n        # take association matrix and all frames off GPU (frames include instances)\n        association_matrix = association_matrix[-1].to(\"cpu\")\n        context_window_frames = [frame.to(\"cpu\") for frame in context_window_frames]\n        frames = [frame.to(\"cpu\") for frame in frames]\n\n        # keep current batch instances in assoc matrix, and remove them after softmax (mirrors the training scheme)\n        pred_frames = self._run_batch_tracker(\n            association_matrix.matrix,\n            context_window_frames,\n            frames,\n            compute_probs_by_frame=True,\n        )\n\n        return pred_frames\n\n    def track_by_frame(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; list[Frame]:\n        \"\"\"Perform sliding inference on the input video (instances) with a given window size.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n        Returns:\n            frames: A list of Frames populated with pred_track_ids and asso_matrices\n        \"\"\"\n        # B: batch size.\n        # D: embedding dimension.\n        # nc: number of channels.\n        # H: height.\n        # W: width.\n\n        for batch_idx, frame_to_track in enumerate(frames):\n            # if we're tracking by frame, it means context length of frames hasn't been reached yet, so context start frame id is 0\n            context_window_frames = self.track_queue.collate_tracks(\n                context_start_frame_id=0, device=frame_to_track.frame_id.device\n            )\n            logger.debug(f\"Current number of tracks is {self.track_queue.n_tracks}\")\n\n            if (\n                self.persistent_tracking and frame_to_track.frame_id == 0\n            ):  # check for new video and clear queue\n                logger.debug(\"New Video! Resetting Track Queue.\")\n                self.track_queue.end_tracks()\n\n            \"\"\"\n            Initialize tracks on first frame where detections appear. This is the first frame of the first batch\n            \"\"\"\n            if len(self.track_queue) == 0:\n                if frame_to_track.has_instances():\n                    logger.debug(\n                        f\"Initializing track on clip ind {batch_idx} frame {frame_to_track.frame_id.item()}\"\n                    )\n\n                    curr_track_id = 0\n                    for i, instance in enumerate(frames[batch_idx].instances):\n                        instance.pred_track_id = instance.gt_track_id\n                        curr_track_id = max(curr_track_id, instance.pred_track_id)\n\n                    for i, instance in enumerate(frames[batch_idx].instances):\n                        if instance.pred_track_id == -1:\n                            curr_track_id += 1\n                            instance.pred_track_id = curr_track_id\n\n            else:\n                if frame_to_track.has_instances():  # Check if there are detections. If there are skip and increment gap count\n                    frames_to_track = context_window_frames + [\n                        frame_to_track\n                    ]  # better var name?\n\n                    frame_to_track = self._run_frame_by_frame_tracker(\n                        model,\n                        frames_to_track,\n                    )\n\n            if frame_to_track.has_instances():\n                self.track_queue.add_frame(frame_to_track)\n                self.num_frames_tracked += 1\n            else:\n                self.track_queue.increment_gaps([])\n\n            frames[batch_idx] = frame_to_track\n\n        return frames\n\n    def _run_batch_tracker(\n        self,\n        association_matrix: torch.Tensor,\n        context_window_frames: list[Frame],\n        frames: list[Frame],\n        compute_probs_by_frame: bool = True,\n    ) -&gt; Frame:\n        \"\"\"Run batch tracker performs track assignment for each frame in the current batch.\n\n        Supports 2 methods for computing association probabilities. First is to softmax each query instance in each query frame in the batch, with only 1 frame at a time from the context window.\n        This is the default method and only supports local track linking.\n        Second is to softmax the entire context + curr batch, then index. This enables global track linking via e.g. ILP. In this case, prob values will be smaller and the overlap thresh should be decreased\n\n        Args:\n            association_matrix: the association matrix to be used for tracking\n            context_window_frames: list of frames in the context window\n            frames: list of frames in the current batch\n            compute_probs_by_frame: Whether to softmax the association matrix logits for each frame in context separately, or globally for the entire context window + current batch\n\n        Returns:\n            List of frames populated with pred_track_ids and asso_matrices\n        \"\"\"\n        tracked_frames = []\n        num_instances_per_frame = [\n            frame.num_detected for frame in context_window_frames + frames\n        ]\n        all_frames = context_window_frames + frames\n        batch_start_ind = len(num_instances_per_frame) - len(frames)\n        overlap_thresh = self.overlap_thresh\n        mult_thresh = self.mult_thresh\n        n_traj = self.track_queue.n_tracks\n        curr_track = self.track_queue.curr_track\n\n        for query_frame_idx, frame in enumerate(\n            frames\n        ):  # only track frames in current batch, not in context window\n            all_prev_instances = [\n                instance\n                for frame in context_window_frames + frames[:query_frame_idx]\n                for instance in frame.instances\n            ]\n            # indices that will be used to index the rows of the association matrix corresponding to the query frame instances\n            query_inds = [\n                x\n                for x in range(\n                    sum(\n                        num_instances_per_frame[\n                            batch_start_ind : batch_start_ind + query_frame_idx\n                        ]\n                    ),\n                    sum(\n                        num_instances_per_frame[\n                            batch_start_ind : batch_start_ind + query_frame_idx + 1\n                        ]\n                    ),\n                )\n            ]\n            # first, slice the association matrix to only include the query frame instances along the rows; these are the 'detections' to be matched to tracks\n            # recall incoming association_matrix is (num_instances_in_batch, num_instances_in_context_window + num_instances_in_batch)\n            assoc_curr_frame = association_matrix[query_inds, :]\n            # discard the columns (ref instances) corresponding to frames including and after the current frame; this means each frame will see previous frames in the batch as well as the context window when linking to tracks\n            # importantly, this means that tracks will be aggregated over a much longer time period than the context window size, making many more tracks visible to each frame to link detections to\n            assoc_curr_frame_by_previous_frames = assoc_curr_frame[\n                :, : sum(num_instances_per_frame[: batch_start_ind + query_frame_idx])\n            ]  # (num_query_instances, num instances in context window + num instances in current batch up till current frame)\n\n            # method 1\n            if compute_probs_by_frame:\n                # for each frame in the context window, split the assoc matrix columns by frame\n                split_assos = assoc_curr_frame_by_previous_frames.split(\n                    num_instances_per_frame[: batch_start_ind + query_frame_idx], dim=1\n                )\n                # compute softmax per-frame\n                softmaxed_asso = model_utils.softmax_asso(split_assos)\n                # merge the softmaxed assoc matrices back together to get (num_query_instances, num_instances_in_context_window)\n                softmaxed_asso = torch.cat(softmaxed_asso, dim=1)\n            # method 2\n            else:\n                # compute softmax across the entire context window and current batch frames\n                softmaxed_asso = model_utils.softmax_asso(\n                    assoc_curr_frame_by_previous_frames\n                )[0]\n\n            # proceed with post processing, LSA, and track assignment (duplicated code with frame by frame tracker)\n\n            # get raw bbox coords of prev frame instances from frame.instances_per_frame\n            prev_frame = all_frames[batch_start_ind + query_frame_idx - 1]\n            prev_frame_instance_ids = torch.cat(\n                [instance.pred_track_id for instance in prev_frame.instances], dim=0\n            )\n            prev_frame_boxes = torch.cat(\n                [instance.bbox for instance in prev_frame.instances], dim=0\n            )\n            all_prev_frames_boxes = torch.cat(\n                [instance.bbox for instance in all_prev_instances], dim=0\n            )\n            curr_frame_boxes = torch.cat(\n                [instance.bbox for instance in frame.instances], dim=0\n            )\n            # get the pred track ids for all instances up until the current frame\n            instance_ids = torch.cat(\n                [\n                    x.get_pred_track_ids()\n                    for x in all_frames[: batch_start_ind + query_frame_idx]\n                ],\n                dim=0,\n            ).view(\n                sum(num_instances_per_frame[: batch_start_ind + query_frame_idx])\n            )  # (n_nonquery,)\n\n            unique_ids = torch.unique(instance_ids)  # (n_nonquery,)\n\n            _, h, w = frame.img_shape.flatten()\n            bbox_scaler = torch.tensor([w, h, w, h])\n            query_boxes = curr_frame_boxes / bbox_scaler  # n_k x 4\n            nonquery_boxes = all_prev_frames_boxes / bbox_scaler  # n_nonquery x 4\n\n            logger.debug(f\"Instance IDs: {instance_ids}\")\n            logger.debug(f\"unique ids: {unique_ids}\")\n\n            id_inds = (\n                unique_ids[None, :] == instance_ids[:, None]\n            ).float()  # (n_nonquery, n_traj)\n\n            prev_frame_id_inds = (\n                unique_ids[None, :] == prev_frame_instance_ids[:, None]\n            ).float()  # (n_prev_frame_instances, n_traj)\n\n            # aggregate the association matrix by tracks; output is shape (num_query_instances, num_tracks)\n            # note the reduce operation is over the context window instances as well as current batch instances up until the current frame\n            # (n_query x n_nonquery) x (n_nonquery x n_traj) --&gt; n_query x n_traj\n            traj_score = torch.mm(softmaxed_asso, id_inds.cpu())  # (n_query, n_traj)\n\n            # post processing\n            if id_inds.numel() &gt; 0:\n                last_inds = (\n                    id_inds\n                    * torch.arange(len(all_prev_instances), device=id_inds.device)[\n                        :, None\n                    ]\n                ).max(dim=0)[1]  # M\n\n                last_boxes = nonquery_boxes[last_inds]  # n_traj x 4\n                last_ious = post_processing._pairwise_iou(\n                    Boxes(query_boxes), Boxes(last_boxes)\n                )  # n_k x M\n            else:\n                last_ious = traj_score.new_zeros(traj_score.shape)\n\n            traj_score = post_processing.weight_iou(traj_score, self.iou, last_ious)\n\n            # threshold for continuing a tracking or starting a new track -&gt; they use 1.0\n            traj_score = post_processing.filter_max_center_dist(\n                traj_score,\n                self.max_center_dist,\n                prev_frame_id_inds,\n                curr_frame_boxes,\n                prev_frame_boxes,\n            )\n\n            match_i, match_j = linear_sum_assignment((-traj_score))\n\n            track_ids = instance_ids.new_full((frame.num_detected,), -1)\n            for i, j in zip(match_i, match_j):\n                # The overlap threshold is multiplied by the number of times the unique track j is matched to an\n                # instance out of all instances in the window excluding the current frame.\n                #\n                # So if this is correct, the threshold is higher for matching an instance from the current frame\n                # to an existing track if that track has already been matched several times.\n                # So if an existing track in the window has been matched a lot, it gets harder to match to that track.\n                thresh = (\n                    overlap_thresh * id_inds[:, j].sum()\n                    if mult_thresh\n                    else overlap_thresh\n                )\n                if n_traj &gt;= self.max_tracks or traj_score[i, j] &gt; thresh:\n                    logger.debug(\n                        f\"Assigning instance {i} to track {j} with id {unique_ids[j]}\"\n                    )\n                    track_ids[i] = unique_ids[j]\n                    frame.instances[i].track_score = traj_score[i, j].item()\n            logger.debug(f\"track_ids: {track_ids}\")\n            for i in range(frame.num_detected):\n                if track_ids[i] &lt; 0:\n                    logger.debug(f\"Creating new track {curr_track}\")\n                    curr_track += 1\n                    track_ids[i] = curr_track\n\n            frame.matches = (match_i, match_j)\n\n            for instance, track_id in zip(frame.instances, track_ids):\n                instance.pred_track_id = track_id\n\n            tracked_frames.append(frame)\n\n            if frame.has_instances():\n                self.track_queue.add_frame(frame)\n                self.num_frames_tracked += 1\n            else:\n                self.track_queue.increment_gaps([])\n\n        return tracked_frames\n\n    def _run_frame_by_frame_tracker(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; Frame:\n        \"\"\"Run global tracker performs the actual tracking.\n\n        Uses Hungarian algorithm to do track assigning.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: A list of Frames containing reid features. See `dreem.io.data_structures` for more info.\n\n        Returns:\n            query_frame: The query frame now populated with the pred_track_ids.\n        \"\"\"\n        # *: each item in frames is a frame in the window. So it follows\n        #    that each frame in the window has * detected instances.\n        # D: embedding dimension.\n        # total_instances: number of instances in the window.\n        # N_i: number of detected instances in i-th frame of window.\n        # instances_per_frame: a list of number of instances in each frame of the window.\n        # n_query: number of instances in current/query frame (rightmost frame of the window).\n        # n_nonquery: number of instances in the window excluding the current/query frame.\n        # window_size: length of window.\n        # L: number of decoder blocks.\n        # n_traj: number of existing tracks within the window so far.\n\n        # Number of instances in each frame of the window.\n        # E.g.: instances_per_frame: [4, 5, 6, 7]; window of length 4 with 4 detected instances in the first frame of the window.\n\n        _ = model.eval()\n\n        query_ind = len(frames) - 1\n        query_frame = frames[query_ind]\n\n        query_instances = query_frame.instances\n        all_instances = [instance for frame in frames for instance in frame.instances]\n\n        logger.debug(f\"Frame {query_frame.frame_id.item()}\")\n\n        instances_per_frame = [frame.num_detected for frame in frames]\n\n        total_instances, window_size = (\n            sum(instances_per_frame),\n            len(instances_per_frame),\n        )  # Number of instances in window; length of window.\n\n        logger.debug(f\"total_instances: {total_instances}\")\n\n        overlap_thresh = self.overlap_thresh\n        mult_thresh = self.mult_thresh\n        n_traj = self.track_queue.n_tracks\n        curr_track = self.track_queue.curr_track\n\n        reid_features = torch.cat([frame.get_features() for frame in frames], dim=0)[\n            None\n        ]  # (1, total_instances, D=512)\n\n        # (L=1, n_query, total_instances)\n        with torch.no_grad():\n            asso_matrix = model(all_instances, query_instances)\n\n        asso_output = asso_matrix[-1].matrix.split(\n            instances_per_frame, dim=1\n        )  # (window_size, n_query, N_i)\n        asso_output = model_utils.softmax_asso(\n            asso_output\n        )  # (window_size, n_query, N_i)\n        asso_output = torch.cat(asso_output, dim=1).cpu()  # (n_query, total_instances)\n\n        asso_output_df = pd.DataFrame(\n            asso_output.clone().numpy(),\n            columns=[f\"Instance {i}\" for i in range(asso_output.shape[-1])],\n        )\n\n        asso_output_df.index.name = \"Instances\"\n        asso_output_df.columns.name = \"Instances\"\n\n        query_frame.add_traj_score(\"asso_output\", asso_output_df)\n        query_frame.asso_output = asso_matrix[-1]\n\n        n_query = (\n            query_frame.num_detected\n        )  # Number of instances in the current/query frame.\n\n        n_nonquery = (\n            total_instances - n_query\n        )  # Number of instances in the window not including the current/query frame.\n\n        logger.debug(f\"n_nonquery: {n_nonquery}\")\n        logger.debug(f\"n_query: {n_query}\")\n\n        instance_ids = torch.cat(\n            [\n                x.get_pred_track_ids()\n                for batch_idx, x in enumerate(frames)\n                if batch_idx != query_ind\n            ],\n            dim=0,\n        ).view(n_nonquery)  # (n_nonquery,)\n\n        query_inds = [\n            x\n            for x in range(\n                sum(instances_per_frame[:query_ind]),\n                sum(instances_per_frame[: query_ind + 1]),\n            )\n        ]\n\n        nonquery_inds = [i for i in range(total_instances) if i not in query_inds]\n\n        # instead should we do model(nonquery_instances, query_instances)?\n        asso_nonquery = asso_output[:, nonquery_inds]  # (n_query, n_nonquery)\n\n        asso_nonquery_df = pd.DataFrame(\n            asso_nonquery.clone().numpy(), columns=nonquery_inds\n        )\n\n        asso_nonquery_df.index.name = \"Current Frame Instances\"\n        asso_nonquery_df.columns.name = \"Nonquery Instances\"\n\n        query_frame.add_traj_score(\"asso_nonquery\", asso_nonquery_df)\n\n        # get raw bbox coords of prev frame instances from frame.instances_per_frame\n        query_boxes_px = torch.cat(\n            [instance.bbox for instance in query_frame.instances], dim=0\n        )\n        nonquery_boxes_px = torch.cat(\n            [\n                instance.bbox\n                for nonquery_frame in frames\n                if nonquery_frame.frame_id != query_frame.frame_id\n                for instance in nonquery_frame.instances\n            ],\n            dim=0,\n        )\n\n        pred_boxes = model_utils.get_boxes(all_instances)\n        query_boxes = pred_boxes[query_inds]  # n_k x 4\n        nonquery_boxes = pred_boxes[nonquery_inds]  # n_nonquery x 4\n\n        unique_ids = torch.unique(instance_ids)  # (n_nonquery,)\n\n        logger.debug(f\"Instance IDs: {instance_ids}\")\n        logger.debug(f\"unique ids: {unique_ids}\")\n\n        id_inds = (\n            unique_ids[None, :] == instance_ids[:, None]\n        ).float()  # (n_nonquery, n_traj)\n\n        ################################################################################\n\n        # reweighting hyper-parameters for association -&gt; they use 0.9\n\n        traj_score = post_processing.weight_decay_time(\n            asso_nonquery, self.decay_time, reid_features, window_size, query_ind\n        )\n\n        if self.decay_time is not None and self.decay_time &gt; 0:\n            decay_time_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=nonquery_inds\n            )\n\n            decay_time_traj_score.index.name = \"Query Instances\"\n            decay_time_traj_score.columns.name = \"Nonquery Instances\"\n\n            query_frame.add_traj_score(\"decay_time\", decay_time_traj_score)\n        ################################################################################\n\n        # (n_query x n_nonquery) x (n_nonquery x n_traj) --&gt; n_query x n_traj\n        traj_score = torch.mm(traj_score, id_inds.cpu())  # (n_query, n_traj)\n\n        traj_score_df = pd.DataFrame(\n            traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n        )\n\n        traj_score_df.index.name = \"Current Frame Instances\"\n        traj_score_df.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"traj_score\", traj_score_df)\n        ################################################################################\n\n        # with iou -&gt; combining with location in tracker, they set to True\n        # todo -&gt; should also work without pos_embed\n\n        if id_inds.numel() &gt; 0:\n            # this throws error, think we need to slice?\n            # last_inds = (id_inds * torch.arange(\n            #    n_nonquery, device=id_inds.device)[:, None]).max(dim=0)[1] # n_traj\n\n            last_inds = (\n                id_inds * torch.arange(n_nonquery, device=id_inds.device)[:, None]\n            ).max(dim=0)[1]  # M\n\n            last_boxes = nonquery_boxes[last_inds]  # n_traj x 4\n            last_ious = post_processing._pairwise_iou(\n                Boxes(query_boxes), Boxes(last_boxes)\n            )  # n_k x M\n        else:\n            last_ious = traj_score.new_zeros(traj_score.shape)\n\n        traj_score = post_processing.weight_iou(traj_score, self.iou, last_ious.cpu())\n\n        if self.iou is not None and self.iou != \"\":\n            iou_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n            )\n\n            iou_traj_score.index.name = \"Current Frame Instances\"\n            iou_traj_score.columns.name = \"Unique IDs\"\n\n            query_frame.add_traj_score(\"weight_iou\", iou_traj_score)\n        ################################################################################\n\n        # threshold for continuing a tracking or starting a new track -&gt; they use 1.0\n        # todo -&gt; should also work without pos_embed\n        traj_score = post_processing.filter_max_center_dist(\n            traj_score,\n            self.max_center_dist,\n            id_inds,\n            query_boxes_px,\n            nonquery_boxes_px,\n        )\n\n        if self.max_center_dist is not None and self.max_center_dist &gt; 0:\n            max_center_dist_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n            )\n\n            max_center_dist_traj_score.index.name = \"Current Frame Instances\"\n            max_center_dist_traj_score.columns.name = \"Unique IDs\"\n\n            query_frame.add_traj_score(\"max_center_dist\", max_center_dist_traj_score)\n\n        ################################################################################\n        scaled_traj_score = torch.softmax(traj_score, dim=1)\n        scaled_traj_score_df = pd.DataFrame(\n            scaled_traj_score.numpy(), columns=unique_ids.cpu().numpy()\n        )\n        scaled_traj_score_df.index.name = \"Current Frame Instances\"\n        scaled_traj_score_df.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"scaled\", scaled_traj_score_df)\n        ################################################################################\n\n        match_i, match_j = linear_sum_assignment((-traj_score))\n\n        track_ids = instance_ids.new_full((n_query,), -1)\n        for i, j in zip(match_i, match_j):\n            # The overlap threshold is multiplied by the number of times the unique track j is matched to an\n            # instance out of all instances in the window excluding the current frame.\n            #\n            # So if this is correct, the threshold is higher for matching an instance from the current frame\n            # to an existing track if that track has already been matched several times.\n            # So if an existing track in the window has been matched a lot, it gets harder to match to that track.\n            thresh = (\n                overlap_thresh * id_inds[:, j].sum() if mult_thresh else overlap_thresh\n            )\n            if n_traj &gt;= self.max_tracks or traj_score[i, j] &gt; thresh:\n                logger.debug(\n                    f\"Assigning instance {i} to track {j} with id {unique_ids[j]}\"\n                )\n                track_ids[i] = unique_ids[j]\n                query_frame.instances[i].track_score = scaled_traj_score[i, j].item()\n        logger.debug(f\"track_ids: {track_ids}\")\n        for i in range(n_query):\n            if track_ids[i] &lt; 0:\n                logger.debug(f\"Creating new track {curr_track}\")\n                curr_track += 1\n                track_ids[i] = curr_track\n\n        query_frame.matches = (match_i, match_j)\n\n        for instance, track_id in zip(query_frame.instances, track_ids):\n            instance.pred_track_id = track_id\n\n        final_traj_score = pd.DataFrame(\n            traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n        )\n        final_traj_score.index.name = \"Current Frame Instances\"\n        final_traj_score.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"final\", final_traj_score)\n        return query_frame\n</code></pre>"},{"location":"reference/dreem/inference/batch_tracker/#dreem.inference.batch_tracker.BatchTracker.__call__","title":"<code>__call__(model, frames)</code>","text":"<p>Wrap around <code>track</code> to enable <code>tracker()</code> instead of <code>tracker.track()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[Frame]</code> <p>list of Frames to run inference on</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>List of frames containing association matrix scores and instances populated with pred track ids.</p> Source code in <code>dreem/inference/batch_tracker.py</code> <pre><code>def __call__(\n    self, model: GlobalTrackingTransformer, frames: list[Frame]\n) -&gt; list[Frame]:\n    \"\"\"Wrap around `track` to enable `tracker()` instead of `tracker.track()`.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: list of Frames to run inference on\n\n    Returns:\n        List of frames containing association matrix scores and instances populated with pred track ids.\n    \"\"\"\n    _ = model.eval()\n\n    for frame in frames:\n        if frame.has_instances():\n            if not self.use_vis_feats:\n                for instance in frame.instances:\n                    instance.features = torch.zeros(1, model.d_model)\n                # frame[\"features\"] = torch.randn(\n                #     num_frame_instances, self.model.d_model\n                # )\n\n            # comment out to turn encoder off\n\n            # Assuming the encoder is already trained or train encoder jointly.\n            elif not frame.has_features():\n                with torch.no_grad():\n                    crops = frame.get_crops()\n                    z = model.visual_encoder(crops)\n\n                    for i, z_i in enumerate(z):\n                        frame.instances[i].features = z_i\n\n    instances_pred = self.track(model, frames)\n    # no more persistent tracking. It is on by default\n    # if not self.persistent_tracking:\n    #     logger.debug(f\"Clearing Queue after tracking\")\n    #     self.track_queue.end_tracks()\n\n    return instances_pred\n</code></pre>"},{"location":"reference/dreem/inference/batch_tracker/#dreem.inference.batch_tracker.BatchTracker.__init__","title":"<code>__init__(window_size=8, use_vis_feats=True, overlap_thresh=0.01, mult_thresh=True, decay_time=None, iou=None, max_center_dist=None, persistent_tracking=True, max_gap=inf, max_tracks=inf, verbose=False, **kwargs)</code>","text":"<p>Initialize a tracker to run inference.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>the size of the window used during sliding inference.</p> <code>8</code> <code>use_vis_feats</code> <code>bool</code> <p>Whether or not to use visual feature extractor.</p> <code>True</code> <code>overlap_thresh</code> <code>float</code> <p>the trajectory overlap threshold to be used for assignment.</p> <code>0.01</code> <code>mult_thresh</code> <code>bool</code> <p>Whether or not to use weight threshold.</p> <code>True</code> <code>decay_time</code> <code>float | None</code> <p>weight for <code>decay_time</code> postprocessing.</p> <code>None</code> <code>iou</code> <code>str | None</code> <p>Either [None, '', \"mult\" or \"max\"]  Whether to use multiplicative or max iou reweighting.</p> <code>None</code> <code>max_center_dist</code> <code>float | None</code> <p>distance threshold for filtering trajectory score matrix.</p> <code>None</code> <code>persistent_tracking</code> <code>bool</code> <p>whether to keep a buffer across chunks or not.</p> <code>True</code> <code>max_gap</code> <code>int</code> <p>the max number of frames a trajectory can be missing before termination.</p> <code>inf</code> <code>max_tracks</code> <code>int</code> <p>the maximum number of tracks that can be created while tracking. We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.</p> <code>inf</code> <code>verbose</code> <code>bool</code> <p>Whether or not to turn on debug printing after each operation.</p> <code>False</code> Source code in <code>dreem/inference/batch_tracker.py</code> <pre><code>def __init__(\n    self,\n    window_size: int = 8,\n    use_vis_feats: bool = True,\n    overlap_thresh: float = 0.01,\n    mult_thresh: bool = True,\n    decay_time: float | None = None,\n    iou: str | None = None,\n    max_center_dist: float | None = None,\n    persistent_tracking: bool = True,\n    max_gap: int = inf,\n    max_tracks: int = inf,\n    verbose: bool = False,\n    **kwargs,\n):\n    \"\"\"Initialize a tracker to run inference.\n\n    Args:\n        window_size: the size of the window used during sliding inference.\n        use_vis_feats: Whether or not to use visual feature extractor.\n        overlap_thresh: the trajectory overlap threshold to be used for assignment.\n        mult_thresh: Whether or not to use weight threshold.\n        decay_time: weight for `decay_time` postprocessing.\n        iou: Either [None, '', \"mult\" or \"max\"]\n             Whether to use multiplicative or max iou reweighting.\n        max_center_dist: distance threshold for filtering trajectory score matrix.\n        persistent_tracking: whether to keep a buffer across chunks or not.\n        max_gap: the max number of frames a trajectory can be missing before termination.\n        max_tracks: the maximum number of tracks that can be created while tracking.\n            We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.\n        verbose: Whether or not to turn on debug printing after each operation.\n    \"\"\"\n    self.track_queue = TrackQueue(\n        window_size=window_size, max_gap=max_gap, verbose=verbose\n    )\n    self.num_frames_tracked = 0\n    self.window_size = window_size\n    self.use_vis_feats = use_vis_feats\n    self.overlap_thresh = overlap_thresh\n    self.mult_thresh = mult_thresh\n    self.decay_time = decay_time\n    self.iou = iou\n    self.max_center_dist = max_center_dist\n    self.persistent_tracking = persistent_tracking\n    self.verbose = verbose\n    self.max_tracks = max_tracks\n</code></pre>"},{"location":"reference/dreem/inference/batch_tracker/#dreem.inference.batch_tracker.BatchTracker.__repr__","title":"<code>__repr__()</code>","text":"<p>Get string representation of tracker.</p> <p>Returns: the string representation of the tracker</p> Source code in <code>dreem/inference/batch_tracker.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get string representation of tracker.\n\n    Returns: the string representation of the tracker\n    \"\"\"\n    return (\n        \"Tracker(\"\n        f\"persistent_tracking={self.persistent_tracking}, \"\n        f\"max_tracks={self.max_tracks}, \"\n        f\"use_vis_feats={self.use_vis_feats}, \"\n        f\"overlap_thresh={self.overlap_thresh}, \"\n        f\"mult_thresh={self.mult_thresh}, \"\n        f\"decay_time={self.decay_time}, \"\n        f\"max_center_dist={self.max_center_dist}, \"\n        f\"verbose={self.verbose}, \"\n        f\"queue={self.track_queue}\"\n    )\n</code></pre>"},{"location":"reference/dreem/inference/batch_tracker/#dreem.inference.batch_tracker.BatchTracker.track","title":"<code>track(model, frames)</code>","text":"<p>Perform sliding inference on the input video (instances) with a given window size. This method is called once per batch.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[Frame]</code> <p>A list of Frames (See <code>dreem.io.Frame</code> for more info).</p> required <p>Returns:</p> Name Type Description <code>frames</code> <code>list[Frame]</code> <p>A list of Frames populated with pred_track_ids and asso_matrices</p> Source code in <code>dreem/inference/batch_tracker.py</code> <pre><code>def track(\n    self, model: GlobalTrackingTransformer, frames: list[Frame]\n) -&gt; list[Frame]:\n    \"\"\"Perform sliding inference on the input video (instances) with a given window size. This method is called once per batch.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n    Returns:\n        frames: A list of Frames populated with pred_track_ids and asso_matrices\n    \"\"\"\n    # all batches up until context_length number of frames have been tracked, will be tracked frame-by-frame\n    if self.num_frames_tracked &lt; self.window_size:\n        frames = self.track_by_frame(model, frames)\n    else:\n        frames = self.track_by_batch(model, frames)\n\n    return frames\n</code></pre>"},{"location":"reference/dreem/inference/batch_tracker/#dreem.inference.batch_tracker.BatchTracker.track_by_batch","title":"<code>track_by_batch(model, frames)</code>","text":"<p>Perform sliding inference, on an entire batch of frames, on the input video (instances) with a given context length (window size).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[Frame]</code> <p>A list of Frames (See <code>dreem.io.Frame</code> for more info).</p> required <p>Returns:</p> Name Type Description <code>frames</code> <code>list[Frame]</code> <p>A list of Frames populated with pred_track_ids and asso_matrices</p> Source code in <code>dreem/inference/batch_tracker.py</code> <pre><code>def track_by_batch(\n    self, model: GlobalTrackingTransformer, frames: list[Frame]\n) -&gt; list[Frame]:\n    \"\"\"Perform sliding inference, on an entire batch of frames, on the input video (instances) with a given context length (window size).\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n    Returns:\n        frames: A list of Frames populated with pred_track_ids and asso_matrices\n    \"\"\"\n    # context window starts from last frame just before start of current batch, to window_size frames preceding it\n    # note; can't use last frame of previous batch, because there could be empty frames in between batches that must\n    # be part of the context window for consistency\n    context_window_frames = self.track_queue.collate_tracks(\n        context_start_frame_id=frames[0].frame_id.item()\n        - 1,  # switched off in collate_tracks; there is no cutoff for context, only until the deque gets filled\n        device=frames[0].frame_id.device,\n    )\n\n    context_window_instances = []\n    context_window_instance_frame_ids = []\n    for frame in context_window_frames:\n        context_window_instances.extend(frame.instances)\n        context_window_instance_frame_ids.extend(\n            [frame.frame_id] * len(frame.instances)\n        )\n\n    current_batch_instances = []\n    current_batch_instance_frame_ids = []\n    for frame in frames:\n        current_batch_instances.extend(frame.instances)\n        current_batch_instance_frame_ids.extend(\n            [frame.frame_id] * len(frame.instances)\n        )\n\n    # query is current batch instances, key is context window and current batch instances\n    association_matrix = model(\n        context_window_instances + current_batch_instances, current_batch_instances\n    )\n\n    # take association matrix and all frames off GPU (frames include instances)\n    association_matrix = association_matrix[-1].to(\"cpu\")\n    context_window_frames = [frame.to(\"cpu\") for frame in context_window_frames]\n    frames = [frame.to(\"cpu\") for frame in frames]\n\n    # keep current batch instances in assoc matrix, and remove them after softmax (mirrors the training scheme)\n    pred_frames = self._run_batch_tracker(\n        association_matrix.matrix,\n        context_window_frames,\n        frames,\n        compute_probs_by_frame=True,\n    )\n\n    return pred_frames\n</code></pre>"},{"location":"reference/dreem/inference/batch_tracker/#dreem.inference.batch_tracker.BatchTracker.track_by_frame","title":"<code>track_by_frame(model, frames)</code>","text":"<p>Perform sliding inference on the input video (instances) with a given window size.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[Frame]</code> <p>A list of Frames (See <code>dreem.io.Frame</code> for more info).</p> required <p>Returns:</p> Name Type Description <code>frames</code> <code>list[Frame]</code> <p>A list of Frames populated with pred_track_ids and asso_matrices</p> Source code in <code>dreem/inference/batch_tracker.py</code> <pre><code>def track_by_frame(\n    self, model: GlobalTrackingTransformer, frames: list[Frame]\n) -&gt; list[Frame]:\n    \"\"\"Perform sliding inference on the input video (instances) with a given window size.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n    Returns:\n        frames: A list of Frames populated with pred_track_ids and asso_matrices\n    \"\"\"\n    # B: batch size.\n    # D: embedding dimension.\n    # nc: number of channels.\n    # H: height.\n    # W: width.\n\n    for batch_idx, frame_to_track in enumerate(frames):\n        # if we're tracking by frame, it means context length of frames hasn't been reached yet, so context start frame id is 0\n        context_window_frames = self.track_queue.collate_tracks(\n            context_start_frame_id=0, device=frame_to_track.frame_id.device\n        )\n        logger.debug(f\"Current number of tracks is {self.track_queue.n_tracks}\")\n\n        if (\n            self.persistent_tracking and frame_to_track.frame_id == 0\n        ):  # check for new video and clear queue\n            logger.debug(\"New Video! Resetting Track Queue.\")\n            self.track_queue.end_tracks()\n\n        \"\"\"\n        Initialize tracks on first frame where detections appear. This is the first frame of the first batch\n        \"\"\"\n        if len(self.track_queue) == 0:\n            if frame_to_track.has_instances():\n                logger.debug(\n                    f\"Initializing track on clip ind {batch_idx} frame {frame_to_track.frame_id.item()}\"\n                )\n\n                curr_track_id = 0\n                for i, instance in enumerate(frames[batch_idx].instances):\n                    instance.pred_track_id = instance.gt_track_id\n                    curr_track_id = max(curr_track_id, instance.pred_track_id)\n\n                for i, instance in enumerate(frames[batch_idx].instances):\n                    if instance.pred_track_id == -1:\n                        curr_track_id += 1\n                        instance.pred_track_id = curr_track_id\n\n        else:\n            if frame_to_track.has_instances():  # Check if there are detections. If there are skip and increment gap count\n                frames_to_track = context_window_frames + [\n                    frame_to_track\n                ]  # better var name?\n\n                frame_to_track = self._run_frame_by_frame_tracker(\n                    model,\n                    frames_to_track,\n                )\n\n        if frame_to_track.has_instances():\n            self.track_queue.add_frame(frame_to_track)\n            self.num_frames_tracked += 1\n        else:\n            self.track_queue.increment_gaps([])\n\n        frames[batch_idx] = frame_to_track\n\n    return frames\n</code></pre>"},{"location":"reference/dreem/inference/boxes/","title":"boxes","text":""},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes","title":"<code>dreem.inference.boxes</code>","text":"<p>Module containing Boxes class.</p> <p>Classes:</p> Name Description <code>Boxes</code> <p>Adapted from https://github.com/facebookresearch/detectron2/blob/main/detectron2/structures/boxes.py.</p>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes","title":"<code>Boxes</code>","text":"<p>Adapted from https://github.com/facebookresearch/detectron2/blob/main/detectron2/structures/boxes.py.</p> <p>This structure stores a list of boxes as a Nx4 torch.Tensor. It supports some common methods about boxes (<code>area</code>, <code>clip</code>, <code>nonempty</code>, etc), and also behaves like a Tensor (support indexing, <code>to(device)</code>, <code>.device</code>, and iteration over all boxes)</p> <p>Attributes:</p> Name Type Description <code>tensor</code> <code>Tensor</code> <p>float matrix of Nx4. Each row is (x1, y1, x2, y2).</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Getter for boxes.</p> <code>__init__</code> <p>Initialize Boxes.</p> <code>__iter__</code> <p>Yield a box as a Tensor of shape (4,) at a time.</p> <code>__len__</code> <p>Get the number of boxes stored in this object.</p> <code>__repr__</code> <p>Override representation for printing.</p> <code>area</code> <p>Compute the area of all the boxes.</p> <code>cat</code> <p>Concatenates a list of Boxes into a single Boxes.</p> <code>clip</code> <p>Clip (in place) the boxes.</p> <code>clone</code> <p>Clone the Boxes.</p> <code>get_centers</code> <p>Get the centroid of the bbox.</p> <code>inside_box</code> <p>Check if box is inside reference box.</p> <code>nonempty</code> <p>Find boxes that are non-empty.</p> <code>scale</code> <p>Scale the box with horizontal and vertical scaling factors.</p> <code>to</code> <p>Load boxes to gpu/cpu.</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>class Boxes:\n    \"\"\"Adapted from https://github.com/facebookresearch/detectron2/blob/main/detectron2/structures/boxes.py.\n\n    This structure stores a list of boxes as a Nx4 torch.Tensor.\n    It supports some common methods about boxes\n    (`area`, `clip`, `nonempty`, etc),\n    and also behaves like a Tensor\n    (support indexing, `to(device)`, `.device`, and iteration over all boxes)\n\n    Attributes:\n        tensor (torch.Tensor): float matrix of Nx4. Each row is (x1, y1, x2, y2).\n    \"\"\"\n\n    def __init__(self, tensor: torch.Tensor):\n        \"\"\"Initialize Boxes.\n\n        Args:\n            tensor (Tensor[float]): a Nx4 matrix.  Each row is (x1, y1, x2, y2).\n        \"\"\"\n        if not isinstance(tensor, torch.Tensor):\n            tensor = torch.as_tensor(\n                tensor, dtype=torch.float32, device=torch.device(\"cpu\")\n            )\n        else:\n            tensor = tensor.to(torch.float32)\n        if tensor.numel() == 0:\n            # Use reshape, so we don't end up creating a new tensor that does not depend on\n            # the inputs (and consequently confuses jit)\n            tensor = tensor.reshape((-1, 4)).to(dtype=torch.float32)\n        assert tensor.dim() == 3 and tensor.size(-1) == 4, tensor.size()\n\n        self.tensor = tensor\n\n    def clone(self) -&gt; Self:\n        \"\"\"Clone the Boxes.\n\n        Returns:\n            Boxes\n        \"\"\"\n        return Boxes(self.tensor.clone())\n\n    def to(self, device: torch.device) -&gt; Self:\n        \"\"\"Load boxes to gpu/cpu.\n\n        Args:\n            device: The device to load the boxes to\n\n        Returns: Boxes on device.\n        \"\"\"\n        # Boxes are assumed float32 and does not support to(dtype)\n        return Boxes(self.tensor.to(device=device))\n\n    def area(self) -&gt; torch.Tensor:\n        \"\"\"Compute the area of all the boxes.\n\n        Returns:\n            torch.Tensor: a vector with areas of each box.\n        \"\"\"\n        box = self.tensor\n        area = (box[:, :, 2] - box[:, :, 0]) * (box[:, :, 3] - box[:, :, 1])\n        return area\n\n    def clip(self, box_size: list[int, int]) -&gt; None:\n        \"\"\"Clip (in place) the boxes.\n\n        Limits x coordinates to the range [0, width]\n        and y coordinates to the range [0, height].\n\n        Args:\n            box_size (height, width): The clipping box's size.\n        \"\"\"\n        assert torch.isfinite(self.tensor).all(), \"Box tensor contains infinite or NaN!\"\n        h, w = box_size\n        x1 = self.tensor[:, :, 0].clamp(min=0, max=w)\n        y1 = self.tensor[:, :, 1].clamp(min=0, max=h)\n        x2 = self.tensor[:, :, 2].clamp(min=0, max=w)\n        y2 = self.tensor[:, :, 3].clamp(min=0, max=h)\n        self.tensor = torch.stack((x1, y1, x2, y2), dim=-1)\n\n    def nonempty(self, threshold: float = 0.0) -&gt; torch.Tensor:\n        \"\"\"Find boxes that are non-empty.\n\n        A box is considered empty, if either of its side is no larger than threshold.\n\n        Args:\n            threshold: the smallest a box can be.\n\n        Returns:\n            Tensor:\n                a binary vector which represents whether each box is empty\n                (False) or non-empty (True).\n        \"\"\"\n        box = self.tensor\n        widths = box[:, :, 2] - box[:, :, 0]\n        heights = box[:, :, 3] - box[:, :, 1]\n        keep = (widths &gt; threshold) &amp; (heights &gt; threshold)\n        return keep\n\n    def __getitem__(self, item: int | slice | torch.BoolTensor) -&gt; \"Boxes\":\n        \"\"\"Getter for boxes.\n\n        Args:\n            item: int, slice, or a BoolTensor\n\n        Returns:\n            Boxes: Create a new :class:`Boxes` by indexing.\n\n        Usage:\n            The following usage are allowed:\n            1. `new_boxes = boxes[3]`: return a `Boxes` which contains only one box.\n            2. `new_boxes = boxes[2:10]`: return a slice of boxes.\n            3. `new_boxes = boxes[vector]`, where vector is a torch.BoolTensor\n            with `length = len(boxes)`. Nonzero elements in the vector will be selected.\n\n        NOTE: that the returned Boxes might share storage with this Boxes,\n        subject to Pytorch's indexing semantics.\n        \"\"\"\n        if isinstance(item, int):\n            return Boxes(self.tensor[item])\n        b = self.tensor[item]\n        assert b.dim() == 3, (\n            \"Indexing on Boxes with {} failed to return a matrix!\".format(item)\n        )\n        return Boxes(b)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the number of boxes stored in this object.\n\n        Returns:\n            the number of boxes stored in this object\n        \"\"\"\n        return self.tensor.shape[0]\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Override representation for printing.\n\n        Returns:\n            'Boxes(tensor)'\n        \"\"\"\n        return \"Boxes(\" + str(self.tensor) + \")\"\n\n    def inside_box(\n        self, box_size: tuple[int, int], boundary_threshold: int = 0\n    ) -&gt; torch.Tensor:\n        \"\"\"Check if box is inside reference box.\n\n        Args:\n            box_size (height, width): Size of the reference box.\n            boundary_threshold (int): Boxes that extend beyond the reference box\n                boundary by more than boundary_threshold are considered \"outside\".\n\n        Returns:\n            a binary vector, indicating whether each box is inside the reference box.\n        \"\"\"\n        height, width = box_size\n        inds_inside = (\n            (self.tensor[..., 0] &gt;= -boundary_threshold)\n            &amp; (self.tensor[..., 1] &gt;= -boundary_threshold)\n            &amp; (self.tensor[..., 2] &lt; width + boundary_threshold)\n            &amp; (self.tensor[..., 3] &lt; height + boundary_threshold)\n        )\n        return inds_inside\n\n    def get_centers(self) -&gt; torch.Tensor:\n        \"\"\"Get the centroid of the bbox.\n\n        Returns:\n            The box centers in a Nx2 array of (x, y).\n        \"\"\"\n        return (self.tensor[:, :, :2] + self.tensor[:, :, 2:]) / 2\n\n    def scale(self, scale_x: float, scale_y: float) -&gt; None:\n        \"\"\"Scale the box with horizontal and vertical scaling factors.\"\"\"\n        self.tensor[:, :, 0::2] *= scale_x\n        self.tensor[:, :, 1::2] *= scale_y\n\n    @classmethod\n    def cat(cls, boxes_list: list[\"Boxes\"]) -&gt; \"Boxes\":\n        \"\"\"Concatenates a list of Boxes into a single Boxes.\n\n        Arguments:\n            boxes_list: list of `Boxes`\n\n        Returns:\n            Boxes: the concatenated Boxes\n        \"\"\"\n        assert isinstance(boxes_list, (list, tuple))\n        if len(boxes_list) == 0:\n            return cls(torch.empty(0))\n        assert all([isinstance(box, Boxes) for box in boxes_list])\n\n        # use torch.cat (v.s. layers.cat) so the returned boxes never share storage with input\n        cat_boxes = cls(torch.cat([b.tensor for b in boxes_list], dim=0))\n        return cat_boxes\n\n    @property\n    def device(self) -&gt; torch.device:\n        \"\"\"Get the device the box is on.\n\n        Returns: the device the box is on\n        \"\"\"\n        return self.tensor.device\n\n    # type \"Iterator[torch.Tensor]\", yield, and iter() not supported by torchscript\n    # https://github.com/pytorch/pytorch/issues/18627\n    @torch.jit.unused\n    def __iter__(self):\n        \"\"\"Yield a box as a Tensor of shape (4,) at a time.\"\"\"\n        yield from self.tensor\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.device","title":"<code>device</code>  <code>property</code>","text":"<p>Get the device the box is on.</p> <p>Returns: the device the box is on</p>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.__getitem__","title":"<code>__getitem__(item)</code>","text":"<p>Getter for boxes.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>int | slice | BoolTensor</code> <p>int, slice, or a BoolTensor</p> required <p>Returns:</p> Name Type Description <code>Boxes</code> <code>Boxes</code> <p>Create a new :class:<code>Boxes</code> by indexing.</p> Usage <p>The following usage are allowed: 1. <code>new_boxes = boxes[3]</code>: return a <code>Boxes</code> which contains only one box. 2. <code>new_boxes = boxes[2:10]</code>: return a slice of boxes. 3. <code>new_boxes = boxes[vector]</code>, where vector is a torch.BoolTensor with <code>length = len(boxes)</code>. Nonzero elements in the vector will be selected.</p> <p>NOTE: that the returned Boxes might share storage with this Boxes, subject to Pytorch's indexing semantics.</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def __getitem__(self, item: int | slice | torch.BoolTensor) -&gt; \"Boxes\":\n    \"\"\"Getter for boxes.\n\n    Args:\n        item: int, slice, or a BoolTensor\n\n    Returns:\n        Boxes: Create a new :class:`Boxes` by indexing.\n\n    Usage:\n        The following usage are allowed:\n        1. `new_boxes = boxes[3]`: return a `Boxes` which contains only one box.\n        2. `new_boxes = boxes[2:10]`: return a slice of boxes.\n        3. `new_boxes = boxes[vector]`, where vector is a torch.BoolTensor\n        with `length = len(boxes)`. Nonzero elements in the vector will be selected.\n\n    NOTE: that the returned Boxes might share storage with this Boxes,\n    subject to Pytorch's indexing semantics.\n    \"\"\"\n    if isinstance(item, int):\n        return Boxes(self.tensor[item])\n    b = self.tensor[item]\n    assert b.dim() == 3, (\n        \"Indexing on Boxes with {} failed to return a matrix!\".format(item)\n    )\n    return Boxes(b)\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.__init__","title":"<code>__init__(tensor)</code>","text":"<p>Initialize Boxes.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor[float]</code> <p>a Nx4 matrix.  Each row is (x1, y1, x2, y2).</p> required Source code in <code>dreem/inference/boxes.py</code> <pre><code>def __init__(self, tensor: torch.Tensor):\n    \"\"\"Initialize Boxes.\n\n    Args:\n        tensor (Tensor[float]): a Nx4 matrix.  Each row is (x1, y1, x2, y2).\n    \"\"\"\n    if not isinstance(tensor, torch.Tensor):\n        tensor = torch.as_tensor(\n            tensor, dtype=torch.float32, device=torch.device(\"cpu\")\n        )\n    else:\n        tensor = tensor.to(torch.float32)\n    if tensor.numel() == 0:\n        # Use reshape, so we don't end up creating a new tensor that does not depend on\n        # the inputs (and consequently confuses jit)\n        tensor = tensor.reshape((-1, 4)).to(dtype=torch.float32)\n    assert tensor.dim() == 3 and tensor.size(-1) == 4, tensor.size()\n\n    self.tensor = tensor\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.__iter__","title":"<code>__iter__()</code>","text":"<p>Yield a box as a Tensor of shape (4,) at a time.</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>@torch.jit.unused\ndef __iter__(self):\n    \"\"\"Yield a box as a Tensor of shape (4,) at a time.\"\"\"\n    yield from self.tensor\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of boxes stored in this object.</p> <p>Returns:</p> Type Description <code>int</code> <p>the number of boxes stored in this object</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the number of boxes stored in this object.\n\n    Returns:\n        the number of boxes stored in this object\n    \"\"\"\n    return self.tensor.shape[0]\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.__repr__","title":"<code>__repr__()</code>","text":"<p>Override representation for printing.</p> <p>Returns:</p> Type Description <code>str</code> <p>'Boxes(tensor)'</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Override representation for printing.\n\n    Returns:\n        'Boxes(tensor)'\n    \"\"\"\n    return \"Boxes(\" + str(self.tensor) + \")\"\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.area","title":"<code>area()</code>","text":"<p>Compute the area of all the boxes.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: a vector with areas of each box.</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def area(self) -&gt; torch.Tensor:\n    \"\"\"Compute the area of all the boxes.\n\n    Returns:\n        torch.Tensor: a vector with areas of each box.\n    \"\"\"\n    box = self.tensor\n    area = (box[:, :, 2] - box[:, :, 0]) * (box[:, :, 3] - box[:, :, 1])\n    return area\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.cat","title":"<code>cat(boxes_list)</code>  <code>classmethod</code>","text":"<p>Concatenates a list of Boxes into a single Boxes.</p> <p>Parameters:</p> Name Type Description Default <code>boxes_list</code> <code>list[Boxes]</code> <p>list of <code>Boxes</code></p> required <p>Returns:</p> Name Type Description <code>Boxes</code> <code>Boxes</code> <p>the concatenated Boxes</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>@classmethod\ndef cat(cls, boxes_list: list[\"Boxes\"]) -&gt; \"Boxes\":\n    \"\"\"Concatenates a list of Boxes into a single Boxes.\n\n    Arguments:\n        boxes_list: list of `Boxes`\n\n    Returns:\n        Boxes: the concatenated Boxes\n    \"\"\"\n    assert isinstance(boxes_list, (list, tuple))\n    if len(boxes_list) == 0:\n        return cls(torch.empty(0))\n    assert all([isinstance(box, Boxes) for box in boxes_list])\n\n    # use torch.cat (v.s. layers.cat) so the returned boxes never share storage with input\n    cat_boxes = cls(torch.cat([b.tensor for b in boxes_list], dim=0))\n    return cat_boxes\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.clip","title":"<code>clip(box_size)</code>","text":"<p>Clip (in place) the boxes.</p> <p>Limits x coordinates to the range [0, width] and y coordinates to the range [0, height].</p> <p>Parameters:</p> Name Type Description Default <code>box_size</code> <code>(height, width)</code> <p>The clipping box's size.</p> required Source code in <code>dreem/inference/boxes.py</code> <pre><code>def clip(self, box_size: list[int, int]) -&gt; None:\n    \"\"\"Clip (in place) the boxes.\n\n    Limits x coordinates to the range [0, width]\n    and y coordinates to the range [0, height].\n\n    Args:\n        box_size (height, width): The clipping box's size.\n    \"\"\"\n    assert torch.isfinite(self.tensor).all(), \"Box tensor contains infinite or NaN!\"\n    h, w = box_size\n    x1 = self.tensor[:, :, 0].clamp(min=0, max=w)\n    y1 = self.tensor[:, :, 1].clamp(min=0, max=h)\n    x2 = self.tensor[:, :, 2].clamp(min=0, max=w)\n    y2 = self.tensor[:, :, 3].clamp(min=0, max=h)\n    self.tensor = torch.stack((x1, y1, x2, y2), dim=-1)\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.clone","title":"<code>clone()</code>","text":"<p>Clone the Boxes.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Boxes</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def clone(self) -&gt; Self:\n    \"\"\"Clone the Boxes.\n\n    Returns:\n        Boxes\n    \"\"\"\n    return Boxes(self.tensor.clone())\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.get_centers","title":"<code>get_centers()</code>","text":"<p>Get the centroid of the bbox.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The box centers in a Nx2 array of (x, y).</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def get_centers(self) -&gt; torch.Tensor:\n    \"\"\"Get the centroid of the bbox.\n\n    Returns:\n        The box centers in a Nx2 array of (x, y).\n    \"\"\"\n    return (self.tensor[:, :, :2] + self.tensor[:, :, 2:]) / 2\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.inside_box","title":"<code>inside_box(box_size, boundary_threshold=0)</code>","text":"<p>Check if box is inside reference box.</p> <p>Parameters:</p> Name Type Description Default <code>box_size</code> <code>(height, width)</code> <p>Size of the reference box.</p> required <code>boundary_threshold</code> <code>int</code> <p>Boxes that extend beyond the reference box boundary by more than boundary_threshold are considered \"outside\".</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>a binary vector, indicating whether each box is inside the reference box.</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def inside_box(\n    self, box_size: tuple[int, int], boundary_threshold: int = 0\n) -&gt; torch.Tensor:\n    \"\"\"Check if box is inside reference box.\n\n    Args:\n        box_size (height, width): Size of the reference box.\n        boundary_threshold (int): Boxes that extend beyond the reference box\n            boundary by more than boundary_threshold are considered \"outside\".\n\n    Returns:\n        a binary vector, indicating whether each box is inside the reference box.\n    \"\"\"\n    height, width = box_size\n    inds_inside = (\n        (self.tensor[..., 0] &gt;= -boundary_threshold)\n        &amp; (self.tensor[..., 1] &gt;= -boundary_threshold)\n        &amp; (self.tensor[..., 2] &lt; width + boundary_threshold)\n        &amp; (self.tensor[..., 3] &lt; height + boundary_threshold)\n    )\n    return inds_inside\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.nonempty","title":"<code>nonempty(threshold=0.0)</code>","text":"<p>Find boxes that are non-empty.</p> <p>A box is considered empty, if either of its side is no larger than threshold.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>the smallest a box can be.</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <pre><code>a binary vector which represents whether each box is empty\n(False) or non-empty (True).\n</code></pre> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def nonempty(self, threshold: float = 0.0) -&gt; torch.Tensor:\n    \"\"\"Find boxes that are non-empty.\n\n    A box is considered empty, if either of its side is no larger than threshold.\n\n    Args:\n        threshold: the smallest a box can be.\n\n    Returns:\n        Tensor:\n            a binary vector which represents whether each box is empty\n            (False) or non-empty (True).\n    \"\"\"\n    box = self.tensor\n    widths = box[:, :, 2] - box[:, :, 0]\n    heights = box[:, :, 3] - box[:, :, 1]\n    keep = (widths &gt; threshold) &amp; (heights &gt; threshold)\n    return keep\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.scale","title":"<code>scale(scale_x, scale_y)</code>","text":"<p>Scale the box with horizontal and vertical scaling factors.</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def scale(self, scale_x: float, scale_y: float) -&gt; None:\n    \"\"\"Scale the box with horizontal and vertical scaling factors.\"\"\"\n    self.tensor[:, :, 0::2] *= scale_x\n    self.tensor[:, :, 1::2] *= scale_y\n</code></pre>"},{"location":"reference/dreem/inference/boxes/#dreem.inference.boxes.Boxes.to","title":"<code>to(device)</code>","text":"<p>Load boxes to gpu/cpu.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>The device to load the boxes to</p> required <p>Returns: Boxes on device.</p> Source code in <code>dreem/inference/boxes.py</code> <pre><code>def to(self, device: torch.device) -&gt; Self:\n    \"\"\"Load boxes to gpu/cpu.\n\n    Args:\n        device: The device to load the boxes to\n\n    Returns: Boxes on device.\n    \"\"\"\n    # Boxes are assumed float32 and does not support to(dtype)\n    return Boxes(self.tensor.to(device=device))\n</code></pre>"},{"location":"reference/dreem/inference/eval/","title":"eval","text":""},{"location":"reference/dreem/inference/eval/#dreem.inference.eval","title":"<code>dreem.inference.eval</code>","text":"<p>Script to evaluate model.</p> <p>Functions:</p> Name Description <code>run</code> <p>Run inference based on config file.</p>"},{"location":"reference/dreem/inference/eval/#dreem.inference.eval.run","title":"<code>run(cfg)</code>","text":"<p>Run inference based on config file.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>A dictconfig loaded from hydra containing checkpoint path and data</p> required Source code in <code>dreem/inference/eval.py</code> <pre><code>@hydra.main(config_path=None, config_name=None, version_base=None)\ndef run(cfg: DictConfig) -&gt; dict[int, sio.Labels]:\n    \"\"\"Run inference based on config file.\n\n    Args:\n        cfg: A dictconfig loaded from hydra containing checkpoint path and data\n    \"\"\"\n    eval_cfg = Config(cfg)\n\n    if \"checkpoints\" in cfg.keys():\n        try:\n            index = int(os.environ[\"POD_INDEX\"])\n        # For testing without deploying a job on runai\n        except KeyError:\n            index = input(\"Pod Index Not found! Please choose a pod index: \")\n\n        logger.info(f\"Pod Index: {index}\")\n\n        checkpoints = pd.read_csv(cfg.checkpoints)\n        checkpoint = checkpoints.iloc[index]\n    else:\n        checkpoint = eval_cfg.get(\"ckpt_path\", None)\n        if checkpoint is None:\n            raise ValueError(\"Checkpoint path not found in config\")\n\n    logging.getLogger().setLevel(level=cfg.get(\"log_level\", \"INFO\").upper())\n\n    model = GTRRunner.load_from_checkpoint(checkpoint, strict=False)\n    model.tracker_cfg = eval_cfg.cfg.tracker\n    if model.tracker_cfg.get(\"tracker_type\", \"standard\") == \"batch\":\n        model.tracker = BatchTracker(**model.tracker_cfg)\n    else:\n        model.tracker = Tracker(**model.tracker_cfg)\n    logger.info(\"Using the following tracker:\")\n    logger.info(model.tracker)\n    model.metrics[\"test\"] = eval_cfg.get(\"metrics\", {}).get(\"test\", \"all\")\n    model.persistent_tracking[\"test\"] = True\n    logger.info(\"Computing the following metrics:\")\n    logger.info(model.metrics[\"test\"])\n    model.test_results[\"save_path\"] = eval_cfg.get(\"outdir\", \".\")\n    os.makedirs(model.test_results[\"save_path\"], exist_ok=True)\n    logger.info(\n        f\"Saving tracking results and metrics to {model.test_results['save_path']}\"\n    )\n\n    labels_files, vid_files = eval_cfg.get_data_paths(\n        \"test\", eval_cfg.cfg.dataset.test_dataset\n    )\n    trainer = eval_cfg.get_trainer()\n    for label_file, vid_file in zip(labels_files, vid_files):\n        dataset = eval_cfg.get_dataset(\n            label_files=[label_file], vid_files=[vid_file], mode=\"test\"\n        )\n        dataloader = eval_cfg.get_dataloader(dataset, mode=\"test\")\n        _ = trainer.test(model, dataloader)\n</code></pre>"},{"location":"reference/dreem/inference/metrics/","title":"metrics","text":""},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics","title":"<code>dreem.inference.metrics</code>","text":"<p>Helper functions for calculating mot metrics.</p> <p>Functions:</p> Name Description <code>compute_global_tracking_accuracy</code> <p>Compute global tracking accuracy for each ground truth track. Average the results to get overall accuracy.</p> <code>compute_motmetrics</code> <p>Get pymotmetrics summary and mot_events.</p> <code>evaluate</code> <p>Evaluate metrics for a list of frames.</p> <code>get_matches</code> <p>Get comparison between predicted and gt trajectory labels. Deprecated.</p> <code>get_switch_count</code> <p>Get the number of mislabeled predicted trajectories. Deprecated.</p> <code>get_switches</code> <p>Get misassigned predicted trajectory labels. Deprecated.</p>"},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics.compute_global_tracking_accuracy","title":"<code>compute_global_tracking_accuracy(df)</code>","text":"<p>Compute global tracking accuracy for each ground truth track. Average the results to get overall accuracy.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>dataframe with ground truth and predicted centroids and track ids</p> required <p>Returns:</p> Name Type Description <code>gta_by_gt_track_filt</code> <p>global tracking accuracy for each ground truth track</p> Source code in <code>dreem/inference/metrics.py</code> <pre><code>def compute_global_tracking_accuracy(df):\n    \"\"\"Compute global tracking accuracy for each ground truth track. Average the results to get overall accuracy.\n\n    Args:\n        df: dataframe with ground truth and predicted centroids and track ids\n\n    Returns:\n        gta_by_gt_track_filt: global tracking accuracy for each ground truth track\n    \"\"\"\n    track_confusion_dict = {i: [] for i in df.gt_track_id.unique()}\n    gt_track_len = df.gt_track_id.value_counts().to_dict()\n    gta_by_gt_track = {}\n\n    for idx, row in df.iterrows():\n        if ~np.isnan(row[\"gt_track_id\"]) and ~np.isnan(row[\"pred_track_id\"]):\n            track_confusion_dict[int(row[\"gt_track_id\"])].append(\n                int(row[\"pred_track_id\"])\n            )\n\n    for gt_track_id, pred_track_ids in track_confusion_dict.items():\n        # Use numpy's mode function to find the most common predicted track ID\n        if pred_track_ids:\n            # Get the most frequent prediction using numpy's mode\n            most_common_pred, count = np.unique(pred_track_ids, return_counts=True)\n            gta_by_gt_track[gt_track_id] = np.max(count) / float(\n                gt_track_len[gt_track_id]\n            )\n        else:\n            gta_by_gt_track[gt_track_id] = 0\n\n    return gta_by_gt_track\n</code></pre>"},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics.compute_motmetrics","title":"<code>compute_motmetrics(df)</code>","text":"<p>Get pymotmetrics summary and mot_events.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>dataframe with ground truth and predicted centroids matched from match_centroids</p> required <p>Returns:</p> Type Description <p>Tuple containing: summary_dreem: Motmetrics summary acc_dreem.mot_events: Frame by frame MOT events log</p> Source code in <code>dreem/inference/metrics.py</code> <pre><code>def compute_motmetrics(df):\n    \"\"\"Get pymotmetrics summary and mot_events.\n\n    Args:\n        df: dataframe with ground truth and predicted centroids matched from match_centroids\n\n    Returns:\n        Tuple containing:\n        summary_dreem: Motmetrics summary\n        acc_dreem.mot_events: Frame by frame MOT events log\n    \"\"\"\n    summary_dreem = {}\n    acc_dreem = mm.MOTAccumulator(auto_id=True)\n    frame_switch_map = {}\n    for frame, framedf in df.groupby(\"frame_id\"):\n        gt_ids = framedf[\"gt_track_id\"].values\n        pred_tracks = framedf[\"pred_track_id\"].values\n        # if no matching preds, fill with nan to let motmetrics handle it\n        if (pred_tracks == -1).all():\n            pred_tracks = np.full(len(gt_ids), np.nan)\n\n        # expected_tm_ids = []  # Currently unused but may be needed for future TRA metric calculations\n        cost_gt_dreem = np.full((len(gt_ids), len(gt_ids)), np.nan)\n        np.fill_diagonal(cost_gt_dreem, 1)\n        acc_dreem.update(\n            oids=gt_ids,\n            hids=pred_tracks,\n            dists=cost_gt_dreem,\n        )\n\n    # get pymotmetrics summary\n    mh = mm.metrics.create()\n    summary_dreem = mh.compute(acc_dreem, name=\"acc\").transpose()\n    motevents = acc_dreem.mot_events.reset_index()\n    for idx, row in motevents.iterrows():\n        if row[\"Type\"] == \"SWITCH\":\n            frame_switch_map[int(row[\"FrameId\"])] = True\n        else:\n            frame_switch_map[int(row[\"FrameId\"])] = False\n\n    return summary_dreem, motevents, frame_switch_map\n</code></pre>"},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics.evaluate","title":"<code>evaluate(preds, metrics)</code>","text":"<p>Evaluate metrics for a list of frames.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <p>list of Frame objects with gt and pred track ids</p> required <code>metrics</code> <p>list of metrics to compute</p> required <p>Returns:</p> Type Description <p>A dict of metrics with key being the metric, and value being the metric value computed.</p> Source code in <code>dreem/inference/metrics.py</code> <pre><code>def evaluate(preds, metrics):\n    \"\"\"Evaluate metrics for a list of frames.\n\n    Args:\n        preds: list of Frame objects with gt and pred track ids\n        metrics: list of metrics to compute\n\n    Returns:\n        A dict of metrics with key being the metric, and value being the metric value computed.\n    \"\"\"\n    metric_fcn_map = {\n        \"motmetrics\": compute_motmetrics,\n        \"global_tracking_accuracy\": compute_global_tracking_accuracy,\n    }\n    list_frame_info = []\n    test_results = {}\n\n    # create gt/pred df\n    for frame in preds:\n        for instance in frame.instances:\n            anchor = instance.anchor[0]\n            if anchor in instance.centroid:\n                centroid = instance.centroid[anchor]\n            else:  # if for some reason the anchor is not in the centroid dict, use the first key-value pair\n                for key, value in instance.centroid.items():\n                    centroid = value\n                    break\n            centroid_x, centroid_y = centroid[0], centroid[1]\n            list_frame_info.append(\n                {\n                    \"frame_id\": frame.frame_id.item(),\n                    \"gt_track_id\": instance.gt_track_id.item(),\n                    \"pred_track_id\": instance.pred_track_id.item(),\n                    \"centroid_x\": centroid_x,\n                    \"centroid_y\": centroid_y,\n                }\n            )\n\n    df = pd.DataFrame(list_frame_info)\n\n    for metric in metrics:\n        result = metric_fcn_map[metric](df)\n        test_results[metric] = result\n\n    return test_results\n</code></pre>"},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics.get_matches","title":"<code>get_matches(frames)</code>","text":"<p>Get comparison between predicted and gt trajectory labels. Deprecated.</p> <p>Parameters:</p> Name Type Description Default <code>frames</code> <code>list[Frame]</code> <p>a list of Frames containing the video_id, frame_id, gt labels and predicted labels</p> required <p>Returns:</p> Name Type Description <code>matches</code> <code>tuple[dict, list, int]</code> <p>a dict containing predicted and gt trajectory labels indices: the frame indices being compared video_id: the video being</p> Source code in <code>dreem/inference/metrics.py</code> <pre><code>def get_matches(frames: list[\"dreem.io.Frame\"]) -&gt; tuple[dict, list, int]:\n    \"\"\"Get comparison between predicted and gt trajectory labels. Deprecated.\n\n    Args:\n        frames: a list of Frames containing the video_id, frame_id,\n            gt labels and predicted labels\n\n    Returns:\n        matches: a dict containing predicted and gt trajectory labels\n        indices: the frame indices being compared\n        video_id: the video being\n    \"\"\"\n    matches = {}\n    indices = []\n\n    video_id = frames[0].video_id.item()\n\n    if any([frame.has_instances() for frame in frames]):\n        for idx, frame in enumerate(frames):\n            indices.append(frame.frame_id.item())\n            for gt_track_id, pred_track_id in zip(\n                frame.get_gt_track_ids(), frame.get_pred_track_ids()\n            ):\n                match = f\"{gt_track_id} -&gt; {pred_track_id}\"\n\n                if match not in matches:\n                    matches[match] = np.full(len(frames), 0)\n\n                matches[match][idx] = 1\n    else:\n        logger.debug(\"No instances detected!\")\n    return matches, indices, video_id\n</code></pre>"},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics.get_switch_count","title":"<code>get_switch_count(switches)</code>","text":"<p>Get the number of mislabeled predicted trajectories. Deprecated.</p> <p>Parameters:</p> Name Type Description Default <code>switches</code> <code>dict</code> <p>a dict of dicts containing the mislabeled trajectories and the frames at which they occur</p> required <p>Returns:</p> Type Description <code>int</code> <p>the number of switched labels in the video chunk</p> Source code in <code>dreem/inference/metrics.py</code> <pre><code>def get_switch_count(switches: dict) -&gt; int:\n    \"\"\"Get the number of mislabeled predicted trajectories. Deprecated.\n\n    Args:\n        switches: a dict of dicts containing the mislabeled trajectories\n            and the frames at which they occur\n\n    Returns:\n        the number of switched labels in the video chunk\n    \"\"\"\n    only_switches = {k: v for k, v in switches.items() if v != {}}\n    sw_cnt = sum([len(i) for i in list(only_switches.values())])\n    return sw_cnt\n</code></pre>"},{"location":"reference/dreem/inference/metrics/#dreem.inference.metrics.get_switches","title":"<code>get_switches(matches, indices)</code>","text":"<p>Get misassigned predicted trajectory labels. Deprecated.</p> <p>Parameters:</p> Name Type Description Default <code>matches</code> <code>dict</code> <p>a dict containing the gt and predicted labels</p> required <code>indices</code> <code>list</code> <p>a list of frame indices being used</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dict of dicts containing the frame at which the switch occurred and the change in labels</p> Source code in <code>dreem/inference/metrics.py</code> <pre><code>def get_switches(matches: dict, indices: list) -&gt; dict:\n    \"\"\"Get misassigned predicted trajectory labels. Deprecated.\n\n    Args:\n        matches: a dict containing the gt and predicted labels\n        indices: a list of frame indices being used\n\n    Returns:\n        A dict of dicts containing the frame at which the switch occurred\n        and the change in labels\n    \"\"\"\n    track, switches = {}, {}\n    if len(matches) &gt; 0 and len(indices) &gt; 0:\n        matches_key = np.array(list(matches.keys()))\n        matches = np.array(list(matches.values()))\n        num_frames = matches.shape[1]\n\n        assert num_frames == len(indices)\n\n        for i, idx in zip(range(num_frames), indices):\n            switches[idx] = {}\n\n            col = matches[:, i]\n            match_indices = np.where(col == 1)[0]\n            match_i = [\n                (m.split(\" \")[0], m.split(\" \")[-1]) for m in matches_key[match_indices]\n            ]\n\n            for m in match_i:\n                gt, pred = m\n\n                if gt in track and track[gt] != pred:\n                    switches[idx][gt] = {\n                        \"frames\": (idx - 1, idx),\n                        \"pred tracks (from, to)\": (track[gt], pred),\n                    }\n\n                track[gt] = pred\n\n    return switches\n</code></pre>"},{"location":"reference/dreem/inference/post_processing/","title":"post_processing","text":""},{"location":"reference/dreem/inference/post_processing/#dreem.inference.post_processing","title":"<code>dreem.inference.post_processing</code>","text":"<p>Helper functions for post-processing association matrix pre-tracking.</p> <p>Functions:</p> Name Description <code>filter_max_center_dist</code> <p>Filter trajectory score by distances between objects across frames.</p> <code>weight_decay_time</code> <p>Weight association matrix by time.</p> <code>weight_iou</code> <p>Weight the association matrix by the IOU between object bboxes across frames.</p>"},{"location":"reference/dreem/inference/post_processing/#dreem.inference.post_processing.filter_max_center_dist","title":"<code>filter_max_center_dist(asso_output, max_center_dist=0, id_inds=None, query_boxes_px=None, nonquery_boxes_px=None)</code>","text":"<p>Filter trajectory score by distances between objects across frames.</p> <p>Parameters:</p> Name Type Description Default <code>asso_output</code> <code>Tensor</code> <p>An N_t x N association matrix</p> required <code>max_center_dist</code> <code>float</code> <p>The euclidean distance threshold between bboxes</p> <code>0</code> <code>id_inds</code> <code>Tensor | None</code> <p>track ids</p> <code>None</code> <code>query_boxes_px</code> <code>Tensor | None</code> <p>the raw bbox coords of the current frame instances</p> <code>None</code> <code>nonquery_boxes_px</code> <code>Tensor | None</code> <p>the raw bbox coords of the instances in the nonquery frames (context window)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>An N_t x N association matrix</p> Source code in <code>dreem/inference/post_processing.py</code> <pre><code>def filter_max_center_dist(\n    asso_output: torch.Tensor,\n    max_center_dist: float = 0,\n    id_inds: torch.Tensor | None = None,\n    query_boxes_px: torch.Tensor | None = None,\n    nonquery_boxes_px: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Filter trajectory score by distances between objects across frames.\n\n    Args:\n        asso_output: An N_t x N association matrix\n        max_center_dist: The euclidean distance threshold between bboxes\n        id_inds: track ids\n        query_boxes_px: the raw bbox coords of the current frame instances\n        nonquery_boxes_px: the raw bbox coords of the instances in the nonquery frames (context window)\n\n    Returns:\n        An N_t x N association matrix\n    \"\"\"\n    if max_center_dist is not None and max_center_dist &gt; 0:\n        assert query_boxes_px is not None and nonquery_boxes_px is not None, (\n            \"Need `query_boxes_px`, and `nonquery_boxes_px` to filter by `max_center_dist`\"\n        )\n\n        k_ct = (query_boxes_px[:, :, :2] + query_boxes_px[:, :, 2:]) / 2\n        # k_s = ((curr_frame_boxes[:, :, 2:] - curr_frame_boxes[:, :, :2]) ** 2).sum(dim=2)  # n_k\n        # nonk boxes are only from previous frame rather than entire window\n        nonk_ct = (nonquery_boxes_px[:, :, :2] + nonquery_boxes_px[:, :, 2:]) / 2\n\n        # pairwise euclidean distance in units of pixels\n        dist = ((k_ct[:, None, :, :] - nonk_ct[None, :, :, :]) ** 2).sum(dim=-1) ** (\n            1 / 2\n        )  # n_k x n_nonk\n        # norm_dist = dist / (k_s[:, None, :] + 1e-8)\n\n        valid = dist.squeeze() &lt; max_center_dist  # n_k x n_nonk\n        # handle case where id_inds and valid is a single value\n        # handle this better\n        if valid.ndim == 0:\n            valid = valid.unsqueeze(0)\n        if valid.ndim == 1:\n            if id_inds.shape[0] == 1:\n                valid_mult = valid.float().unsqueeze(-1)\n            else:\n                valid_mult = valid.float().unsqueeze(0)\n        else:\n            valid_mult = valid.float()\n\n        valid_assn = (\n            torch.mm(valid_mult, id_inds.to(valid.device)).clamp_(max=1.0).long().bool()\n        )  # n_k x M\n        asso_output_filtered = asso_output.clone()\n        asso_output_filtered[~valid_assn] = 0  # n_k x M\n        return asso_output_filtered\n    else:\n        return asso_output\n</code></pre>"},{"location":"reference/dreem/inference/post_processing/#dreem.inference.post_processing.weight_decay_time","title":"<code>weight_decay_time(asso_output, decay_time=0, reid_features=None, T=None, k=None)</code>","text":"<p>Weight association matrix by time.</p> <p>Weighs matrix by number of frames the ith object is from the jth object in the association matrix.</p> <p>Parameters:</p> Name Type Description Default <code>asso_output</code> <code>Tensor</code> <p>the association matrix to be reweighted</p> required <code>decay_time</code> <code>float</code> <p>the scale to weight the asso_output by</p> <code>0</code> <code>reid_features</code> <code>Tensor | None</code> <p>The n x d matrix of feature vectors for each object</p> <code>None</code> <code>T</code> <code>int | None</code> <p>The length of the window</p> <code>None</code> <code>k</code> <code>int | None</code> <p>an integer for the query frame within the window of instances</p> <code>None</code> <p>Returns: The N_t x N association matrix weighted by decay time</p> Source code in <code>dreem/inference/post_processing.py</code> <pre><code>def weight_decay_time(\n    asso_output: torch.Tensor,\n    decay_time: float = 0,\n    reid_features: torch.Tensor | None = None,\n    T: int | None = None,\n    k: int | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Weight association matrix by time.\n\n    Weighs matrix by number of frames the ith object is from the jth object\n    in the association matrix.\n\n    Args:\n        asso_output: the association matrix to be reweighted\n        decay_time: the scale to weight the asso_output by\n        reid_features: The n x d matrix of feature vectors for each object\n        T: The length of the window\n        k: an integer for the query frame within the window of instances\n    Returns: The N_t x N association matrix weighted by decay time\n    \"\"\"\n    if decay_time is not None and decay_time &gt; 0:\n        assert reid_features is not None and T is not None and k is not None, (\n            \"Need reid_features to weight traj_score by `decay_time`!\"\n        )\n        N_t = asso_output.shape[0]\n        dts = torch.cat(\n            [\n                x.new_full((N_t,), T - t - 2)\n                for t, x in enumerate(reid_features)\n                if t != k\n            ],\n            dim=0,\n        ).cpu()  # Np\n        # asso_output = asso_output.to(self.device) * (self.decay_time ** dts[None, :])\n        asso_output = asso_output * (decay_time ** dts[:, None])\n    return asso_output\n</code></pre>"},{"location":"reference/dreem/inference/post_processing/#dreem.inference.post_processing.weight_iou","title":"<code>weight_iou(asso_output, method=None, last_ious=None)</code>","text":"<p>Weight the association matrix by the IOU between object bboxes across frames.</p> <p>Parameters:</p> Name Type Description Default <code>asso_output</code> <code>Tensor</code> <p>An N_t x N association matrix</p> required <code>method</code> <code>str | None</code> <p>string indicating whether to use a max weighting or multiplicative weighting     Max weighting: take <code>max(traj_score, iou)</code>     multiplicative weighting: <code>iou*weight + traj_score</code></p> <code>None</code> <code>last_ious</code> <code>Tensor</code> <p>torch Tensor containing the ious between current and previous frames</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>An N_t x N association matrix weighted by the IOU</p> Source code in <code>dreem/inference/post_processing.py</code> <pre><code>def weight_iou(\n    asso_output: torch.Tensor, method: str | None = None, last_ious: torch.Tensor = None\n) -&gt; torch.Tensor:\n    \"\"\"Weight the association matrix by the IOU between object bboxes across frames.\n\n    Args:\n        asso_output: An N_t x N association matrix\n        method: string indicating whether to use a max weighting or multiplicative weighting\n                Max weighting: take `max(traj_score, iou)`\n                multiplicative weighting: `iou*weight + traj_score`\n        last_ious: torch Tensor containing the ious between current and previous frames\n\n    Returns:\n        An N_t x N association matrix weighted by the IOU\n    \"\"\"\n    if method is not None and method != \"\":\n        assert last_ious is not None, \"Need `last_ious` to weight traj_score by `IOU`\"\n        if method.lower() == \"mult\":\n            weights = torch.abs(last_ious - asso_output)\n            weighted_iou = weights * last_ious\n            weighted_iou = torch.nan_to_num(weighted_iou, 0)\n            asso_output = asso_output + weighted_iou\n        elif method.lower() == \"max\":\n            asso_output = torch.max(asso_output, last_ious)\n        else:\n            raise ValueError(\n                f\"`method` must be one of ['mult' or 'max'] got '{method.lower()}'\"\n            )\n    return asso_output\n</code></pre>"},{"location":"reference/dreem/inference/track/","title":"track","text":""},{"location":"reference/dreem/inference/track/#dreem.inference.track","title":"<code>dreem.inference.track</code>","text":"<p>Script to run inference and get out tracks.</p> <p>Functions:</p> Name Description <code>export_trajectories</code> <p>Convert trajectories to data frame and save as .csv.</p> <code>get_timestamp</code> <p>Get current timestamp.</p> <code>run</code> <p>Run inference based on config file.</p> <code>track</code> <p>Run Inference.</p> <code>track_ctc</code> <p>Run Inference.</p>"},{"location":"reference/dreem/inference/track/#dreem.inference.track.export_trajectories","title":"<code>export_trajectories(frames_pred, save_path=None)</code>","text":"<p>Convert trajectories to data frame and save as .csv.</p> <p>Parameters:</p> Name Type Description Default <code>frames_pred</code> <code>list[Frame]</code> <p>A list of Frames with predicted track ids.</p> required <code>save_path</code> <code>str | None</code> <p>The path to save the predicted trajectories to.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dictionary containing the predicted track id and centroid coordinates for each instance in the video.</p> Source code in <code>dreem/inference/track.py</code> <pre><code>def export_trajectories(\n    frames_pred: list[\"dreem.io.Frame\"], save_path: str | None = None\n) -&gt; pd.DataFrame:\n    \"\"\"Convert trajectories to data frame and save as .csv.\n\n    Args:\n        frames_pred: A list of Frames with predicted track ids.\n        save_path: The path to save the predicted trajectories to.\n\n    Returns:\n        A dictionary containing the predicted track id and centroid coordinates for each instance in the video.\n    \"\"\"\n    save_dict = {}\n    frame_ids = []\n    X, Y = [], []\n    pred_track_ids = []\n    track_scores = []\n    for frame in frames_pred:\n        for i, instance in enumerate(frame.instances):\n            frame_ids.append(frame.frame_id.item())\n            bbox = instance.bbox.squeeze()\n            y = (bbox[2] + bbox[0]) / 2\n            x = (bbox[3] + bbox[1]) / 2\n            X.append(x.item())\n            Y.append(y.item())\n            track_scores.append(instance.track_score)\n            pred_track_ids.append(instance.pred_track_id.item())\n\n    save_dict[\"Frame\"] = frame_ids\n    save_dict[\"X\"] = X\n    save_dict[\"Y\"] = Y\n    save_dict[\"Pred_track_id\"] = pred_track_ids\n    save_dict[\"Track_score\"] = track_scores\n    save_df = pd.DataFrame(save_dict)\n    if save_path:\n        save_df.to_csv(save_path, index=False)\n    return save_df\n</code></pre>"},{"location":"reference/dreem/inference/track/#dreem.inference.track.get_timestamp","title":"<code>get_timestamp()</code>","text":"<p>Get current timestamp.</p> <p>Returns:</p> Type Description <code>str</code> <p>the current timestamp in /m/d/y-H:M:S format</p> Source code in <code>dreem/inference/track.py</code> <pre><code>def get_timestamp() -&gt; str:\n    \"\"\"Get current timestamp.\n\n    Returns:\n        the current timestamp in /m/d/y-H:M:S format\n    \"\"\"\n    date_time = datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")\n    return date_time\n</code></pre>"},{"location":"reference/dreem/inference/track/#dreem.inference.track.run","title":"<code>run(cfg)</code>","text":"<p>Run inference based on config file.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>A dictconfig loaded from hydra containing checkpoint path and data</p> required Source code in <code>dreem/inference/track.py</code> <pre><code>@hydra.main(config_path=None, config_name=None, version_base=None)\ndef run(cfg: DictConfig) -&gt; dict[int, sio.Labels]:\n    \"\"\"Run inference based on config file.\n\n    Args:\n        cfg: A dictconfig loaded from hydra containing checkpoint path and data\n    \"\"\"\n    pred_cfg = Config(cfg)\n\n    if \"checkpoints\" in cfg.keys():\n        try:\n            index = int(os.environ[\"POD_INDEX\"])\n        # For testing without deploying a job on runai\n        except KeyError:\n            index = input(\"Pod Index Not found! Please choose a pod index: \")\n\n        logger.info(f\"Pod Index: {index}\")\n\n        checkpoints = pd.read_csv(cfg.checkpoints)\n        checkpoint = checkpoints.iloc[index]\n    else:\n        checkpoint = pred_cfg.cfg.ckpt_path\n\n    logging.getLogger().setLevel(level=cfg.get(\"log_level\", \"INFO\").upper())\n\n    model = GTRRunner.load_from_checkpoint(checkpoint, strict=False)\n    tracker_cfg = pred_cfg.get_tracker_cfg()\n    logger.info(\"Updating tracker hparams\")\n    model.tracker_cfg = tracker_cfg\n    if model.tracker_cfg.get(\"tracker_type\", \"standard\") == \"batch\":\n        model.tracker = BatchTracker(**model.tracker_cfg)\n    else:\n        model.tracker = Tracker(**model.tracker_cfg)\n    logger.info(\"Using the following tracker:\")\n    logger.info(model.tracker)\n\n    labels_files, vid_files = pred_cfg.get_data_paths(\n        \"test\", pred_cfg.cfg.dataset.test_dataset\n    )\n    trainer = pred_cfg.get_trainer()\n    outdir = pred_cfg.cfg.outdir if \"outdir\" in pred_cfg.cfg else \"./results\"\n    os.makedirs(outdir, exist_ok=True)\n\n    for label_file, vid_file in zip(labels_files, vid_files):\n        dataset = pred_cfg.get_dataset(\n            label_files=[label_file], vid_files=[vid_file], mode=\"test\"\n        )\n        dataloader = pred_cfg.get_dataloader(dataset, mode=\"test\")\n        if isinstance(vid_file, list):\n            save_file_name = vid_file[0].split(\"/\")[-2]\n        else:\n            save_file_name = vid_file\n\n        if isinstance(dataset, CellTrackingDataset):\n            preds = track_ctc(model, trainer, dataloader)\n            outpath = os.path.join(\n                outdir,\n                f\"{Path(save_file_name).stem}.dreem_inference.{get_timestamp()}.tif\",\n            )\n            tifffile.imwrite(outpath, preds.astype(np.uint16))\n        else:\n            preds = track(model, trainer, dataloader)\n            outpath = os.path.join(\n                outdir,\n                f\"{Path(save_file_name).stem}.dreem_inference.{get_timestamp()}.slp\",\n            )\n            preds.save(outpath)\n\n    return preds\n</code></pre>"},{"location":"reference/dreem/inference/track/#dreem.inference.track.track","title":"<code>track(model, trainer, dataloader)</code>","text":"<p>Run Inference.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GTRRunner</code> <p>GTRRunner model loaded from checkpoint used for inference</p> required <code>trainer</code> <code>Trainer</code> <p>lighting Trainer object used for handling inference log.</p> required <code>dataloader</code> <code>DataLoader</code> <p>dataloader containing inference data</p> required Return <p>List of DataFrames containing prediction results for each video</p> Source code in <code>dreem/inference/track.py</code> <pre><code>def track(\n    model: GTRRunner, trainer: pl.Trainer, dataloader: torch.utils.data.DataLoader\n) -&gt; list[pd.DataFrame]:\n    \"\"\"Run Inference.\n\n    Args:\n        model: GTRRunner model loaded from checkpoint used for inference\n        trainer: lighting Trainer object used for handling inference log.\n        dataloader: dataloader containing inference data\n\n    Return:\n        List of DataFrames containing prediction results for each video\n    \"\"\"\n    preds = trainer.predict(model, dataloader)\n    pred_slp = []\n    tracks = {}\n    for batch in preds:\n        for frame in batch:\n            if frame.frame_id.item() == 0:\n                video = (\n                    sio.Video(frame.video)\n                    if isinstance(frame.video, str)\n                    else sio.Video\n                )\n            lf, tracks = frame.to_slp(tracks, video=video)\n            pred_slp.append(lf)\n    pred_slp = sio.Labels(pred_slp)\n    print(pred_slp)\n    return pred_slp\n</code></pre>"},{"location":"reference/dreem/inference/track/#dreem.inference.track.track_ctc","title":"<code>track_ctc(model, trainer, dataloader)</code>","text":"<p>Run Inference.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GTRRunner</code> <p>GTRRunner model loaded from checkpoint used for inference</p> required <code>trainer</code> <code>Trainer</code> <p>lighting Trainer object used for handling inference log.</p> required <code>dataloader</code> <code>DataLoader</code> <p>dataloader containing inference data</p> required Source code in <code>dreem/inference/track.py</code> <pre><code>def track_ctc(\n    model: GTRRunner, trainer: pl.Trainer, dataloader: torch.utils.data.DataLoader\n) -&gt; list[pd.DataFrame]:\n    \"\"\"Run Inference.\n\n    Args:\n        model: GTRRunner model loaded from checkpoint used for inference\n        trainer: lighting Trainer object used for handling inference log.\n        dataloader: dataloader containing inference data\n    \"\"\"\n    preds = trainer.predict(model, dataloader)\n    pred_imgs = []\n    for batch in preds:\n        for frame in batch:\n            frame_masks = []\n            for instance in frame.instances:\n                # centroid = instance.centroid[\"centroid\"]  # Currently unused but available if needed\n                mask = instance.mask.cpu().numpy()\n                track_id = instance.pred_track_id.cpu().numpy().item()\n                mask = mask.astype(np.uint8)\n                mask[mask != 0] = track_id  # label the mask with the track id\n                frame_masks.append(mask)\n                # combine masks by merging into single mask using max; this will cause overlap\n            frame_mask = np.max(frame_masks, axis=0)\n            pred_imgs.append(frame_mask)\n    pred_imgs = np.stack(pred_imgs)\n    return pred_imgs\n</code></pre>"},{"location":"reference/dreem/inference/track_queue/","title":"track_queue","text":""},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue","title":"<code>dreem.inference.track_queue</code>","text":"<p>Module handling sliding window tracking.</p> <p>Classes:</p> Name Description <code>TrackQueue</code> <p>Class handling track local queue system for sliding window.</p>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue","title":"<code>TrackQueue</code>","text":"<p>Class handling track local queue system for sliding window.</p> <p>Each trajectory has its own deque based queue of size <code>window_size - 1</code>. Elements of the queue are Instance objects that have already been tracked and will be compared against later frames for assignment.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize track queue.</p> <code>__len__</code> <p>Get length of the queue.</p> <code>__repr__</code> <p>Return the string representation of the TrackQueue.</p> <code>add_frame</code> <p>Add frames to the queue.</p> <code>collate_tracks</code> <p>Merge queues into a single list of Frames containing corresponding instances.</p> <code>end_tracks</code> <p>Terminate tracks and removing them from the queue.</p> <code>increment_gaps</code> <p>Keep track of number of consecutive frames each trajectory has been missing from the queue.</p> <p>Attributes:</p> Name Type Description <code>curr_track</code> <code>int</code> <p>The newest created trajectory in the queue.</p> <code>max_gap</code> <code>int</code> <p>The maximum number of consecutive frames an trajectory can fail to appear before termination.</p> <code>n_tracks</code> <code>int</code> <p>The current number of trajectories in the queue.</p> <code>tracks</code> <code>list</code> <p>A list of the track ids currently in the queue.</p> <code>verbose</code> <code>bool</code> <p>Indicate whether or not to print outputs along operations. Mostly used for debugging.</p> <code>window_size</code> <code>int</code> <p>The maximum number of instances allowed in a sub-queue to be compared against.</p> Source code in <code>dreem/inference/track_queue.py</code> <pre><code>class TrackQueue:\n    \"\"\"Class handling track local queue system for sliding window.\n\n    Each trajectory has its own deque based queue of size `window_size - 1`.\n    Elements of the queue are Instance objects that have already been tracked\n    and will be compared against later frames for assignment.\n    \"\"\"\n\n    def __init__(\n        self, window_size: int, max_gap: int = np.inf, verbose: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize track queue.\n\n        Args:\n            window_size: The number of instances per trajectory allowed in the\n                queue to be compared against.\n            max_gap: The number of consecutive frames a trajectory can fail to\n                appear in before terminating the track.\n            verbose: Whether to print info during operations.\n        \"\"\"\n        self._window_size = window_size\n        self._queues = {}\n        self._max_gap = max_gap\n        self._curr_gap = {}\n        if self._max_gap &lt;= self._window_size:\n            self._max_gap = self._window_size\n        self._curr_track = -1\n        self._verbose = verbose\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get length of the queue.\n\n        Returns:\n            The total number of instances in every sub-queue.\n        \"\"\"\n        return sum([len(queue) for queue in self._queues.values()])\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the TrackQueue.\n\n        Returns:\n            The string representation of the current state of the queue.\n        \"\"\"\n        return (\n            \"TrackQueue(\"\n            f\"window_size={self.window_size}, \"\n            f\"max_gap={self.max_gap}, \"\n            f\"n_tracks={self.n_tracks}, \"\n            f\"curr_track={self.curr_track}, \"\n            f\"queues={[(key, len(queue)) for key, queue in self._queues.items()]}, \"\n            f\"curr_gap:{self._curr_gap}\"\n            \")\"\n        )\n\n    @property\n    def window_size(self) -&gt; int:\n        \"\"\"The maximum number of instances allowed in a sub-queue to be compared against.\n\n        Returns:\n            An int representing The maximum number of instances allowed in a\n                sub-queue to be compared against.\n        \"\"\"\n        return self._window_size\n\n    @window_size.setter\n    def window_size(self, window_size: int) -&gt; None:\n        \"\"\"Set the window size of the queue.\n\n        Args:\n            window_size: An int representing The maximum number of instances\n                allowed in a sub-queue to be compared against.\n        \"\"\"\n        self._window_size = window_size\n\n    @property\n    def max_gap(self) -&gt; int:\n        \"\"\"The maximum number of consecutive frames an trajectory can fail to appear before termination.\n\n        Returns:\n            An int representing the maximum number of consecutive frames an trajectory can fail to\n                appear before termination.\n        \"\"\"\n        return self._max_gap\n\n    @max_gap.setter\n    def max_gap(self, max_gap: int) -&gt; None:\n        \"\"\"Set the max consecutive frame gap allowed for a trajectory.\n\n        Args:\n            max_gap: An int representing the maximum number of consecutive frames an trajectory can fail to\n                appear before termination.\n        \"\"\"\n        self._max_gap = max_gap\n\n    @property\n    def curr_track(self) -&gt; int:\n        \"\"\"The newest *created* trajectory in the queue.\n\n        Returns:\n            The latest *created* trajectory in the queue.\n        \"\"\"\n        return self._curr_track\n\n    @curr_track.setter\n    def curr_track(self, curr_track: int) -&gt; None:\n        \"\"\"Set the newest *created* trajectory in the queue.\n\n        Args:\n            curr_track: The latest *created* trajectory in the queue.\n        \"\"\"\n        self._curr_track = curr_track\n\n    @property\n    def n_tracks(self) -&gt; int:\n        \"\"\"The current number of trajectories in the queue.\n\n        Returns:\n            An int representing the current number of trajectories in the queue.\n        \"\"\"\n        return len(self._queues.keys())\n\n    @property\n    def tracks(self) -&gt; list:\n        \"\"\"A list of the track ids currently in the queue.\n\n        Returns:\n            A list containing the track ids currently in the queue.\n        \"\"\"\n        return list(self._queues.keys())\n\n    @property\n    def verbose(self) -&gt; bool:\n        \"\"\"Indicate whether or not to print outputs along operations. Mostly used for debugging.\n\n        Returns:\n            A boolean representing whether or not printing is turned on.\n        \"\"\"\n        return self._verbose\n\n    @verbose.setter\n    def verbose(self, verbose: bool) -&gt; None:\n        \"\"\"Turn on/off printing.\n\n        Args:\n            verbose: A boolean representing whether printing should be on or off.\n        \"\"\"\n        self._verbose = verbose\n\n    def end_tracks(self, track_id: int | None = None) -&gt; bool:\n        \"\"\"Terminate tracks and removing them from the queue.\n\n        Args:\n            track_id: The index of the trajectory to be ended and removed.\n                If `None` then then every trajectory is removed and the track queue is reset.\n\n        Returns:\n            True if the track is successively removed, otherwise False.\n                (ie if the track doesn't exist in the queue.)\n        \"\"\"\n        if track_id is None:\n            self._queues = {}\n            self._curr_gap = {}\n            self.curr_track = -1\n        else:\n            try:\n                self._queues.pop(track_id)\n                self._curr_gap.pop(track_id)\n            except KeyError:\n                logger.exception(f\"Track ID {track_id} not found in queue!\")\n                return False\n        return True\n\n    def add_frame(self, frame: Frame) -&gt; None:\n        \"\"\"Add frames to the queue.\n\n        Each instance from the frame is added to the queue according to its pred_track_id.\n        If the corresponding trajectory is not already in the queue then create a new queue for the track.\n\n        Args:\n            frame: A Frame object containing instances that have already been tracked.\n        \"\"\"\n        if frame.num_detected == 0:  # only add frames with instances.\n            return\n        vid_id = frame.video_id.item()\n        frame_id = frame.frame_id.item()\n        img_shape = frame.img_shape\n        if isinstance(frame.video, str):\n            vid_name = frame.video\n        else:\n            vid_name = frame.video.filename\n        # traj_score = frame.get_traj_score()  TODO: figure out better way to save trajectory scores.\n        frame_meta = (vid_id, frame_id, vid_name, img_shape.cpu().tolist())\n\n        pred_tracks = []\n        for instance in frame.instances:\n            pred_track_id = instance.pred_track_id.item()\n            pred_tracks.append(pred_track_id)\n\n            if pred_track_id not in self._queues.keys():\n                self._queues[pred_track_id] = deque(\n                    [(*frame_meta, instance)], maxlen=self.window_size\n                )  # dumb work around to retain `img_shape`\n                self.curr_track = pred_track_id\n\n                logger.debug(\n                    f\"New track = {pred_track_id} on frame {frame_id}! Current number of tracks = {self.n_tracks}\"\n                )\n\n            else:\n                self._queues[pred_track_id].append((*frame_meta, instance))\n        self.increment_gaps(\n            pred_tracks\n        )  # should this be done in the tracker or the queue?\n\n    def collate_tracks(\n        self,\n        context_start_frame_id: int | None = None,\n        track_ids: list[int] | None = None,\n        device: str | device | None = None,\n    ) -&gt; list[Frame]:\n        \"\"\"Merge queues into a single list of Frames containing corresponding instances.\n\n        Args:\n            context_start_frame_id: The frame_id of the last frame in the context i.e. just before the start of the current batch\n            track_ids: A list of trajectorys to merge. If None, then merge all\n                queues, otherwise filter queues by track_ids then merge.\n            device: A str representation of the device the frames should be on after merging\n                since all instances in the queue are kept on the cpu.\n\n        Returns:\n            A sorted list of Frame objects from which each instance came from,\n            containing the corresponding instances.\n        \"\"\"\n        if len(self._queues) == 0:\n            return []\n\n        frames = {}\n\n        tracks_to_convert = (\n            {track: queue for track, queue in self._queues if track in track_ids}\n            if track_ids is not None\n            else self._queues\n        )\n        for track, instances in tracks_to_convert.items():\n            for video_id, frame_id, vid_name, img_shape, instance in instances:\n                # if frame_id &lt; context_start_frame_id - self.window_size:\n                #     continue\n                if (video_id, frame_id) not in frames.keys():\n                    frame = Frame(\n                        video_id,\n                        frame_id,\n                        img_shape=img_shape,\n                        instances=[instance],\n                        vid_file=vid_name,\n                    )\n                    frames[(video_id, frame_id)] = frame\n                else:\n                    frames[(video_id, frame_id)].instances.append(instance)\n        return [frames[frame].to(device) for frame in sorted(frames.keys())]\n\n    def increment_gaps(self, pred_track_ids: list[int]) -&gt; dict[int, bool]:\n        \"\"\"Keep track of number of consecutive frames each trajectory has been missing from the queue.\n\n        If a trajectory has exceeded the `max_gap` then terminate the track and remove it from the queue.\n\n        Args:\n            pred_track_ids: A list of track_ids to be matched against the trajectories in the queue.\n                If a trajectory is in `pred_track_ids` then its gap counter is reset,\n                otherwise its incremented by 1.\n\n        Returns:\n            A dictionary containing the trajectory id and a boolean value representing\n            whether or not it has exceeded the max allowed gap and been\n            terminated.\n        \"\"\"\n        exceeded_gap = {}\n\n        for track in pred_track_ids:\n            if track not in self._curr_gap:\n                self._curr_gap[track] = 0\n\n        for track in self._curr_gap:\n            if track not in pred_track_ids:\n                self._curr_gap[track] += 1\n                logger.debug(\n                    f\"Track {track} has not been seen for {self._curr_gap[track]} frames.\"\n                )\n            else:\n                self._curr_gap[track] = 0\n            if self._curr_gap[track] &gt;= self.max_gap:\n                exceeded_gap[track] = True\n            else:\n                exceeded_gap[track] = False\n\n        for track, gap_exceeded in exceeded_gap.items():\n            if gap_exceeded:\n                logger.debug(\n                    f\"Track {track} has not been seen for {self._curr_gap[track]} frames! Terminating Track...Current number of tracks = {self.n_tracks}.\"\n                )\n                self._queues.pop(track)\n                self._curr_gap.pop(track)\n\n        return exceeded_gap\n</code></pre>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.curr_track","title":"<code>curr_track</code>  <code>property</code> <code>writable</code>","text":"<p>The newest created trajectory in the queue.</p> <p>Returns:</p> Type Description <code>int</code> <p>The latest created trajectory in the queue.</p>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.max_gap","title":"<code>max_gap</code>  <code>property</code> <code>writable</code>","text":"<p>The maximum number of consecutive frames an trajectory can fail to appear before termination.</p> <p>Returns:</p> Type Description <code>int</code> <p>An int representing the maximum number of consecutive frames an trajectory can fail to     appear before termination.</p>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.n_tracks","title":"<code>n_tracks</code>  <code>property</code>","text":"<p>The current number of trajectories in the queue.</p> <p>Returns:</p> Type Description <code>int</code> <p>An int representing the current number of trajectories in the queue.</p>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.tracks","title":"<code>tracks</code>  <code>property</code>","text":"<p>A list of the track ids currently in the queue.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list containing the track ids currently in the queue.</p>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.verbose","title":"<code>verbose</code>  <code>property</code> <code>writable</code>","text":"<p>Indicate whether or not to print outputs along operations. Mostly used for debugging.</p> <p>Returns:</p> Type Description <code>bool</code> <p>A boolean representing whether or not printing is turned on.</p>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.window_size","title":"<code>window_size</code>  <code>property</code> <code>writable</code>","text":"<p>The maximum number of instances allowed in a sub-queue to be compared against.</p> <p>Returns:</p> Type Description <code>int</code> <p>An int representing The maximum number of instances allowed in a     sub-queue to be compared against.</p>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.__init__","title":"<code>__init__(window_size, max_gap=np.inf, verbose=False)</code>","text":"<p>Initialize track queue.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>The number of instances per trajectory allowed in the queue to be compared against.</p> required <code>max_gap</code> <code>int</code> <p>The number of consecutive frames a trajectory can fail to appear in before terminating the track.</p> <code>inf</code> <code>verbose</code> <code>bool</code> <p>Whether to print info during operations.</p> <code>False</code> Source code in <code>dreem/inference/track_queue.py</code> <pre><code>def __init__(\n    self, window_size: int, max_gap: int = np.inf, verbose: bool = False\n) -&gt; None:\n    \"\"\"Initialize track queue.\n\n    Args:\n        window_size: The number of instances per trajectory allowed in the\n            queue to be compared against.\n        max_gap: The number of consecutive frames a trajectory can fail to\n            appear in before terminating the track.\n        verbose: Whether to print info during operations.\n    \"\"\"\n    self._window_size = window_size\n    self._queues = {}\n    self._max_gap = max_gap\n    self._curr_gap = {}\n    if self._max_gap &lt;= self._window_size:\n        self._max_gap = self._window_size\n    self._curr_track = -1\n    self._verbose = verbose\n</code></pre>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.__len__","title":"<code>__len__()</code>","text":"<p>Get length of the queue.</p> <p>Returns:</p> Type Description <code>int</code> <p>The total number of instances in every sub-queue.</p> Source code in <code>dreem/inference/track_queue.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get length of the queue.\n\n    Returns:\n        The total number of instances in every sub-queue.\n    \"\"\"\n    return sum([len(queue) for queue in self._queues.values()])\n</code></pre>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the TrackQueue.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the current state of the queue.</p> Source code in <code>dreem/inference/track_queue.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the TrackQueue.\n\n    Returns:\n        The string representation of the current state of the queue.\n    \"\"\"\n    return (\n        \"TrackQueue(\"\n        f\"window_size={self.window_size}, \"\n        f\"max_gap={self.max_gap}, \"\n        f\"n_tracks={self.n_tracks}, \"\n        f\"curr_track={self.curr_track}, \"\n        f\"queues={[(key, len(queue)) for key, queue in self._queues.items()]}, \"\n        f\"curr_gap:{self._curr_gap}\"\n        \")\"\n    )\n</code></pre>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.add_frame","title":"<code>add_frame(frame)</code>","text":"<p>Add frames to the queue.</p> <p>Each instance from the frame is added to the queue according to its pred_track_id. If the corresponding trajectory is not already in the queue then create a new queue for the track.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>Frame</code> <p>A Frame object containing instances that have already been tracked.</p> required Source code in <code>dreem/inference/track_queue.py</code> <pre><code>def add_frame(self, frame: Frame) -&gt; None:\n    \"\"\"Add frames to the queue.\n\n    Each instance from the frame is added to the queue according to its pred_track_id.\n    If the corresponding trajectory is not already in the queue then create a new queue for the track.\n\n    Args:\n        frame: A Frame object containing instances that have already been tracked.\n    \"\"\"\n    if frame.num_detected == 0:  # only add frames with instances.\n        return\n    vid_id = frame.video_id.item()\n    frame_id = frame.frame_id.item()\n    img_shape = frame.img_shape\n    if isinstance(frame.video, str):\n        vid_name = frame.video\n    else:\n        vid_name = frame.video.filename\n    # traj_score = frame.get_traj_score()  TODO: figure out better way to save trajectory scores.\n    frame_meta = (vid_id, frame_id, vid_name, img_shape.cpu().tolist())\n\n    pred_tracks = []\n    for instance in frame.instances:\n        pred_track_id = instance.pred_track_id.item()\n        pred_tracks.append(pred_track_id)\n\n        if pred_track_id not in self._queues.keys():\n            self._queues[pred_track_id] = deque(\n                [(*frame_meta, instance)], maxlen=self.window_size\n            )  # dumb work around to retain `img_shape`\n            self.curr_track = pred_track_id\n\n            logger.debug(\n                f\"New track = {pred_track_id} on frame {frame_id}! Current number of tracks = {self.n_tracks}\"\n            )\n\n        else:\n            self._queues[pred_track_id].append((*frame_meta, instance))\n    self.increment_gaps(\n        pred_tracks\n    )  # should this be done in the tracker or the queue?\n</code></pre>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.collate_tracks","title":"<code>collate_tracks(context_start_frame_id=None, track_ids=None, device=None)</code>","text":"<p>Merge queues into a single list of Frames containing corresponding instances.</p> <p>Parameters:</p> Name Type Description Default <code>context_start_frame_id</code> <code>int | None</code> <p>The frame_id of the last frame in the context i.e. just before the start of the current batch</p> <code>None</code> <code>track_ids</code> <code>list[int] | None</code> <p>A list of trajectorys to merge. If None, then merge all queues, otherwise filter queues by track_ids then merge.</p> <code>None</code> <code>device</code> <code>str | device | None</code> <p>A str representation of the device the frames should be on after merging since all instances in the queue are kept on the cpu.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A sorted list of Frame objects from which each instance came from, containing the corresponding instances.</p> Source code in <code>dreem/inference/track_queue.py</code> <pre><code>def collate_tracks(\n    self,\n    context_start_frame_id: int | None = None,\n    track_ids: list[int] | None = None,\n    device: str | device | None = None,\n) -&gt; list[Frame]:\n    \"\"\"Merge queues into a single list of Frames containing corresponding instances.\n\n    Args:\n        context_start_frame_id: The frame_id of the last frame in the context i.e. just before the start of the current batch\n        track_ids: A list of trajectorys to merge. If None, then merge all\n            queues, otherwise filter queues by track_ids then merge.\n        device: A str representation of the device the frames should be on after merging\n            since all instances in the queue are kept on the cpu.\n\n    Returns:\n        A sorted list of Frame objects from which each instance came from,\n        containing the corresponding instances.\n    \"\"\"\n    if len(self._queues) == 0:\n        return []\n\n    frames = {}\n\n    tracks_to_convert = (\n        {track: queue for track, queue in self._queues if track in track_ids}\n        if track_ids is not None\n        else self._queues\n    )\n    for track, instances in tracks_to_convert.items():\n        for video_id, frame_id, vid_name, img_shape, instance in instances:\n            # if frame_id &lt; context_start_frame_id - self.window_size:\n            #     continue\n            if (video_id, frame_id) not in frames.keys():\n                frame = Frame(\n                    video_id,\n                    frame_id,\n                    img_shape=img_shape,\n                    instances=[instance],\n                    vid_file=vid_name,\n                )\n                frames[(video_id, frame_id)] = frame\n            else:\n                frames[(video_id, frame_id)].instances.append(instance)\n    return [frames[frame].to(device) for frame in sorted(frames.keys())]\n</code></pre>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.end_tracks","title":"<code>end_tracks(track_id=None)</code>","text":"<p>Terminate tracks and removing them from the queue.</p> <p>Parameters:</p> Name Type Description Default <code>track_id</code> <code>int | None</code> <p>The index of the trajectory to be ended and removed. If <code>None</code> then then every trajectory is removed and the track queue is reset.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the track is successively removed, otherwise False.     (ie if the track doesn't exist in the queue.)</p> Source code in <code>dreem/inference/track_queue.py</code> <pre><code>def end_tracks(self, track_id: int | None = None) -&gt; bool:\n    \"\"\"Terminate tracks and removing them from the queue.\n\n    Args:\n        track_id: The index of the trajectory to be ended and removed.\n            If `None` then then every trajectory is removed and the track queue is reset.\n\n    Returns:\n        True if the track is successively removed, otherwise False.\n            (ie if the track doesn't exist in the queue.)\n    \"\"\"\n    if track_id is None:\n        self._queues = {}\n        self._curr_gap = {}\n        self.curr_track = -1\n    else:\n        try:\n            self._queues.pop(track_id)\n            self._curr_gap.pop(track_id)\n        except KeyError:\n            logger.exception(f\"Track ID {track_id} not found in queue!\")\n            return False\n    return True\n</code></pre>"},{"location":"reference/dreem/inference/track_queue/#dreem.inference.track_queue.TrackQueue.increment_gaps","title":"<code>increment_gaps(pred_track_ids)</code>","text":"<p>Keep track of number of consecutive frames each trajectory has been missing from the queue.</p> <p>If a trajectory has exceeded the <code>max_gap</code> then terminate the track and remove it from the queue.</p> <p>Parameters:</p> Name Type Description Default <code>pred_track_ids</code> <code>list[int]</code> <p>A list of track_ids to be matched against the trajectories in the queue. If a trajectory is in <code>pred_track_ids</code> then its gap counter is reset, otherwise its incremented by 1.</p> required <p>Returns:</p> Type Description <code>dict[int, bool]</code> <p>A dictionary containing the trajectory id and a boolean value representing whether or not it has exceeded the max allowed gap and been terminated.</p> Source code in <code>dreem/inference/track_queue.py</code> <pre><code>def increment_gaps(self, pred_track_ids: list[int]) -&gt; dict[int, bool]:\n    \"\"\"Keep track of number of consecutive frames each trajectory has been missing from the queue.\n\n    If a trajectory has exceeded the `max_gap` then terminate the track and remove it from the queue.\n\n    Args:\n        pred_track_ids: A list of track_ids to be matched against the trajectories in the queue.\n            If a trajectory is in `pred_track_ids` then its gap counter is reset,\n            otherwise its incremented by 1.\n\n    Returns:\n        A dictionary containing the trajectory id and a boolean value representing\n        whether or not it has exceeded the max allowed gap and been\n        terminated.\n    \"\"\"\n    exceeded_gap = {}\n\n    for track in pred_track_ids:\n        if track not in self._curr_gap:\n            self._curr_gap[track] = 0\n\n    for track in self._curr_gap:\n        if track not in pred_track_ids:\n            self._curr_gap[track] += 1\n            logger.debug(\n                f\"Track {track} has not been seen for {self._curr_gap[track]} frames.\"\n            )\n        else:\n            self._curr_gap[track] = 0\n        if self._curr_gap[track] &gt;= self.max_gap:\n            exceeded_gap[track] = True\n        else:\n            exceeded_gap[track] = False\n\n    for track, gap_exceeded in exceeded_gap.items():\n        if gap_exceeded:\n            logger.debug(\n                f\"Track {track} has not been seen for {self._curr_gap[track]} frames! Terminating Track...Current number of tracks = {self.n_tracks}.\"\n            )\n            self._queues.pop(track)\n            self._curr_gap.pop(track)\n\n    return exceeded_gap\n</code></pre>"},{"location":"reference/dreem/inference/tracker/","title":"tracker","text":""},{"location":"reference/dreem/inference/tracker/#dreem.inference.tracker","title":"<code>dreem.inference.tracker</code>","text":"<p>Module containing logic for going from association -&gt; assignment.</p> <p>Classes:</p> Name Description <code>Tracker</code> <p>Tracker class used for assignment based on sliding inference from GTR.</p>"},{"location":"reference/dreem/inference/tracker/#dreem.inference.tracker.Tracker","title":"<code>Tracker</code>","text":"<p>Tracker class used for assignment based on sliding inference from GTR.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Wrap around <code>track</code> to enable <code>tracker()</code> instead of <code>tracker.track()</code>.</p> <code>__init__</code> <p>Initialize a tracker to run inference.</p> <code>__repr__</code> <p>Get string representation of tracker.</p> <code>sliding_inference</code> <p>Perform sliding inference on the input video (instances) with a given window size.</p> <code>track</code> <p>Run tracker and get predicted trajectories.</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>class Tracker:\n    \"\"\"Tracker class used for assignment based on sliding inference from GTR.\"\"\"\n\n    def __init__(\n        self,\n        window_size: int = 8,\n        use_vis_feats: bool = True,\n        overlap_thresh: float = 0.01,\n        mult_thresh: bool = True,\n        decay_time: float | None = None,\n        iou: str | None = None,\n        max_center_dist: float | None = None,\n        persistent_tracking: bool = False,\n        max_gap: int = inf,\n        max_tracks: int = inf,\n        verbose: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize a tracker to run inference.\n\n        Args:\n            window_size: the size of the window used during sliding inference.\n            use_vis_feats: Whether or not to use visual feature extractor.\n            overlap_thresh: the trajectory overlap threshold to be used for assignment.\n            mult_thresh: Whether or not to use weight threshold.\n            decay_time: weight for `decay_time` postprocessing.\n            iou: Either [None, '', \"mult\" or \"max\"]\n                 Whether to use multiplicative or max iou reweighting.\n            max_center_dist: distance threshold for filtering trajectory score matrix.\n            persistent_tracking: whether to keep a buffer across chunks or not.\n            max_gap: the max number of frames a trajectory can be missing before termination.\n            max_tracks: the maximum number of tracks that can be created while tracking.\n                We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.\n            verbose: Whether or not to turn on debug printing after each operation.\n        \"\"\"\n        self.track_queue = TrackQueue(\n            window_size=window_size, max_gap=max_gap, verbose=verbose\n        )\n        self.use_vis_feats = use_vis_feats\n        self.overlap_thresh = overlap_thresh\n        self.mult_thresh = mult_thresh\n        self.decay_time = decay_time\n        self.iou = iou\n        self.max_center_dist = max_center_dist\n        self.persistent_tracking = persistent_tracking\n        self.verbose = verbose\n        self.max_tracks = max_tracks\n\n    def __call__(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; list[Frame]:\n        \"\"\"Wrap around `track` to enable `tracker()` instead of `tracker.track()`.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: list of Frames to run inference on\n\n        Returns:\n            List of frames containing association matrix scores and instances populated with pred track ids.\n        \"\"\"\n        return self.track(model, frames)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get string representation of tracker.\n\n        Returns: the string representation of the tracker\n        \"\"\"\n        return (\n            \"Tracker(\"\n            f\"persistent_tracking={self.persistent_tracking}, \"\n            f\"max_tracks={self.max_tracks}, \"\n            f\"use_vis_feats={self.use_vis_feats}, \"\n            f\"overlap_thresh={self.overlap_thresh}, \"\n            f\"mult_thresh={self.mult_thresh}, \"\n            f\"decay_time={self.decay_time}, \"\n            f\"max_center_dist={self.max_center_dist}, \"\n            f\"verbose={self.verbose}, \"\n            f\"queue={self.track_queue}\"\n        )\n\n    def track(\n        self, model: GlobalTrackingTransformer, frames: list[dict]\n    ) -&gt; list[Frame]:\n        \"\"\"Run tracker and get predicted trajectories.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: data dict to run inference on\n\n        Returns:\n            List of Frames populated with pred track ids and association matrix scores\n        \"\"\"\n        # Extract feature representations with pre-trained encoder.\n\n        _ = model.eval()\n\n        for frame in frames:\n            if frame.has_instances():\n                if not self.use_vis_feats:\n                    for instance in frame.instances:\n                        instance.features = torch.zeros(1, model.d_model)\n                    # frame[\"features\"] = torch.randn(\n                    #     num_frame_instances, self.model.d_model\n                    # )\n\n                # comment out to turn encoder off\n\n                # Assuming the encoder is already trained or train encoder jointly.\n                elif not frame.has_features():\n                    with torch.no_grad():\n                        crops = frame.get_crops()\n                        z = model.visual_encoder(crops)\n\n                        for i, z_i in enumerate(z):\n                            frame.instances[i].features = z_i\n\n        # I feel like this chunk is unnecessary:\n        # reid_features = torch.cat(\n        #     [frame[\"features\"] for frame in instances], dim=0\n        # ).unsqueeze(0)\n\n        # asso_preds, pred_boxes, pred_time, embeddings = self.model(\n        #     instances, reid_features\n        # )\n        instances_pred = self.sliding_inference(model, frames)\n\n        if not self.persistent_tracking:\n            logger.debug(\"Clearing Queue after tracking\")\n            self.track_queue.end_tracks()\n\n        return instances_pred\n\n    def sliding_inference(\n        self, model: GlobalTrackingTransformer, frames: list[Frame]\n    ) -&gt; list[Frame]:\n        \"\"\"Perform sliding inference on the input video (instances) with a given window size.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n        Returns:\n            frames: A list of Frames populated with pred_track_ids and asso_matrices\n        \"\"\"\n        # B: batch size.\n        # D: embedding dimension.\n        # nc: number of channels.\n        # H: height.\n        # W: width.\n\n        for batch_idx, frame_to_track in enumerate(frames):\n            tracked_frames = self.track_queue.collate_tracks(\n                device=frame_to_track.frame_id.device\n            )\n            logger.debug(f\"Current number of tracks is {self.track_queue.n_tracks}\")\n\n            if (\n                self.persistent_tracking and frame_to_track.frame_id == 0\n            ):  # check for new video and clear queue\n                logger.debug(\"New Video! Resetting Track Queue.\")\n                self.track_queue.end_tracks()\n\n            \"\"\"\n            Initialize tracks on first frame where detections appear.\n            \"\"\"\n            if len(self.track_queue) == 0:\n                if frame_to_track.has_instances():\n                    logger.debug(\n                        f\"Initializing track on clip ind {batch_idx} frame {frame_to_track.frame_id.item()}\"\n                    )\n\n                    curr_track_id = 0\n                    for i, instance in enumerate(frames[batch_idx].instances):\n                        instance.pred_track_id = instance.gt_track_id\n                        curr_track_id = max(curr_track_id, instance.pred_track_id)\n\n                    for i, instance in enumerate(frames[batch_idx].instances):\n                        if instance.pred_track_id == -1:\n                            curr_track_id += 1\n                            instance.pred_track_id = curr_track_id\n\n            else:\n                if frame_to_track.has_instances():  # Check if there are detections. If there are skip and increment gap count\n                    frames_to_track = tracked_frames + [\n                        frame_to_track\n                    ]  # better var name?\n\n                    query_ind = len(frames_to_track) - 1\n\n                    frame_to_track = self._run_global_tracker(\n                        model,\n                        frames_to_track,\n                        query_ind=query_ind,\n                    )\n\n            if frame_to_track.has_instances():\n                self.track_queue.add_frame(frame_to_track)\n            else:\n                self.track_queue.increment_gaps([])\n\n            frames[batch_idx] = frame_to_track\n        return frames\n\n    def _run_global_tracker(\n        self, model: GlobalTrackingTransformer, frames: list[Frame], query_ind: int\n    ) -&gt; Frame:\n        \"\"\"Run global tracker performs the actual tracking.\n\n        Uses Hungarian algorithm to do track assigning.\n\n        Args:\n            model: the pretrained GlobalTrackingTransformer to be used for inference\n            frames: A list of Frames containing reid features. See `dreem.io.data_structures` for more info.\n            query_ind: An integer for the query frame within the window of instances.\n\n        Returns:\n            query_frame: The query frame now populated with the pred_track_ids.\n        \"\"\"\n        # *: each item in frames is a frame in the window. So it follows\n        #    that each frame in the window has * detected instances.\n        # D: embedding dimension.\n        # total_instances: number of instances in the window.\n        # N_i: number of detected instances in i-th frame of window.\n        # instances_per_frame: a list of number of instances in each frame of the window.\n        # n_query: number of instances in current/query frame (rightmost frame of the window).\n        # n_nonquery: number of instances in the window excluding the current/query frame.\n        # window_size: length of window.\n        # L: number of decoder blocks.\n        # n_traj: number of existing tracks within the window so far.\n\n        # Number of instances in each frame of the window.\n        # E.g.: instances_per_frame: [4, 5, 6, 7]; window of length 4 with 4 detected instances in the first frame of the window.\n\n        _ = model.eval()\n\n        query_frame = frames[query_ind]\n\n        query_instances = query_frame.instances\n        all_instances = [instance for frame in frames for instance in frame.instances]\n\n        logger.debug(f\"Frame {query_frame.frame_id.item()}\")\n\n        instances_per_frame = [frame.num_detected for frame in frames]\n\n        total_instances, window_size = (\n            sum(instances_per_frame),\n            len(instances_per_frame),\n        )  # Number of instances in window; length of window.\n\n        logger.debug(f\"total_instances: {total_instances}\")\n\n        overlap_thresh = self.overlap_thresh\n        mult_thresh = self.mult_thresh\n        n_traj = self.track_queue.n_tracks\n        curr_track = self.track_queue.curr_track\n\n        reid_features = torch.cat([frame.get_features() for frame in frames], dim=0)[\n            None\n        ]  # (1, total_instances, D=512)\n\n        # (L=1, n_query, total_instances)\n        with torch.no_grad():\n            asso_matrix = model(all_instances, query_instances)\n\n        asso_output = asso_matrix[-1].matrix.split(\n            instances_per_frame, dim=1\n        )  # (window_size, n_query, N_i)\n        asso_output = model_utils.softmax_asso(\n            asso_output\n        )  # (window_size, n_query, N_i)\n        asso_output = torch.cat(asso_output, dim=1).cpu()  # (n_query, total_instances)\n\n        asso_output_df = pd.DataFrame(\n            asso_output.clone().numpy(),\n            columns=[f\"Instance {i}\" for i in range(asso_output.shape[-1])],\n        )\n\n        asso_output_df.index.name = \"Instances\"\n        asso_output_df.columns.name = \"Instances\"\n\n        query_frame.add_traj_score(\"asso_output\", asso_output_df)\n        query_frame.asso_output = asso_matrix[-1]\n\n        n_query = (\n            query_frame.num_detected\n        )  # Number of instances in the current/query frame.\n\n        n_nonquery = (\n            total_instances - n_query\n        )  # Number of instances in the window not including the current/query frame.\n\n        logger.debug(f\"n_nonquery: {n_nonquery}\")\n        logger.debug(f\"n_query: {n_query}\")\n\n        instance_ids = torch.cat(\n            [\n                x.get_pred_track_ids()\n                for batch_idx, x in enumerate(frames)\n                if batch_idx != query_ind\n            ],\n            dim=0,\n        ).view(n_nonquery)  # (n_nonquery,)\n\n        query_inds = [\n            x\n            for x in range(\n                sum(instances_per_frame[:query_ind]),\n                sum(instances_per_frame[: query_ind + 1]),\n            )\n        ]\n\n        nonquery_inds = [i for i in range(total_instances) if i not in query_inds]\n\n        # instead should we do model(nonquery_instances, query_instances)?\n        asso_nonquery = asso_output[:, nonquery_inds]  # (n_query, n_nonquery)\n\n        asso_nonquery_df = pd.DataFrame(\n            asso_nonquery.clone().numpy(), columns=nonquery_inds\n        )\n\n        asso_nonquery_df.index.name = \"Current Frame Instances\"\n        asso_nonquery_df.columns.name = \"Nonquery Instances\"\n\n        query_frame.add_traj_score(\"asso_nonquery\", asso_nonquery_df)\n\n        # get raw bbox coords of prev frame instances from frame.instances_per_frame\n        query_boxes_px = torch.cat(\n            [instance.bbox for instance in query_frame.instances], dim=0\n        )\n        nonquery_boxes_px = torch.cat(\n            [\n                instance.bbox\n                for nonquery_frame in frames\n                if nonquery_frame.frame_id != query_frame.frame_id\n                for instance in nonquery_frame.instances\n            ],\n            dim=0,\n        )\n\n        pred_boxes = model_utils.get_boxes(all_instances)\n        query_boxes = pred_boxes[query_inds]  # n_k x 4\n        nonquery_boxes = pred_boxes[nonquery_inds]  # n_nonquery x 4\n\n        unique_ids = torch.unique(instance_ids)  # (n_nonquery,)\n\n        logger.debug(f\"Instance IDs: {instance_ids}\")\n        logger.debug(f\"unique ids: {unique_ids}\")\n\n        id_inds = (\n            unique_ids[None, :] == instance_ids[:, None]\n        ).float()  # (n_nonquery, n_traj)\n\n        ################################################################################\n\n        # reweighting hyper-parameters for association -&gt; they use 0.9\n\n        traj_score = post_processing.weight_decay_time(\n            asso_nonquery, self.decay_time, reid_features, window_size, query_ind\n        )\n\n        if self.decay_time is not None and self.decay_time &gt; 0:\n            decay_time_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=nonquery_inds\n            )\n\n            decay_time_traj_score.index.name = \"Query Instances\"\n            decay_time_traj_score.columns.name = \"Nonquery Instances\"\n\n            query_frame.add_traj_score(\"decay_time\", decay_time_traj_score)\n        ################################################################################\n\n        # (n_query x n_nonquery) x (n_nonquery x n_traj) --&gt; n_query x n_traj\n        traj_score = torch.mm(traj_score, id_inds.cpu())  # (n_query, n_traj)\n\n        traj_score_df = pd.DataFrame(\n            traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n        )\n\n        traj_score_df.index.name = \"Current Frame Instances\"\n        traj_score_df.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"traj_score\", traj_score_df)\n        ################################################################################\n\n        # with iou -&gt; combining with location in tracker, they set to True\n        # todo -&gt; should also work without pos_embed\n\n        if id_inds.numel() &gt; 0:\n            # this throws error, think we need to slice?\n            # last_inds = (id_inds * torch.arange(\n            #    n_nonquery, device=id_inds.device)[:, None]).max(dim=0)[1] # n_traj\n\n            last_inds = (\n                id_inds * torch.arange(n_nonquery, device=id_inds.device)[:, None]\n            ).max(dim=0)[1]  # M\n\n            last_boxes = nonquery_boxes[last_inds]  # n_traj x 4\n            last_ious = post_processing._pairwise_iou(\n                Boxes(query_boxes), Boxes(last_boxes)\n            )  # n_k x M\n        else:\n            last_ious = traj_score.new_zeros(traj_score.shape)\n\n        traj_score = post_processing.weight_iou(traj_score, self.iou, last_ious.cpu())\n\n        if self.iou is not None and self.iou != \"\":\n            iou_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n            )\n\n            iou_traj_score.index.name = \"Current Frame Instances\"\n            iou_traj_score.columns.name = \"Unique IDs\"\n\n            query_frame.add_traj_score(\"weight_iou\", iou_traj_score)\n        ################################################################################\n\n        # threshold for continuing a tracking or starting a new track -&gt; they use 1.0\n        # todo -&gt; should also work without pos_embed\n        traj_score = post_processing.filter_max_center_dist(\n            traj_score,\n            self.max_center_dist,\n            id_inds,\n            query_boxes_px,\n            nonquery_boxes_px,\n        )\n\n        if self.max_center_dist is not None and self.max_center_dist &gt; 0:\n            max_center_dist_traj_score = pd.DataFrame(\n                traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n            )\n\n            max_center_dist_traj_score.index.name = \"Current Frame Instances\"\n            max_center_dist_traj_score.columns.name = \"Unique IDs\"\n\n            query_frame.add_traj_score(\"max_center_dist\", max_center_dist_traj_score)\n\n        ################################################################################\n        scaled_traj_score = torch.softmax(traj_score, dim=1)\n        scaled_traj_score_df = pd.DataFrame(\n            scaled_traj_score.numpy(), columns=unique_ids.cpu().numpy()\n        )\n        scaled_traj_score_df.index.name = \"Current Frame Instances\"\n        scaled_traj_score_df.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"scaled\", scaled_traj_score_df)\n        ################################################################################\n\n        match_i, match_j = linear_sum_assignment((-traj_score))\n\n        track_ids = instance_ids.new_full((n_query,), -1)\n        for i, j in zip(match_i, match_j):\n            # The overlap threshold is multiplied by the number of times the unique track j is matched to an\n            # instance out of all instances in the window excluding the current frame.\n            #\n            # So if this is correct, the threshold is higher for matching an instance from the current frame\n            # to an existing track if that track has already been matched several times.\n            # So if an existing track in the window has been matched a lot, it gets harder to match to that track.\n            thresh = (\n                overlap_thresh * id_inds[:, j].sum() if mult_thresh else overlap_thresh\n            )\n            if n_traj &gt;= self.max_tracks or traj_score[i, j] &gt; thresh:\n                logger.debug(\n                    f\"Assigning instance {i} to track {j} with id {unique_ids[j]}\"\n                )\n                track_ids[i] = unique_ids[j]\n                query_frame.instances[i].track_score = scaled_traj_score[i, j].item()\n        logger.debug(f\"track_ids: {track_ids}\")\n        for i in range(n_query):\n            if track_ids[i] &lt; 0:\n                logger.debug(f\"Creating new track {curr_track}\")\n                curr_track += 1\n                track_ids[i] = curr_track\n\n        query_frame.matches = (match_i, match_j)\n\n        for instance, track_id in zip(query_frame.instances, track_ids):\n            instance.pred_track_id = track_id\n\n        final_traj_score = pd.DataFrame(\n            traj_score.clone().numpy(), columns=unique_ids.cpu().numpy()\n        )\n        final_traj_score.index.name = \"Current Frame Instances\"\n        final_traj_score.columns.name = \"Unique IDs\"\n\n        query_frame.add_traj_score(\"final\", final_traj_score)\n        return query_frame\n</code></pre>"},{"location":"reference/dreem/inference/tracker/#dreem.inference.tracker.Tracker.__call__","title":"<code>__call__(model, frames)</code>","text":"<p>Wrap around <code>track</code> to enable <code>tracker()</code> instead of <code>tracker.track()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[Frame]</code> <p>list of Frames to run inference on</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>List of frames containing association matrix scores and instances populated with pred track ids.</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def __call__(\n    self, model: GlobalTrackingTransformer, frames: list[Frame]\n) -&gt; list[Frame]:\n    \"\"\"Wrap around `track` to enable `tracker()` instead of `tracker.track()`.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: list of Frames to run inference on\n\n    Returns:\n        List of frames containing association matrix scores and instances populated with pred track ids.\n    \"\"\"\n    return self.track(model, frames)\n</code></pre>"},{"location":"reference/dreem/inference/tracker/#dreem.inference.tracker.Tracker.__init__","title":"<code>__init__(window_size=8, use_vis_feats=True, overlap_thresh=0.01, mult_thresh=True, decay_time=None, iou=None, max_center_dist=None, persistent_tracking=False, max_gap=inf, max_tracks=inf, verbose=False, **kwargs)</code>","text":"<p>Initialize a tracker to run inference.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>the size of the window used during sliding inference.</p> <code>8</code> <code>use_vis_feats</code> <code>bool</code> <p>Whether or not to use visual feature extractor.</p> <code>True</code> <code>overlap_thresh</code> <code>float</code> <p>the trajectory overlap threshold to be used for assignment.</p> <code>0.01</code> <code>mult_thresh</code> <code>bool</code> <p>Whether or not to use weight threshold.</p> <code>True</code> <code>decay_time</code> <code>float | None</code> <p>weight for <code>decay_time</code> postprocessing.</p> <code>None</code> <code>iou</code> <code>str | None</code> <p>Either [None, '', \"mult\" or \"max\"]  Whether to use multiplicative or max iou reweighting.</p> <code>None</code> <code>max_center_dist</code> <code>float | None</code> <p>distance threshold for filtering trajectory score matrix.</p> <code>None</code> <code>persistent_tracking</code> <code>bool</code> <p>whether to keep a buffer across chunks or not.</p> <code>False</code> <code>max_gap</code> <code>int</code> <p>the max number of frames a trajectory can be missing before termination.</p> <code>inf</code> <code>max_tracks</code> <code>int</code> <p>the maximum number of tracks that can be created while tracking. We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.</p> <code>inf</code> <code>verbose</code> <code>bool</code> <p>Whether or not to turn on debug printing after each operation.</p> <code>False</code> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def __init__(\n    self,\n    window_size: int = 8,\n    use_vis_feats: bool = True,\n    overlap_thresh: float = 0.01,\n    mult_thresh: bool = True,\n    decay_time: float | None = None,\n    iou: str | None = None,\n    max_center_dist: float | None = None,\n    persistent_tracking: bool = False,\n    max_gap: int = inf,\n    max_tracks: int = inf,\n    verbose: bool = False,\n    **kwargs,\n):\n    \"\"\"Initialize a tracker to run inference.\n\n    Args:\n        window_size: the size of the window used during sliding inference.\n        use_vis_feats: Whether or not to use visual feature extractor.\n        overlap_thresh: the trajectory overlap threshold to be used for assignment.\n        mult_thresh: Whether or not to use weight threshold.\n        decay_time: weight for `decay_time` postprocessing.\n        iou: Either [None, '', \"mult\" or \"max\"]\n             Whether to use multiplicative or max iou reweighting.\n        max_center_dist: distance threshold for filtering trajectory score matrix.\n        persistent_tracking: whether to keep a buffer across chunks or not.\n        max_gap: the max number of frames a trajectory can be missing before termination.\n        max_tracks: the maximum number of tracks that can be created while tracking.\n            We force the tracker to assign instances to a track instead of creating a new track if max_tracks has been reached.\n        verbose: Whether or not to turn on debug printing after each operation.\n    \"\"\"\n    self.track_queue = TrackQueue(\n        window_size=window_size, max_gap=max_gap, verbose=verbose\n    )\n    self.use_vis_feats = use_vis_feats\n    self.overlap_thresh = overlap_thresh\n    self.mult_thresh = mult_thresh\n    self.decay_time = decay_time\n    self.iou = iou\n    self.max_center_dist = max_center_dist\n    self.persistent_tracking = persistent_tracking\n    self.verbose = verbose\n    self.max_tracks = max_tracks\n</code></pre>"},{"location":"reference/dreem/inference/tracker/#dreem.inference.tracker.Tracker.__repr__","title":"<code>__repr__()</code>","text":"<p>Get string representation of tracker.</p> <p>Returns: the string representation of the tracker</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get string representation of tracker.\n\n    Returns: the string representation of the tracker\n    \"\"\"\n    return (\n        \"Tracker(\"\n        f\"persistent_tracking={self.persistent_tracking}, \"\n        f\"max_tracks={self.max_tracks}, \"\n        f\"use_vis_feats={self.use_vis_feats}, \"\n        f\"overlap_thresh={self.overlap_thresh}, \"\n        f\"mult_thresh={self.mult_thresh}, \"\n        f\"decay_time={self.decay_time}, \"\n        f\"max_center_dist={self.max_center_dist}, \"\n        f\"verbose={self.verbose}, \"\n        f\"queue={self.track_queue}\"\n    )\n</code></pre>"},{"location":"reference/dreem/inference/tracker/#dreem.inference.tracker.Tracker.sliding_inference","title":"<code>sliding_inference(model, frames)</code>","text":"<p>Perform sliding inference on the input video (instances) with a given window size.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[Frame]</code> <p>A list of Frames (See <code>dreem.io.Frame</code> for more info).</p> required <p>Returns:</p> Name Type Description <code>frames</code> <code>list[Frame]</code> <p>A list of Frames populated with pred_track_ids and asso_matrices</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def sliding_inference(\n    self, model: GlobalTrackingTransformer, frames: list[Frame]\n) -&gt; list[Frame]:\n    \"\"\"Perform sliding inference on the input video (instances) with a given window size.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: A list of Frames (See `dreem.io.Frame` for more info).\n\n    Returns:\n        frames: A list of Frames populated with pred_track_ids and asso_matrices\n    \"\"\"\n    # B: batch size.\n    # D: embedding dimension.\n    # nc: number of channels.\n    # H: height.\n    # W: width.\n\n    for batch_idx, frame_to_track in enumerate(frames):\n        tracked_frames = self.track_queue.collate_tracks(\n            device=frame_to_track.frame_id.device\n        )\n        logger.debug(f\"Current number of tracks is {self.track_queue.n_tracks}\")\n\n        if (\n            self.persistent_tracking and frame_to_track.frame_id == 0\n        ):  # check for new video and clear queue\n            logger.debug(\"New Video! Resetting Track Queue.\")\n            self.track_queue.end_tracks()\n\n        \"\"\"\n        Initialize tracks on first frame where detections appear.\n        \"\"\"\n        if len(self.track_queue) == 0:\n            if frame_to_track.has_instances():\n                logger.debug(\n                    f\"Initializing track on clip ind {batch_idx} frame {frame_to_track.frame_id.item()}\"\n                )\n\n                curr_track_id = 0\n                for i, instance in enumerate(frames[batch_idx].instances):\n                    instance.pred_track_id = instance.gt_track_id\n                    curr_track_id = max(curr_track_id, instance.pred_track_id)\n\n                for i, instance in enumerate(frames[batch_idx].instances):\n                    if instance.pred_track_id == -1:\n                        curr_track_id += 1\n                        instance.pred_track_id = curr_track_id\n\n        else:\n            if frame_to_track.has_instances():  # Check if there are detections. If there are skip and increment gap count\n                frames_to_track = tracked_frames + [\n                    frame_to_track\n                ]  # better var name?\n\n                query_ind = len(frames_to_track) - 1\n\n                frame_to_track = self._run_global_tracker(\n                    model,\n                    frames_to_track,\n                    query_ind=query_ind,\n                )\n\n        if frame_to_track.has_instances():\n            self.track_queue.add_frame(frame_to_track)\n        else:\n            self.track_queue.increment_gaps([])\n\n        frames[batch_idx] = frame_to_track\n    return frames\n</code></pre>"},{"location":"reference/dreem/inference/tracker/#dreem.inference.tracker.Tracker.track","title":"<code>track(model, frames)</code>","text":"<p>Run tracker and get predicted trajectories.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GlobalTrackingTransformer</code> <p>the pretrained GlobalTrackingTransformer to be used for inference</p> required <code>frames</code> <code>list[dict]</code> <p>data dict to run inference on</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>List of Frames populated with pred track ids and association matrix scores</p> Source code in <code>dreem/inference/tracker.py</code> <pre><code>def track(\n    self, model: GlobalTrackingTransformer, frames: list[dict]\n) -&gt; list[Frame]:\n    \"\"\"Run tracker and get predicted trajectories.\n\n    Args:\n        model: the pretrained GlobalTrackingTransformer to be used for inference\n        frames: data dict to run inference on\n\n    Returns:\n        List of Frames populated with pred track ids and association matrix scores\n    \"\"\"\n    # Extract feature representations with pre-trained encoder.\n\n    _ = model.eval()\n\n    for frame in frames:\n        if frame.has_instances():\n            if not self.use_vis_feats:\n                for instance in frame.instances:\n                    instance.features = torch.zeros(1, model.d_model)\n                # frame[\"features\"] = torch.randn(\n                #     num_frame_instances, self.model.d_model\n                # )\n\n            # comment out to turn encoder off\n\n            # Assuming the encoder is already trained or train encoder jointly.\n            elif not frame.has_features():\n                with torch.no_grad():\n                    crops = frame.get_crops()\n                    z = model.visual_encoder(crops)\n\n                    for i, z_i in enumerate(z):\n                        frame.instances[i].features = z_i\n\n    # I feel like this chunk is unnecessary:\n    # reid_features = torch.cat(\n    #     [frame[\"features\"] for frame in instances], dim=0\n    # ).unsqueeze(0)\n\n    # asso_preds, pred_boxes, pred_time, embeddings = self.model(\n    #     instances, reid_features\n    # )\n    instances_pred = self.sliding_inference(model, frames)\n\n    if not self.persistent_tracking:\n        logger.debug(\"Clearing Queue after tracking\")\n        self.track_queue.end_tracks()\n\n    return instances_pred\n</code></pre>"},{"location":"reference/dreem/io/","title":"io","text":""},{"location":"reference/dreem/io/#dreem.io","title":"<code>dreem.io</code>","text":"<p>Module containing input/output data structures for easy storage and manipulation.</p> <p>Modules:</p> Name Description <code>association_matrix</code> <p>Module containing class for storing and looking up association scores.</p> <code>config</code> <p>Data structures for handling config parsing.</p> <code>frame</code> <p>Module containing data classes such as Instances and Frames.</p> <code>instance</code> <p>Module containing data class for storing detections.</p> <code>track</code> <p>Module containing data structures for storing instances of the same Track.</p> <code>visualize</code> <p>Helper functions for visualizing tracking.</p> <p>Classes:</p> Name Description <code>AssociationMatrix</code> <p>Class representing the associations between detections.</p> <code>Config</code> <p>Class handling loading components based on config params.</p> <code>Frame</code> <p>Data structure containing metadata for a single frame of a video.</p> <code>Instance</code> <p>Class representing a single instance to be tracked.</p> <code>Track</code> <p>Object for storing instances of the same track.</p>"},{"location":"reference/dreem/io/#dreem.io.AssociationMatrix","title":"<code>AssociationMatrix</code>","text":"<p>Class representing the associations between detections.</p> <p>Attributes:</p> Name Type Description <code>matrix</code> <code>ndarray | Tensor</code> <p>the <code>n_query x n_ref</code> association matrix`</p> <code>ref_instances</code> <code>list[Instance]</code> <p>all instances used to associate against.</p> <code>query_instances</code> <code>list[Instance]</code> <p>query instances that were associated against ref instances.</p> <p>Methods:</p> Name Description <code>__getindices__</code> <p>Get the indices of the instance for lookup.</p> <code>__getitem__</code> <p>Get elements of the association matrix.</p> <code>__repr__</code> <p>Get the string representation of the Association Matrix.</p> <code>get_tracks</code> <p>Group instances by track.</p> <code>numpy</code> <p>Convert association matrix to a numpy array.</p> <code>reduce</code> <p>Aggregate the association matrix by specified dimensions and grouping.</p> <code>to</code> <p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <code>to_dataframe</code> <p>Convert the association matrix to a pandas DataFrame.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>@attrs.define\nclass AssociationMatrix:\n    \"\"\"Class representing the associations between detections.\n\n    Attributes:\n        matrix: the `n_query x n_ref` association matrix`\n        ref_instances: all instances used to associate against.\n        query_instances: query instances that were associated against ref instances.\n    \"\"\"\n\n    matrix: np.ndarray | torch.Tensor\n    ref_instances: list[Instance] = attrs.field()\n    query_instances: list[Instance] = attrs.field()\n\n    @ref_instances.validator\n    def _check_ref_instances(self, attribute, value):\n        \"\"\"Check to ensure that the number of association matrix columns and reference instances match.\n\n        Args:\n            attribute: The ref instances.\n            value: the list of ref instances.\n\n        Raises:\n            ValueError if the number of columns and reference instances don't match.\n        \"\"\"\n        if len(value) != self.matrix.shape[-1]:\n            raise ValueError(\n                (\n                    \"Ref instances must equal number of columns in Association matrix\"\n                    f\"Found {len(value)} ref instances but {self.matrix.shape[-1]} columns.\"\n                )\n            )\n\n    @query_instances.validator\n    def _check_query_instances(self, attribute, value):\n        \"\"\"Check to ensure that the number of association matrix rows and query instances match.\n\n        Args:\n            attribute: The query instances.\n            value: the list of query instances.\n\n        Raises:\n            ValueError if the number of rows and query instances don't match.\n        \"\"\"\n        if len(value) != self.matrix.shape[0]:\n            raise ValueError(\n                (\n                    \"Query instances must equal number of rows in Association matrix\"\n                    f\"Found {len(value)} query instances but {self.matrix.shape[0]} rows.\"\n                )\n            )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get the string representation of the Association Matrix.\n\n        Returns:\n            the string representation of the association matrix.\n        \"\"\"\n        return (\n            f\"AssociationMatrix({self.matrix},\"\n            f\"query_instances={len(self.query_instances)},\"\n            f\"ref_instances={len(self.ref_instances)})\"\n        )\n\n    def numpy(self) -&gt; np.ndarray:\n        \"\"\"Convert association matrix to a numpy array.\n\n        Returns:\n            The association matrix as a numpy array.\n        \"\"\"\n        if isinstance(self.matrix, torch.Tensor):\n            return self.matrix.detach().cpu().numpy()\n        return self.matrix\n\n    def to_dataframe(\n        self, row_labels: str = \"gt\", col_labels: str = \"gt\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Convert the association matrix to a pandas DataFrame.\n\n        Args:\n            row_labels: How to label the rows(queries).\n                If list, then must match # of rows/queries\n                If `\"gt\"` then label by gt track id.\n                If `\"pred\"` then label by pred track id.\n                Otherwise label by the query_instance indices\n            col_labels: How to label the columns(references).\n                If list, then must match # of columns/refs\n                If `\"gt\"` then label by gt track id.\n                If `\"pred\"` then label by pred track id.\n                Otherwise label by the ref_instance indices\n\n        Returns:\n            The association matrix as a pandas dataframe.\n        \"\"\"\n        matrix = self.numpy()\n\n        if not isinstance(row_labels, str):\n            if len(row_labels) == len(self.query_instances):\n                row_inds = row_labels\n\n            else:\n                raise ValueError(\n                    (\n                        \"Mismatched # of rows and labels!\",\n                        f\"Found {len(row_labels)} with {len(self.query_instances)} rows\",\n                    )\n                )\n\n        else:\n            if row_labels == \"gt\":\n                row_inds = [\n                    instance.gt_track_id.item() for instance in self.query_instances\n                ]\n\n            elif row_labels == \"pred\":\n                row_inds = [\n                    instance.pred_track_id.item() for instance in self.query_instances\n                ]\n\n            else:\n                row_inds = np.arange(len(self.query_instances))\n\n        if not isinstance(col_labels, str):\n            if len(col_labels) == len(self.ref_instances):\n                col_inds = col_labels\n\n            else:\n                raise ValueError(\n                    (\n                        \"Mismatched # of columns and labels!\",\n                        f\"Found {len(col_labels)} with {len(self.ref_instances)} columns\",\n                    )\n                )\n\n        else:\n            if col_labels == \"gt\":\n                col_inds = [\n                    instance.gt_track_id.item() for instance in self.ref_instances\n                ]\n\n            elif col_labels == \"pred\":\n                col_inds = [\n                    instance.pred_track_id.item() for instance in self.ref_instances\n                ]\n\n            else:\n                col_inds = np.arange(len(self.ref_instances))\n\n        asso_df = pd.DataFrame(matrix, index=row_inds, columns=col_inds)\n\n        return asso_df\n\n    def reduce(\n        self,\n        row_dims: str = \"instance\",\n        col_dims: str = \"track\",\n        row_grouping: str | None = None,\n        col_grouping: str = \"pred\",\n        reduce_method: callable = np.sum,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Aggregate the association matrix by specified dimensions and grouping.\n\n        Args:\n           row_dims: A str indicating how to what dimensions to reduce rows to.\n                Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).\n           col_dims: A str indicating how to dimensions to reduce rows to.\n                Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)\n           row_grouping: A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".\n           col_grouping: A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".\n           reduce_method: A callable function that operates on numpy matrices and can take an `axis` arg for reducing.\n\n        Returns:\n            The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.\n        \"\"\"\n        n_rows = len(self.query_instances)\n        n_cols = len(self.ref_instances)\n\n        col_tracks = {-1: self.ref_instances}\n        row_tracks = {-1: self.query_instances}\n\n        col_inds = [i for i in range(len(self.ref_instances))]\n        row_inds = [i for i in range(len(self.query_instances))]\n\n        if col_dims == \"track\":\n            col_tracks = self.get_tracks(self.ref_instances, col_grouping)\n            col_inds = list(col_tracks.keys())\n            n_cols = len(col_inds)\n\n        if row_dims == \"track\":\n            row_tracks = self.get_tracks(self.query_instances, row_grouping)\n            row_inds = list(row_tracks.keys())\n            n_rows = len(row_inds)\n\n        reduced_matrix = []\n        for row_track, row_instances in row_tracks.items():\n            for col_track, col_instances in col_tracks.items():\n                asso_matrix = self[row_instances, col_instances]\n\n                if col_dims == \"track\":\n                    asso_matrix = reduce_method(asso_matrix, axis=1)\n\n                if row_dims == \"track\":\n                    asso_matrix = reduce_method(asso_matrix, axis=0)\n\n                reduced_matrix.append(asso_matrix)\n\n        reduced_matrix = np.array(reduced_matrix).reshape(n_cols, n_rows).T\n\n        return pd.DataFrame(reduced_matrix, index=row_inds, columns=col_inds)\n\n    def __getitem__(\n        self, inds: tuple[int | Instance | list[int | Instance]]\n    ) -&gt; np.ndarray:\n        \"\"\"Get elements of the association matrix.\n\n        Args:\n            inds: A tuple of query indices and reference indices.\n                Indices can be either:\n                    A single instance or integer.\n                    A list of instances or integers.\n\n        Returns:\n            An np.ndarray containing the elements requested.\n        \"\"\"\n        query_inst, ref_inst = inds\n\n        query_ind = self.__getindices__(query_inst, self.query_instances)\n        ref_ind = self.__getindices__(ref_inst, self.ref_instances)\n\n        try:\n            return self.numpy()[query_ind[:, None], ref_ind].squeeze()\n        except IndexError as e:\n            logger.exception(f\"Query_insts: {type(query_inst)}\")\n            logger.exception(f\"Query_inds: {query_ind}\")\n            logger.exception(f\"Ref_insts: {type(ref_inst)}\")\n            logger.exception(f\"Ref_ind: {ref_ind}\")\n            logger.exception(e)\n            raise (e)\n\n    def __getindices__(\n        self,\n        instance: Instance | int | np.typing.ArrayLike,\n        instance_lookup: list[Instance],\n    ) -&gt; np.ndarray:\n        \"\"\"Get the indices of the instance for lookup.\n\n        Args:\n            instance: The instance(s) to be retrieved\n                Can either be a single int/instance or a list of int/instances\n            instance_lookup: A list of Instances to be used to retrieve indices\n\n        Returns:\n            A np array of indices.\n        \"\"\"\n        if isinstance(instance, Instance):\n            ind = np.array([instance_lookup.index(instance)])\n\n        elif instance is None:\n            ind = np.arange(len(instance_lookup))\n\n        elif np.isscalar(instance):\n            ind = np.array([instance])\n\n        else:\n            instances = instance\n            if not [isinstance(inst, (Instance, int)) for inst in instance]:\n                raise ValueError(\n                    f\"List of indices must be `int` or `Instance`. Found {set([type(inst) for inst in instance])}\"\n                )\n            ind = np.array(\n                [\n                    (\n                        instance_lookup.index(instance)\n                        if isinstance(instance, Instance)\n                        else instance\n                    )\n                    for instance in instances\n                ]\n            )\n\n        return ind\n\n    def get_tracks(\n        self, instances: list[\"Instance\"], label: str = \"pred\"\n    ) -&gt; dict[int, list[\"Instance\"]]:\n        \"\"\"Group instances by track.\n\n        Args:\n            instances: The list of instances to group\n            label: the track id type to group by. Either `pred` or `gt`.\n\n        Returns:\n            A dictionary of track_id:instances\n        \"\"\"\n        if label == \"pred\":\n            traj_ids = set([instance.pred_track_id.item() for instance in instances])\n            traj = {\n                track_id: [\n                    instance\n                    for instance in instances\n                    if instance.pred_track_id.item() == track_id\n                ]\n                for track_id in traj_ids\n            }\n\n        elif label == \"gt\":\n            traj_ids = set(\n                [instance.gt_track_id.item() for instance in self.ref_instances]\n            )\n            traj = {\n                track_id: [\n                    instance\n                    for instance in self.ref_instances\n                    if instance.gt_track_id.item() == track_id\n                ]\n                for track_id in traj_ids\n            }\n\n        else:\n            raise ValueError(f\"Unsupported label '{label}'. Expected 'pred' or 'gt'.\")\n\n        return traj\n\n    def to(self, map_location: str | torch.device) -&gt; Self:\n        \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n        Args:\n            map_location: Either the device or dtype for the instance to be moved.\n\n        Returns:\n            self: reference to the instance moved to correct device/dtype.\n        \"\"\"\n        self.matrix = self.matrix.to(map_location)\n        self.ref_instances = [\n            instance.to(map_location) for instance in self.ref_instances\n        ]\n        self.query_instances = [\n            instance.to(map_location) for instance in self.query_instances\n        ]\n\n        return self\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.AssociationMatrix.__getindices__","title":"<code>__getindices__(instance, instance_lookup)</code>","text":"<p>Get the indices of the instance for lookup.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>Instance | int | ArrayLike</code> <p>The instance(s) to be retrieved Can either be a single int/instance or a list of int/instances</p> required <code>instance_lookup</code> <code>list[Instance]</code> <p>A list of Instances to be used to retrieve indices</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A np array of indices.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __getindices__(\n    self,\n    instance: Instance | int | np.typing.ArrayLike,\n    instance_lookup: list[Instance],\n) -&gt; np.ndarray:\n    \"\"\"Get the indices of the instance for lookup.\n\n    Args:\n        instance: The instance(s) to be retrieved\n            Can either be a single int/instance or a list of int/instances\n        instance_lookup: A list of Instances to be used to retrieve indices\n\n    Returns:\n        A np array of indices.\n    \"\"\"\n    if isinstance(instance, Instance):\n        ind = np.array([instance_lookup.index(instance)])\n\n    elif instance is None:\n        ind = np.arange(len(instance_lookup))\n\n    elif np.isscalar(instance):\n        ind = np.array([instance])\n\n    else:\n        instances = instance\n        if not [isinstance(inst, (Instance, int)) for inst in instance]:\n            raise ValueError(\n                f\"List of indices must be `int` or `Instance`. Found {set([type(inst) for inst in instance])}\"\n            )\n        ind = np.array(\n            [\n                (\n                    instance_lookup.index(instance)\n                    if isinstance(instance, Instance)\n                    else instance\n                )\n                for instance in instances\n            ]\n        )\n\n    return ind\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.AssociationMatrix.__getitem__","title":"<code>__getitem__(inds)</code>","text":"<p>Get elements of the association matrix.</p> <p>Parameters:</p> Name Type Description Default <code>inds</code> <code>tuple[int | Instance | list[int | Instance]]</code> <p>A tuple of query indices and reference indices. Indices can be either:     A single instance or integer.     A list of instances or integers.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>An np.ndarray containing the elements requested.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __getitem__(\n    self, inds: tuple[int | Instance | list[int | Instance]]\n) -&gt; np.ndarray:\n    \"\"\"Get elements of the association matrix.\n\n    Args:\n        inds: A tuple of query indices and reference indices.\n            Indices can be either:\n                A single instance or integer.\n                A list of instances or integers.\n\n    Returns:\n        An np.ndarray containing the elements requested.\n    \"\"\"\n    query_inst, ref_inst = inds\n\n    query_ind = self.__getindices__(query_inst, self.query_instances)\n    ref_ind = self.__getindices__(ref_inst, self.ref_instances)\n\n    try:\n        return self.numpy()[query_ind[:, None], ref_ind].squeeze()\n    except IndexError as e:\n        logger.exception(f\"Query_insts: {type(query_inst)}\")\n        logger.exception(f\"Query_inds: {query_ind}\")\n        logger.exception(f\"Ref_insts: {type(ref_inst)}\")\n        logger.exception(f\"Ref_ind: {ref_ind}\")\n        logger.exception(e)\n        raise (e)\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.AssociationMatrix.__repr__","title":"<code>__repr__()</code>","text":"<p>Get the string representation of the Association Matrix.</p> <p>Returns:</p> Type Description <code>str</code> <p>the string representation of the association matrix.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get the string representation of the Association Matrix.\n\n    Returns:\n        the string representation of the association matrix.\n    \"\"\"\n    return (\n        f\"AssociationMatrix({self.matrix},\"\n        f\"query_instances={len(self.query_instances)},\"\n        f\"ref_instances={len(self.ref_instances)})\"\n    )\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.AssociationMatrix.get_tracks","title":"<code>get_tracks(instances, label='pred')</code>","text":"<p>Group instances by track.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[Instance]</code> <p>The list of instances to group</p> required <code>label</code> <code>str</code> <p>the track id type to group by. Either <code>pred</code> or <code>gt</code>.</p> <code>'pred'</code> <p>Returns:</p> Type Description <code>dict[int, list[Instance]]</code> <p>A dictionary of track_id:instances</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def get_tracks(\n    self, instances: list[\"Instance\"], label: str = \"pred\"\n) -&gt; dict[int, list[\"Instance\"]]:\n    \"\"\"Group instances by track.\n\n    Args:\n        instances: The list of instances to group\n        label: the track id type to group by. Either `pred` or `gt`.\n\n    Returns:\n        A dictionary of track_id:instances\n    \"\"\"\n    if label == \"pred\":\n        traj_ids = set([instance.pred_track_id.item() for instance in instances])\n        traj = {\n            track_id: [\n                instance\n                for instance in instances\n                if instance.pred_track_id.item() == track_id\n            ]\n            for track_id in traj_ids\n        }\n\n    elif label == \"gt\":\n        traj_ids = set(\n            [instance.gt_track_id.item() for instance in self.ref_instances]\n        )\n        traj = {\n            track_id: [\n                instance\n                for instance in self.ref_instances\n                if instance.gt_track_id.item() == track_id\n            ]\n            for track_id in traj_ids\n        }\n\n    else:\n        raise ValueError(f\"Unsupported label '{label}'. Expected 'pred' or 'gt'.\")\n\n    return traj\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.AssociationMatrix.numpy","title":"<code>numpy()</code>","text":"<p>Convert association matrix to a numpy array.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The association matrix as a numpy array.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def numpy(self) -&gt; np.ndarray:\n    \"\"\"Convert association matrix to a numpy array.\n\n    Returns:\n        The association matrix as a numpy array.\n    \"\"\"\n    if isinstance(self.matrix, torch.Tensor):\n        return self.matrix.detach().cpu().numpy()\n    return self.matrix\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.AssociationMatrix.reduce","title":"<code>reduce(row_dims='instance', col_dims='track', row_grouping=None, col_grouping='pred', reduce_method=np.sum)</code>","text":"<p>Aggregate the association matrix by specified dimensions and grouping.</p> <p>Parameters:</p> Name Type Description Default <code>row_dims</code> <code>str</code> <p>A str indicating how to what dimensions to reduce rows to.   Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).</p> <code>'instance'</code> <code>col_dims</code> <code>str</code> <p>A str indicating how to dimensions to reduce rows to.   Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)</p> <code>'track'</code> <code>row_grouping</code> <code>str | None</code> <p>A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".</p> <code>None</code> <code>col_grouping</code> <code>str</code> <p>A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".</p> <code>'pred'</code> <code>reduce_method</code> <code>callable</code> <p>A callable function that operates on numpy matrices and can take an <code>axis</code> arg for reducing.</p> <code>sum</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def reduce(\n    self,\n    row_dims: str = \"instance\",\n    col_dims: str = \"track\",\n    row_grouping: str | None = None,\n    col_grouping: str = \"pred\",\n    reduce_method: callable = np.sum,\n) -&gt; pd.DataFrame:\n    \"\"\"Aggregate the association matrix by specified dimensions and grouping.\n\n    Args:\n       row_dims: A str indicating how to what dimensions to reduce rows to.\n            Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).\n       col_dims: A str indicating how to dimensions to reduce rows to.\n            Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)\n       row_grouping: A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".\n       col_grouping: A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".\n       reduce_method: A callable function that operates on numpy matrices and can take an `axis` arg for reducing.\n\n    Returns:\n        The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.\n    \"\"\"\n    n_rows = len(self.query_instances)\n    n_cols = len(self.ref_instances)\n\n    col_tracks = {-1: self.ref_instances}\n    row_tracks = {-1: self.query_instances}\n\n    col_inds = [i for i in range(len(self.ref_instances))]\n    row_inds = [i for i in range(len(self.query_instances))]\n\n    if col_dims == \"track\":\n        col_tracks = self.get_tracks(self.ref_instances, col_grouping)\n        col_inds = list(col_tracks.keys())\n        n_cols = len(col_inds)\n\n    if row_dims == \"track\":\n        row_tracks = self.get_tracks(self.query_instances, row_grouping)\n        row_inds = list(row_tracks.keys())\n        n_rows = len(row_inds)\n\n    reduced_matrix = []\n    for row_track, row_instances in row_tracks.items():\n        for col_track, col_instances in col_tracks.items():\n            asso_matrix = self[row_instances, col_instances]\n\n            if col_dims == \"track\":\n                asso_matrix = reduce_method(asso_matrix, axis=1)\n\n            if row_dims == \"track\":\n                asso_matrix = reduce_method(asso_matrix, axis=0)\n\n            reduced_matrix.append(asso_matrix)\n\n    reduced_matrix = np.array(reduced_matrix).reshape(n_cols, n_rows).T\n\n    return pd.DataFrame(reduced_matrix, index=row_inds, columns=col_inds)\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.AssociationMatrix.to","title":"<code>to(map_location)</code>","text":"<p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <p>Parameters:</p> Name Type Description Default <code>map_location</code> <code>str | device</code> <p>Either the device or dtype for the instance to be moved.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>Self</code> <p>reference to the instance moved to correct device/dtype.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def to(self, map_location: str | torch.device) -&gt; Self:\n    \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n    Args:\n        map_location: Either the device or dtype for the instance to be moved.\n\n    Returns:\n        self: reference to the instance moved to correct device/dtype.\n    \"\"\"\n    self.matrix = self.matrix.to(map_location)\n    self.ref_instances = [\n        instance.to(map_location) for instance in self.ref_instances\n    ]\n    self.query_instances = [\n        instance.to(map_location) for instance in self.query_instances\n    ]\n\n    return self\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.AssociationMatrix.to_dataframe","title":"<code>to_dataframe(row_labels='gt', col_labels='gt')</code>","text":"<p>Convert the association matrix to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>row_labels</code> <code>str</code> <p>How to label the rows(queries). If list, then must match # of rows/queries If <code>\"gt\"</code> then label by gt track id. If <code>\"pred\"</code> then label by pred track id. Otherwise label by the query_instance indices</p> <code>'gt'</code> <code>col_labels</code> <code>str</code> <p>How to label the columns(references). If list, then must match # of columns/refs If <code>\"gt\"</code> then label by gt track id. If <code>\"pred\"</code> then label by pred track id. Otherwise label by the ref_instance indices</p> <code>'gt'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The association matrix as a pandas dataframe.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def to_dataframe(\n    self, row_labels: str = \"gt\", col_labels: str = \"gt\"\n) -&gt; pd.DataFrame:\n    \"\"\"Convert the association matrix to a pandas DataFrame.\n\n    Args:\n        row_labels: How to label the rows(queries).\n            If list, then must match # of rows/queries\n            If `\"gt\"` then label by gt track id.\n            If `\"pred\"` then label by pred track id.\n            Otherwise label by the query_instance indices\n        col_labels: How to label the columns(references).\n            If list, then must match # of columns/refs\n            If `\"gt\"` then label by gt track id.\n            If `\"pred\"` then label by pred track id.\n            Otherwise label by the ref_instance indices\n\n    Returns:\n        The association matrix as a pandas dataframe.\n    \"\"\"\n    matrix = self.numpy()\n\n    if not isinstance(row_labels, str):\n        if len(row_labels) == len(self.query_instances):\n            row_inds = row_labels\n\n        else:\n            raise ValueError(\n                (\n                    \"Mismatched # of rows and labels!\",\n                    f\"Found {len(row_labels)} with {len(self.query_instances)} rows\",\n                )\n            )\n\n    else:\n        if row_labels == \"gt\":\n            row_inds = [\n                instance.gt_track_id.item() for instance in self.query_instances\n            ]\n\n        elif row_labels == \"pred\":\n            row_inds = [\n                instance.pred_track_id.item() for instance in self.query_instances\n            ]\n\n        else:\n            row_inds = np.arange(len(self.query_instances))\n\n    if not isinstance(col_labels, str):\n        if len(col_labels) == len(self.ref_instances):\n            col_inds = col_labels\n\n        else:\n            raise ValueError(\n                (\n                    \"Mismatched # of columns and labels!\",\n                    f\"Found {len(col_labels)} with {len(self.ref_instances)} columns\",\n                )\n            )\n\n    else:\n        if col_labels == \"gt\":\n            col_inds = [\n                instance.gt_track_id.item() for instance in self.ref_instances\n            ]\n\n        elif col_labels == \"pred\":\n            col_inds = [\n                instance.pred_track_id.item() for instance in self.ref_instances\n            ]\n\n        else:\n            col_inds = np.arange(len(self.ref_instances))\n\n    asso_df = pd.DataFrame(matrix, index=row_inds, columns=col_inds)\n\n    return asso_df\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config","title":"<code>Config</code>","text":"<p>Class handling loading components based on config params.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class with config from hydra/omega conf.</p> <code>__repr__</code> <p>Object representation of config class.</p> <code>__str__</code> <p>Return a string representation of config class.</p> <code>from_yaml</code> <p>Load config directly from yaml.</p> <code>get</code> <p>Get config item.</p> <code>get_checkpointing</code> <p>Getter for lightning checkpointing callback.</p> <code>get_ctc_paths</code> <p>Get file paths from directory. Only for CTC datasets.</p> <code>get_data_paths</code> <p>Get file paths from directory. Only for SLEAP datasets.</p> <code>get_dataloader</code> <p>Getter for dataloader.</p> <code>get_dataset</code> <p>Getter for datasets.</p> <code>get_early_stopping</code> <p>Getter for lightning early stopping callback.</p> <code>get_gtr_runner</code> <p>Get lightning module for training, validation, and inference.</p> <code>get_logger</code> <p>Getter for logging callback.</p> <code>get_loss</code> <p>Getter for loss functions.</p> <code>get_model</code> <p>Getter for gtr model.</p> <code>get_optimizer</code> <p>Getter for optimizer.</p> <code>get_scheduler</code> <p>Getter for lr scheduler.</p> <code>get_tracker_cfg</code> <p>Getter for tracker config params.</p> <code>get_trainer</code> <p>Getter for the lightning trainer.</p> <code>set_hparams</code> <p>Setter function for overwriting specific hparams.</p> <p>Attributes:</p> Name Type Description <code>data_paths</code> <p>Get data paths.</p> Source code in <code>dreem/io/config.py</code> <pre><code>class Config:\n    \"\"\"Class handling loading components based on config params.\"\"\"\n\n    def __init__(self, cfg: DictConfig, params_cfg: DictConfig | None = None):\n        \"\"\"Initialize the class with config from hydra/omega conf.\n\n        First uses `base_param` file then overwrites with specific `params_config`.\n\n        Args:\n            cfg: The `DictConfig` containing all the hyperparameters needed for\n                training/evaluation.\n            params_cfg: The `DictConfig` containing subset of hyperparameters to override.\n                training/evaluation\n        \"\"\"\n        base_cfg = cfg\n        logger.info(f\"Base Config: {cfg}\")\n\n        if \"params_config\" in cfg:\n            params_cfg = OmegaConf.load(cfg.params_config)\n\n        if params_cfg:\n            logger.info(f\"Overwriting base config with {params_cfg}\")\n            with open_dict(base_cfg):\n                self.cfg = OmegaConf.merge(base_cfg, params_cfg)  # merge configs\n        else:\n            self.cfg = cfg\n\n        OmegaConf.set_struct(self.cfg, False)\n\n        self._vid_files = {}\n\n    def __repr__(self):\n        \"\"\"Object representation of config class.\"\"\"\n        return f\"Config({self.cfg})\"\n\n    def __str__(self):\n        \"\"\"Return a string representation of config class.\"\"\"\n        return f\"Config({self.cfg})\"\n\n    @classmethod\n    def from_yaml(cls, base_cfg_path: str, params_cfg_path: str | None = None) -&gt; None:\n        \"\"\"Load config directly from yaml.\n\n        Args:\n            base_cfg_path: path to base config file.\n            params_cfg_path: path to override params.\n        \"\"\"\n        base_cfg = OmegaConf.load(base_cfg_path)\n        params_cfg = OmegaConf.load(params_cfg_path) if params_cfg_path else None\n        return cls(base_cfg, params_cfg)\n\n    def set_hparams(self, hparams: dict) -&gt; bool:\n        \"\"\"Setter function for overwriting specific hparams.\n\n        Useful for changing 1 or 2 hyperparameters such as dataset.\n\n        Args:\n            hparams: A dict containing the hyperparameter to be overwritten and\n                the value to be changed\n\n        Returns:\n            `True` if config is successfully updated, `False` otherwise\n        \"\"\"\n        if hparams == {} or hparams is None:\n            logger.warning(\"Nothing to update!\")\n            return False\n        for hparam, val in hparams.items():\n            try:\n                OmegaConf.update(self.cfg, hparam, val)\n            except Exception as e:\n                logger.exception(f\"Failed to update {hparam} to {val} due to {e}\")\n                return False\n        return True\n\n    def get(self, key: str, default=None, cfg: dict = None):\n        \"\"\"Get config item.\n\n        Args:\n            key: key of item to return\n            default: default value to return if key is missing.\n            cfg: the config dict from which to retrieve an item\n        \"\"\"\n        if cfg is None:\n            cfg = self.cfg\n\n        param = cfg.get(key, default)\n\n        if isinstance(param, DictConfig):\n            param = OmegaConf.to_container(param, resolve=True)\n\n        return param\n\n    def get_model(self) -&gt; \"GlobalTrackingTransformer\":\n        \"\"\"Getter for gtr model.\n\n        Returns:\n            A global tracking transformer with parameters indicated by cfg\n        \"\"\"\n        from dreem.models import GlobalTrackingTransformer, GTRRunner\n\n        model_params = self.get(\"model\", {})\n\n        ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n        if ckpt_path is not None and len(ckpt_path) &gt; 0:\n            return GTRRunner.load_from_checkpoint(ckpt_path).model\n\n        return GlobalTrackingTransformer(**model_params)\n\n    def get_tracker_cfg(self) -&gt; dict:\n        \"\"\"Getter for tracker config params.\n\n        Returns:\n            A dict containing the init params for `Tracker`.\n        \"\"\"\n        return self.get(\"tracker\", {})\n\n    def get_gtr_runner(self, ckpt_path: str | None = None) -&gt; \"GTRRunner\":\n        \"\"\"Get lightning module for training, validation, and inference.\n\n        Args:\n            ckpt_path: path to checkpoint for override\n\n        Returns:\n            a gtr runner model\n        \"\"\"\n        from dreem.models import GTRRunner\n\n        keys = [\"tracker\", \"optimizer\", \"scheduler\", \"loss\", \"runner\", \"model\"]\n        args = [key + \"_cfg\" if key != \"runner\" else key for key in keys]\n\n        params = {}\n        for key, arg in zip(keys, args):\n            sub_params = self.get(key, {})\n\n            if len(sub_params) == 0:\n                logger.warning(\n                    f\"`{key}` not found in config or is empty. Using defaults for {arg}!\"\n                )\n\n            if key == \"runner\":\n                runner_params = sub_params\n                for k, v in runner_params.items():\n                    params[k] = v\n            else:\n                params[arg] = sub_params\n\n        ckpt_path = params[\"model_cfg\"].pop(\"ckpt_path\", None)\n\n        if ckpt_path is not None and ckpt_path != \"\":\n            model = GTRRunner.load_from_checkpoint(\n                ckpt_path, tracker_cfg=params[\"tracker_cfg\"], **runner_params\n            )\n\n        else:\n            model = GTRRunner(**params)\n\n        return model\n\n    def get_ctc_paths(\n        self, list_dir_path: list[str]\n    ) -&gt; tuple[list[str], list[str], list[str]]:\n        \"\"\"Get file paths from directory. Only for CTC datasets.\n\n        Args:\n            list_dir_path: list of directories to search for labels and videos\n\n        Returns:\n            lists of labels file paths and video file paths\n        \"\"\"\n        gt_list = []\n        raw_img_list = []\n        ctc_track_meta = []\n        # user can specify a list of directories, each of which can contain several subdirectories that come in pairs of (dset_name, dset_name_GT/TRA)\n        for dir_path in list_dir_path:\n            for subdir in os.listdir(dir_path):\n                if subdir.endswith(\"_GT\"):\n                    gt_path = os.path.join(dir_path, subdir, \"TRA\")\n                    raw_img_path = os.path.join(dir_path, subdir.replace(\"_GT\", \"\"))\n                    # get filepaths for all tif files in gt_path\n                    gt_list.append(glob.glob(os.path.join(gt_path, \"*.tif\")))\n                    # get filepaths for all tif files in raw_img_path\n                    raw_img_list.append(glob.glob(os.path.join(raw_img_path, \"*.tif\")))\n                    man_track_file = glob.glob(os.path.join(gt_path, \"man_track.txt\"))\n                    if len(man_track_file) &gt; 0:\n                        ctc_track_meta.append(man_track_file[0])\n                    else:\n                        logger.debug(\n                            f\"No man_track.txt file found in {gt_path}. Continuing...\"\n                        )\n                else:\n                    continue\n\n        return gt_list, raw_img_list, ctc_track_meta\n\n    def get_data_paths(self, mode: str, data_cfg: dict) -&gt; tuple[list[str], list[str]]:\n        \"\"\"Get file paths from directory. Only for SLEAP datasets.\n\n        Args:\n            mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n                train, val, or test params for dataset\n            data_cfg: Config for the dataset containing \"dir\" key.\n\n        Returns:\n            lists of labels file paths and video file paths respectively\n        \"\"\"\n        # hack to get around the fact that for test mode, get_data_paths is called before get_dataset.\n        # also, for train/val mode, data_cfg has had the dir key popped through self.get() called in get_dataset()\n        if mode == \"test\":\n            list_dir_path = data_cfg.get(\"dir\", {}).get(\"path\", None)\n            if list_dir_path is None:\n                raise ValueError(\n                    \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n                )\n            self.labels_suffix = data_cfg.get(\"dir\", {}).get(\"labels_suffix\")\n            self.vid_suffix = data_cfg.get(\"dir\", {}).get(\"vid_suffix\")\n        else:\n            list_dir_path = self.data_dirs\n        if not isinstance(list_dir_path, list):\n            list_dir_path = [list_dir_path]\n\n        if self.labels_suffix == \".slp\":\n            label_files = []\n            vid_files = []\n            for dir_path in list_dir_path:\n                logger.debug(f\"Searching `{dir_path}` directory\")\n                labels_path = f\"{dir_path}/*{self.labels_suffix}\"\n                vid_path = f\"{dir_path}/*{self.vid_suffix}\"\n                logger.debug(f\"Searching for labels matching {labels_path}\")\n                label_files.extend(glob.glob(labels_path))\n                logger.debug(f\"Searching for videos matching {vid_path}\")\n                vid_files.extend(glob.glob(vid_path))\n\n        elif self.labels_suffix == \".tif\":\n            label_files, vid_files, ctc_track_meta = self.get_ctc_paths(list_dir_path)\n\n        logger.debug(f\"Found {len(label_files)} labels and {len(vid_files)} videos\")\n\n        # backdoor to set label files directly in the configs (i.e. bypass dir.path)\n        if data_cfg.get(\"slp_files\", None):\n            logger.debug(\"Overriding label files with user provided list\")\n            slp_files = data_cfg.get(\"slp_files\")\n            if len(slp_files) &gt; 0:\n                label_files = slp_files\n        if data_cfg.get(\"video_files\", None):\n            individual_video_files = data_cfg.get(\"video_files\")\n            if len(individual_video_files) &gt; 0:\n                vid_files = individual_video_files\n        return label_files, vid_files\n\n    def get_dataset(\n        self,\n        mode: str,\n        label_files: list[str] | None = None,\n        vid_files: list[str | list[str]] = None,\n    ) -&gt; \"SleapDataset\" | \"CellTrackingDataset\":\n        \"\"\"Getter for datasets.\n\n        Args:\n            mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n                train, val, or test params for dataset\n            label_files: path to label_files for override\n            vid_files: path to vid_files for override\n\n        Returns:\n            Either a `SleapDataset` or `CellTrackingDataset` with params indicated by cfg\n        \"\"\"\n        from dreem.datasets import CellTrackingDataset, SleapDataset\n\n        dataset_params = self.get(\"dataset\")\n        if dataset_params is None:\n            raise KeyError(\"`dataset` key is missing from cfg!\")\n\n        if mode.lower() == \"train\":\n            dataset_params = self.get(\"train_dataset\", {}, dataset_params)\n        elif mode.lower() == \"val\":\n            dataset_params = self.get(\"val_dataset\", {}, dataset_params)\n        elif mode.lower() == \"test\":\n            dataset_params = self.get(\"test_dataset\", {}, dataset_params)\n        else:\n            raise ValueError(\n                \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n            )\n\n        # input validation\n        self.data_dirs = dataset_params.get(\"dir\", {}).get(\"path\", None)\n        self.labels_suffix = dataset_params.get(\"dir\", {}).get(\"labels_suffix\")\n        self.vid_suffix = dataset_params.get(\"dir\", {}).get(\"vid_suffix\")\n        if self.data_dirs is None:\n            raise ValueError(\n                \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n            )\n        if self.labels_suffix is None or self.vid_suffix is None:\n            raise KeyError(\n                f\"Must provide a labels suffix and vid suffix to search for but found {self.labels_suffix} and {self.vid_suffix}\"\n            )\n\n        # infer dataset type from the user provided suffix\n        if self.labels_suffix == \".slp\":\n            # during training, multiple files can be used at once, so label_files is not passed in\n            # during inference, a single label_files string can be passed in as get_data_paths is\n            # called before get_dataset, hence the check\n            if label_files is None or vid_files is None:\n                label_files, vid_files = self.get_data_paths(mode, dataset_params)\n            dataset_params[\"slp_files\"] = label_files\n            dataset_params[\"video_files\"] = vid_files\n            dataset_params[\"data_dirs\"] = self.data_dirs\n            self.data_paths = (mode, vid_files)\n\n            return SleapDataset(**dataset_params)\n\n        elif self.labels_suffix == \".tif\":\n            # for CTC datasets, pass in a list of gt and raw image directories, eaech of which contain tifs\n            ctc_track_meta = None\n            list_dir_path = self.data_dirs  # don't modify self.data_dirs\n            if not isinstance(list_dir_path, list):\n                list_dir_path = [list_dir_path]\n            if label_files is None or vid_files is None:\n                label_files, vid_files, ctc_track_meta = self.get_ctc_paths(\n                    list_dir_path\n                )\n            dataset_params[\"data_dirs\"] = self.data_dirs\n            # extract filepaths of all raw images and gt images (i.e. labelled masks)\n            dataset_params[\"gt_list\"] = label_files\n            dataset_params[\"raw_img_list\"] = vid_files\n            dataset_params[\"ctc_track_meta\"] = ctc_track_meta\n\n            return CellTrackingDataset(**dataset_params)\n\n        else:\n            raise ValueError(\n                \"Could not resolve dataset type from Config! Only .slp (SLEAP) and .tif (Cell Tracking Challenge) data formats are supported.\"\n            )\n\n    @property\n    def data_paths(self):\n        \"\"\"Get data paths.\"\"\"\n        return self._vid_files\n\n    @data_paths.setter\n    def data_paths(self, paths: tuple[str, list[str]]):\n        \"\"\"Set data paths.\n\n        Args:\n            paths: A tuple containing (mode, vid_files)\n        \"\"\"\n        mode, vid_files = paths\n        self._vid_files[mode] = vid_files\n\n    def get_dataloader(\n        self,\n        dataset: \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\",\n        mode: str,\n    ) -&gt; torch.utils.data.DataLoader:\n        \"\"\"Getter for dataloader.\n\n        Args:\n            dataset: the Sleap or Microscopy Dataset used to initialize the dataloader\n            mode: either [\"train\", \"val\", or \"test\"] indicates which dataset\n                config to use\n\n        Returns:\n            A torch dataloader for `dataset` with parameters configured as specified\n        \"\"\"\n        dataloader_params = self.get(\"dataloader\", {})\n        if mode.lower() == \"train\":\n            dataloader_params = self.get(\"train_dataloader\", {}, dataloader_params)\n        elif mode.lower() == \"val\":\n            dataloader_params = self.get(\"val_dataloader\", {}, dataloader_params)\n        elif mode.lower() == \"test\":\n            dataloader_params = self.get(\"test_dataloader\", {}, dataloader_params)\n        else:\n            raise ValueError(\n                \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n            )\n        if dataloader_params.get(\"num_workers\", 0) &gt; 0:\n            # prevent too many open files error\n            pin_memory = True\n            torch.multiprocessing.set_sharing_strategy(\"file_system\")\n        else:\n            pin_memory = False\n\n        return torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size=1,\n            pin_memory=pin_memory,\n            collate_fn=dataset.no_batching_fn,\n            **dataloader_params,\n        )\n\n    def get_optimizer(self, params: Iterable) -&gt; torch.optim.Optimizer:\n        \"\"\"Getter for optimizer.\n\n        Args:\n            params: iterable of model parameters to optimize or dicts defining\n                parameter groups\n\n        Returns:\n            A torch Optimizer with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_optimizer\n\n        optimizer_params = self.get(\"optimizer\")\n\n        return init_optimizer(params, optimizer_params)\n\n    def get_scheduler(\n        self, optimizer: torch.optim.Optimizer\n    ) -&gt; torch.optim.lr_scheduler.LRScheduler | None:\n        \"\"\"Getter for lr scheduler.\n\n        Args:\n            optimizer: The optimizer to wrap the scheduler around\n\n        Returns:\n            A torch learning rate scheduler with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_scheduler\n\n        lr_scheduler_params = self.get(\"scheduler\")\n\n        if lr_scheduler_params is None:\n            logger.warning(\n                \"`scheduler` key not found in cfg or is empty. No scheduler will be returned!\"\n            )\n            return None\n        return init_scheduler(optimizer, lr_scheduler_params)\n\n    def get_loss(self) -&gt; \"dreem.training.losses.AssoLoss\":\n        \"\"\"Getter for loss functions.\n\n        Returns:\n            An AssoLoss with specified params\n        \"\"\"\n        from dreem.training.losses import AssoLoss\n\n        loss_params = self.get(\"loss\", {})\n\n        if len(loss_params) == 0:\n            logger.warning(\n                \"`loss` key not found in cfg. Using default params for `AssoLoss`\"\n            )\n\n        return AssoLoss(**loss_params)\n\n    def get_logger(self) -&gt; pl.loggers.Logger:\n        \"\"\"Getter for logging callback.\n\n        Returns:\n            A Logger with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_logger\n\n        logger_params = self.get(\"logging\", {})\n        if len(logger_params) == 0:\n            logger.warning(\n                \"`logging` key not found in cfg. No logger will be configured!\"\n            )\n\n        return init_logger(\n            logger_params, OmegaConf.to_container(self.cfg, resolve=True)\n        )\n\n    def get_early_stopping(self) -&gt; pl.callbacks.EarlyStopping:\n        \"\"\"Getter for lightning early stopping callback.\n\n        Returns:\n            A lightning early stopping callback with specified params\n        \"\"\"\n        early_stopping_params = self.get(\"early_stopping\", None)\n\n        if early_stopping_params is None:\n            logger.warning(\n                \"`early_stopping` was not found in cfg or was `null`. Early stopping will not be used!\"\n            )\n            return None\n        elif len(early_stopping_params) == 0:\n            logger.warning(\"`early_stopping` cfg is empty! Using defaults\")\n        return pl.callbacks.EarlyStopping(**early_stopping_params)\n\n    def get_checkpointing(self) -&gt; pl.callbacks.ModelCheckpoint:\n        \"\"\"Getter for lightning checkpointing callback.\n\n        Returns:\n            A lightning checkpointing callback with specified params\n        \"\"\"\n        # convert to dict to enable extracting/removing params\n        checkpoint_params = self.get(\"checkpointing\", {})\n        logging_params = self.get(\"logging\", {})\n\n        dirpath = checkpoint_params.pop(\"dirpath\", None)\n\n        if dirpath is None:\n            dirpath = f\"./models/{self.get('group', '', logging_params)}/{self.get('name', '', logging_params)}\"\n\n        dirpath = Path(dirpath).resolve()\n        if not Path(dirpath).exists():\n            try:\n                Path(dirpath).mkdir(parents=True, exist_ok=True)\n            except OSError as e:\n                logger.exception(\n                    f\"Cannot create a new folder!. Check the permissions to {dirpath}. \\n {e}\"\n                )\n\n        _ = checkpoint_params.pop(\"dirpath\", None)\n        monitor = checkpoint_params.pop(\"monitor\", [\"val_loss\"])\n        checkpointers = []\n\n        logger.info(\n            f\"Saving checkpoints to `{dirpath}` based on the following metrics: {monitor}\"\n        )\n        if len(checkpoint_params) == 0:\n            logger.warning(\n                \"\"\"`checkpointing` key was not found in cfg or was empty!\n                Configuring checkpointing to use default params!\"\"\"\n            )\n\n        for metric in monitor:\n            checkpointer = pl.callbacks.ModelCheckpoint(\n                monitor=metric,\n                dirpath=dirpath,\n                filename=f\"{{epoch}}-{{{metric}}}\",\n                **checkpoint_params,\n            )\n            checkpointer.CHECKPOINT_NAME_LAST = f\"{{epoch}}-final-{{{metric}}}\"\n            checkpointers.append(checkpointer)\n        return checkpointers\n\n    def get_trainer(\n        self,\n        callbacks: list[pl.callbacks.Callback] | None = None,\n        logger: pl.loggers.WandbLogger | None = None,\n        devices: int = 1,\n        accelerator: str = \"auto\",\n    ) -&gt; pl.Trainer:\n        \"\"\"Getter for the lightning trainer.\n\n        Args:\n            callbacks: a list of lightning callbacks preconfigured to be used\n                for training\n            logger: the Wandb logger used for logging during training\n            devices: The number of gpus to be used. 0 means cpu\n            accelerator: either \"gpu\" or \"cpu\" specifies which device to use\n\n        Returns:\n            A lightning Trainer with specified params\n        \"\"\"\n        trainer_params = self.get(\"trainer\", {})\n        profiler = trainer_params.pop(\"profiler\", None)\n        if len(trainer_params) == 0:\n            print(\n                \"`trainer` key was not found in cfg or was empty. Using defaults for `pl.Trainer`!\"\n            )\n\n        if \"accelerator\" not in trainer_params:\n            trainer_params[\"accelerator\"] = accelerator\n        if \"devices\" not in trainer_params:\n            trainer_params[\"devices\"] = devices\n\n        map_profiler = {\n            \"advanced\": pl.profilers.AdvancedProfiler,\n            \"simple\": pl.profilers.SimpleProfiler,\n            \"pytorch\": pl.profilers.PyTorchProfiler,\n            \"passthrough\": pl.profilers.PassThroughProfiler,\n            \"xla\": pl.profilers.XLAProfiler,\n        }\n\n        if profiler:\n            if profiler in map_profiler:\n                profiler = map_profiler[profiler](filename=\"profile\")\n            else:\n                raise ValueError(\n                    f\"Profiler {profiler} not supported! Please use one of {list(map_profiler.keys())}\"\n                )\n\n        return pl.Trainer(\n            callbacks=callbacks,\n            logger=logger,\n            profiler=profiler,\n            **trainer_params,\n        )\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.data_paths","title":"<code>data_paths</code>  <code>property</code> <code>writable</code>","text":"<p>Get data paths.</p>"},{"location":"reference/dreem/io/#dreem.io.Config.__init__","title":"<code>__init__(cfg, params_cfg=None)</code>","text":"<p>Initialize the class with config from hydra/omega conf.</p> <p>First uses <code>base_param</code> file then overwrites with specific <code>params_config</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>The <code>DictConfig</code> containing all the hyperparameters needed for training/evaluation.</p> required <code>params_cfg</code> <code>DictConfig | None</code> <p>The <code>DictConfig</code> containing subset of hyperparameters to override. training/evaluation</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>def __init__(self, cfg: DictConfig, params_cfg: DictConfig | None = None):\n    \"\"\"Initialize the class with config from hydra/omega conf.\n\n    First uses `base_param` file then overwrites with specific `params_config`.\n\n    Args:\n        cfg: The `DictConfig` containing all the hyperparameters needed for\n            training/evaluation.\n        params_cfg: The `DictConfig` containing subset of hyperparameters to override.\n            training/evaluation\n    \"\"\"\n    base_cfg = cfg\n    logger.info(f\"Base Config: {cfg}\")\n\n    if \"params_config\" in cfg:\n        params_cfg = OmegaConf.load(cfg.params_config)\n\n    if params_cfg:\n        logger.info(f\"Overwriting base config with {params_cfg}\")\n        with open_dict(base_cfg):\n            self.cfg = OmegaConf.merge(base_cfg, params_cfg)  # merge configs\n    else:\n        self.cfg = cfg\n\n    OmegaConf.set_struct(self.cfg, False)\n\n    self._vid_files = {}\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.__repr__","title":"<code>__repr__()</code>","text":"<p>Object representation of config class.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def __repr__(self):\n    \"\"\"Object representation of config class.\"\"\"\n    return f\"Config({self.cfg})\"\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of config class.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def __str__(self):\n    \"\"\"Return a string representation of config class.\"\"\"\n    return f\"Config({self.cfg})\"\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.from_yaml","title":"<code>from_yaml(base_cfg_path, params_cfg_path=None)</code>  <code>classmethod</code>","text":"<p>Load config directly from yaml.</p> <p>Parameters:</p> Name Type Description Default <code>base_cfg_path</code> <code>str</code> <p>path to base config file.</p> required <code>params_cfg_path</code> <code>str | None</code> <p>path to override params.</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>@classmethod\ndef from_yaml(cls, base_cfg_path: str, params_cfg_path: str | None = None) -&gt; None:\n    \"\"\"Load config directly from yaml.\n\n    Args:\n        base_cfg_path: path to base config file.\n        params_cfg_path: path to override params.\n    \"\"\"\n    base_cfg = OmegaConf.load(base_cfg_path)\n    params_cfg = OmegaConf.load(params_cfg_path) if params_cfg_path else None\n    return cls(base_cfg, params_cfg)\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.get","title":"<code>get(key, default=None, cfg=None)</code>","text":"<p>Get config item.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key of item to return</p> required <code>default</code> <p>default value to return if key is missing.</p> <code>None</code> <code>cfg</code> <code>dict</code> <p>the config dict from which to retrieve an item</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>def get(self, key: str, default=None, cfg: dict = None):\n    \"\"\"Get config item.\n\n    Args:\n        key: key of item to return\n        default: default value to return if key is missing.\n        cfg: the config dict from which to retrieve an item\n    \"\"\"\n    if cfg is None:\n        cfg = self.cfg\n\n    param = cfg.get(key, default)\n\n    if isinstance(param, DictConfig):\n        param = OmegaConf.to_container(param, resolve=True)\n\n    return param\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.get_checkpointing","title":"<code>get_checkpointing()</code>","text":"<p>Getter for lightning checkpointing callback.</p> <p>Returns:</p> Type Description <code>ModelCheckpoint</code> <p>A lightning checkpointing callback with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_checkpointing(self) -&gt; pl.callbacks.ModelCheckpoint:\n    \"\"\"Getter for lightning checkpointing callback.\n\n    Returns:\n        A lightning checkpointing callback with specified params\n    \"\"\"\n    # convert to dict to enable extracting/removing params\n    checkpoint_params = self.get(\"checkpointing\", {})\n    logging_params = self.get(\"logging\", {})\n\n    dirpath = checkpoint_params.pop(\"dirpath\", None)\n\n    if dirpath is None:\n        dirpath = f\"./models/{self.get('group', '', logging_params)}/{self.get('name', '', logging_params)}\"\n\n    dirpath = Path(dirpath).resolve()\n    if not Path(dirpath).exists():\n        try:\n            Path(dirpath).mkdir(parents=True, exist_ok=True)\n        except OSError as e:\n            logger.exception(\n                f\"Cannot create a new folder!. Check the permissions to {dirpath}. \\n {e}\"\n            )\n\n    _ = checkpoint_params.pop(\"dirpath\", None)\n    monitor = checkpoint_params.pop(\"monitor\", [\"val_loss\"])\n    checkpointers = []\n\n    logger.info(\n        f\"Saving checkpoints to `{dirpath}` based on the following metrics: {monitor}\"\n    )\n    if len(checkpoint_params) == 0:\n        logger.warning(\n            \"\"\"`checkpointing` key was not found in cfg or was empty!\n            Configuring checkpointing to use default params!\"\"\"\n        )\n\n    for metric in monitor:\n        checkpointer = pl.callbacks.ModelCheckpoint(\n            monitor=metric,\n            dirpath=dirpath,\n            filename=f\"{{epoch}}-{{{metric}}}\",\n            **checkpoint_params,\n        )\n        checkpointer.CHECKPOINT_NAME_LAST = f\"{{epoch}}-final-{{{metric}}}\"\n        checkpointers.append(checkpointer)\n    return checkpointers\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.get_ctc_paths","title":"<code>get_ctc_paths(list_dir_path)</code>","text":"<p>Get file paths from directory. Only for CTC datasets.</p> <p>Parameters:</p> Name Type Description Default <code>list_dir_path</code> <code>list[str]</code> <p>list of directories to search for labels and videos</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str], list[str]]</code> <p>lists of labels file paths and video file paths</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_ctc_paths(\n    self, list_dir_path: list[str]\n) -&gt; tuple[list[str], list[str], list[str]]:\n    \"\"\"Get file paths from directory. Only for CTC datasets.\n\n    Args:\n        list_dir_path: list of directories to search for labels and videos\n\n    Returns:\n        lists of labels file paths and video file paths\n    \"\"\"\n    gt_list = []\n    raw_img_list = []\n    ctc_track_meta = []\n    # user can specify a list of directories, each of which can contain several subdirectories that come in pairs of (dset_name, dset_name_GT/TRA)\n    for dir_path in list_dir_path:\n        for subdir in os.listdir(dir_path):\n            if subdir.endswith(\"_GT\"):\n                gt_path = os.path.join(dir_path, subdir, \"TRA\")\n                raw_img_path = os.path.join(dir_path, subdir.replace(\"_GT\", \"\"))\n                # get filepaths for all tif files in gt_path\n                gt_list.append(glob.glob(os.path.join(gt_path, \"*.tif\")))\n                # get filepaths for all tif files in raw_img_path\n                raw_img_list.append(glob.glob(os.path.join(raw_img_path, \"*.tif\")))\n                man_track_file = glob.glob(os.path.join(gt_path, \"man_track.txt\"))\n                if len(man_track_file) &gt; 0:\n                    ctc_track_meta.append(man_track_file[0])\n                else:\n                    logger.debug(\n                        f\"No man_track.txt file found in {gt_path}. Continuing...\"\n                    )\n            else:\n                continue\n\n    return gt_list, raw_img_list, ctc_track_meta\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.get_data_paths","title":"<code>get_data_paths(mode, data_cfg)</code>","text":"<p>Get file paths from directory. Only for SLEAP datasets.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>[None, \"train\", \"test\", \"val\"]. Indicates whether to use train, val, or test params for dataset</p> required <code>data_cfg</code> <code>dict</code> <p>Config for the dataset containing \"dir\" key.</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str]]</code> <p>lists of labels file paths and video file paths respectively</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_data_paths(self, mode: str, data_cfg: dict) -&gt; tuple[list[str], list[str]]:\n    \"\"\"Get file paths from directory. Only for SLEAP datasets.\n\n    Args:\n        mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n            train, val, or test params for dataset\n        data_cfg: Config for the dataset containing \"dir\" key.\n\n    Returns:\n        lists of labels file paths and video file paths respectively\n    \"\"\"\n    # hack to get around the fact that for test mode, get_data_paths is called before get_dataset.\n    # also, for train/val mode, data_cfg has had the dir key popped through self.get() called in get_dataset()\n    if mode == \"test\":\n        list_dir_path = data_cfg.get(\"dir\", {}).get(\"path\", None)\n        if list_dir_path is None:\n            raise ValueError(\n                \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n            )\n        self.labels_suffix = data_cfg.get(\"dir\", {}).get(\"labels_suffix\")\n        self.vid_suffix = data_cfg.get(\"dir\", {}).get(\"vid_suffix\")\n    else:\n        list_dir_path = self.data_dirs\n    if not isinstance(list_dir_path, list):\n        list_dir_path = [list_dir_path]\n\n    if self.labels_suffix == \".slp\":\n        label_files = []\n        vid_files = []\n        for dir_path in list_dir_path:\n            logger.debug(f\"Searching `{dir_path}` directory\")\n            labels_path = f\"{dir_path}/*{self.labels_suffix}\"\n            vid_path = f\"{dir_path}/*{self.vid_suffix}\"\n            logger.debug(f\"Searching for labels matching {labels_path}\")\n            label_files.extend(glob.glob(labels_path))\n            logger.debug(f\"Searching for videos matching {vid_path}\")\n            vid_files.extend(glob.glob(vid_path))\n\n    elif self.labels_suffix == \".tif\":\n        label_files, vid_files, ctc_track_meta = self.get_ctc_paths(list_dir_path)\n\n    logger.debug(f\"Found {len(label_files)} labels and {len(vid_files)} videos\")\n\n    # backdoor to set label files directly in the configs (i.e. bypass dir.path)\n    if data_cfg.get(\"slp_files\", None):\n        logger.debug(\"Overriding label files with user provided list\")\n        slp_files = data_cfg.get(\"slp_files\")\n        if len(slp_files) &gt; 0:\n            label_files = slp_files\n    if data_cfg.get(\"video_files\", None):\n        individual_video_files = data_cfg.get(\"video_files\")\n        if len(individual_video_files) &gt; 0:\n            vid_files = individual_video_files\n    return label_files, vid_files\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.get_dataloader","title":"<code>get_dataloader(dataset, mode)</code>","text":"<p>Getter for dataloader.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>'SleapDataset' | 'MicroscopyDataset' | 'CellTrackingDataset'</code> <p>the Sleap or Microscopy Dataset used to initialize the dataloader</p> required <code>mode</code> <code>str</code> <p>either [\"train\", \"val\", or \"test\"] indicates which dataset config to use</p> required <p>Returns:</p> Type Description <code>DataLoader</code> <p>A torch dataloader for <code>dataset</code> with parameters configured as specified</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_dataloader(\n    self,\n    dataset: \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\",\n    mode: str,\n) -&gt; torch.utils.data.DataLoader:\n    \"\"\"Getter for dataloader.\n\n    Args:\n        dataset: the Sleap or Microscopy Dataset used to initialize the dataloader\n        mode: either [\"train\", \"val\", or \"test\"] indicates which dataset\n            config to use\n\n    Returns:\n        A torch dataloader for `dataset` with parameters configured as specified\n    \"\"\"\n    dataloader_params = self.get(\"dataloader\", {})\n    if mode.lower() == \"train\":\n        dataloader_params = self.get(\"train_dataloader\", {}, dataloader_params)\n    elif mode.lower() == \"val\":\n        dataloader_params = self.get(\"val_dataloader\", {}, dataloader_params)\n    elif mode.lower() == \"test\":\n        dataloader_params = self.get(\"test_dataloader\", {}, dataloader_params)\n    else:\n        raise ValueError(\n            \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n        )\n    if dataloader_params.get(\"num_workers\", 0) &gt; 0:\n        # prevent too many open files error\n        pin_memory = True\n        torch.multiprocessing.set_sharing_strategy(\"file_system\")\n    else:\n        pin_memory = False\n\n    return torch.utils.data.DataLoader(\n        dataset=dataset,\n        batch_size=1,\n        pin_memory=pin_memory,\n        collate_fn=dataset.no_batching_fn,\n        **dataloader_params,\n    )\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.get_dataset","title":"<code>get_dataset(mode, label_files=None, vid_files=None)</code>","text":"<p>Getter for datasets.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>[None, \"train\", \"test\", \"val\"]. Indicates whether to use train, val, or test params for dataset</p> required <code>label_files</code> <code>list[str] | None</code> <p>path to label_files for override</p> <code>None</code> <code>vid_files</code> <code>list[str | list[str]]</code> <p>path to vid_files for override</p> <code>None</code> <p>Returns:</p> Type Description <code>'SleapDataset' | 'CellTrackingDataset'</code> <p>Either a <code>SleapDataset</code> or <code>CellTrackingDataset</code> with params indicated by cfg</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_dataset(\n    self,\n    mode: str,\n    label_files: list[str] | None = None,\n    vid_files: list[str | list[str]] = None,\n) -&gt; \"SleapDataset\" | \"CellTrackingDataset\":\n    \"\"\"Getter for datasets.\n\n    Args:\n        mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n            train, val, or test params for dataset\n        label_files: path to label_files for override\n        vid_files: path to vid_files for override\n\n    Returns:\n        Either a `SleapDataset` or `CellTrackingDataset` with params indicated by cfg\n    \"\"\"\n    from dreem.datasets import CellTrackingDataset, SleapDataset\n\n    dataset_params = self.get(\"dataset\")\n    if dataset_params is None:\n        raise KeyError(\"`dataset` key is missing from cfg!\")\n\n    if mode.lower() == \"train\":\n        dataset_params = self.get(\"train_dataset\", {}, dataset_params)\n    elif mode.lower() == \"val\":\n        dataset_params = self.get(\"val_dataset\", {}, dataset_params)\n    elif mode.lower() == \"test\":\n        dataset_params = self.get(\"test_dataset\", {}, dataset_params)\n    else:\n        raise ValueError(\n            \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n        )\n\n    # input validation\n    self.data_dirs = dataset_params.get(\"dir\", {}).get(\"path\", None)\n    self.labels_suffix = dataset_params.get(\"dir\", {}).get(\"labels_suffix\")\n    self.vid_suffix = dataset_params.get(\"dir\", {}).get(\"vid_suffix\")\n    if self.data_dirs is None:\n        raise ValueError(\n            \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n        )\n    if self.labels_suffix is None or self.vid_suffix is None:\n        raise KeyError(\n            f\"Must provide a labels suffix and vid suffix to search for but found {self.labels_suffix} and {self.vid_suffix}\"\n        )\n\n    # infer dataset type from the user provided suffix\n    if self.labels_suffix == \".slp\":\n        # during training, multiple files can be used at once, so label_files is not passed in\n        # during inference, a single label_files string can be passed in as get_data_paths is\n        # called before get_dataset, hence the check\n        if label_files is None or vid_files is None:\n            label_files, vid_files = self.get_data_paths(mode, dataset_params)\n        dataset_params[\"slp_files\"] = label_files\n        dataset_params[\"video_files\"] = vid_files\n        dataset_params[\"data_dirs\"] = self.data_dirs\n        self.data_paths = (mode, vid_files)\n\n        return SleapDataset(**dataset_params)\n\n    elif self.labels_suffix == \".tif\":\n        # for CTC datasets, pass in a list of gt and raw image directories, eaech of which contain tifs\n        ctc_track_meta = None\n        list_dir_path = self.data_dirs  # don't modify self.data_dirs\n        if not isinstance(list_dir_path, list):\n            list_dir_path = [list_dir_path]\n        if label_files is None or vid_files is None:\n            label_files, vid_files, ctc_track_meta = self.get_ctc_paths(\n                list_dir_path\n            )\n        dataset_params[\"data_dirs\"] = self.data_dirs\n        # extract filepaths of all raw images and gt images (i.e. labelled masks)\n        dataset_params[\"gt_list\"] = label_files\n        dataset_params[\"raw_img_list\"] = vid_files\n        dataset_params[\"ctc_track_meta\"] = ctc_track_meta\n\n        return CellTrackingDataset(**dataset_params)\n\n    else:\n        raise ValueError(\n            \"Could not resolve dataset type from Config! Only .slp (SLEAP) and .tif (Cell Tracking Challenge) data formats are supported.\"\n        )\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.get_early_stopping","title":"<code>get_early_stopping()</code>","text":"<p>Getter for lightning early stopping callback.</p> <p>Returns:</p> Type Description <code>EarlyStopping</code> <p>A lightning early stopping callback with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_early_stopping(self) -&gt; pl.callbacks.EarlyStopping:\n    \"\"\"Getter for lightning early stopping callback.\n\n    Returns:\n        A lightning early stopping callback with specified params\n    \"\"\"\n    early_stopping_params = self.get(\"early_stopping\", None)\n\n    if early_stopping_params is None:\n        logger.warning(\n            \"`early_stopping` was not found in cfg or was `null`. Early stopping will not be used!\"\n        )\n        return None\n    elif len(early_stopping_params) == 0:\n        logger.warning(\"`early_stopping` cfg is empty! Using defaults\")\n    return pl.callbacks.EarlyStopping(**early_stopping_params)\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.get_gtr_runner","title":"<code>get_gtr_runner(ckpt_path=None)</code>","text":"<p>Get lightning module for training, validation, and inference.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_path</code> <code>str | None</code> <p>path to checkpoint for override</p> <code>None</code> <p>Returns:</p> Type Description <code>'GTRRunner'</code> <p>a gtr runner model</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_gtr_runner(self, ckpt_path: str | None = None) -&gt; \"GTRRunner\":\n    \"\"\"Get lightning module for training, validation, and inference.\n\n    Args:\n        ckpt_path: path to checkpoint for override\n\n    Returns:\n        a gtr runner model\n    \"\"\"\n    from dreem.models import GTRRunner\n\n    keys = [\"tracker\", \"optimizer\", \"scheduler\", \"loss\", \"runner\", \"model\"]\n    args = [key + \"_cfg\" if key != \"runner\" else key for key in keys]\n\n    params = {}\n    for key, arg in zip(keys, args):\n        sub_params = self.get(key, {})\n\n        if len(sub_params) == 0:\n            logger.warning(\n                f\"`{key}` not found in config or is empty. Using defaults for {arg}!\"\n            )\n\n        if key == \"runner\":\n            runner_params = sub_params\n            for k, v in runner_params.items():\n                params[k] = v\n        else:\n            params[arg] = sub_params\n\n    ckpt_path = params[\"model_cfg\"].pop(\"ckpt_path\", None)\n\n    if ckpt_path is not None and ckpt_path != \"\":\n        model = GTRRunner.load_from_checkpoint(\n            ckpt_path, tracker_cfg=params[\"tracker_cfg\"], **runner_params\n        )\n\n    else:\n        model = GTRRunner(**params)\n\n    return model\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.get_logger","title":"<code>get_logger()</code>","text":"<p>Getter for logging callback.</p> <p>Returns:</p> Type Description <code>Logger</code> <p>A Logger with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_logger(self) -&gt; pl.loggers.Logger:\n    \"\"\"Getter for logging callback.\n\n    Returns:\n        A Logger with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_logger\n\n    logger_params = self.get(\"logging\", {})\n    if len(logger_params) == 0:\n        logger.warning(\n            \"`logging` key not found in cfg. No logger will be configured!\"\n        )\n\n    return init_logger(\n        logger_params, OmegaConf.to_container(self.cfg, resolve=True)\n    )\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.get_loss","title":"<code>get_loss()</code>","text":"<p>Getter for loss functions.</p> <p>Returns:</p> Type Description <code>'dreem.training.losses.AssoLoss'</code> <p>An AssoLoss with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_loss(self) -&gt; \"dreem.training.losses.AssoLoss\":\n    \"\"\"Getter for loss functions.\n\n    Returns:\n        An AssoLoss with specified params\n    \"\"\"\n    from dreem.training.losses import AssoLoss\n\n    loss_params = self.get(\"loss\", {})\n\n    if len(loss_params) == 0:\n        logger.warning(\n            \"`loss` key not found in cfg. Using default params for `AssoLoss`\"\n        )\n\n    return AssoLoss(**loss_params)\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.get_model","title":"<code>get_model()</code>","text":"<p>Getter for gtr model.</p> <p>Returns:</p> Type Description <code>'GlobalTrackingTransformer'</code> <p>A global tracking transformer with parameters indicated by cfg</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_model(self) -&gt; \"GlobalTrackingTransformer\":\n    \"\"\"Getter for gtr model.\n\n    Returns:\n        A global tracking transformer with parameters indicated by cfg\n    \"\"\"\n    from dreem.models import GlobalTrackingTransformer, GTRRunner\n\n    model_params = self.get(\"model\", {})\n\n    ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n    if ckpt_path is not None and len(ckpt_path) &gt; 0:\n        return GTRRunner.load_from_checkpoint(ckpt_path).model\n\n    return GlobalTrackingTransformer(**model_params)\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.get_optimizer","title":"<code>get_optimizer(params)</code>","text":"<p>Getter for optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable</code> <p>iterable of model parameters to optimize or dicts defining parameter groups</p> required <p>Returns:</p> Type Description <code>Optimizer</code> <p>A torch Optimizer with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_optimizer(self, params: Iterable) -&gt; torch.optim.Optimizer:\n    \"\"\"Getter for optimizer.\n\n    Args:\n        params: iterable of model parameters to optimize or dicts defining\n            parameter groups\n\n    Returns:\n        A torch Optimizer with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_optimizer\n\n    optimizer_params = self.get(\"optimizer\")\n\n    return init_optimizer(params, optimizer_params)\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.get_scheduler","title":"<code>get_scheduler(optimizer)</code>","text":"<p>Getter for lr scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to wrap the scheduler around</p> required <p>Returns:</p> Type Description <code>LRScheduler | None</code> <p>A torch learning rate scheduler with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_scheduler(\n    self, optimizer: torch.optim.Optimizer\n) -&gt; torch.optim.lr_scheduler.LRScheduler | None:\n    \"\"\"Getter for lr scheduler.\n\n    Args:\n        optimizer: The optimizer to wrap the scheduler around\n\n    Returns:\n        A torch learning rate scheduler with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_scheduler\n\n    lr_scheduler_params = self.get(\"scheduler\")\n\n    if lr_scheduler_params is None:\n        logger.warning(\n            \"`scheduler` key not found in cfg or is empty. No scheduler will be returned!\"\n        )\n        return None\n    return init_scheduler(optimizer, lr_scheduler_params)\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.get_tracker_cfg","title":"<code>get_tracker_cfg()</code>","text":"<p>Getter for tracker config params.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict containing the init params for <code>Tracker</code>.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_tracker_cfg(self) -&gt; dict:\n    \"\"\"Getter for tracker config params.\n\n    Returns:\n        A dict containing the init params for `Tracker`.\n    \"\"\"\n    return self.get(\"tracker\", {})\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.get_trainer","title":"<code>get_trainer(callbacks=None, logger=None, devices=1, accelerator='auto')</code>","text":"<p>Getter for the lightning trainer.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[Callback] | None</code> <p>a list of lightning callbacks preconfigured to be used for training</p> <code>None</code> <code>logger</code> <code>WandbLogger | None</code> <p>the Wandb logger used for logging during training</p> <code>None</code> <code>devices</code> <code>int</code> <p>The number of gpus to be used. 0 means cpu</p> <code>1</code> <code>accelerator</code> <code>str</code> <p>either \"gpu\" or \"cpu\" specifies which device to use</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>Trainer</code> <p>A lightning Trainer with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_trainer(\n    self,\n    callbacks: list[pl.callbacks.Callback] | None = None,\n    logger: pl.loggers.WandbLogger | None = None,\n    devices: int = 1,\n    accelerator: str = \"auto\",\n) -&gt; pl.Trainer:\n    \"\"\"Getter for the lightning trainer.\n\n    Args:\n        callbacks: a list of lightning callbacks preconfigured to be used\n            for training\n        logger: the Wandb logger used for logging during training\n        devices: The number of gpus to be used. 0 means cpu\n        accelerator: either \"gpu\" or \"cpu\" specifies which device to use\n\n    Returns:\n        A lightning Trainer with specified params\n    \"\"\"\n    trainer_params = self.get(\"trainer\", {})\n    profiler = trainer_params.pop(\"profiler\", None)\n    if len(trainer_params) == 0:\n        print(\n            \"`trainer` key was not found in cfg or was empty. Using defaults for `pl.Trainer`!\"\n        )\n\n    if \"accelerator\" not in trainer_params:\n        trainer_params[\"accelerator\"] = accelerator\n    if \"devices\" not in trainer_params:\n        trainer_params[\"devices\"] = devices\n\n    map_profiler = {\n        \"advanced\": pl.profilers.AdvancedProfiler,\n        \"simple\": pl.profilers.SimpleProfiler,\n        \"pytorch\": pl.profilers.PyTorchProfiler,\n        \"passthrough\": pl.profilers.PassThroughProfiler,\n        \"xla\": pl.profilers.XLAProfiler,\n    }\n\n    if profiler:\n        if profiler in map_profiler:\n            profiler = map_profiler[profiler](filename=\"profile\")\n        else:\n            raise ValueError(\n                f\"Profiler {profiler} not supported! Please use one of {list(map_profiler.keys())}\"\n            )\n\n    return pl.Trainer(\n        callbacks=callbacks,\n        logger=logger,\n        profiler=profiler,\n        **trainer_params,\n    )\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Config.set_hparams","title":"<code>set_hparams(hparams)</code>","text":"<p>Setter function for overwriting specific hparams.</p> <p>Useful for changing 1 or 2 hyperparameters such as dataset.</p> <p>Parameters:</p> Name Type Description Default <code>hparams</code> <code>dict</code> <p>A dict containing the hyperparameter to be overwritten and the value to be changed</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if config is successfully updated, <code>False</code> otherwise</p> Source code in <code>dreem/io/config.py</code> <pre><code>def set_hparams(self, hparams: dict) -&gt; bool:\n    \"\"\"Setter function for overwriting specific hparams.\n\n    Useful for changing 1 or 2 hyperparameters such as dataset.\n\n    Args:\n        hparams: A dict containing the hyperparameter to be overwritten and\n            the value to be changed\n\n    Returns:\n        `True` if config is successfully updated, `False` otherwise\n    \"\"\"\n    if hparams == {} or hparams is None:\n        logger.warning(\"Nothing to update!\")\n        return False\n    for hparam, val in hparams.items():\n        try:\n            OmegaConf.update(self.cfg, hparam, val)\n        except Exception as e:\n            logger.exception(f\"Failed to update {hparam} to {val} due to {e}\")\n            return False\n    return True\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame","title":"<code>Frame</code>","text":"<p>Data structure containing metadata for a single frame of a video.</p> <p>Attributes:</p> Name Type Description <code>video_id</code> <code>Tensor</code> <p>The video index in the dataset.</p> <code>frame_id</code> <code>Tensor</code> <p>The index of the frame in a video.</p> <code>vid_file</code> <code>Tensor</code> <p>The path to the video the frame is from.</p> <code>img_shape</code> <code>Tensor</code> <p>The shape of the original frame (not the crop).</p> <code>instances</code> <code>list['Instance']</code> <p>A list of Instance objects that appear in the frame.</p> <code>asso_output</code> <code>'AssociationMatrix'</code> <p>The association matrix between instances output directly from the transformer.</p> <code>matches</code> <code>tuple</code> <p>matches from LSA algorithm between the instances and available trajectories during tracking.</p> <code>traj_score</code> <code>tuple</code> <p>Either a dict containing the association matrix between instances and trajectories along postprocessing pipeline or a single association matrix.</p> <code>device</code> <code>str</code> <p>The device the frame should be moved to.</p> <p>Methods:</p> Name Description <code>__attrs_post_init__</code> <p>Handle more intricate default initializations and moving to device.</p> <code>__repr__</code> <p>Return String representation of the Frame.</p> <code>add_traj_score</code> <p>Add trajectory score to dictionary.</p> <code>from_slp</code> <p>Convert <code>sio.LabeledFrame</code> to <code>dreem.io.Frame</code>.</p> <code>get_anchors</code> <p>Get the anchor names of instances in the frame.</p> <code>get_bboxes</code> <p>Get the bounding boxes of all instances in the frame.</p> <code>get_centroids</code> <p>Get the centroids around which each instance's crop was formed.</p> <code>get_crops</code> <p>Get the crops of all instances in the frame.</p> <code>get_features</code> <p>Get the reid feature vectors of all instances in the frame.</p> <code>get_gt_track_ids</code> <p>Get the gt track ids of all instances in the frame.</p> <code>get_pred_track_ids</code> <p>Get the pred track ids of all instances in the frame.</p> <code>get_traj_score</code> <p>Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.</p> <code>has_asso_output</code> <p>Determine whether the frame has an association matrix computed.</p> <code>has_bboxes</code> <p>Check if any of frames instances has a bounding box.</p> <code>has_crops</code> <p>Check if any of frames instances has a crop.</p> <code>has_features</code> <p>Check if any of frames instances has reid features already computed.</p> <code>has_gt_track_ids</code> <p>Check if any of frames instances has a gt track id.</p> <code>has_instances</code> <p>Determine whether there are instances in the frame.</p> <code>has_matches</code> <p>Check whether or not matches have been computed for frame.</p> <code>has_pred_track_ids</code> <p>Check if any of frames instances has a pred track id.</p> <code>has_traj_score</code> <p>Check if any trajectory association matrix has been saved.</p> <code>to</code> <p>Move frame to different device or dtype (See <code>torch.to</code> for more info).</p> <code>to_h5</code> <p>Convert frame to h5py group.</p> <code>to_slp</code> <p>Convert Frame to sleap_io.LabeledFrame object.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>@attrs.define(eq=False)\nclass Frame:\n    \"\"\"Data structure containing metadata for a single frame of a video.\n\n    Attributes:\n        video_id: The video index in the dataset.\n        frame_id: The index of the frame in a video.\n        vid_file: The path to the video the frame is from.\n        img_shape: The shape of the original frame (not the crop).\n        instances: A list of Instance objects that appear in the frame.\n        asso_output: The association matrix between instances\n            output directly from the transformer.\n        matches: matches from LSA algorithm between the instances and\n            available trajectories during tracking.\n        traj_score: Either a dict containing the association matrix\n            between instances and trajectories along postprocessing pipeline\n            or a single association matrix.\n        device: The device the frame should be moved to.\n    \"\"\"\n\n    _video_id: int = attrs.field(alias=\"video_id\", converter=_to_tensor)\n    _frame_id: int = attrs.field(alias=\"frame_id\", converter=_to_tensor)\n    _video: str = attrs.field(alias=\"vid_file\", default=\"\")\n    _img_shape: ArrayLike = attrs.field(\n        alias=\"img_shape\", converter=_to_tensor, factory=list\n    )\n\n    _instances: list[\"Instance\"] = attrs.field(alias=\"instances\", factory=list)\n    _asso_output: \"AssociationMatrix\" | None = attrs.field(\n        alias=\"asso_output\", default=None\n    )\n    _matches: tuple = attrs.field(alias=\"matches\", factory=tuple)\n    _traj_score: dict = attrs.field(alias=\"traj_score\", factory=dict)\n    _device: str | torch.device | None = attrs.field(alias=\"device\", default=None)\n\n    def __attrs_post_init__(self) -&gt; None:\n        \"\"\"Handle more intricate default initializations and moving to device.\"\"\"\n        if len(self.img_shape) == 0:\n            self.img_shape = torch.tensor([0, 0, 0])\n\n        for instance in self.instances:\n            instance.frame = self\n\n        self.to(self.device)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return String representation of the Frame.\n\n        Returns:\n            The string representation of the frame.\n        \"\"\"\n        return (\n            \"Frame(\"\n            f\"video={self._video.filename if isinstance(self._video, sio.Video) else self._video}, \"\n            f\"video_id={self._video_id.item()}, \"\n            f\"frame_id={self._frame_id.item()}, \"\n            f\"img_shape={self._img_shape}, \"\n            f\"num_detected={self.num_detected}, \"\n            f\"asso_output={self._asso_output}, \"\n            f\"traj_score={self._traj_score}, \"\n            f\"matches={self._matches}, \"\n            f\"instances={self._instances}, \"\n            f\"device={self._device}\"\n            \")\"\n        )\n\n    def to(self, map_location: str | torch.device) -&gt; Self:\n        \"\"\"Move frame to different device or dtype (See `torch.to` for more info).\n\n        Args:\n            map_location: A string representing the device to move to.\n\n        Returns:\n            The frame moved to a different device/dtype.\n        \"\"\"\n        self._video_id = self._video_id.to(map_location)\n        self._frame_id = self._frame_id.to(map_location)\n        self._img_shape = self._img_shape.to(map_location)\n\n        if isinstance(self._asso_output, torch.Tensor):\n            self._asso_output = self._asso_output.to(map_location)\n\n        if isinstance(self._matches, torch.Tensor):\n            self._matches = self._matches.to(map_location)\n\n        for key, val in self._traj_score.items():\n            if isinstance(val, torch.Tensor):\n                self._traj_score[key] = val.to(map_location)\n        for instance in self.instances:\n            instance = instance.to(map_location)\n\n        if isinstance(map_location, (str, torch.device)):\n            self._device = map_location\n\n        return self\n\n    @classmethod\n    def from_slp(\n        cls,\n        lf: sio.LabeledFrame,\n        video_id: int = 0,\n        device: str | None = None,\n        **kwargs,\n    ) -&gt; Self:\n        \"\"\"Convert `sio.LabeledFrame` to `dreem.io.Frame`.\n\n        Args:\n            lf: A sio.LabeledFrame object\n\n        Returns:\n            A dreem.io.Frame object\n        \"\"\"\n        from dreem.io.instance import Instance\n\n        img_shape = lf.image.shape\n        if len(img_shape) == 2:\n            img_shape = (1, *img_shape)\n        elif len(img_shape) &gt; 2 and img_shape[-1] &lt;= 3:\n            img_shape = (lf.image.shape[-1], lf.image.shape[0], lf.image.shape[1])\n        return cls(\n            video_id=video_id,\n            frame_id=(\n                lf.frame_idx.astype(np.int32)\n                if isinstance(lf.frame_idx, np.number)\n                else lf.frame_idx\n            ),\n            vid_file=lf.video.filename,\n            img_shape=img_shape,\n            instances=[Instance.from_slp(instance, **kwargs) for instance in lf],\n            device=device,\n        )\n\n    def to_slp(\n        self,\n        track_lookup: dict[int, sio.Track] | None = None,\n        video: sio.Video | None = None,\n    ) -&gt; tuple[sio.LabeledFrame, dict[int, sio.Track]]:\n        \"\"\"Convert Frame to sleap_io.LabeledFrame object.\n\n        Args:\n            track_lookup: A lookup dictionary containing the track_id and sio.Track for persistence\n            video: An sio.Video object used for overriding.\n\n        Returns: A tuple containing a LabeledFrame object with necessary metadata and\n        a lookup dictionary containing the track_id and sio.Track for persistence\n        \"\"\"\n        if track_lookup is None:\n            track_lookup = {}\n\n        slp_instances = []\n        for instance in self.instances:\n            slp_instance, track_lookup = instance.to_slp(track_lookup=track_lookup)\n            slp_instances.append(slp_instance)\n\n        if video is None:\n            video = (\n                self.video\n                if isinstance(self.video, sio.Video)\n                else sio.load_video(self.video)\n            )\n\n        return (\n            sio.LabeledFrame(\n                video=video,\n                frame_idx=self.frame_id.item(),\n                instances=slp_instances,\n            ),\n            track_lookup,\n        )\n\n    def to_h5(\n        self,\n        clip_group: h5py.Group,\n        instance_labels: list | None = None,\n        save: dict[str, bool] | None = None,\n    ) -&gt; h5py.Group:\n        \"\"\"Convert frame to h5py group.\n\n        Args:\n            clip_group: the h5py group representing the clip (e.g batch/video) the frame belongs to\n            instance_labels: the labels used to create instance group names\n            save: whether to save crops, features and embeddings for the instance\n        Returns:\n            An h5py group containing the frame\n        \"\"\"\n        if save is None:\n            save = {\"crop\": False, \"features\": False, \"embeddings\": False}\n        frame_group = clip_group.require_group(f\"frame_{self.frame_id.item()}\")\n        frame_group.attrs.create(\"frame_id\", self.frame_id.item())\n        frame_group.attrs.create(\"vid_id\", self.video_id.item())\n        frame_group.attrs.create(\"vid_name\", self.vid_name)\n\n        frame_group.create_dataset(\n            \"asso_matrix\",\n            data=self.asso_output.numpy() if self.asso_output is not None else [],\n        )\n        asso_group = frame_group.require_group(\"traj_scores\")\n        for key, value in self.get_traj_score().items():\n            asso_group.create_dataset(\n                key, data=value.to_numpy() if value is not None else []\n            )\n\n        if instance_labels is None:\n            instance_labels = self.get_gt_track_ids.cpu().numpy()\n        for instance_label, instance in zip(instance_labels, self.instances):\n            kwargs = {}\n            if save.get(\"crop\", False):\n                kwargs[\"crop\"] = instance.crop.cpu().numpy()\n            if save.get(\"features\", False):\n                kwargs[\"features\"] = instance.features.cpu().numpy()\n            if save.get(\"embeddings\", False):\n                for key, val in instance.get_embedding().items():\n                    kwargs[f\"{key}_emb\"] = val.cpu().numpy()\n            _ = instance.to_h5(frame_group, f\"instance_{instance_label}\", **kwargs)\n\n        return frame_group\n\n    @property\n    def device(self) -&gt; str:\n        \"\"\"The device the frame is on.\n\n        Returns:\n            The string representation of the device the frame is on.\n        \"\"\"\n        return self._device\n\n    @device.setter\n    def device(self, device: str) -&gt; None:\n        \"\"\"Set the device.\n\n        Note: Do not set `frame.device = device` normally. Use `frame.to(device)` instead.\n\n        Args:\n            device: the device the function should be on.\n        \"\"\"\n        self._device = device\n\n    @property\n    def video_id(self) -&gt; torch.Tensor:\n        \"\"\"The index of the video the frame comes from.\n\n        Returns:\n            A tensor containing the video index.\n        \"\"\"\n        return self._video_id\n\n    @video_id.setter\n    def video_id(self, video_id: int) -&gt; None:\n        \"\"\"Set the video index.\n\n        Note: Generally the video_id should be immutable after initialization.\n\n        Args:\n            video_id: an int representing the index of the video that the frame came from.\n        \"\"\"\n        self._video_id = torch.tensor([video_id])\n\n    @property\n    def frame_id(self) -&gt; torch.Tensor:\n        \"\"\"The index of the frame in a full video.\n\n        Returns:\n            A torch tensor containing the index of the frame in the video.\n        \"\"\"\n        return self._frame_id\n\n    @frame_id.setter\n    def frame_id(self, frame_id: int) -&gt; None:\n        \"\"\"Set the frame index of the frame.\n\n        Note: The frame_id should generally be immutable after initialization.\n\n        Args:\n            frame_id: The int index of the frame in the full video.\n        \"\"\"\n        self._frame_id = torch.tensor([frame_id])\n\n    @property\n    def video(self) -&gt; sio.Video | str:\n        \"\"\"Get the video associated with the frame.\n\n        Returns: An sio.Video object representing the video or a placeholder string\n        if it is not possible to create the sio.Video\n        \"\"\"\n        return self._video\n\n    @video.setter\n    def video(self, video: sio.Video | str) -&gt; None:\n        \"\"\"Set the video associated with the frame.\n\n        Note: we try to store the video in an sio.Video object.\n        However, if this is not possible (e.g. incompatible format or missing filepath)\n        then we simply store the string.\n\n        Args:\n            video: sio.Video containing the vid reader or string path to video_file\n        \"\"\"\n        if isinstance(video, sio.Video):\n            self._video = video\n        else:\n            try:\n                self._video = sio.load_video(video)\n            except ValueError:\n                self._video = video\n\n    @property\n    def vid_name(self) -&gt; str:\n        \"\"\"Get the path to the video corresponding to this frame.\n\n        Returns: A str file path corresponding to the frame.\n        \"\"\"\n        if isinstance(self.video, str):\n            return self.video\n        else:\n            return self.video.name\n\n    @property\n    def img_shape(self) -&gt; torch.Tensor:\n        \"\"\"The shape of the pre-cropped frame.\n\n        Returns:\n            A torch tensor containing the shape of the frame. Should generally be (c, h, w)\n        \"\"\"\n        return self._img_shape\n\n    @img_shape.setter\n    def img_shape(self, img_shape: ArrayLike) -&gt; None:\n        \"\"\"Set the shape of the frame image.\n\n        Note: the img_shape should generally be immutable after initialization.\n\n        Args:\n            img_shape: an ArrayLike object containing the shape of the frame image.\n        \"\"\"\n        self._img_shape = _to_tensor(img_shape)\n\n    @property\n    def instances(self) -&gt; list[\"Instance\"]:\n        \"\"\"A list of instances in the frame.\n\n        Returns:\n            The list of instances that appear in the frame.\n        \"\"\"\n        return self._instances\n\n    @instances.setter\n    def instances(self, instances: list[\"Instance\"]) -&gt; None:\n        \"\"\"Set the frame's instance.\n\n        Args:\n            instances: A list of Instances that appear in the frame.\n        \"\"\"\n        for instance in instances:\n            instance.frame = self\n        self._instances = instances\n\n    def has_instances(self) -&gt; bool:\n        \"\"\"Determine whether there are instances in the frame.\n\n        Returns:\n            True if there are instances in the frame, otherwise False.\n        \"\"\"\n        if self.num_detected == 0:\n            return False\n        return True\n\n    @property\n    def num_detected(self) -&gt; int:\n        \"\"\"The number of instances in the frame.\n\n        Returns:\n            the number of instances in the frame.\n        \"\"\"\n        return len(self.instances)\n\n    @property\n    def asso_output(self) -&gt; \"AssociationMatrix\":\n        \"\"\"The association matrix between instances outputted directly by transformer.\n\n        Returns:\n            An arraylike (n_query, n_nonquery) association matrix between instances.\n        \"\"\"\n        return self._asso_output\n\n    def has_asso_output(self) -&gt; bool:\n        \"\"\"Determine whether the frame has an association matrix computed.\n\n        Returns:\n            True if the frame has an association matrix otherwise, False.\n        \"\"\"\n        if self._asso_output is None or len(self._asso_output.matrix) == 0:\n            return False\n        return True\n\n    @asso_output.setter\n    def asso_output(self, asso_output: \"AssociationMatrix\") -&gt; None:\n        \"\"\"Set the association matrix of a frame.\n\n        Args:\n            asso_output: An arraylike (n_query, n_nonquery) association matrix between instances.\n        \"\"\"\n        self._asso_output = asso_output\n\n    @property\n    def matches(self) -&gt; tuple:\n        \"\"\"Matches between frame instances and available trajectories.\n\n        Returns:\n            A tuple containing the instance idx and trajectory idx for the matched instance.\n        \"\"\"\n        return self._matches\n\n    @matches.setter\n    def matches(self, matches: tuple) -&gt; None:\n        \"\"\"Set the frame matches.\n\n        Args:\n            matches: A tuple containing the instance idx and trajectory idx for the matched instance.\n        \"\"\"\n        self._matches = matches\n\n    def has_matches(self) -&gt; bool:\n        \"\"\"Check whether or not matches have been computed for frame.\n\n        Returns:\n            True if frame contains matches otherwise False.\n        \"\"\"\n        if self._matches is not None and len(self._matches) &gt; 0:\n            return True\n        return False\n\n    def get_traj_score(self, key: str | None = None) -&gt; dict | ArrayLike | None:\n        \"\"\"Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.\n\n        Args:\n            key: The key of the trajectory score to be accessed.\n                Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}\n\n        Returns:\n            - dictionary containing all trajectory scores if key is None\n            - trajectory score associated with key\n            - None if the key is not found\n        \"\"\"\n        if key is None:\n            return self._traj_score\n        else:\n            try:\n                return self._traj_score[key]\n            except KeyError as e:\n                logger.exception(f\"Could not access {key} traj_score due to {e}\")\n                return None\n\n    def add_traj_score(self, key: str, traj_score: ArrayLike) -&gt; None:\n        \"\"\"Add trajectory score to dictionary.\n\n        Args:\n            key: key associated with traj score to be used in dictionary\n            traj_score: association matrix between instances and trajectories\n        \"\"\"\n        self._traj_score[key] = traj_score\n\n    def has_traj_score(self) -&gt; bool:\n        \"\"\"Check if any trajectory association matrix has been saved.\n\n        Returns:\n            True there is at least one association matrix otherwise, false.\n        \"\"\"\n        if len(self._traj_score) == 0:\n            return False\n        return True\n\n    def has_gt_track_ids(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a gt track id.\n\n        Returns:\n            True if at least 1 instance has a gt track id otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_gt_track_id() for instance in self.instances])\n        return False\n\n    def get_gt_track_ids(self) -&gt; torch.Tensor:\n        \"\"\"Get the gt track ids of all instances in the frame.\n\n        Returns:\n            an (N,) shaped tensor with the gt track ids of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.gt_track_id for instance in self.instances])\n\n    def has_pred_track_ids(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a pred track id.\n\n        Returns:\n            True if at least 1 instance has a pred track id otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_pred_track_id() for instance in self.instances])\n        return False\n\n    def get_pred_track_ids(self) -&gt; torch.Tensor:\n        \"\"\"Get the pred track ids of all instances in the frame.\n\n        Returns:\n            an (N,) shaped tensor with the pred track ids of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.pred_track_id for instance in self.instances])\n\n    def has_bboxes(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a bounding box.\n\n        Returns:\n            True if at least 1 instance has a bounding box otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_bboxes() for instance in self.instances])\n        return False\n\n    def get_bboxes(self) -&gt; torch.Tensor:\n        \"\"\"Get the bounding boxes of all instances in the frame.\n\n        Returns:\n            an (N,4) shaped tensor with bounding boxes of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.empty(0, 4)\n        return torch.cat([instance.bbox for instance in self.instances], dim=0)\n\n    def has_crops(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a crop.\n\n        Returns:\n            True if at least 1 instance has a crop otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_crop() for instance in self.instances])\n        return False\n\n    def get_crops(self) -&gt; torch.Tensor:\n        \"\"\"Get the crops of all instances in the frame.\n\n        Returns:\n            an (N, C, H, W) shaped tensor with crops of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n\n        return torch.cat([instance.crop for instance in self.instances], dim=0)\n\n    def has_features(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has reid features already computed.\n\n        Returns:\n            True if at least 1 instance have reid features otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_features() for instance in self.instances])\n        return False\n\n    def get_features(self) -&gt; torch.Tensor:\n        \"\"\"Get the reid feature vectors of all instances in the frame.\n\n        Returns:\n            an (N, D) shaped tensor with reid feature vectors of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.features for instance in self.instances], dim=0)\n\n    def get_anchors(self) -&gt; list[str]:\n        \"\"\"Get the anchor names of instances in the frame.\n\n        Returns:\n            A list of anchor names used by the instances to get the crop.\n        \"\"\"\n        return [instance.anchor for instance in self.instances]\n\n    def get_centroids(self) -&gt; tuple[list[str], ArrayLike]:\n        \"\"\"Get the centroids around which each instance's crop was formed.\n\n        Returns:\n            anchors: the node names for the corresponding point\n            points: an n_instances x 2 array containing the centroids\n        \"\"\"\n        anchors = [\n            anchor for instance in self.instances for anchor in instance.centroid.keys()\n        ]\n\n        points = np.array(\n            [\n                point\n                for instance in self.instances\n                for point in instance.centroid.values()\n            ]\n        )\n\n        return (anchors, points)\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.asso_output","title":"<code>asso_output</code>  <code>property</code> <code>writable</code>","text":"<p>The association matrix between instances outputted directly by transformer.</p> <p>Returns:</p> Type Description <code>'AssociationMatrix'</code> <p>An arraylike (n_query, n_nonquery) association matrix between instances.</p>"},{"location":"reference/dreem/io/#dreem.io.Frame.device","title":"<code>device</code>  <code>property</code> <code>writable</code>","text":"<p>The device the frame is on.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the device the frame is on.</p>"},{"location":"reference/dreem/io/#dreem.io.Frame.frame_id","title":"<code>frame_id</code>  <code>property</code> <code>writable</code>","text":"<p>The index of the frame in a full video.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor containing the index of the frame in the video.</p>"},{"location":"reference/dreem/io/#dreem.io.Frame.img_shape","title":"<code>img_shape</code>  <code>property</code> <code>writable</code>","text":"<p>The shape of the pre-cropped frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor containing the shape of the frame. Should generally be (c, h, w)</p>"},{"location":"reference/dreem/io/#dreem.io.Frame.instances","title":"<code>instances</code>  <code>property</code> <code>writable</code>","text":"<p>A list of instances in the frame.</p> <p>Returns:</p> Type Description <code>list['Instance']</code> <p>The list of instances that appear in the frame.</p>"},{"location":"reference/dreem/io/#dreem.io.Frame.matches","title":"<code>matches</code>  <code>property</code> <code>writable</code>","text":"<p>Matches between frame instances and available trajectories.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the instance idx and trajectory idx for the matched instance.</p>"},{"location":"reference/dreem/io/#dreem.io.Frame.num_detected","title":"<code>num_detected</code>  <code>property</code>","text":"<p>The number of instances in the frame.</p> <p>Returns:</p> Type Description <code>int</code> <p>the number of instances in the frame.</p>"},{"location":"reference/dreem/io/#dreem.io.Frame.vid_name","title":"<code>vid_name</code>  <code>property</code>","text":"<p>Get the path to the video corresponding to this frame.</p> <p>Returns: A str file path corresponding to the frame.</p>"},{"location":"reference/dreem/io/#dreem.io.Frame.video","title":"<code>video</code>  <code>property</code> <code>writable</code>","text":"<p>Get the video associated with the frame.</p> <p>Returns: An sio.Video object representing the video or a placeholder string if it is not possible to create the sio.Video</p>"},{"location":"reference/dreem/io/#dreem.io.Frame.video_id","title":"<code>video_id</code>  <code>property</code> <code>writable</code>","text":"<p>The index of the video the frame comes from.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the video index.</p>"},{"location":"reference/dreem/io/#dreem.io.Frame.__attrs_post_init__","title":"<code>__attrs_post_init__()</code>","text":"<p>Handle more intricate default initializations and moving to device.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def __attrs_post_init__(self) -&gt; None:\n    \"\"\"Handle more intricate default initializations and moving to device.\"\"\"\n    if len(self.img_shape) == 0:\n        self.img_shape = torch.tensor([0, 0, 0])\n\n    for instance in self.instances:\n        instance.frame = self\n\n    self.to(self.device)\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.__repr__","title":"<code>__repr__()</code>","text":"<p>Return String representation of the Frame.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return String representation of the Frame.\n\n    Returns:\n        The string representation of the frame.\n    \"\"\"\n    return (\n        \"Frame(\"\n        f\"video={self._video.filename if isinstance(self._video, sio.Video) else self._video}, \"\n        f\"video_id={self._video_id.item()}, \"\n        f\"frame_id={self._frame_id.item()}, \"\n        f\"img_shape={self._img_shape}, \"\n        f\"num_detected={self.num_detected}, \"\n        f\"asso_output={self._asso_output}, \"\n        f\"traj_score={self._traj_score}, \"\n        f\"matches={self._matches}, \"\n        f\"instances={self._instances}, \"\n        f\"device={self._device}\"\n        \")\"\n    )\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.add_traj_score","title":"<code>add_traj_score(key, traj_score)</code>","text":"<p>Add trajectory score to dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key associated with traj score to be used in dictionary</p> required <code>traj_score</code> <code>ArrayLike</code> <p>association matrix between instances and trajectories</p> required Source code in <code>dreem/io/frame.py</code> <pre><code>def add_traj_score(self, key: str, traj_score: ArrayLike) -&gt; None:\n    \"\"\"Add trajectory score to dictionary.\n\n    Args:\n        key: key associated with traj score to be used in dictionary\n        traj_score: association matrix between instances and trajectories\n    \"\"\"\n    self._traj_score[key] = traj_score\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.from_slp","title":"<code>from_slp(lf, video_id=0, device=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Convert <code>sio.LabeledFrame</code> to <code>dreem.io.Frame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>lf</code> <code>LabeledFrame</code> <p>A sio.LabeledFrame object</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A dreem.io.Frame object</p> Source code in <code>dreem/io/frame.py</code> <pre><code>@classmethod\ndef from_slp(\n    cls,\n    lf: sio.LabeledFrame,\n    video_id: int = 0,\n    device: str | None = None,\n    **kwargs,\n) -&gt; Self:\n    \"\"\"Convert `sio.LabeledFrame` to `dreem.io.Frame`.\n\n    Args:\n        lf: A sio.LabeledFrame object\n\n    Returns:\n        A dreem.io.Frame object\n    \"\"\"\n    from dreem.io.instance import Instance\n\n    img_shape = lf.image.shape\n    if len(img_shape) == 2:\n        img_shape = (1, *img_shape)\n    elif len(img_shape) &gt; 2 and img_shape[-1] &lt;= 3:\n        img_shape = (lf.image.shape[-1], lf.image.shape[0], lf.image.shape[1])\n    return cls(\n        video_id=video_id,\n        frame_id=(\n            lf.frame_idx.astype(np.int32)\n            if isinstance(lf.frame_idx, np.number)\n            else lf.frame_idx\n        ),\n        vid_file=lf.video.filename,\n        img_shape=img_shape,\n        instances=[Instance.from_slp(instance, **kwargs) for instance in lf],\n        device=device,\n    )\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.get_anchors","title":"<code>get_anchors()</code>","text":"<p>Get the anchor names of instances in the frame.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of anchor names used by the instances to get the crop.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_anchors(self) -&gt; list[str]:\n    \"\"\"Get the anchor names of instances in the frame.\n\n    Returns:\n        A list of anchor names used by the instances to get the crop.\n    \"\"\"\n    return [instance.anchor for instance in self.instances]\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.get_bboxes","title":"<code>get_bboxes()</code>","text":"<p>Get the bounding boxes of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,4) shaped tensor with bounding boxes of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_bboxes(self) -&gt; torch.Tensor:\n    \"\"\"Get the bounding boxes of all instances in the frame.\n\n    Returns:\n        an (N,4) shaped tensor with bounding boxes of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.empty(0, 4)\n    return torch.cat([instance.bbox for instance in self.instances], dim=0)\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.get_centroids","title":"<code>get_centroids()</code>","text":"<p>Get the centroids around which each instance's crop was formed.</p> <p>Returns:</p> Name Type Description <code>anchors</code> <code>tuple[list[str], ArrayLike]</code> <p>the node names for the corresponding point points: an n_instances x 2 array containing the centroids</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_centroids(self) -&gt; tuple[list[str], ArrayLike]:\n    \"\"\"Get the centroids around which each instance's crop was formed.\n\n    Returns:\n        anchors: the node names for the corresponding point\n        points: an n_instances x 2 array containing the centroids\n    \"\"\"\n    anchors = [\n        anchor for instance in self.instances for anchor in instance.centroid.keys()\n    ]\n\n    points = np.array(\n        [\n            point\n            for instance in self.instances\n            for point in instance.centroid.values()\n        ]\n    )\n\n    return (anchors, points)\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.get_crops","title":"<code>get_crops()</code>","text":"<p>Get the crops of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N, C, H, W) shaped tensor with crops of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_crops(self) -&gt; torch.Tensor:\n    \"\"\"Get the crops of all instances in the frame.\n\n    Returns:\n        an (N, C, H, W) shaped tensor with crops of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n\n    return torch.cat([instance.crop for instance in self.instances], dim=0)\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.get_features","title":"<code>get_features()</code>","text":"<p>Get the reid feature vectors of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N, D) shaped tensor with reid feature vectors of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_features(self) -&gt; torch.Tensor:\n    \"\"\"Get the reid feature vectors of all instances in the frame.\n\n    Returns:\n        an (N, D) shaped tensor with reid feature vectors of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.features for instance in self.instances], dim=0)\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.get_gt_track_ids","title":"<code>get_gt_track_ids()</code>","text":"<p>Get the gt track ids of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,) shaped tensor with the gt track ids of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_gt_track_ids(self) -&gt; torch.Tensor:\n    \"\"\"Get the gt track ids of all instances in the frame.\n\n    Returns:\n        an (N,) shaped tensor with the gt track ids of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.gt_track_id for instance in self.instances])\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.get_pred_track_ids","title":"<code>get_pred_track_ids()</code>","text":"<p>Get the pred track ids of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,) shaped tensor with the pred track ids of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_pred_track_ids(self) -&gt; torch.Tensor:\n    \"\"\"Get the pred track ids of all instances in the frame.\n\n    Returns:\n        an (N,) shaped tensor with the pred track ids of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.pred_track_id for instance in self.instances])\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.get_traj_score","title":"<code>get_traj_score(key=None)</code>","text":"<p>Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str | None</code> <p>The key of the trajectory score to be accessed. Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}</p> <code>None</code> <p>Returns:</p> Type Description <code>dict | ArrayLike | None</code> <ul> <li>dictionary containing all trajectory scores if key is None</li> <li>trajectory score associated with key</li> <li>None if the key is not found</li> </ul> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_traj_score(self, key: str | None = None) -&gt; dict | ArrayLike | None:\n    \"\"\"Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.\n\n    Args:\n        key: The key of the trajectory score to be accessed.\n            Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}\n\n    Returns:\n        - dictionary containing all trajectory scores if key is None\n        - trajectory score associated with key\n        - None if the key is not found\n    \"\"\"\n    if key is None:\n        return self._traj_score\n    else:\n        try:\n            return self._traj_score[key]\n        except KeyError as e:\n            logger.exception(f\"Could not access {key} traj_score due to {e}\")\n            return None\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.has_asso_output","title":"<code>has_asso_output()</code>","text":"<p>Determine whether the frame has an association matrix computed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the frame has an association matrix otherwise, False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_asso_output(self) -&gt; bool:\n    \"\"\"Determine whether the frame has an association matrix computed.\n\n    Returns:\n        True if the frame has an association matrix otherwise, False.\n    \"\"\"\n    if self._asso_output is None or len(self._asso_output.matrix) == 0:\n        return False\n    return True\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.has_bboxes","title":"<code>has_bboxes()</code>","text":"<p>Check if any of frames instances has a bounding box.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a bounding box otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_bboxes(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a bounding box.\n\n    Returns:\n        True if at least 1 instance has a bounding box otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_bboxes() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.has_crops","title":"<code>has_crops()</code>","text":"<p>Check if any of frames instances has a crop.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a crop otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_crops(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a crop.\n\n    Returns:\n        True if at least 1 instance has a crop otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_crop() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.has_features","title":"<code>has_features()</code>","text":"<p>Check if any of frames instances has reid features already computed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance have reid features otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_features(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has reid features already computed.\n\n    Returns:\n        True if at least 1 instance have reid features otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_features() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.has_gt_track_ids","title":"<code>has_gt_track_ids()</code>","text":"<p>Check if any of frames instances has a gt track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a gt track id otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_gt_track_ids(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a gt track id.\n\n    Returns:\n        True if at least 1 instance has a gt track id otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_gt_track_id() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.has_instances","title":"<code>has_instances()</code>","text":"<p>Determine whether there are instances in the frame.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if there are instances in the frame, otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_instances(self) -&gt; bool:\n    \"\"\"Determine whether there are instances in the frame.\n\n    Returns:\n        True if there are instances in the frame, otherwise False.\n    \"\"\"\n    if self.num_detected == 0:\n        return False\n    return True\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.has_matches","title":"<code>has_matches()</code>","text":"<p>Check whether or not matches have been computed for frame.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if frame contains matches otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_matches(self) -&gt; bool:\n    \"\"\"Check whether or not matches have been computed for frame.\n\n    Returns:\n        True if frame contains matches otherwise False.\n    \"\"\"\n    if self._matches is not None and len(self._matches) &gt; 0:\n        return True\n    return False\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.has_pred_track_ids","title":"<code>has_pred_track_ids()</code>","text":"<p>Check if any of frames instances has a pred track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a pred track id otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_pred_track_ids(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a pred track id.\n\n    Returns:\n        True if at least 1 instance has a pred track id otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_pred_track_id() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.has_traj_score","title":"<code>has_traj_score()</code>","text":"<p>Check if any trajectory association matrix has been saved.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True there is at least one association matrix otherwise, false.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_traj_score(self) -&gt; bool:\n    \"\"\"Check if any trajectory association matrix has been saved.\n\n    Returns:\n        True there is at least one association matrix otherwise, false.\n    \"\"\"\n    if len(self._traj_score) == 0:\n        return False\n    return True\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.to","title":"<code>to(map_location)</code>","text":"<p>Move frame to different device or dtype (See <code>torch.to</code> for more info).</p> <p>Parameters:</p> Name Type Description Default <code>map_location</code> <code>str | device</code> <p>A string representing the device to move to.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The frame moved to a different device/dtype.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to(self, map_location: str | torch.device) -&gt; Self:\n    \"\"\"Move frame to different device or dtype (See `torch.to` for more info).\n\n    Args:\n        map_location: A string representing the device to move to.\n\n    Returns:\n        The frame moved to a different device/dtype.\n    \"\"\"\n    self._video_id = self._video_id.to(map_location)\n    self._frame_id = self._frame_id.to(map_location)\n    self._img_shape = self._img_shape.to(map_location)\n\n    if isinstance(self._asso_output, torch.Tensor):\n        self._asso_output = self._asso_output.to(map_location)\n\n    if isinstance(self._matches, torch.Tensor):\n        self._matches = self._matches.to(map_location)\n\n    for key, val in self._traj_score.items():\n        if isinstance(val, torch.Tensor):\n            self._traj_score[key] = val.to(map_location)\n    for instance in self.instances:\n        instance = instance.to(map_location)\n\n    if isinstance(map_location, (str, torch.device)):\n        self._device = map_location\n\n    return self\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.to_h5","title":"<code>to_h5(clip_group, instance_labels=None, save=None)</code>","text":"<p>Convert frame to h5py group.</p> <p>Parameters:</p> Name Type Description Default <code>clip_group</code> <code>Group</code> <p>the h5py group representing the clip (e.g batch/video) the frame belongs to</p> required <code>instance_labels</code> <code>list | None</code> <p>the labels used to create instance group names</p> <code>None</code> <code>save</code> <code>dict[str, bool] | None</code> <p>whether to save crops, features and embeddings for the instance</p> <code>None</code> <p>Returns:     An h5py group containing the frame</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to_h5(\n    self,\n    clip_group: h5py.Group,\n    instance_labels: list | None = None,\n    save: dict[str, bool] | None = None,\n) -&gt; h5py.Group:\n    \"\"\"Convert frame to h5py group.\n\n    Args:\n        clip_group: the h5py group representing the clip (e.g batch/video) the frame belongs to\n        instance_labels: the labels used to create instance group names\n        save: whether to save crops, features and embeddings for the instance\n    Returns:\n        An h5py group containing the frame\n    \"\"\"\n    if save is None:\n        save = {\"crop\": False, \"features\": False, \"embeddings\": False}\n    frame_group = clip_group.require_group(f\"frame_{self.frame_id.item()}\")\n    frame_group.attrs.create(\"frame_id\", self.frame_id.item())\n    frame_group.attrs.create(\"vid_id\", self.video_id.item())\n    frame_group.attrs.create(\"vid_name\", self.vid_name)\n\n    frame_group.create_dataset(\n        \"asso_matrix\",\n        data=self.asso_output.numpy() if self.asso_output is not None else [],\n    )\n    asso_group = frame_group.require_group(\"traj_scores\")\n    for key, value in self.get_traj_score().items():\n        asso_group.create_dataset(\n            key, data=value.to_numpy() if value is not None else []\n        )\n\n    if instance_labels is None:\n        instance_labels = self.get_gt_track_ids.cpu().numpy()\n    for instance_label, instance in zip(instance_labels, self.instances):\n        kwargs = {}\n        if save.get(\"crop\", False):\n            kwargs[\"crop\"] = instance.crop.cpu().numpy()\n        if save.get(\"features\", False):\n            kwargs[\"features\"] = instance.features.cpu().numpy()\n        if save.get(\"embeddings\", False):\n            for key, val in instance.get_embedding().items():\n                kwargs[f\"{key}_emb\"] = val.cpu().numpy()\n        _ = instance.to_h5(frame_group, f\"instance_{instance_label}\", **kwargs)\n\n    return frame_group\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Frame.to_slp","title":"<code>to_slp(track_lookup=None, video=None)</code>","text":"<p>Convert Frame to sleap_io.LabeledFrame object.</p> <p>Parameters:</p> Name Type Description Default <code>track_lookup</code> <code>dict[int, Track] | None</code> <p>A lookup dictionary containing the track_id and sio.Track for persistence</p> <code>None</code> <code>video</code> <code>Video | None</code> <p>An sio.Video object used for overriding.</p> <code>None</code> <p>Returns: A tuple containing a LabeledFrame object with necessary metadata and a lookup dictionary containing the track_id and sio.Track for persistence</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to_slp(\n    self,\n    track_lookup: dict[int, sio.Track] | None = None,\n    video: sio.Video | None = None,\n) -&gt; tuple[sio.LabeledFrame, dict[int, sio.Track]]:\n    \"\"\"Convert Frame to sleap_io.LabeledFrame object.\n\n    Args:\n        track_lookup: A lookup dictionary containing the track_id and sio.Track for persistence\n        video: An sio.Video object used for overriding.\n\n    Returns: A tuple containing a LabeledFrame object with necessary metadata and\n    a lookup dictionary containing the track_id and sio.Track for persistence\n    \"\"\"\n    if track_lookup is None:\n        track_lookup = {}\n\n    slp_instances = []\n    for instance in self.instances:\n        slp_instance, track_lookup = instance.to_slp(track_lookup=track_lookup)\n        slp_instances.append(slp_instance)\n\n    if video is None:\n        video = (\n            self.video\n            if isinstance(self.video, sio.Video)\n            else sio.load_video(self.video)\n        )\n\n    return (\n        sio.LabeledFrame(\n            video=video,\n            frame_idx=self.frame_id.item(),\n            instances=slp_instances,\n        ),\n        track_lookup,\n    )\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Instance","title":"<code>Instance</code>","text":"<p>Class representing a single instance to be tracked.</p> <p>Attributes:</p> Name Type Description <code>gt_track_id</code> <code>Tensor</code> <p>Ground truth track id - only used for train/eval.</p> <code>pred_track_id</code> <code>Tensor</code> <p>Predicted track id. Untracked instance is represented by -1.</p> <code>bbox</code> <code>Tensor</code> <p>The bounding box coordinate of the instance. Defaults to an empty tensor.</p> <code>crop</code> <code>Tensor</code> <p>The crop of the instance.</p> <code>centroid</code> <code>dict[str, ArrayLike]</code> <p>the centroid around which the bbox was cropped.</p> <code>features</code> <code>Tensor</code> <p>The reid features extracted from the CNN backbone used in the transformer.</p> <code>track_score</code> <code>float</code> <p>The track score output from the association matrix.</p> <code>point_scores</code> <code>ArrayLike</code> <p>The point scores from sleap.</p> <code>instance_score</code> <code>float</code> <p>The instance scores from sleap.</p> <code>skeleton</code> <code>Skeleton</code> <p>The sleap skeleton used for the instance.</p> <code>pose</code> <code>dict[str, ArrayLike]</code> <p>A dictionary containing the node name and corresponding point.</p> <code>device</code> <code>str</code> <p>String representation of the device the instance should be on.</p> <p>Methods:</p> Name Description <code>__attrs_post_init__</code> <p>Handle dimensionality and more intricate default initializations post-init.</p> <code>__repr__</code> <p>Return string representation of the Instance.</p> <code>add_embedding</code> <p>Save embedding to instance embedding dictionary.</p> <code>from_slp</code> <p>Convert a slp instance to a dreem instance.</p> <code>get_embedding</code> <p>Retrieve instance's spatial/temporal embedding.</p> <code>has_bbox</code> <p>Determine if the instance has a bbox.</p> <code>has_crop</code> <p>Determine if the instance has a crop.</p> <code>has_embedding</code> <p>Determine if the instance has embedding type requested.</p> <code>has_features</code> <p>Determine if the instance has computed reid features.</p> <code>has_gt_track_id</code> <p>Determine if instance has a gt track assignment.</p> <code>has_pose</code> <p>Check if the instance has a pose.</p> <code>has_pred_track_id</code> <p>Determine whether instance has predicted track id.</p> <code>to</code> <p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <code>to_h5</code> <p>Convert instance to an h5 group\".</p> <code>to_slp</code> <p>Convert instance to sleap_io.PredictedInstance object.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>@attrs.define(eq=False)\nclass Instance:\n    \"\"\"Class representing a single instance to be tracked.\n\n    Attributes:\n        gt_track_id: Ground truth track id - only used for train/eval.\n        pred_track_id: Predicted track id. Untracked instance is represented by -1.\n        bbox: The bounding box coordinate of the instance. Defaults to an empty tensor.\n        crop: The crop of the instance.\n        centroid: the centroid around which the bbox was cropped.\n        features: The reid features extracted from the CNN backbone used in the transformer.\n        track_score: The track score output from the association matrix.\n        point_scores: The point scores from sleap.\n        instance_score: The instance scores from sleap.\n        skeleton: The sleap skeleton used for the instance.\n        pose: A dictionary containing the node name and corresponding point.\n        device: String representation of the device the instance should be on.\n    \"\"\"\n\n    _gt_track_id: int = attrs.field(\n        alias=\"gt_track_id\", default=-1, converter=_to_tensor\n    )\n    _pred_track_id: int = attrs.field(\n        alias=\"pred_track_id\", default=-1, converter=_to_tensor\n    )\n    _bbox: ArrayLike = attrs.field(alias=\"bbox\", factory=list, converter=_to_tensor)\n    _crop: ArrayLike = attrs.field(alias=\"crop\", factory=list, converter=_to_tensor)\n    _centroid: dict[str, ArrayLike] = attrs.field(alias=\"centroid\", factory=dict)\n    _features: ArrayLike = attrs.field(\n        alias=\"features\", factory=list, converter=_to_tensor\n    )\n    _embeddings: dict = attrs.field(alias=\"embeddings\", factory=dict)\n    _track_score: float = attrs.field(alias=\"track_score\", default=-1.0)\n    _instance_score: float = attrs.field(alias=\"instance_score\", default=-1.0)\n    _point_scores: ArrayLike | None = attrs.field(alias=\"point_scores\", default=None)\n    _skeleton: sio.Skeleton | None = attrs.field(alias=\"skeleton\", default=None)\n    _mask: ArrayLike | None = attrs.field(\n        alias=\"mask\", converter=_to_tensor, default=None\n    )\n    _pose: dict[str, ArrayLike] = attrs.field(alias=\"pose\", factory=dict)\n    _device: str | torch.device | None = attrs.field(alias=\"device\", default=None)\n    _frame: \"Frame\" = None\n\n    def __attrs_post_init__(self) -&gt; None:\n        \"\"\"Handle dimensionality and more intricate default initializations post-init.\"\"\"\n        self.bbox = _expand_to_rank(self.bbox, 3)\n        self.crop = _expand_to_rank(self.crop, 4)\n        self.features = _expand_to_rank(self.features, 2)\n\n        if self.skeleton is None:\n            self.skeleton = sio.Skeleton([\"centroid\"])\n\n        if self.bbox.shape[-1] == 0:\n            self.bbox = torch.empty([1, 0, 4])\n\n        if self.crop.shape[-1] == 0 and self.bbox.shape[1] != 0:\n            y1, x1, y2, x2 = self.bbox.squeeze(dim=0).nanmean(dim=0)\n            self.centroid = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        if len(self.pose) == 0 and self.bbox.shape[1]:\n            y1, x1, y2, x2 = self.bbox.squeeze(dim=0).mean(dim=0)\n            self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        if self.point_scores is None and len(self.pose) != 0:\n            self._point_scores = np.zeros((len(self.pose), 2))\n\n        self.to(self.device)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation of the Instance.\"\"\"\n        return (\n            \"Instance(\"\n            f\"gt_track_id={self._gt_track_id.item()}, \"\n            f\"pred_track_id={self._pred_track_id.item()}, \"\n            f\"bbox={self._bbox}, \"\n            f\"centroid={self._centroid}, \"\n            f\"crop={self._crop.shape}, \"\n            f\"features={self._features.shape}, \"\n            f\"device={self._device}\"\n            \")\"\n        )\n\n    def to(self, map_location: str | torch.device) -&gt; Self:\n        \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n        Args:\n            map_location: Either the device or dtype for the instance to be moved.\n\n        Returns:\n            self: reference to the instance moved to correct device/dtype.\n        \"\"\"\n        if map_location is not None and map_location != \"\":\n            self._gt_track_id = self._gt_track_id.to(map_location)\n            self._pred_track_id = self._pred_track_id.to(map_location)\n            self._bbox = self._bbox.to(map_location)\n            self._crop = self._crop.to(map_location)\n            self._features = self._features.to(map_location)\n            if isinstance(map_location, (str, torch.device)):\n                self.device = map_location\n\n        return self\n\n    @classmethod\n    def from_slp(\n        cls,\n        slp_instance: sio.PredictedInstance | sio.Instance,\n        bbox_size: int | tuple[int, int] = 64,\n        crop: ArrayLike | None = None,\n        device: str | None = None,\n    ) -&gt; Self:\n        \"\"\"Convert a slp instance to a dreem instance.\n\n        Args:\n            slp_instance: A `sleap_io.Instance` object representing a detection\n            bbox_size: size of the pose-centered bbox to form.\n            crop: The corresponding crop of the bbox\n            device: which device to keep the instance on\n        Returns:\n            A dreem.Instance object with a pose-centered bbox and no crop.\n        \"\"\"\n        try:\n            track_id = int(slp_instance.track.name)\n        except ValueError:\n            track_id = int(\n                \"\".join([str(ord(c)) for c in slp_instance.track.name])\n            )  # better way to handle this?\n        if isinstance(bbox_size, int):\n            bbox_size = (bbox_size, bbox_size)\n\n        track_score = -1.0\n        point_scores = np.full(len(slp_instance.points), -1)\n        instance_score = -1\n        if isinstance(slp_instance, sio.PredictedInstance):\n            track_score = slp_instance.tracking_score\n            point_scores = slp_instance.numpy()[:, -1]\n            instance_score = slp_instance.score\n\n        centroid = np.nanmean(slp_instance.numpy(), axis=1)\n        bbox = [\n            centroid[1] - bbox_size[1],\n            centroid[0] - bbox_size[0],\n            centroid[1] + bbox_size[1],\n            centroid[0] + bbox_size[0],\n        ]\n        return cls(\n            gt_track_id=track_id,\n            bbox=bbox,\n            crop=crop,\n            centroid={\"centroid\": centroid},\n            track_score=track_score,\n            point_scores=point_scores,\n            instance_score=instance_score,\n            skeleton=slp_instance.skeleton,\n            pose={\n                node.name: point.numpy() for node, point in slp_instance.points.items()\n            },\n            device=device,\n        )\n\n    def to_slp(\n        self, track_lookup: dict[int, sio.Track] = {}\n    ) -&gt; tuple[sio.PredictedInstance, dict[int, sio.Track]]:\n        \"\"\"Convert instance to sleap_io.PredictedInstance object.\n\n        Args:\n            track_lookup: A track look up dictionary containing track_id:sio.Track.\n        Returns: A sleap_io.PredictedInstance with necessary metadata\n            and a track_lookup dictionary to persist tracks.\n        \"\"\"\n        try:\n            track_id = self.pred_track_id.item()\n            if track_id not in track_lookup:\n                track_lookup[track_id] = sio.Track(name=self.pred_track_id.item())\n\n            track = track_lookup[track_id]\n\n            return (\n                sio.PredictedInstance.from_numpy(\n                    points_data=np.array(list(self.pose.values())),\n                    skeleton=self.skeleton,\n                    point_scores=self.point_scores,\n                    score=self.instance_score,\n                    tracking_score=self.track_score,\n                    track=track,\n                ),\n                track_lookup,\n            )\n        except Exception as e:\n            logger.exception(\n                f\"Pose: {np.array(list(self.pose.values())).shape}, Pose score shape {self.point_scores.shape}\"\n            )\n            raise RuntimeError(f\"Failed to convert to sio.PredictedInstance: {e}\")\n\n    def to_h5(\n        self, frame_group: h5py.Group, label: Any = None, **kwargs: dict\n    ) -&gt; h5py.Group:\n        \"\"\"Convert instance to an h5 group\".\n\n        By default we always save:\n            - the gt/pred track id\n            - bbox\n            - centroid\n            - pose\n            - instance/traj/points score\n        Larger arrays (crops/features/embeddings) can be saved by passing as kwargs\n\n        Args:\n            frame_group: the h5py group representing the frame the instance appears on\n            label: the name of the instance group that will be created\n            **kwargs: additional key:value pairs to be saved as datasets.\n\n        Returns:\n            The h5 group representing this instance.\n        \"\"\"\n        if label is None:\n            if pred_track_id != -1:\n                label = f\"instance_{self.pred_track_id.item()}\"\n            else:\n                label = f\"instance_{self.gt_track_id.item()}\"\n        instance_group = frame_group.create_group(label)\n        instance_group.attrs.create(\"gt_track_id\", self.gt_track_id.item())\n        instance_group.attrs.create(\"pred_track_id\", self.pred_track_id.item())\n        instance_group.attrs.create(\"track_score\", self.track_score)\n        instance_group.attrs.create(\"instance_score\", self.instance_score)\n\n        instance_group.create_dataset(\"bbox\", data=self.bbox.cpu().numpy())\n\n        pose_group = instance_group.create_group(\"pose\")\n        pose_group.create_dataset(\"points\", data=np.array(list(self.pose.values())))\n        pose_group.attrs.create(\"nodes\", list(self.pose.keys()))\n        pose_group.create_dataset(\"scores\", data=self.point_scores)\n\n        for key, value in kwargs.items():\n            if \"emb\" in key:\n                emb_group = instance_group.require_group(\"emb\")\n                emb_group.create_dataset(key, data=value)\n            else:\n                instance_group.create_dataset(key, data=value)\n\n        return instance_group\n\n    @property\n    def device(self) -&gt; str:\n        \"\"\"The device the instance is on.\n\n        Returns:\n            The str representation of the device the gpu is on.\n        \"\"\"\n        return self._device\n\n    @device.setter\n    def device(self, device) -&gt; None:\n        \"\"\"Set for the device property.\n\n        Args:\n            device: The str representation of the device.\n        \"\"\"\n        self._device = device\n\n    @property\n    def gt_track_id(self) -&gt; torch.Tensor:\n        \"\"\"The ground truth track id of the instance.\n\n        Returns:\n            A tensor containing the ground truth track id\n        \"\"\"\n        return self._gt_track_id\n\n    @gt_track_id.setter\n    def gt_track_id(self, track: int):\n        \"\"\"Set the instance ground-truth track id.\n\n        Args:\n           track: An int representing the ground-truth track id.\n        \"\"\"\n        if track is not None:\n            self._gt_track_id = torch.tensor([track])\n        else:\n            self._gt_track_id = torch.tensor([])\n\n    def has_gt_track_id(self) -&gt; bool:\n        \"\"\"Determine if instance has a gt track assignment.\n\n        Returns:\n            True if the gt track id is set, otherwise False.\n        \"\"\"\n        if self._gt_track_id.shape[0] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def pred_track_id(self) -&gt; torch.Tensor:\n        \"\"\"The track id predicted by the tracker using asso_output from model.\n\n        Returns:\n            A tensor containing the predicted track id.\n        \"\"\"\n        return self._pred_track_id\n\n    @pred_track_id.setter\n    def pred_track_id(self, track: int) -&gt; None:\n        \"\"\"Set predicted track id.\n\n        Args:\n            track: an int representing the predicted track id.\n        \"\"\"\n        if track is not None:\n            self._pred_track_id = torch.tensor([track])\n        else:\n            self._pred_track_id = torch.tensor([])\n\n    def has_pred_track_id(self) -&gt; bool:\n        \"\"\"Determine whether instance has predicted track id.\n\n        Returns:\n            True if instance has a pred track id, False otherwise.\n        \"\"\"\n        if self._pred_track_id.item() == -1 or self._pred_track_id.shape[0] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def bbox(self) -&gt; torch.Tensor:\n        \"\"\"The bounding box coordinates of the instance in the original frame.\n\n        Returns:\n            A (1,4) tensor containing the bounding box coordinates.\n        \"\"\"\n        return self._bbox\n\n    @bbox.setter\n    def bbox(self, bbox: ArrayLike) -&gt; None:\n        \"\"\"Set the instance bounding box.\n\n        Args:\n            bbox: an arraylike object containing the bounding box coordinates.\n        \"\"\"\n        if bbox is None or len(bbox) == 0:\n            self._bbox = torch.empty((0, 4))\n        else:\n            if not isinstance(bbox, torch.Tensor):\n                self._bbox = torch.tensor(bbox)\n            else:\n                self._bbox = bbox\n\n        if self._bbox.shape[0] and len(self._bbox.shape) == 1:\n            self._bbox = self._bbox.unsqueeze(0)\n        if self._bbox.shape[1] and len(self._bbox.shape) == 2:\n            self._bbox = self._bbox.unsqueeze(0)\n\n    def has_bbox(self) -&gt; bool:\n        \"\"\"Determine if the instance has a bbox.\n\n        Returns:\n            True if the instance has a bounding box, false otherwise.\n        \"\"\"\n        if self._bbox.shape[1] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def centroid(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"The centroid around which the crop was formed.\n\n        Returns:\n            A dict containing the anchor name and the x, y bbox midpoint.\n        \"\"\"\n        return self._centroid\n\n    @centroid.setter\n    def centroid(self, centroid: dict[str, ArrayLike]) -&gt; None:\n        \"\"\"Set the centroid of the instance.\n\n        Args:\n            centroid: A dict containing the anchor name and points.\n        \"\"\"\n        self._centroid = centroid\n\n    @property\n    def anchor(self) -&gt; list[str]:\n        \"\"\"The anchor node name around which the crop was formed.\n\n        Returns:\n            the list of anchors around which each crop was formed\n            the list of anchors around which each crop was formed\n        \"\"\"\n        if self.centroid:\n            return list(self.centroid.keys())\n        return \"\"\n\n    @property\n    def mask(self) -&gt; torch.Tensor:\n        \"\"\"The mask of the instance.\n\n        Returns:\n            A (h, w) tensor containing the mask of the instance.\n        \"\"\"\n        return self._mask\n\n    @mask.setter\n    def mask(self, mask: ArrayLike) -&gt; None:\n        \"\"\"Set the mask of the instance.\n\n        Args:\n            mask: an arraylike object containing the mask of the instance.\n        \"\"\"\n        if mask is None or len(mask) == 0:\n            self._mask = torch.tensor([])\n        else:\n            if not isinstance(mask, torch.Tensor):\n                self._mask = torch.tensor(mask)\n            else:\n                self._mask = mask\n\n    @property\n    def crop(self) -&gt; torch.Tensor:\n        \"\"\"The crop of the instance.\n\n        Returns:\n            A (1, c, h , w) tensor containing the cropped image centered around the instance.\n        \"\"\"\n        return self._crop\n\n    @crop.setter\n    def crop(self, crop: ArrayLike) -&gt; None:\n        \"\"\"Set the crop of the instance.\n\n        Args:\n            crop: an arraylike object containing the cropped image of the centered instance.\n        \"\"\"\n        if crop is None or len(crop) == 0:\n            self._crop = torch.tensor([])\n        else:\n            if not isinstance(crop, torch.Tensor):\n                self._crop = torch.tensor(crop)\n            else:\n                self._crop = crop\n\n        if len(self._crop.shape) == 2:\n            self._crop = self._crop.unsqueeze(0)\n        if len(self._crop.shape) == 3:\n            self._crop = self._crop.unsqueeze(0)\n\n    def has_crop(self) -&gt; bool:\n        \"\"\"Determine if the instance has a crop.\n\n        Returns:\n            True if the instance has an image otherwise False.\n        \"\"\"\n        if self._crop.shape[-1] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def features(self) -&gt; torch.Tensor:\n        \"\"\"Re-ID feature vector from backbone model to be used as input to transformer.\n\n        Returns:\n            a (1, d) tensor containing the reid feature vector.\n        \"\"\"\n        return self._features\n\n    @features.setter\n    def features(self, features: ArrayLike) -&gt; None:\n        \"\"\"Set the reid feature vector of the instance.\n\n        Args:\n            features: a (1,d) array like object containing the reid features for the instance.\n        \"\"\"\n        if features is None or len(features) == 0:\n            self._features = torch.tensor([])\n\n        elif not isinstance(features, torch.Tensor):\n            self._features = torch.tensor(features)\n        else:\n            self._features = features\n\n        if self._features.shape[0] and len(self._features.shape) == 1:\n            self._features = self._features.unsqueeze(0)\n\n    def has_features(self) -&gt; bool:\n        \"\"\"Determine if the instance has computed reid features.\n\n        Returns:\n            True if the instance has reid features, False otherwise.\n        \"\"\"\n        if self._features.shape[-1] == 0:\n            return False\n        else:\n            return True\n\n    def has_embedding(self, emb_type: str | None = None) -&gt; bool:\n        \"\"\"Determine if the instance has embedding type requested.\n\n        Args:\n            emb_type: The key to check in the embedding dictionary.\n\n        Returns:\n            True if `emb_type` in embedding_dict else false\n        \"\"\"\n        return emb_type in self._embeddings\n\n    def get_embedding(\n        self, emb_type: str = \"all\"\n    ) -&gt; dict[str, torch.Tensor] | torch.Tensor | None:\n        \"\"\"Retrieve instance's spatial/temporal embedding.\n\n        Args:\n            emb_type: The string key of the embedding to retrieve. Should be \"pos\", \"temp\"\n\n        Returns:\n            * A torch tensor representing the spatial/temporal location of the instance.\n            * None if the embedding is not stored\n        \"\"\"\n        if emb_type.lower() == \"all\":\n            return self._embeddings\n        else:\n            try:\n                return self._embeddings[emb_type]\n            except KeyError:\n                logger.exception(\n                    f\"{emb_type} not saved! Only {list(self._embeddings.keys())} are available\"\n                )\n        return None\n\n    def add_embedding(self, emb_type: str, embedding: torch.Tensor) -&gt; None:\n        \"\"\"Save embedding to instance embedding dictionary.\n\n        Args:\n            emb_type: Key/embedding type to be saved to dictionary\n            embedding: The actual torch tensor embedding.\n        \"\"\"\n        embedding = _expand_to_rank(embedding, 2)\n        self._embeddings[emb_type] = embedding\n\n    @property\n    def frame(self) -&gt; \"Frame\":\n        \"\"\"Get the frame the instance belongs to.\n\n        Returns:\n            The back reference to the `Frame` that this `Instance` belongs to.\n        \"\"\"\n        return self._frame\n\n    @frame.setter\n    def frame(self, frame: \"Frame\") -&gt; None:\n        \"\"\"Set the back reference to the `Frame` that this `Instance` belongs to.\n\n        This field is set when instances are added to `Frame` object.\n\n        Args:\n            frame: A `Frame` object containing the metadata for the frame that the instance belongs to\n        \"\"\"\n        self._frame = frame\n\n    @property\n    def pose(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"Get the pose of the instance.\n\n        Returns:\n            A dictionary containing the node and corresponding x,y points\n        \"\"\"\n        return self._pose\n\n    @pose.setter\n    def pose(self, pose: dict[str, ArrayLike]) -&gt; None:\n        \"\"\"Set the pose of the instance.\n\n        Args:\n            pose: A nodes x 2 array containing the pose coordinates.\n        \"\"\"\n        if pose is not None:\n            self._pose = pose\n\n        elif self.bbox.shape[0]:\n            y1, x1, y2, x2 = self.bbox.squeeze()\n            self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        else:\n            self._pose = {}\n\n    def has_pose(self) -&gt; bool:\n        \"\"\"Check if the instance has a pose.\n\n        Returns True if the instance has a pose.\n        \"\"\"\n        if len(self.pose):\n            return True\n        return False\n\n    @property\n    def shown_pose(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"Get the pose with shown nodes only.\n\n        Returns: A dictionary filtered by nodes that are shown (points are not nan).\n        \"\"\"\n        pose = self.pose\n        return {node: point for node, point in pose.items() if not np.isna(point).any()}\n\n    @property\n    def skeleton(self) -&gt; sio.Skeleton:\n        \"\"\"Get the skeleton associated with the instance.\n\n        Returns: The sio.Skeleton associated with the instance.\n        \"\"\"\n        return self._skeleton\n\n    @skeleton.setter\n    def skeleton(self, skeleton: sio.Skeleton) -&gt; None:\n        \"\"\"Set the skeleton associated with the instance.\n\n        Args:\n            skeleton: The sio.Skeleton associated with the instance.\n        \"\"\"\n        self._skeleton = skeleton\n\n    @property\n    def point_scores(self) -&gt; ArrayLike:\n        \"\"\"Get the point scores associated with the pose prediction.\n\n        Returns: a vector of shape n containing the point scores outputted from sleap associated with pose predictions.\n        \"\"\"\n        return self._point_scores\n\n    @point_scores.setter\n    def point_scores(self, point_scores: ArrayLike) -&gt; None:\n        \"\"\"Set the point scores associated with the pose prediction.\n\n        Args:\n            point_scores: a vector of shape n containing the point scores\n            outputted from sleap associated with pose predictions.\n        \"\"\"\n        self._point_scores = point_scores\n\n    @property\n    def instance_score(self) -&gt; float:\n        \"\"\"Get the pose prediction score associated with the instance.\n\n        Returns: a float from 0-1 representing an instance_score.\n        \"\"\"\n        return self._instance_score\n\n    @instance_score.setter\n    def instance_score(self, instance_score: float) -&gt; None:\n        \"\"\"Set the pose prediction score associated with the instance.\n\n        Args:\n            instance_score: a float from 0-1 representing an instance_score.\n        \"\"\"\n        self._instance_score = instance_score\n\n    @property\n    def track_score(self) -&gt; float:\n        \"\"\"Get the track_score of the instance.\n\n        Returns: A float from 0-1 representing the output used in the tracker for assignment.\n        \"\"\"\n        return self._track_score\n\n    @track_score.setter\n    def track_score(self, track_score: float) -&gt; None:\n        \"\"\"Set the track_score of the instance.\n\n        Args:\n            track_score: A float from 0-1 representing the output used in the tracker for assignment.\n        \"\"\"\n        self._track_score = track_score\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Instance.anchor","title":"<code>anchor</code>  <code>property</code>","text":"<p>The anchor node name around which the crop was formed.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>the list of anchors around which each crop was formed the list of anchors around which each crop was formed</p>"},{"location":"reference/dreem/io/#dreem.io.Instance.bbox","title":"<code>bbox</code>  <code>property</code> <code>writable</code>","text":"<p>The bounding box coordinates of the instance in the original frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A (1,4) tensor containing the bounding box coordinates.</p>"},{"location":"reference/dreem/io/#dreem.io.Instance.centroid","title":"<code>centroid</code>  <code>property</code> <code>writable</code>","text":"<p>The centroid around which the crop was formed.</p> <p>Returns:</p> Type Description <code>dict[str, ArrayLike]</code> <p>A dict containing the anchor name and the x, y bbox midpoint.</p>"},{"location":"reference/dreem/io/#dreem.io.Instance.crop","title":"<code>crop</code>  <code>property</code> <code>writable</code>","text":"<p>The crop of the instance.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A (1, c, h , w) tensor containing the cropped image centered around the instance.</p>"},{"location":"reference/dreem/io/#dreem.io.Instance.device","title":"<code>device</code>  <code>property</code> <code>writable</code>","text":"<p>The device the instance is on.</p> <p>Returns:</p> Type Description <code>str</code> <p>The str representation of the device the gpu is on.</p>"},{"location":"reference/dreem/io/#dreem.io.Instance.features","title":"<code>features</code>  <code>property</code> <code>writable</code>","text":"<p>Re-ID feature vector from backbone model to be used as input to transformer.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>a (1, d) tensor containing the reid feature vector.</p>"},{"location":"reference/dreem/io/#dreem.io.Instance.frame","title":"<code>frame</code>  <code>property</code> <code>writable</code>","text":"<p>Get the frame the instance belongs to.</p> <p>Returns:</p> Type Description <code>Frame</code> <p>The back reference to the <code>Frame</code> that this <code>Instance</code> belongs to.</p>"},{"location":"reference/dreem/io/#dreem.io.Instance.gt_track_id","title":"<code>gt_track_id</code>  <code>property</code> <code>writable</code>","text":"<p>The ground truth track id of the instance.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the ground truth track id</p>"},{"location":"reference/dreem/io/#dreem.io.Instance.instance_score","title":"<code>instance_score</code>  <code>property</code> <code>writable</code>","text":"<p>Get the pose prediction score associated with the instance.</p> <p>Returns: a float from 0-1 representing an instance_score.</p>"},{"location":"reference/dreem/io/#dreem.io.Instance.mask","title":"<code>mask</code>  <code>property</code> <code>writable</code>","text":"<p>The mask of the instance.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A (h, w) tensor containing the mask of the instance.</p>"},{"location":"reference/dreem/io/#dreem.io.Instance.point_scores","title":"<code>point_scores</code>  <code>property</code> <code>writable</code>","text":"<p>Get the point scores associated with the pose prediction.</p> <p>Returns: a vector of shape n containing the point scores outputted from sleap associated with pose predictions.</p>"},{"location":"reference/dreem/io/#dreem.io.Instance.pose","title":"<code>pose</code>  <code>property</code> <code>writable</code>","text":"<p>Get the pose of the instance.</p> <p>Returns:</p> Type Description <code>dict[str, ArrayLike]</code> <p>A dictionary containing the node and corresponding x,y points</p>"},{"location":"reference/dreem/io/#dreem.io.Instance.pred_track_id","title":"<code>pred_track_id</code>  <code>property</code> <code>writable</code>","text":"<p>The track id predicted by the tracker using asso_output from model.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the predicted track id.</p>"},{"location":"reference/dreem/io/#dreem.io.Instance.shown_pose","title":"<code>shown_pose</code>  <code>property</code>","text":"<p>Get the pose with shown nodes only.</p> <p>Returns: A dictionary filtered by nodes that are shown (points are not nan).</p>"},{"location":"reference/dreem/io/#dreem.io.Instance.skeleton","title":"<code>skeleton</code>  <code>property</code> <code>writable</code>","text":"<p>Get the skeleton associated with the instance.</p> <p>Returns: The sio.Skeleton associated with the instance.</p>"},{"location":"reference/dreem/io/#dreem.io.Instance.track_score","title":"<code>track_score</code>  <code>property</code> <code>writable</code>","text":"<p>Get the track_score of the instance.</p> <p>Returns: A float from 0-1 representing the output used in the tracker for assignment.</p>"},{"location":"reference/dreem/io/#dreem.io.Instance.__attrs_post_init__","title":"<code>__attrs_post_init__()</code>","text":"<p>Handle dimensionality and more intricate default initializations post-init.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def __attrs_post_init__(self) -&gt; None:\n    \"\"\"Handle dimensionality and more intricate default initializations post-init.\"\"\"\n    self.bbox = _expand_to_rank(self.bbox, 3)\n    self.crop = _expand_to_rank(self.crop, 4)\n    self.features = _expand_to_rank(self.features, 2)\n\n    if self.skeleton is None:\n        self.skeleton = sio.Skeleton([\"centroid\"])\n\n    if self.bbox.shape[-1] == 0:\n        self.bbox = torch.empty([1, 0, 4])\n\n    if self.crop.shape[-1] == 0 and self.bbox.shape[1] != 0:\n        y1, x1, y2, x2 = self.bbox.squeeze(dim=0).nanmean(dim=0)\n        self.centroid = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n    if len(self.pose) == 0 and self.bbox.shape[1]:\n        y1, x1, y2, x2 = self.bbox.squeeze(dim=0).mean(dim=0)\n        self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n    if self.point_scores is None and len(self.pose) != 0:\n        self._point_scores = np.zeros((len(self.pose), 2))\n\n    self.to(self.device)\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Instance.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of the Instance.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation of the Instance.\"\"\"\n    return (\n        \"Instance(\"\n        f\"gt_track_id={self._gt_track_id.item()}, \"\n        f\"pred_track_id={self._pred_track_id.item()}, \"\n        f\"bbox={self._bbox}, \"\n        f\"centroid={self._centroid}, \"\n        f\"crop={self._crop.shape}, \"\n        f\"features={self._features.shape}, \"\n        f\"device={self._device}\"\n        \")\"\n    )\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Instance.add_embedding","title":"<code>add_embedding(emb_type, embedding)</code>","text":"<p>Save embedding to instance embedding dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>Key/embedding type to be saved to dictionary</p> required <code>embedding</code> <code>Tensor</code> <p>The actual torch tensor embedding.</p> required Source code in <code>dreem/io/instance.py</code> <pre><code>def add_embedding(self, emb_type: str, embedding: torch.Tensor) -&gt; None:\n    \"\"\"Save embedding to instance embedding dictionary.\n\n    Args:\n        emb_type: Key/embedding type to be saved to dictionary\n        embedding: The actual torch tensor embedding.\n    \"\"\"\n    embedding = _expand_to_rank(embedding, 2)\n    self._embeddings[emb_type] = embedding\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Instance.from_slp","title":"<code>from_slp(slp_instance, bbox_size=64, crop=None, device=None)</code>  <code>classmethod</code>","text":"<p>Convert a slp instance to a dreem instance.</p> <p>Parameters:</p> Name Type Description Default <code>slp_instance</code> <code>PredictedInstance | Instance</code> <p>A <code>sleap_io.Instance</code> object representing a detection</p> required <code>bbox_size</code> <code>int | tuple[int, int]</code> <p>size of the pose-centered bbox to form.</p> <code>64</code> <code>crop</code> <code>ArrayLike | None</code> <p>The corresponding crop of the bbox</p> <code>None</code> <code>device</code> <code>str | None</code> <p>which device to keep the instance on</p> <code>None</code> <p>Returns:     A dreem.Instance object with a pose-centered bbox and no crop.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>@classmethod\ndef from_slp(\n    cls,\n    slp_instance: sio.PredictedInstance | sio.Instance,\n    bbox_size: int | tuple[int, int] = 64,\n    crop: ArrayLike | None = None,\n    device: str | None = None,\n) -&gt; Self:\n    \"\"\"Convert a slp instance to a dreem instance.\n\n    Args:\n        slp_instance: A `sleap_io.Instance` object representing a detection\n        bbox_size: size of the pose-centered bbox to form.\n        crop: The corresponding crop of the bbox\n        device: which device to keep the instance on\n    Returns:\n        A dreem.Instance object with a pose-centered bbox and no crop.\n    \"\"\"\n    try:\n        track_id = int(slp_instance.track.name)\n    except ValueError:\n        track_id = int(\n            \"\".join([str(ord(c)) for c in slp_instance.track.name])\n        )  # better way to handle this?\n    if isinstance(bbox_size, int):\n        bbox_size = (bbox_size, bbox_size)\n\n    track_score = -1.0\n    point_scores = np.full(len(slp_instance.points), -1)\n    instance_score = -1\n    if isinstance(slp_instance, sio.PredictedInstance):\n        track_score = slp_instance.tracking_score\n        point_scores = slp_instance.numpy()[:, -1]\n        instance_score = slp_instance.score\n\n    centroid = np.nanmean(slp_instance.numpy(), axis=1)\n    bbox = [\n        centroid[1] - bbox_size[1],\n        centroid[0] - bbox_size[0],\n        centroid[1] + bbox_size[1],\n        centroid[0] + bbox_size[0],\n    ]\n    return cls(\n        gt_track_id=track_id,\n        bbox=bbox,\n        crop=crop,\n        centroid={\"centroid\": centroid},\n        track_score=track_score,\n        point_scores=point_scores,\n        instance_score=instance_score,\n        skeleton=slp_instance.skeleton,\n        pose={\n            node.name: point.numpy() for node, point in slp_instance.points.items()\n        },\n        device=device,\n    )\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Instance.get_embedding","title":"<code>get_embedding(emb_type='all')</code>","text":"<p>Retrieve instance's spatial/temporal embedding.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>The string key of the embedding to retrieve. Should be \"pos\", \"temp\"</p> <code>'all'</code> <p>Returns:</p> Type Description <code>dict[str, Tensor] | Tensor | None</code> <ul> <li>A torch tensor representing the spatial/temporal location of the instance.</li> <li>None if the embedding is not stored</li> </ul> Source code in <code>dreem/io/instance.py</code> <pre><code>def get_embedding(\n    self, emb_type: str = \"all\"\n) -&gt; dict[str, torch.Tensor] | torch.Tensor | None:\n    \"\"\"Retrieve instance's spatial/temporal embedding.\n\n    Args:\n        emb_type: The string key of the embedding to retrieve. Should be \"pos\", \"temp\"\n\n    Returns:\n        * A torch tensor representing the spatial/temporal location of the instance.\n        * None if the embedding is not stored\n    \"\"\"\n    if emb_type.lower() == \"all\":\n        return self._embeddings\n    else:\n        try:\n            return self._embeddings[emb_type]\n        except KeyError:\n            logger.exception(\n                f\"{emb_type} not saved! Only {list(self._embeddings.keys())} are available\"\n            )\n    return None\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Instance.has_bbox","title":"<code>has_bbox()</code>","text":"<p>Determine if the instance has a bbox.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has a bounding box, false otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_bbox(self) -&gt; bool:\n    \"\"\"Determine if the instance has a bbox.\n\n    Returns:\n        True if the instance has a bounding box, false otherwise.\n    \"\"\"\n    if self._bbox.shape[1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Instance.has_crop","title":"<code>has_crop()</code>","text":"<p>Determine if the instance has a crop.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has an image otherwise False.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_crop(self) -&gt; bool:\n    \"\"\"Determine if the instance has a crop.\n\n    Returns:\n        True if the instance has an image otherwise False.\n    \"\"\"\n    if self._crop.shape[-1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Instance.has_embedding","title":"<code>has_embedding(emb_type=None)</code>","text":"<p>Determine if the instance has embedding type requested.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str | None</code> <p>The key to check in the embedding dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if <code>emb_type</code> in embedding_dict else false</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_embedding(self, emb_type: str | None = None) -&gt; bool:\n    \"\"\"Determine if the instance has embedding type requested.\n\n    Args:\n        emb_type: The key to check in the embedding dictionary.\n\n    Returns:\n        True if `emb_type` in embedding_dict else false\n    \"\"\"\n    return emb_type in self._embeddings\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Instance.has_features","title":"<code>has_features()</code>","text":"<p>Determine if the instance has computed reid features.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has reid features, False otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_features(self) -&gt; bool:\n    \"\"\"Determine if the instance has computed reid features.\n\n    Returns:\n        True if the instance has reid features, False otherwise.\n    \"\"\"\n    if self._features.shape[-1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Instance.has_gt_track_id","title":"<code>has_gt_track_id()</code>","text":"<p>Determine if instance has a gt track assignment.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the gt track id is set, otherwise False.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_gt_track_id(self) -&gt; bool:\n    \"\"\"Determine if instance has a gt track assignment.\n\n    Returns:\n        True if the gt track id is set, otherwise False.\n    \"\"\"\n    if self._gt_track_id.shape[0] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Instance.has_pose","title":"<code>has_pose()</code>","text":"<p>Check if the instance has a pose.</p> <p>Returns True if the instance has a pose.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_pose(self) -&gt; bool:\n    \"\"\"Check if the instance has a pose.\n\n    Returns True if the instance has a pose.\n    \"\"\"\n    if len(self.pose):\n        return True\n    return False\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Instance.has_pred_track_id","title":"<code>has_pred_track_id()</code>","text":"<p>Determine whether instance has predicted track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if instance has a pred track id, False otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_pred_track_id(self) -&gt; bool:\n    \"\"\"Determine whether instance has predicted track id.\n\n    Returns:\n        True if instance has a pred track id, False otherwise.\n    \"\"\"\n    if self._pred_track_id.item() == -1 or self._pred_track_id.shape[0] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Instance.to","title":"<code>to(map_location)</code>","text":"<p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <p>Parameters:</p> Name Type Description Default <code>map_location</code> <code>str | device</code> <p>Either the device or dtype for the instance to be moved.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>Self</code> <p>reference to the instance moved to correct device/dtype.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to(self, map_location: str | torch.device) -&gt; Self:\n    \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n    Args:\n        map_location: Either the device or dtype for the instance to be moved.\n\n    Returns:\n        self: reference to the instance moved to correct device/dtype.\n    \"\"\"\n    if map_location is not None and map_location != \"\":\n        self._gt_track_id = self._gt_track_id.to(map_location)\n        self._pred_track_id = self._pred_track_id.to(map_location)\n        self._bbox = self._bbox.to(map_location)\n        self._crop = self._crop.to(map_location)\n        self._features = self._features.to(map_location)\n        if isinstance(map_location, (str, torch.device)):\n            self.device = map_location\n\n    return self\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Instance.to_h5","title":"<code>to_h5(frame_group, label=None, **kwargs)</code>","text":"<p>Convert instance to an h5 group\".</p> By default we always save <ul> <li>the gt/pred track id</li> <li>bbox</li> <li>centroid</li> <li>pose</li> <li>instance/traj/points score</li> </ul> <p>Larger arrays (crops/features/embeddings) can be saved by passing as kwargs</p> <p>Parameters:</p> Name Type Description Default <code>frame_group</code> <code>Group</code> <p>the h5py group representing the frame the instance appears on</p> required <code>label</code> <code>Any</code> <p>the name of the instance group that will be created</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>additional key:value pairs to be saved as datasets.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Group</code> <p>The h5 group representing this instance.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to_h5(\n    self, frame_group: h5py.Group, label: Any = None, **kwargs: dict\n) -&gt; h5py.Group:\n    \"\"\"Convert instance to an h5 group\".\n\n    By default we always save:\n        - the gt/pred track id\n        - bbox\n        - centroid\n        - pose\n        - instance/traj/points score\n    Larger arrays (crops/features/embeddings) can be saved by passing as kwargs\n\n    Args:\n        frame_group: the h5py group representing the frame the instance appears on\n        label: the name of the instance group that will be created\n        **kwargs: additional key:value pairs to be saved as datasets.\n\n    Returns:\n        The h5 group representing this instance.\n    \"\"\"\n    if label is None:\n        if pred_track_id != -1:\n            label = f\"instance_{self.pred_track_id.item()}\"\n        else:\n            label = f\"instance_{self.gt_track_id.item()}\"\n    instance_group = frame_group.create_group(label)\n    instance_group.attrs.create(\"gt_track_id\", self.gt_track_id.item())\n    instance_group.attrs.create(\"pred_track_id\", self.pred_track_id.item())\n    instance_group.attrs.create(\"track_score\", self.track_score)\n    instance_group.attrs.create(\"instance_score\", self.instance_score)\n\n    instance_group.create_dataset(\"bbox\", data=self.bbox.cpu().numpy())\n\n    pose_group = instance_group.create_group(\"pose\")\n    pose_group.create_dataset(\"points\", data=np.array(list(self.pose.values())))\n    pose_group.attrs.create(\"nodes\", list(self.pose.keys()))\n    pose_group.create_dataset(\"scores\", data=self.point_scores)\n\n    for key, value in kwargs.items():\n        if \"emb\" in key:\n            emb_group = instance_group.require_group(\"emb\")\n            emb_group.create_dataset(key, data=value)\n        else:\n            instance_group.create_dataset(key, data=value)\n\n    return instance_group\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Instance.to_slp","title":"<code>to_slp(track_lookup={})</code>","text":"<p>Convert instance to sleap_io.PredictedInstance object.</p> <p>Parameters:</p> Name Type Description Default <code>track_lookup</code> <code>dict[int, Track]</code> <p>A track look up dictionary containing track_id:sio.Track.</p> <code>{}</code> <p>Returns: A sleap_io.PredictedInstance with necessary metadata     and a track_lookup dictionary to persist tracks.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to_slp(\n    self, track_lookup: dict[int, sio.Track] = {}\n) -&gt; tuple[sio.PredictedInstance, dict[int, sio.Track]]:\n    \"\"\"Convert instance to sleap_io.PredictedInstance object.\n\n    Args:\n        track_lookup: A track look up dictionary containing track_id:sio.Track.\n    Returns: A sleap_io.PredictedInstance with necessary metadata\n        and a track_lookup dictionary to persist tracks.\n    \"\"\"\n    try:\n        track_id = self.pred_track_id.item()\n        if track_id not in track_lookup:\n            track_lookup[track_id] = sio.Track(name=self.pred_track_id.item())\n\n        track = track_lookup[track_id]\n\n        return (\n            sio.PredictedInstance.from_numpy(\n                points_data=np.array(list(self.pose.values())),\n                skeleton=self.skeleton,\n                point_scores=self.point_scores,\n                score=self.instance_score,\n                tracking_score=self.track_score,\n                track=track,\n            ),\n            track_lookup,\n        )\n    except Exception as e:\n        logger.exception(\n            f\"Pose: {np.array(list(self.pose.values())).shape}, Pose score shape {self.point_scores.shape}\"\n        )\n        raise RuntimeError(f\"Failed to convert to sio.PredictedInstance: {e}\")\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Track","title":"<code>Track</code>","text":"<p>Object for storing instances of the same track.</p> <p>Attributes:</p> Name Type Description <code>id</code> <p>the track label.</p> <code>instances</code> <code>list['Instances']</code> <p>A list of instances belonging to the track.</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Get an instance from the track.</p> <code>__len__</code> <p>Get the length of the track.</p> <code>__repr__</code> <p>Get the string representation of the track.</p> Source code in <code>dreem/io/track.py</code> <pre><code>@attrs.define(eq=False)\nclass Track:\n    \"\"\"Object for storing instances of the same track.\n\n    Attributes:\n        id: the track label.\n        instances: A list of instances belonging to the track.\n    \"\"\"\n\n    _id: int = attrs.field(alias=\"id\")\n    _instances: list[\"Instance\"] = attrs.field(alias=\"instances\", factory=list)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get the string representation of the track.\n\n        Returns:\n            the string representation of the Track.\n        \"\"\"\n        return f\"Track(id={self.id}, len={len(self)})\"\n\n    @property\n    def track_id(self) -&gt; int:\n        \"\"\"Get the id of the track.\n\n        Returns:\n            The integer id of the track.\n        \"\"\"\n        return self._id\n\n    @track_id.setter\n    def track_id(self, track_id: int) -&gt; None:\n        \"\"\"Set the id of the track.\n\n        Args:\n            track_id: the int id of the track.\n        \"\"\"\n        self._id = track_id\n\n    @property\n    def instances(self) -&gt; list[\"Instances\"]:\n        \"\"\"Get the instances belonging to this track.\n\n        Returns:\n            A list of instances with this track id.\n        \"\"\"\n        return self._instances\n\n    @instances.setter\n    def instances(self, instances) -&gt; None:\n        \"\"\"Set the instances belonging to this track.\n\n        Args:\n            instances: A list of instances that belong to the same track.\n        \"\"\"\n        self._instances = instances\n\n    @property\n    def frames(self) -&gt; set[\"Frame\"]:\n        \"\"\"Get the frames where this track appears.\n\n        Returns:\n            A set of `Frame` objects where this track appears.\n        \"\"\"\n        return set([instance.frame for instance in self.instances])\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the length of the track.\n\n        Returns:\n            The number of instances/frames in the track.\n        \"\"\"\n        return len(self.instances)\n\n    def __getitem__(self, ind: int | list[int]) -&gt; \"Instance\" | list[\"Instance\"]:\n        \"\"\"Get an instance from the track.\n\n        Args:\n            ind: Either a single int or list of int indices.\n\n        Returns:\n            the instance at that index of the track.instances.\n        \"\"\"\n        if isinstance(ind, int):\n            return self.instances[ind]\n        elif isinstance(ind, list):\n            return [self.instances[i] for i in ind]\n        else:\n            raise ValueError(f\"Ind must be an int or list of ints, found {type(ind)}\")\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Track.frames","title":"<code>frames</code>  <code>property</code>","text":"<p>Get the frames where this track appears.</p> <p>Returns:</p> Type Description <code>set['Frame']</code> <p>A set of <code>Frame</code> objects where this track appears.</p>"},{"location":"reference/dreem/io/#dreem.io.Track.instances","title":"<code>instances</code>  <code>property</code> <code>writable</code>","text":"<p>Get the instances belonging to this track.</p> <p>Returns:</p> Type Description <code>list['Instances']</code> <p>A list of instances with this track id.</p>"},{"location":"reference/dreem/io/#dreem.io.Track.track_id","title":"<code>track_id</code>  <code>property</code> <code>writable</code>","text":"<p>Get the id of the track.</p> <p>Returns:</p> Type Description <code>int</code> <p>The integer id of the track.</p>"},{"location":"reference/dreem/io/#dreem.io.Track.__getitem__","title":"<code>__getitem__(ind)</code>","text":"<p>Get an instance from the track.</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> <code>int | list[int]</code> <p>Either a single int or list of int indices.</p> required <p>Returns:</p> Type Description <code>'Instance' | list['Instance']</code> <p>the instance at that index of the track.instances.</p> Source code in <code>dreem/io/track.py</code> <pre><code>def __getitem__(self, ind: int | list[int]) -&gt; \"Instance\" | list[\"Instance\"]:\n    \"\"\"Get an instance from the track.\n\n    Args:\n        ind: Either a single int or list of int indices.\n\n    Returns:\n        the instance at that index of the track.instances.\n    \"\"\"\n    if isinstance(ind, int):\n        return self.instances[ind]\n    elif isinstance(ind, list):\n        return [self.instances[i] for i in ind]\n    else:\n        raise ValueError(f\"Ind must be an int or list of ints, found {type(ind)}\")\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Track.__len__","title":"<code>__len__()</code>","text":"<p>Get the length of the track.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of instances/frames in the track.</p> Source code in <code>dreem/io/track.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the length of the track.\n\n    Returns:\n        The number of instances/frames in the track.\n    \"\"\"\n    return len(self.instances)\n</code></pre>"},{"location":"reference/dreem/io/#dreem.io.Track.__repr__","title":"<code>__repr__()</code>","text":"<p>Get the string representation of the track.</p> <p>Returns:</p> Type Description <code>str</code> <p>the string representation of the Track.</p> Source code in <code>dreem/io/track.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get the string representation of the track.\n\n    Returns:\n        the string representation of the Track.\n    \"\"\"\n    return f\"Track(id={self.id}, len={len(self)})\"\n</code></pre>"},{"location":"reference/dreem/io/association_matrix/","title":"association_matrix","text":""},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix","title":"<code>dreem.io.association_matrix</code>","text":"<p>Module containing class for storing and looking up association scores.</p> <p>Classes:</p> Name Description <code>AssociationMatrix</code> <p>Class representing the associations between detections.</p>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix","title":"<code>AssociationMatrix</code>","text":"<p>Class representing the associations between detections.</p> <p>Attributes:</p> Name Type Description <code>matrix</code> <code>ndarray | Tensor</code> <p>the <code>n_query x n_ref</code> association matrix`</p> <code>ref_instances</code> <code>list[Instance]</code> <p>all instances used to associate against.</p> <code>query_instances</code> <code>list[Instance]</code> <p>query instances that were associated against ref instances.</p> <p>Methods:</p> Name Description <code>__getindices__</code> <p>Get the indices of the instance for lookup.</p> <code>__getitem__</code> <p>Get elements of the association matrix.</p> <code>__repr__</code> <p>Get the string representation of the Association Matrix.</p> <code>get_tracks</code> <p>Group instances by track.</p> <code>numpy</code> <p>Convert association matrix to a numpy array.</p> <code>reduce</code> <p>Aggregate the association matrix by specified dimensions and grouping.</p> <code>to</code> <p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <code>to_dataframe</code> <p>Convert the association matrix to a pandas DataFrame.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>@attrs.define\nclass AssociationMatrix:\n    \"\"\"Class representing the associations between detections.\n\n    Attributes:\n        matrix: the `n_query x n_ref` association matrix`\n        ref_instances: all instances used to associate against.\n        query_instances: query instances that were associated against ref instances.\n    \"\"\"\n\n    matrix: np.ndarray | torch.Tensor\n    ref_instances: list[Instance] = attrs.field()\n    query_instances: list[Instance] = attrs.field()\n\n    @ref_instances.validator\n    def _check_ref_instances(self, attribute, value):\n        \"\"\"Check to ensure that the number of association matrix columns and reference instances match.\n\n        Args:\n            attribute: The ref instances.\n            value: the list of ref instances.\n\n        Raises:\n            ValueError if the number of columns and reference instances don't match.\n        \"\"\"\n        if len(value) != self.matrix.shape[-1]:\n            raise ValueError(\n                (\n                    \"Ref instances must equal number of columns in Association matrix\"\n                    f\"Found {len(value)} ref instances but {self.matrix.shape[-1]} columns.\"\n                )\n            )\n\n    @query_instances.validator\n    def _check_query_instances(self, attribute, value):\n        \"\"\"Check to ensure that the number of association matrix rows and query instances match.\n\n        Args:\n            attribute: The query instances.\n            value: the list of query instances.\n\n        Raises:\n            ValueError if the number of rows and query instances don't match.\n        \"\"\"\n        if len(value) != self.matrix.shape[0]:\n            raise ValueError(\n                (\n                    \"Query instances must equal number of rows in Association matrix\"\n                    f\"Found {len(value)} query instances but {self.matrix.shape[0]} rows.\"\n                )\n            )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get the string representation of the Association Matrix.\n\n        Returns:\n            the string representation of the association matrix.\n        \"\"\"\n        return (\n            f\"AssociationMatrix({self.matrix},\"\n            f\"query_instances={len(self.query_instances)},\"\n            f\"ref_instances={len(self.ref_instances)})\"\n        )\n\n    def numpy(self) -&gt; np.ndarray:\n        \"\"\"Convert association matrix to a numpy array.\n\n        Returns:\n            The association matrix as a numpy array.\n        \"\"\"\n        if isinstance(self.matrix, torch.Tensor):\n            return self.matrix.detach().cpu().numpy()\n        return self.matrix\n\n    def to_dataframe(\n        self, row_labels: str = \"gt\", col_labels: str = \"gt\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"Convert the association matrix to a pandas DataFrame.\n\n        Args:\n            row_labels: How to label the rows(queries).\n                If list, then must match # of rows/queries\n                If `\"gt\"` then label by gt track id.\n                If `\"pred\"` then label by pred track id.\n                Otherwise label by the query_instance indices\n            col_labels: How to label the columns(references).\n                If list, then must match # of columns/refs\n                If `\"gt\"` then label by gt track id.\n                If `\"pred\"` then label by pred track id.\n                Otherwise label by the ref_instance indices\n\n        Returns:\n            The association matrix as a pandas dataframe.\n        \"\"\"\n        matrix = self.numpy()\n\n        if not isinstance(row_labels, str):\n            if len(row_labels) == len(self.query_instances):\n                row_inds = row_labels\n\n            else:\n                raise ValueError(\n                    (\n                        \"Mismatched # of rows and labels!\",\n                        f\"Found {len(row_labels)} with {len(self.query_instances)} rows\",\n                    )\n                )\n\n        else:\n            if row_labels == \"gt\":\n                row_inds = [\n                    instance.gt_track_id.item() for instance in self.query_instances\n                ]\n\n            elif row_labels == \"pred\":\n                row_inds = [\n                    instance.pred_track_id.item() for instance in self.query_instances\n                ]\n\n            else:\n                row_inds = np.arange(len(self.query_instances))\n\n        if not isinstance(col_labels, str):\n            if len(col_labels) == len(self.ref_instances):\n                col_inds = col_labels\n\n            else:\n                raise ValueError(\n                    (\n                        \"Mismatched # of columns and labels!\",\n                        f\"Found {len(col_labels)} with {len(self.ref_instances)} columns\",\n                    )\n                )\n\n        else:\n            if col_labels == \"gt\":\n                col_inds = [\n                    instance.gt_track_id.item() for instance in self.ref_instances\n                ]\n\n            elif col_labels == \"pred\":\n                col_inds = [\n                    instance.pred_track_id.item() for instance in self.ref_instances\n                ]\n\n            else:\n                col_inds = np.arange(len(self.ref_instances))\n\n        asso_df = pd.DataFrame(matrix, index=row_inds, columns=col_inds)\n\n        return asso_df\n\n    def reduce(\n        self,\n        row_dims: str = \"instance\",\n        col_dims: str = \"track\",\n        row_grouping: str | None = None,\n        col_grouping: str = \"pred\",\n        reduce_method: callable = np.sum,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Aggregate the association matrix by specified dimensions and grouping.\n\n        Args:\n           row_dims: A str indicating how to what dimensions to reduce rows to.\n                Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).\n           col_dims: A str indicating how to dimensions to reduce rows to.\n                Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)\n           row_grouping: A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".\n           col_grouping: A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".\n           reduce_method: A callable function that operates on numpy matrices and can take an `axis` arg for reducing.\n\n        Returns:\n            The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.\n        \"\"\"\n        n_rows = len(self.query_instances)\n        n_cols = len(self.ref_instances)\n\n        col_tracks = {-1: self.ref_instances}\n        row_tracks = {-1: self.query_instances}\n\n        col_inds = [i for i in range(len(self.ref_instances))]\n        row_inds = [i for i in range(len(self.query_instances))]\n\n        if col_dims == \"track\":\n            col_tracks = self.get_tracks(self.ref_instances, col_grouping)\n            col_inds = list(col_tracks.keys())\n            n_cols = len(col_inds)\n\n        if row_dims == \"track\":\n            row_tracks = self.get_tracks(self.query_instances, row_grouping)\n            row_inds = list(row_tracks.keys())\n            n_rows = len(row_inds)\n\n        reduced_matrix = []\n        for row_track, row_instances in row_tracks.items():\n            for col_track, col_instances in col_tracks.items():\n                asso_matrix = self[row_instances, col_instances]\n\n                if col_dims == \"track\":\n                    asso_matrix = reduce_method(asso_matrix, axis=1)\n\n                if row_dims == \"track\":\n                    asso_matrix = reduce_method(asso_matrix, axis=0)\n\n                reduced_matrix.append(asso_matrix)\n\n        reduced_matrix = np.array(reduced_matrix).reshape(n_cols, n_rows).T\n\n        return pd.DataFrame(reduced_matrix, index=row_inds, columns=col_inds)\n\n    def __getitem__(\n        self, inds: tuple[int | Instance | list[int | Instance]]\n    ) -&gt; np.ndarray:\n        \"\"\"Get elements of the association matrix.\n\n        Args:\n            inds: A tuple of query indices and reference indices.\n                Indices can be either:\n                    A single instance or integer.\n                    A list of instances or integers.\n\n        Returns:\n            An np.ndarray containing the elements requested.\n        \"\"\"\n        query_inst, ref_inst = inds\n\n        query_ind = self.__getindices__(query_inst, self.query_instances)\n        ref_ind = self.__getindices__(ref_inst, self.ref_instances)\n\n        try:\n            return self.numpy()[query_ind[:, None], ref_ind].squeeze()\n        except IndexError as e:\n            logger.exception(f\"Query_insts: {type(query_inst)}\")\n            logger.exception(f\"Query_inds: {query_ind}\")\n            logger.exception(f\"Ref_insts: {type(ref_inst)}\")\n            logger.exception(f\"Ref_ind: {ref_ind}\")\n            logger.exception(e)\n            raise (e)\n\n    def __getindices__(\n        self,\n        instance: Instance | int | np.typing.ArrayLike,\n        instance_lookup: list[Instance],\n    ) -&gt; np.ndarray:\n        \"\"\"Get the indices of the instance for lookup.\n\n        Args:\n            instance: The instance(s) to be retrieved\n                Can either be a single int/instance or a list of int/instances\n            instance_lookup: A list of Instances to be used to retrieve indices\n\n        Returns:\n            A np array of indices.\n        \"\"\"\n        if isinstance(instance, Instance):\n            ind = np.array([instance_lookup.index(instance)])\n\n        elif instance is None:\n            ind = np.arange(len(instance_lookup))\n\n        elif np.isscalar(instance):\n            ind = np.array([instance])\n\n        else:\n            instances = instance\n            if not [isinstance(inst, (Instance, int)) for inst in instance]:\n                raise ValueError(\n                    f\"List of indices must be `int` or `Instance`. Found {set([type(inst) for inst in instance])}\"\n                )\n            ind = np.array(\n                [\n                    (\n                        instance_lookup.index(instance)\n                        if isinstance(instance, Instance)\n                        else instance\n                    )\n                    for instance in instances\n                ]\n            )\n\n        return ind\n\n    def get_tracks(\n        self, instances: list[\"Instance\"], label: str = \"pred\"\n    ) -&gt; dict[int, list[\"Instance\"]]:\n        \"\"\"Group instances by track.\n\n        Args:\n            instances: The list of instances to group\n            label: the track id type to group by. Either `pred` or `gt`.\n\n        Returns:\n            A dictionary of track_id:instances\n        \"\"\"\n        if label == \"pred\":\n            traj_ids = set([instance.pred_track_id.item() for instance in instances])\n            traj = {\n                track_id: [\n                    instance\n                    for instance in instances\n                    if instance.pred_track_id.item() == track_id\n                ]\n                for track_id in traj_ids\n            }\n\n        elif label == \"gt\":\n            traj_ids = set(\n                [instance.gt_track_id.item() for instance in self.ref_instances]\n            )\n            traj = {\n                track_id: [\n                    instance\n                    for instance in self.ref_instances\n                    if instance.gt_track_id.item() == track_id\n                ]\n                for track_id in traj_ids\n            }\n\n        else:\n            raise ValueError(f\"Unsupported label '{label}'. Expected 'pred' or 'gt'.\")\n\n        return traj\n\n    def to(self, map_location: str | torch.device) -&gt; Self:\n        \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n        Args:\n            map_location: Either the device or dtype for the instance to be moved.\n\n        Returns:\n            self: reference to the instance moved to correct device/dtype.\n        \"\"\"\n        self.matrix = self.matrix.to(map_location)\n        self.ref_instances = [\n            instance.to(map_location) for instance in self.ref_instances\n        ]\n        self.query_instances = [\n            instance.to(map_location) for instance in self.query_instances\n        ]\n\n        return self\n</code></pre>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix.__getindices__","title":"<code>__getindices__(instance, instance_lookup)</code>","text":"<p>Get the indices of the instance for lookup.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>Instance | int | ArrayLike</code> <p>The instance(s) to be retrieved Can either be a single int/instance or a list of int/instances</p> required <code>instance_lookup</code> <code>list[Instance]</code> <p>A list of Instances to be used to retrieve indices</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A np array of indices.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __getindices__(\n    self,\n    instance: Instance | int | np.typing.ArrayLike,\n    instance_lookup: list[Instance],\n) -&gt; np.ndarray:\n    \"\"\"Get the indices of the instance for lookup.\n\n    Args:\n        instance: The instance(s) to be retrieved\n            Can either be a single int/instance or a list of int/instances\n        instance_lookup: A list of Instances to be used to retrieve indices\n\n    Returns:\n        A np array of indices.\n    \"\"\"\n    if isinstance(instance, Instance):\n        ind = np.array([instance_lookup.index(instance)])\n\n    elif instance is None:\n        ind = np.arange(len(instance_lookup))\n\n    elif np.isscalar(instance):\n        ind = np.array([instance])\n\n    else:\n        instances = instance\n        if not [isinstance(inst, (Instance, int)) for inst in instance]:\n            raise ValueError(\n                f\"List of indices must be `int` or `Instance`. Found {set([type(inst) for inst in instance])}\"\n            )\n        ind = np.array(\n            [\n                (\n                    instance_lookup.index(instance)\n                    if isinstance(instance, Instance)\n                    else instance\n                )\n                for instance in instances\n            ]\n        )\n\n    return ind\n</code></pre>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix.__getitem__","title":"<code>__getitem__(inds)</code>","text":"<p>Get elements of the association matrix.</p> <p>Parameters:</p> Name Type Description Default <code>inds</code> <code>tuple[int | Instance | list[int | Instance]]</code> <p>A tuple of query indices and reference indices. Indices can be either:     A single instance or integer.     A list of instances or integers.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>An np.ndarray containing the elements requested.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __getitem__(\n    self, inds: tuple[int | Instance | list[int | Instance]]\n) -&gt; np.ndarray:\n    \"\"\"Get elements of the association matrix.\n\n    Args:\n        inds: A tuple of query indices and reference indices.\n            Indices can be either:\n                A single instance or integer.\n                A list of instances or integers.\n\n    Returns:\n        An np.ndarray containing the elements requested.\n    \"\"\"\n    query_inst, ref_inst = inds\n\n    query_ind = self.__getindices__(query_inst, self.query_instances)\n    ref_ind = self.__getindices__(ref_inst, self.ref_instances)\n\n    try:\n        return self.numpy()[query_ind[:, None], ref_ind].squeeze()\n    except IndexError as e:\n        logger.exception(f\"Query_insts: {type(query_inst)}\")\n        logger.exception(f\"Query_inds: {query_ind}\")\n        logger.exception(f\"Ref_insts: {type(ref_inst)}\")\n        logger.exception(f\"Ref_ind: {ref_ind}\")\n        logger.exception(e)\n        raise (e)\n</code></pre>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix.__repr__","title":"<code>__repr__()</code>","text":"<p>Get the string representation of the Association Matrix.</p> <p>Returns:</p> Type Description <code>str</code> <p>the string representation of the association matrix.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get the string representation of the Association Matrix.\n\n    Returns:\n        the string representation of the association matrix.\n    \"\"\"\n    return (\n        f\"AssociationMatrix({self.matrix},\"\n        f\"query_instances={len(self.query_instances)},\"\n        f\"ref_instances={len(self.ref_instances)})\"\n    )\n</code></pre>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix.get_tracks","title":"<code>get_tracks(instances, label='pred')</code>","text":"<p>Group instances by track.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[Instance]</code> <p>The list of instances to group</p> required <code>label</code> <code>str</code> <p>the track id type to group by. Either <code>pred</code> or <code>gt</code>.</p> <code>'pred'</code> <p>Returns:</p> Type Description <code>dict[int, list[Instance]]</code> <p>A dictionary of track_id:instances</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def get_tracks(\n    self, instances: list[\"Instance\"], label: str = \"pred\"\n) -&gt; dict[int, list[\"Instance\"]]:\n    \"\"\"Group instances by track.\n\n    Args:\n        instances: The list of instances to group\n        label: the track id type to group by. Either `pred` or `gt`.\n\n    Returns:\n        A dictionary of track_id:instances\n    \"\"\"\n    if label == \"pred\":\n        traj_ids = set([instance.pred_track_id.item() for instance in instances])\n        traj = {\n            track_id: [\n                instance\n                for instance in instances\n                if instance.pred_track_id.item() == track_id\n            ]\n            for track_id in traj_ids\n        }\n\n    elif label == \"gt\":\n        traj_ids = set(\n            [instance.gt_track_id.item() for instance in self.ref_instances]\n        )\n        traj = {\n            track_id: [\n                instance\n                for instance in self.ref_instances\n                if instance.gt_track_id.item() == track_id\n            ]\n            for track_id in traj_ids\n        }\n\n    else:\n        raise ValueError(f\"Unsupported label '{label}'. Expected 'pred' or 'gt'.\")\n\n    return traj\n</code></pre>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix.numpy","title":"<code>numpy()</code>","text":"<p>Convert association matrix to a numpy array.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The association matrix as a numpy array.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def numpy(self) -&gt; np.ndarray:\n    \"\"\"Convert association matrix to a numpy array.\n\n    Returns:\n        The association matrix as a numpy array.\n    \"\"\"\n    if isinstance(self.matrix, torch.Tensor):\n        return self.matrix.detach().cpu().numpy()\n    return self.matrix\n</code></pre>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix.reduce","title":"<code>reduce(row_dims='instance', col_dims='track', row_grouping=None, col_grouping='pred', reduce_method=np.sum)</code>","text":"<p>Aggregate the association matrix by specified dimensions and grouping.</p> <p>Parameters:</p> Name Type Description Default <code>row_dims</code> <code>str</code> <p>A str indicating how to what dimensions to reduce rows to.   Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).</p> <code>'instance'</code> <code>col_dims</code> <code>str</code> <p>A str indicating how to dimensions to reduce rows to.   Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)</p> <code>'track'</code> <code>row_grouping</code> <code>str | None</code> <p>A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".</p> <code>None</code> <code>col_grouping</code> <code>str</code> <p>A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".</p> <code>'pred'</code> <code>reduce_method</code> <code>callable</code> <p>A callable function that operates on numpy matrices and can take an <code>axis</code> arg for reducing.</p> <code>sum</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def reduce(\n    self,\n    row_dims: str = \"instance\",\n    col_dims: str = \"track\",\n    row_grouping: str | None = None,\n    col_grouping: str = \"pred\",\n    reduce_method: callable = np.sum,\n) -&gt; pd.DataFrame:\n    \"\"\"Aggregate the association matrix by specified dimensions and grouping.\n\n    Args:\n       row_dims: A str indicating how to what dimensions to reduce rows to.\n            Either \"instance\" (remains unchanged), or \"track\" (n_rows=n_traj).\n       col_dims: A str indicating how to dimensions to reduce rows to.\n            Either \"instance\" (remains unchanged), or \"track\" (n_cols=n_traj)\n       row_grouping: A str indicating how to group rows when aggregating. Either \"pred\" or \"gt\".\n       col_grouping: A str indicating how to group columns when aggregating. Either \"pred\" or \"gt\".\n       reduce_method: A callable function that operates on numpy matrices and can take an `axis` arg for reducing.\n\n    Returns:\n        The association matrix reduced to an inst/traj x traj/inst association matrix as a dataframe.\n    \"\"\"\n    n_rows = len(self.query_instances)\n    n_cols = len(self.ref_instances)\n\n    col_tracks = {-1: self.ref_instances}\n    row_tracks = {-1: self.query_instances}\n\n    col_inds = [i for i in range(len(self.ref_instances))]\n    row_inds = [i for i in range(len(self.query_instances))]\n\n    if col_dims == \"track\":\n        col_tracks = self.get_tracks(self.ref_instances, col_grouping)\n        col_inds = list(col_tracks.keys())\n        n_cols = len(col_inds)\n\n    if row_dims == \"track\":\n        row_tracks = self.get_tracks(self.query_instances, row_grouping)\n        row_inds = list(row_tracks.keys())\n        n_rows = len(row_inds)\n\n    reduced_matrix = []\n    for row_track, row_instances in row_tracks.items():\n        for col_track, col_instances in col_tracks.items():\n            asso_matrix = self[row_instances, col_instances]\n\n            if col_dims == \"track\":\n                asso_matrix = reduce_method(asso_matrix, axis=1)\n\n            if row_dims == \"track\":\n                asso_matrix = reduce_method(asso_matrix, axis=0)\n\n            reduced_matrix.append(asso_matrix)\n\n    reduced_matrix = np.array(reduced_matrix).reshape(n_cols, n_rows).T\n\n    return pd.DataFrame(reduced_matrix, index=row_inds, columns=col_inds)\n</code></pre>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix.to","title":"<code>to(map_location)</code>","text":"<p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <p>Parameters:</p> Name Type Description Default <code>map_location</code> <code>str | device</code> <p>Either the device or dtype for the instance to be moved.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>Self</code> <p>reference to the instance moved to correct device/dtype.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def to(self, map_location: str | torch.device) -&gt; Self:\n    \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n    Args:\n        map_location: Either the device or dtype for the instance to be moved.\n\n    Returns:\n        self: reference to the instance moved to correct device/dtype.\n    \"\"\"\n    self.matrix = self.matrix.to(map_location)\n    self.ref_instances = [\n        instance.to(map_location) for instance in self.ref_instances\n    ]\n    self.query_instances = [\n        instance.to(map_location) for instance in self.query_instances\n    ]\n\n    return self\n</code></pre>"},{"location":"reference/dreem/io/association_matrix/#dreem.io.association_matrix.AssociationMatrix.to_dataframe","title":"<code>to_dataframe(row_labels='gt', col_labels='gt')</code>","text":"<p>Convert the association matrix to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>row_labels</code> <code>str</code> <p>How to label the rows(queries). If list, then must match # of rows/queries If <code>\"gt\"</code> then label by gt track id. If <code>\"pred\"</code> then label by pred track id. Otherwise label by the query_instance indices</p> <code>'gt'</code> <code>col_labels</code> <code>str</code> <p>How to label the columns(references). If list, then must match # of columns/refs If <code>\"gt\"</code> then label by gt track id. If <code>\"pred\"</code> then label by pred track id. Otherwise label by the ref_instance indices</p> <code>'gt'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The association matrix as a pandas dataframe.</p> Source code in <code>dreem/io/association_matrix.py</code> <pre><code>def to_dataframe(\n    self, row_labels: str = \"gt\", col_labels: str = \"gt\"\n) -&gt; pd.DataFrame:\n    \"\"\"Convert the association matrix to a pandas DataFrame.\n\n    Args:\n        row_labels: How to label the rows(queries).\n            If list, then must match # of rows/queries\n            If `\"gt\"` then label by gt track id.\n            If `\"pred\"` then label by pred track id.\n            Otherwise label by the query_instance indices\n        col_labels: How to label the columns(references).\n            If list, then must match # of columns/refs\n            If `\"gt\"` then label by gt track id.\n            If `\"pred\"` then label by pred track id.\n            Otherwise label by the ref_instance indices\n\n    Returns:\n        The association matrix as a pandas dataframe.\n    \"\"\"\n    matrix = self.numpy()\n\n    if not isinstance(row_labels, str):\n        if len(row_labels) == len(self.query_instances):\n            row_inds = row_labels\n\n        else:\n            raise ValueError(\n                (\n                    \"Mismatched # of rows and labels!\",\n                    f\"Found {len(row_labels)} with {len(self.query_instances)} rows\",\n                )\n            )\n\n    else:\n        if row_labels == \"gt\":\n            row_inds = [\n                instance.gt_track_id.item() for instance in self.query_instances\n            ]\n\n        elif row_labels == \"pred\":\n            row_inds = [\n                instance.pred_track_id.item() for instance in self.query_instances\n            ]\n\n        else:\n            row_inds = np.arange(len(self.query_instances))\n\n    if not isinstance(col_labels, str):\n        if len(col_labels) == len(self.ref_instances):\n            col_inds = col_labels\n\n        else:\n            raise ValueError(\n                (\n                    \"Mismatched # of columns and labels!\",\n                    f\"Found {len(col_labels)} with {len(self.ref_instances)} columns\",\n                )\n            )\n\n    else:\n        if col_labels == \"gt\":\n            col_inds = [\n                instance.gt_track_id.item() for instance in self.ref_instances\n            ]\n\n        elif col_labels == \"pred\":\n            col_inds = [\n                instance.pred_track_id.item() for instance in self.ref_instances\n            ]\n\n        else:\n            col_inds = np.arange(len(self.ref_instances))\n\n    asso_df = pd.DataFrame(matrix, index=row_inds, columns=col_inds)\n\n    return asso_df\n</code></pre>"},{"location":"reference/dreem/io/config/","title":"config","text":""},{"location":"reference/dreem/io/config/#dreem.io.config","title":"<code>dreem.io.config</code>","text":"<p>Data structures for handling config parsing.</p> <p>Classes:</p> Name Description <code>Config</code> <p>Class handling loading components based on config params.</p>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config","title":"<code>Config</code>","text":"<p>Class handling loading components based on config params.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class with config from hydra/omega conf.</p> <code>__repr__</code> <p>Object representation of config class.</p> <code>__str__</code> <p>Return a string representation of config class.</p> <code>from_yaml</code> <p>Load config directly from yaml.</p> <code>get</code> <p>Get config item.</p> <code>get_checkpointing</code> <p>Getter for lightning checkpointing callback.</p> <code>get_ctc_paths</code> <p>Get file paths from directory. Only for CTC datasets.</p> <code>get_data_paths</code> <p>Get file paths from directory. Only for SLEAP datasets.</p> <code>get_dataloader</code> <p>Getter for dataloader.</p> <code>get_dataset</code> <p>Getter for datasets.</p> <code>get_early_stopping</code> <p>Getter for lightning early stopping callback.</p> <code>get_gtr_runner</code> <p>Get lightning module for training, validation, and inference.</p> <code>get_logger</code> <p>Getter for logging callback.</p> <code>get_loss</code> <p>Getter for loss functions.</p> <code>get_model</code> <p>Getter for gtr model.</p> <code>get_optimizer</code> <p>Getter for optimizer.</p> <code>get_scheduler</code> <p>Getter for lr scheduler.</p> <code>get_tracker_cfg</code> <p>Getter for tracker config params.</p> <code>get_trainer</code> <p>Getter for the lightning trainer.</p> <code>set_hparams</code> <p>Setter function for overwriting specific hparams.</p> <p>Attributes:</p> Name Type Description <code>data_paths</code> <p>Get data paths.</p> Source code in <code>dreem/io/config.py</code> <pre><code>class Config:\n    \"\"\"Class handling loading components based on config params.\"\"\"\n\n    def __init__(self, cfg: DictConfig, params_cfg: DictConfig | None = None):\n        \"\"\"Initialize the class with config from hydra/omega conf.\n\n        First uses `base_param` file then overwrites with specific `params_config`.\n\n        Args:\n            cfg: The `DictConfig` containing all the hyperparameters needed for\n                training/evaluation.\n            params_cfg: The `DictConfig` containing subset of hyperparameters to override.\n                training/evaluation\n        \"\"\"\n        base_cfg = cfg\n        logger.info(f\"Base Config: {cfg}\")\n\n        if \"params_config\" in cfg:\n            params_cfg = OmegaConf.load(cfg.params_config)\n\n        if params_cfg:\n            logger.info(f\"Overwriting base config with {params_cfg}\")\n            with open_dict(base_cfg):\n                self.cfg = OmegaConf.merge(base_cfg, params_cfg)  # merge configs\n        else:\n            self.cfg = cfg\n\n        OmegaConf.set_struct(self.cfg, False)\n\n        self._vid_files = {}\n\n    def __repr__(self):\n        \"\"\"Object representation of config class.\"\"\"\n        return f\"Config({self.cfg})\"\n\n    def __str__(self):\n        \"\"\"Return a string representation of config class.\"\"\"\n        return f\"Config({self.cfg})\"\n\n    @classmethod\n    def from_yaml(cls, base_cfg_path: str, params_cfg_path: str | None = None) -&gt; None:\n        \"\"\"Load config directly from yaml.\n\n        Args:\n            base_cfg_path: path to base config file.\n            params_cfg_path: path to override params.\n        \"\"\"\n        base_cfg = OmegaConf.load(base_cfg_path)\n        params_cfg = OmegaConf.load(params_cfg_path) if params_cfg_path else None\n        return cls(base_cfg, params_cfg)\n\n    def set_hparams(self, hparams: dict) -&gt; bool:\n        \"\"\"Setter function for overwriting specific hparams.\n\n        Useful for changing 1 or 2 hyperparameters such as dataset.\n\n        Args:\n            hparams: A dict containing the hyperparameter to be overwritten and\n                the value to be changed\n\n        Returns:\n            `True` if config is successfully updated, `False` otherwise\n        \"\"\"\n        if hparams == {} or hparams is None:\n            logger.warning(\"Nothing to update!\")\n            return False\n        for hparam, val in hparams.items():\n            try:\n                OmegaConf.update(self.cfg, hparam, val)\n            except Exception as e:\n                logger.exception(f\"Failed to update {hparam} to {val} due to {e}\")\n                return False\n        return True\n\n    def get(self, key: str, default=None, cfg: dict = None):\n        \"\"\"Get config item.\n\n        Args:\n            key: key of item to return\n            default: default value to return if key is missing.\n            cfg: the config dict from which to retrieve an item\n        \"\"\"\n        if cfg is None:\n            cfg = self.cfg\n\n        param = cfg.get(key, default)\n\n        if isinstance(param, DictConfig):\n            param = OmegaConf.to_container(param, resolve=True)\n\n        return param\n\n    def get_model(self) -&gt; \"GlobalTrackingTransformer\":\n        \"\"\"Getter for gtr model.\n\n        Returns:\n            A global tracking transformer with parameters indicated by cfg\n        \"\"\"\n        from dreem.models import GlobalTrackingTransformer, GTRRunner\n\n        model_params = self.get(\"model\", {})\n\n        ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n        if ckpt_path is not None and len(ckpt_path) &gt; 0:\n            return GTRRunner.load_from_checkpoint(ckpt_path).model\n\n        return GlobalTrackingTransformer(**model_params)\n\n    def get_tracker_cfg(self) -&gt; dict:\n        \"\"\"Getter for tracker config params.\n\n        Returns:\n            A dict containing the init params for `Tracker`.\n        \"\"\"\n        return self.get(\"tracker\", {})\n\n    def get_gtr_runner(self, ckpt_path: str | None = None) -&gt; \"GTRRunner\":\n        \"\"\"Get lightning module for training, validation, and inference.\n\n        Args:\n            ckpt_path: path to checkpoint for override\n\n        Returns:\n            a gtr runner model\n        \"\"\"\n        from dreem.models import GTRRunner\n\n        keys = [\"tracker\", \"optimizer\", \"scheduler\", \"loss\", \"runner\", \"model\"]\n        args = [key + \"_cfg\" if key != \"runner\" else key for key in keys]\n\n        params = {}\n        for key, arg in zip(keys, args):\n            sub_params = self.get(key, {})\n\n            if len(sub_params) == 0:\n                logger.warning(\n                    f\"`{key}` not found in config or is empty. Using defaults for {arg}!\"\n                )\n\n            if key == \"runner\":\n                runner_params = sub_params\n                for k, v in runner_params.items():\n                    params[k] = v\n            else:\n                params[arg] = sub_params\n\n        ckpt_path = params[\"model_cfg\"].pop(\"ckpt_path\", None)\n\n        if ckpt_path is not None and ckpt_path != \"\":\n            model = GTRRunner.load_from_checkpoint(\n                ckpt_path, tracker_cfg=params[\"tracker_cfg\"], **runner_params\n            )\n\n        else:\n            model = GTRRunner(**params)\n\n        return model\n\n    def get_ctc_paths(\n        self, list_dir_path: list[str]\n    ) -&gt; tuple[list[str], list[str], list[str]]:\n        \"\"\"Get file paths from directory. Only for CTC datasets.\n\n        Args:\n            list_dir_path: list of directories to search for labels and videos\n\n        Returns:\n            lists of labels file paths and video file paths\n        \"\"\"\n        gt_list = []\n        raw_img_list = []\n        ctc_track_meta = []\n        # user can specify a list of directories, each of which can contain several subdirectories that come in pairs of (dset_name, dset_name_GT/TRA)\n        for dir_path in list_dir_path:\n            for subdir in os.listdir(dir_path):\n                if subdir.endswith(\"_GT\"):\n                    gt_path = os.path.join(dir_path, subdir, \"TRA\")\n                    raw_img_path = os.path.join(dir_path, subdir.replace(\"_GT\", \"\"))\n                    # get filepaths for all tif files in gt_path\n                    gt_list.append(glob.glob(os.path.join(gt_path, \"*.tif\")))\n                    # get filepaths for all tif files in raw_img_path\n                    raw_img_list.append(glob.glob(os.path.join(raw_img_path, \"*.tif\")))\n                    man_track_file = glob.glob(os.path.join(gt_path, \"man_track.txt\"))\n                    if len(man_track_file) &gt; 0:\n                        ctc_track_meta.append(man_track_file[0])\n                    else:\n                        logger.debug(\n                            f\"No man_track.txt file found in {gt_path}. Continuing...\"\n                        )\n                else:\n                    continue\n\n        return gt_list, raw_img_list, ctc_track_meta\n\n    def get_data_paths(self, mode: str, data_cfg: dict) -&gt; tuple[list[str], list[str]]:\n        \"\"\"Get file paths from directory. Only for SLEAP datasets.\n\n        Args:\n            mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n                train, val, or test params for dataset\n            data_cfg: Config for the dataset containing \"dir\" key.\n\n        Returns:\n            lists of labels file paths and video file paths respectively\n        \"\"\"\n        # hack to get around the fact that for test mode, get_data_paths is called before get_dataset.\n        # also, for train/val mode, data_cfg has had the dir key popped through self.get() called in get_dataset()\n        if mode == \"test\":\n            list_dir_path = data_cfg.get(\"dir\", {}).get(\"path\", None)\n            if list_dir_path is None:\n                raise ValueError(\n                    \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n                )\n            self.labels_suffix = data_cfg.get(\"dir\", {}).get(\"labels_suffix\")\n            self.vid_suffix = data_cfg.get(\"dir\", {}).get(\"vid_suffix\")\n        else:\n            list_dir_path = self.data_dirs\n        if not isinstance(list_dir_path, list):\n            list_dir_path = [list_dir_path]\n\n        if self.labels_suffix == \".slp\":\n            label_files = []\n            vid_files = []\n            for dir_path in list_dir_path:\n                logger.debug(f\"Searching `{dir_path}` directory\")\n                labels_path = f\"{dir_path}/*{self.labels_suffix}\"\n                vid_path = f\"{dir_path}/*{self.vid_suffix}\"\n                logger.debug(f\"Searching for labels matching {labels_path}\")\n                label_files.extend(glob.glob(labels_path))\n                logger.debug(f\"Searching for videos matching {vid_path}\")\n                vid_files.extend(glob.glob(vid_path))\n\n        elif self.labels_suffix == \".tif\":\n            label_files, vid_files, ctc_track_meta = self.get_ctc_paths(list_dir_path)\n\n        logger.debug(f\"Found {len(label_files)} labels and {len(vid_files)} videos\")\n\n        # backdoor to set label files directly in the configs (i.e. bypass dir.path)\n        if data_cfg.get(\"slp_files\", None):\n            logger.debug(\"Overriding label files with user provided list\")\n            slp_files = data_cfg.get(\"slp_files\")\n            if len(slp_files) &gt; 0:\n                label_files = slp_files\n        if data_cfg.get(\"video_files\", None):\n            individual_video_files = data_cfg.get(\"video_files\")\n            if len(individual_video_files) &gt; 0:\n                vid_files = individual_video_files\n        return label_files, vid_files\n\n    def get_dataset(\n        self,\n        mode: str,\n        label_files: list[str] | None = None,\n        vid_files: list[str | list[str]] = None,\n    ) -&gt; \"SleapDataset\" | \"CellTrackingDataset\":\n        \"\"\"Getter for datasets.\n\n        Args:\n            mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n                train, val, or test params for dataset\n            label_files: path to label_files for override\n            vid_files: path to vid_files for override\n\n        Returns:\n            Either a `SleapDataset` or `CellTrackingDataset` with params indicated by cfg\n        \"\"\"\n        from dreem.datasets import CellTrackingDataset, SleapDataset\n\n        dataset_params = self.get(\"dataset\")\n        if dataset_params is None:\n            raise KeyError(\"`dataset` key is missing from cfg!\")\n\n        if mode.lower() == \"train\":\n            dataset_params = self.get(\"train_dataset\", {}, dataset_params)\n        elif mode.lower() == \"val\":\n            dataset_params = self.get(\"val_dataset\", {}, dataset_params)\n        elif mode.lower() == \"test\":\n            dataset_params = self.get(\"test_dataset\", {}, dataset_params)\n        else:\n            raise ValueError(\n                \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n            )\n\n        # input validation\n        self.data_dirs = dataset_params.get(\"dir\", {}).get(\"path\", None)\n        self.labels_suffix = dataset_params.get(\"dir\", {}).get(\"labels_suffix\")\n        self.vid_suffix = dataset_params.get(\"dir\", {}).get(\"vid_suffix\")\n        if self.data_dirs is None:\n            raise ValueError(\n                \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n            )\n        if self.labels_suffix is None or self.vid_suffix is None:\n            raise KeyError(\n                f\"Must provide a labels suffix and vid suffix to search for but found {self.labels_suffix} and {self.vid_suffix}\"\n            )\n\n        # infer dataset type from the user provided suffix\n        if self.labels_suffix == \".slp\":\n            # during training, multiple files can be used at once, so label_files is not passed in\n            # during inference, a single label_files string can be passed in as get_data_paths is\n            # called before get_dataset, hence the check\n            if label_files is None or vid_files is None:\n                label_files, vid_files = self.get_data_paths(mode, dataset_params)\n            dataset_params[\"slp_files\"] = label_files\n            dataset_params[\"video_files\"] = vid_files\n            dataset_params[\"data_dirs\"] = self.data_dirs\n            self.data_paths = (mode, vid_files)\n\n            return SleapDataset(**dataset_params)\n\n        elif self.labels_suffix == \".tif\":\n            # for CTC datasets, pass in a list of gt and raw image directories, eaech of which contain tifs\n            ctc_track_meta = None\n            list_dir_path = self.data_dirs  # don't modify self.data_dirs\n            if not isinstance(list_dir_path, list):\n                list_dir_path = [list_dir_path]\n            if label_files is None or vid_files is None:\n                label_files, vid_files, ctc_track_meta = self.get_ctc_paths(\n                    list_dir_path\n                )\n            dataset_params[\"data_dirs\"] = self.data_dirs\n            # extract filepaths of all raw images and gt images (i.e. labelled masks)\n            dataset_params[\"gt_list\"] = label_files\n            dataset_params[\"raw_img_list\"] = vid_files\n            dataset_params[\"ctc_track_meta\"] = ctc_track_meta\n\n            return CellTrackingDataset(**dataset_params)\n\n        else:\n            raise ValueError(\n                \"Could not resolve dataset type from Config! Only .slp (SLEAP) and .tif (Cell Tracking Challenge) data formats are supported.\"\n            )\n\n    @property\n    def data_paths(self):\n        \"\"\"Get data paths.\"\"\"\n        return self._vid_files\n\n    @data_paths.setter\n    def data_paths(self, paths: tuple[str, list[str]]):\n        \"\"\"Set data paths.\n\n        Args:\n            paths: A tuple containing (mode, vid_files)\n        \"\"\"\n        mode, vid_files = paths\n        self._vid_files[mode] = vid_files\n\n    def get_dataloader(\n        self,\n        dataset: \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\",\n        mode: str,\n    ) -&gt; torch.utils.data.DataLoader:\n        \"\"\"Getter for dataloader.\n\n        Args:\n            dataset: the Sleap or Microscopy Dataset used to initialize the dataloader\n            mode: either [\"train\", \"val\", or \"test\"] indicates which dataset\n                config to use\n\n        Returns:\n            A torch dataloader for `dataset` with parameters configured as specified\n        \"\"\"\n        dataloader_params = self.get(\"dataloader\", {})\n        if mode.lower() == \"train\":\n            dataloader_params = self.get(\"train_dataloader\", {}, dataloader_params)\n        elif mode.lower() == \"val\":\n            dataloader_params = self.get(\"val_dataloader\", {}, dataloader_params)\n        elif mode.lower() == \"test\":\n            dataloader_params = self.get(\"test_dataloader\", {}, dataloader_params)\n        else:\n            raise ValueError(\n                \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n            )\n        if dataloader_params.get(\"num_workers\", 0) &gt; 0:\n            # prevent too many open files error\n            pin_memory = True\n            torch.multiprocessing.set_sharing_strategy(\"file_system\")\n        else:\n            pin_memory = False\n\n        return torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size=1,\n            pin_memory=pin_memory,\n            collate_fn=dataset.no_batching_fn,\n            **dataloader_params,\n        )\n\n    def get_optimizer(self, params: Iterable) -&gt; torch.optim.Optimizer:\n        \"\"\"Getter for optimizer.\n\n        Args:\n            params: iterable of model parameters to optimize or dicts defining\n                parameter groups\n\n        Returns:\n            A torch Optimizer with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_optimizer\n\n        optimizer_params = self.get(\"optimizer\")\n\n        return init_optimizer(params, optimizer_params)\n\n    def get_scheduler(\n        self, optimizer: torch.optim.Optimizer\n    ) -&gt; torch.optim.lr_scheduler.LRScheduler | None:\n        \"\"\"Getter for lr scheduler.\n\n        Args:\n            optimizer: The optimizer to wrap the scheduler around\n\n        Returns:\n            A torch learning rate scheduler with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_scheduler\n\n        lr_scheduler_params = self.get(\"scheduler\")\n\n        if lr_scheduler_params is None:\n            logger.warning(\n                \"`scheduler` key not found in cfg or is empty. No scheduler will be returned!\"\n            )\n            return None\n        return init_scheduler(optimizer, lr_scheduler_params)\n\n    def get_loss(self) -&gt; \"dreem.training.losses.AssoLoss\":\n        \"\"\"Getter for loss functions.\n\n        Returns:\n            An AssoLoss with specified params\n        \"\"\"\n        from dreem.training.losses import AssoLoss\n\n        loss_params = self.get(\"loss\", {})\n\n        if len(loss_params) == 0:\n            logger.warning(\n                \"`loss` key not found in cfg. Using default params for `AssoLoss`\"\n            )\n\n        return AssoLoss(**loss_params)\n\n    def get_logger(self) -&gt; pl.loggers.Logger:\n        \"\"\"Getter for logging callback.\n\n        Returns:\n            A Logger with specified params\n        \"\"\"\n        from dreem.models.model_utils import init_logger\n\n        logger_params = self.get(\"logging\", {})\n        if len(logger_params) == 0:\n            logger.warning(\n                \"`logging` key not found in cfg. No logger will be configured!\"\n            )\n\n        return init_logger(\n            logger_params, OmegaConf.to_container(self.cfg, resolve=True)\n        )\n\n    def get_early_stopping(self) -&gt; pl.callbacks.EarlyStopping:\n        \"\"\"Getter for lightning early stopping callback.\n\n        Returns:\n            A lightning early stopping callback with specified params\n        \"\"\"\n        early_stopping_params = self.get(\"early_stopping\", None)\n\n        if early_stopping_params is None:\n            logger.warning(\n                \"`early_stopping` was not found in cfg or was `null`. Early stopping will not be used!\"\n            )\n            return None\n        elif len(early_stopping_params) == 0:\n            logger.warning(\"`early_stopping` cfg is empty! Using defaults\")\n        return pl.callbacks.EarlyStopping(**early_stopping_params)\n\n    def get_checkpointing(self) -&gt; pl.callbacks.ModelCheckpoint:\n        \"\"\"Getter for lightning checkpointing callback.\n\n        Returns:\n            A lightning checkpointing callback with specified params\n        \"\"\"\n        # convert to dict to enable extracting/removing params\n        checkpoint_params = self.get(\"checkpointing\", {})\n        logging_params = self.get(\"logging\", {})\n\n        dirpath = checkpoint_params.pop(\"dirpath\", None)\n\n        if dirpath is None:\n            dirpath = f\"./models/{self.get('group', '', logging_params)}/{self.get('name', '', logging_params)}\"\n\n        dirpath = Path(dirpath).resolve()\n        if not Path(dirpath).exists():\n            try:\n                Path(dirpath).mkdir(parents=True, exist_ok=True)\n            except OSError as e:\n                logger.exception(\n                    f\"Cannot create a new folder!. Check the permissions to {dirpath}. \\n {e}\"\n                )\n\n        _ = checkpoint_params.pop(\"dirpath\", None)\n        monitor = checkpoint_params.pop(\"monitor\", [\"val_loss\"])\n        checkpointers = []\n\n        logger.info(\n            f\"Saving checkpoints to `{dirpath}` based on the following metrics: {monitor}\"\n        )\n        if len(checkpoint_params) == 0:\n            logger.warning(\n                \"\"\"`checkpointing` key was not found in cfg or was empty!\n                Configuring checkpointing to use default params!\"\"\"\n            )\n\n        for metric in monitor:\n            checkpointer = pl.callbacks.ModelCheckpoint(\n                monitor=metric,\n                dirpath=dirpath,\n                filename=f\"{{epoch}}-{{{metric}}}\",\n                **checkpoint_params,\n            )\n            checkpointer.CHECKPOINT_NAME_LAST = f\"{{epoch}}-final-{{{metric}}}\"\n            checkpointers.append(checkpointer)\n        return checkpointers\n\n    def get_trainer(\n        self,\n        callbacks: list[pl.callbacks.Callback] | None = None,\n        logger: pl.loggers.WandbLogger | None = None,\n        devices: int = 1,\n        accelerator: str = \"auto\",\n    ) -&gt; pl.Trainer:\n        \"\"\"Getter for the lightning trainer.\n\n        Args:\n            callbacks: a list of lightning callbacks preconfigured to be used\n                for training\n            logger: the Wandb logger used for logging during training\n            devices: The number of gpus to be used. 0 means cpu\n            accelerator: either \"gpu\" or \"cpu\" specifies which device to use\n\n        Returns:\n            A lightning Trainer with specified params\n        \"\"\"\n        trainer_params = self.get(\"trainer\", {})\n        profiler = trainer_params.pop(\"profiler\", None)\n        if len(trainer_params) == 0:\n            print(\n                \"`trainer` key was not found in cfg or was empty. Using defaults for `pl.Trainer`!\"\n            )\n\n        if \"accelerator\" not in trainer_params:\n            trainer_params[\"accelerator\"] = accelerator\n        if \"devices\" not in trainer_params:\n            trainer_params[\"devices\"] = devices\n\n        map_profiler = {\n            \"advanced\": pl.profilers.AdvancedProfiler,\n            \"simple\": pl.profilers.SimpleProfiler,\n            \"pytorch\": pl.profilers.PyTorchProfiler,\n            \"passthrough\": pl.profilers.PassThroughProfiler,\n            \"xla\": pl.profilers.XLAProfiler,\n        }\n\n        if profiler:\n            if profiler in map_profiler:\n                profiler = map_profiler[profiler](filename=\"profile\")\n            else:\n                raise ValueError(\n                    f\"Profiler {profiler} not supported! Please use one of {list(map_profiler.keys())}\"\n                )\n\n        return pl.Trainer(\n            callbacks=callbacks,\n            logger=logger,\n            profiler=profiler,\n            **trainer_params,\n        )\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.data_paths","title":"<code>data_paths</code>  <code>property</code> <code>writable</code>","text":"<p>Get data paths.</p>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.__init__","title":"<code>__init__(cfg, params_cfg=None)</code>","text":"<p>Initialize the class with config from hydra/omega conf.</p> <p>First uses <code>base_param</code> file then overwrites with specific <code>params_config</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>The <code>DictConfig</code> containing all the hyperparameters needed for training/evaluation.</p> required <code>params_cfg</code> <code>DictConfig | None</code> <p>The <code>DictConfig</code> containing subset of hyperparameters to override. training/evaluation</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>def __init__(self, cfg: DictConfig, params_cfg: DictConfig | None = None):\n    \"\"\"Initialize the class with config from hydra/omega conf.\n\n    First uses `base_param` file then overwrites with specific `params_config`.\n\n    Args:\n        cfg: The `DictConfig` containing all the hyperparameters needed for\n            training/evaluation.\n        params_cfg: The `DictConfig` containing subset of hyperparameters to override.\n            training/evaluation\n    \"\"\"\n    base_cfg = cfg\n    logger.info(f\"Base Config: {cfg}\")\n\n    if \"params_config\" in cfg:\n        params_cfg = OmegaConf.load(cfg.params_config)\n\n    if params_cfg:\n        logger.info(f\"Overwriting base config with {params_cfg}\")\n        with open_dict(base_cfg):\n            self.cfg = OmegaConf.merge(base_cfg, params_cfg)  # merge configs\n    else:\n        self.cfg = cfg\n\n    OmegaConf.set_struct(self.cfg, False)\n\n    self._vid_files = {}\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.__repr__","title":"<code>__repr__()</code>","text":"<p>Object representation of config class.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def __repr__(self):\n    \"\"\"Object representation of config class.\"\"\"\n    return f\"Config({self.cfg})\"\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of config class.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def __str__(self):\n    \"\"\"Return a string representation of config class.\"\"\"\n    return f\"Config({self.cfg})\"\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.from_yaml","title":"<code>from_yaml(base_cfg_path, params_cfg_path=None)</code>  <code>classmethod</code>","text":"<p>Load config directly from yaml.</p> <p>Parameters:</p> Name Type Description Default <code>base_cfg_path</code> <code>str</code> <p>path to base config file.</p> required <code>params_cfg_path</code> <code>str | None</code> <p>path to override params.</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>@classmethod\ndef from_yaml(cls, base_cfg_path: str, params_cfg_path: str | None = None) -&gt; None:\n    \"\"\"Load config directly from yaml.\n\n    Args:\n        base_cfg_path: path to base config file.\n        params_cfg_path: path to override params.\n    \"\"\"\n    base_cfg = OmegaConf.load(base_cfg_path)\n    params_cfg = OmegaConf.load(params_cfg_path) if params_cfg_path else None\n    return cls(base_cfg, params_cfg)\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get","title":"<code>get(key, default=None, cfg=None)</code>","text":"<p>Get config item.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key of item to return</p> required <code>default</code> <p>default value to return if key is missing.</p> <code>None</code> <code>cfg</code> <code>dict</code> <p>the config dict from which to retrieve an item</p> <code>None</code> Source code in <code>dreem/io/config.py</code> <pre><code>def get(self, key: str, default=None, cfg: dict = None):\n    \"\"\"Get config item.\n\n    Args:\n        key: key of item to return\n        default: default value to return if key is missing.\n        cfg: the config dict from which to retrieve an item\n    \"\"\"\n    if cfg is None:\n        cfg = self.cfg\n\n    param = cfg.get(key, default)\n\n    if isinstance(param, DictConfig):\n        param = OmegaConf.to_container(param, resolve=True)\n\n    return param\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_checkpointing","title":"<code>get_checkpointing()</code>","text":"<p>Getter for lightning checkpointing callback.</p> <p>Returns:</p> Type Description <code>ModelCheckpoint</code> <p>A lightning checkpointing callback with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_checkpointing(self) -&gt; pl.callbacks.ModelCheckpoint:\n    \"\"\"Getter for lightning checkpointing callback.\n\n    Returns:\n        A lightning checkpointing callback with specified params\n    \"\"\"\n    # convert to dict to enable extracting/removing params\n    checkpoint_params = self.get(\"checkpointing\", {})\n    logging_params = self.get(\"logging\", {})\n\n    dirpath = checkpoint_params.pop(\"dirpath\", None)\n\n    if dirpath is None:\n        dirpath = f\"./models/{self.get('group', '', logging_params)}/{self.get('name', '', logging_params)}\"\n\n    dirpath = Path(dirpath).resolve()\n    if not Path(dirpath).exists():\n        try:\n            Path(dirpath).mkdir(parents=True, exist_ok=True)\n        except OSError as e:\n            logger.exception(\n                f\"Cannot create a new folder!. Check the permissions to {dirpath}. \\n {e}\"\n            )\n\n    _ = checkpoint_params.pop(\"dirpath\", None)\n    monitor = checkpoint_params.pop(\"monitor\", [\"val_loss\"])\n    checkpointers = []\n\n    logger.info(\n        f\"Saving checkpoints to `{dirpath}` based on the following metrics: {monitor}\"\n    )\n    if len(checkpoint_params) == 0:\n        logger.warning(\n            \"\"\"`checkpointing` key was not found in cfg or was empty!\n            Configuring checkpointing to use default params!\"\"\"\n        )\n\n    for metric in monitor:\n        checkpointer = pl.callbacks.ModelCheckpoint(\n            monitor=metric,\n            dirpath=dirpath,\n            filename=f\"{{epoch}}-{{{metric}}}\",\n            **checkpoint_params,\n        )\n        checkpointer.CHECKPOINT_NAME_LAST = f\"{{epoch}}-final-{{{metric}}}\"\n        checkpointers.append(checkpointer)\n    return checkpointers\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_ctc_paths","title":"<code>get_ctc_paths(list_dir_path)</code>","text":"<p>Get file paths from directory. Only for CTC datasets.</p> <p>Parameters:</p> Name Type Description Default <code>list_dir_path</code> <code>list[str]</code> <p>list of directories to search for labels and videos</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str], list[str]]</code> <p>lists of labels file paths and video file paths</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_ctc_paths(\n    self, list_dir_path: list[str]\n) -&gt; tuple[list[str], list[str], list[str]]:\n    \"\"\"Get file paths from directory. Only for CTC datasets.\n\n    Args:\n        list_dir_path: list of directories to search for labels and videos\n\n    Returns:\n        lists of labels file paths and video file paths\n    \"\"\"\n    gt_list = []\n    raw_img_list = []\n    ctc_track_meta = []\n    # user can specify a list of directories, each of which can contain several subdirectories that come in pairs of (dset_name, dset_name_GT/TRA)\n    for dir_path in list_dir_path:\n        for subdir in os.listdir(dir_path):\n            if subdir.endswith(\"_GT\"):\n                gt_path = os.path.join(dir_path, subdir, \"TRA\")\n                raw_img_path = os.path.join(dir_path, subdir.replace(\"_GT\", \"\"))\n                # get filepaths for all tif files in gt_path\n                gt_list.append(glob.glob(os.path.join(gt_path, \"*.tif\")))\n                # get filepaths for all tif files in raw_img_path\n                raw_img_list.append(glob.glob(os.path.join(raw_img_path, \"*.tif\")))\n                man_track_file = glob.glob(os.path.join(gt_path, \"man_track.txt\"))\n                if len(man_track_file) &gt; 0:\n                    ctc_track_meta.append(man_track_file[0])\n                else:\n                    logger.debug(\n                        f\"No man_track.txt file found in {gt_path}. Continuing...\"\n                    )\n            else:\n                continue\n\n    return gt_list, raw_img_list, ctc_track_meta\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_data_paths","title":"<code>get_data_paths(mode, data_cfg)</code>","text":"<p>Get file paths from directory. Only for SLEAP datasets.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>[None, \"train\", \"test\", \"val\"]. Indicates whether to use train, val, or test params for dataset</p> required <code>data_cfg</code> <code>dict</code> <p>Config for the dataset containing \"dir\" key.</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str]]</code> <p>lists of labels file paths and video file paths respectively</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_data_paths(self, mode: str, data_cfg: dict) -&gt; tuple[list[str], list[str]]:\n    \"\"\"Get file paths from directory. Only for SLEAP datasets.\n\n    Args:\n        mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n            train, val, or test params for dataset\n        data_cfg: Config for the dataset containing \"dir\" key.\n\n    Returns:\n        lists of labels file paths and video file paths respectively\n    \"\"\"\n    # hack to get around the fact that for test mode, get_data_paths is called before get_dataset.\n    # also, for train/val mode, data_cfg has had the dir key popped through self.get() called in get_dataset()\n    if mode == \"test\":\n        list_dir_path = data_cfg.get(\"dir\", {}).get(\"path\", None)\n        if list_dir_path is None:\n            raise ValueError(\n                \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n            )\n        self.labels_suffix = data_cfg.get(\"dir\", {}).get(\"labels_suffix\")\n        self.vid_suffix = data_cfg.get(\"dir\", {}).get(\"vid_suffix\")\n    else:\n        list_dir_path = self.data_dirs\n    if not isinstance(list_dir_path, list):\n        list_dir_path = [list_dir_path]\n\n    if self.labels_suffix == \".slp\":\n        label_files = []\n        vid_files = []\n        for dir_path in list_dir_path:\n            logger.debug(f\"Searching `{dir_path}` directory\")\n            labels_path = f\"{dir_path}/*{self.labels_suffix}\"\n            vid_path = f\"{dir_path}/*{self.vid_suffix}\"\n            logger.debug(f\"Searching for labels matching {labels_path}\")\n            label_files.extend(glob.glob(labels_path))\n            logger.debug(f\"Searching for videos matching {vid_path}\")\n            vid_files.extend(glob.glob(vid_path))\n\n    elif self.labels_suffix == \".tif\":\n        label_files, vid_files, ctc_track_meta = self.get_ctc_paths(list_dir_path)\n\n    logger.debug(f\"Found {len(label_files)} labels and {len(vid_files)} videos\")\n\n    # backdoor to set label files directly in the configs (i.e. bypass dir.path)\n    if data_cfg.get(\"slp_files\", None):\n        logger.debug(\"Overriding label files with user provided list\")\n        slp_files = data_cfg.get(\"slp_files\")\n        if len(slp_files) &gt; 0:\n            label_files = slp_files\n    if data_cfg.get(\"video_files\", None):\n        individual_video_files = data_cfg.get(\"video_files\")\n        if len(individual_video_files) &gt; 0:\n            vid_files = individual_video_files\n    return label_files, vid_files\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_dataloader","title":"<code>get_dataloader(dataset, mode)</code>","text":"<p>Getter for dataloader.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>'SleapDataset' | 'MicroscopyDataset' | 'CellTrackingDataset'</code> <p>the Sleap or Microscopy Dataset used to initialize the dataloader</p> required <code>mode</code> <code>str</code> <p>either [\"train\", \"val\", or \"test\"] indicates which dataset config to use</p> required <p>Returns:</p> Type Description <code>DataLoader</code> <p>A torch dataloader for <code>dataset</code> with parameters configured as specified</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_dataloader(\n    self,\n    dataset: \"SleapDataset\" | \"MicroscopyDataset\" | \"CellTrackingDataset\",\n    mode: str,\n) -&gt; torch.utils.data.DataLoader:\n    \"\"\"Getter for dataloader.\n\n    Args:\n        dataset: the Sleap or Microscopy Dataset used to initialize the dataloader\n        mode: either [\"train\", \"val\", or \"test\"] indicates which dataset\n            config to use\n\n    Returns:\n        A torch dataloader for `dataset` with parameters configured as specified\n    \"\"\"\n    dataloader_params = self.get(\"dataloader\", {})\n    if mode.lower() == \"train\":\n        dataloader_params = self.get(\"train_dataloader\", {}, dataloader_params)\n    elif mode.lower() == \"val\":\n        dataloader_params = self.get(\"val_dataloader\", {}, dataloader_params)\n    elif mode.lower() == \"test\":\n        dataloader_params = self.get(\"test_dataloader\", {}, dataloader_params)\n    else:\n        raise ValueError(\n            \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n        )\n    if dataloader_params.get(\"num_workers\", 0) &gt; 0:\n        # prevent too many open files error\n        pin_memory = True\n        torch.multiprocessing.set_sharing_strategy(\"file_system\")\n    else:\n        pin_memory = False\n\n    return torch.utils.data.DataLoader(\n        dataset=dataset,\n        batch_size=1,\n        pin_memory=pin_memory,\n        collate_fn=dataset.no_batching_fn,\n        **dataloader_params,\n    )\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_dataset","title":"<code>get_dataset(mode, label_files=None, vid_files=None)</code>","text":"<p>Getter for datasets.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>[None, \"train\", \"test\", \"val\"]. Indicates whether to use train, val, or test params for dataset</p> required <code>label_files</code> <code>list[str] | None</code> <p>path to label_files for override</p> <code>None</code> <code>vid_files</code> <code>list[str | list[str]]</code> <p>path to vid_files for override</p> <code>None</code> <p>Returns:</p> Type Description <code>'SleapDataset' | 'CellTrackingDataset'</code> <p>Either a <code>SleapDataset</code> or <code>CellTrackingDataset</code> with params indicated by cfg</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_dataset(\n    self,\n    mode: str,\n    label_files: list[str] | None = None,\n    vid_files: list[str | list[str]] = None,\n) -&gt; \"SleapDataset\" | \"CellTrackingDataset\":\n    \"\"\"Getter for datasets.\n\n    Args:\n        mode: [None, \"train\", \"test\", \"val\"]. Indicates whether to use\n            train, val, or test params for dataset\n        label_files: path to label_files for override\n        vid_files: path to vid_files for override\n\n    Returns:\n        Either a `SleapDataset` or `CellTrackingDataset` with params indicated by cfg\n    \"\"\"\n    from dreem.datasets import CellTrackingDataset, SleapDataset\n\n    dataset_params = self.get(\"dataset\")\n    if dataset_params is None:\n        raise KeyError(\"`dataset` key is missing from cfg!\")\n\n    if mode.lower() == \"train\":\n        dataset_params = self.get(\"train_dataset\", {}, dataset_params)\n    elif mode.lower() == \"val\":\n        dataset_params = self.get(\"val_dataset\", {}, dataset_params)\n    elif mode.lower() == \"test\":\n        dataset_params = self.get(\"test_dataset\", {}, dataset_params)\n    else:\n        raise ValueError(\n            \"`mode` must be one of ['train', 'val','test'], not '{mode}'\"\n        )\n\n    # input validation\n    self.data_dirs = dataset_params.get(\"dir\", {}).get(\"path\", None)\n    self.labels_suffix = dataset_params.get(\"dir\", {}).get(\"labels_suffix\")\n    self.vid_suffix = dataset_params.get(\"dir\", {}).get(\"vid_suffix\")\n    if self.data_dirs is None:\n        raise ValueError(\n            \"`dir` is missing from dataset config. Please provide a path to the directory containing the labels and videos.\"\n        )\n    if self.labels_suffix is None or self.vid_suffix is None:\n        raise KeyError(\n            f\"Must provide a labels suffix and vid suffix to search for but found {self.labels_suffix} and {self.vid_suffix}\"\n        )\n\n    # infer dataset type from the user provided suffix\n    if self.labels_suffix == \".slp\":\n        # during training, multiple files can be used at once, so label_files is not passed in\n        # during inference, a single label_files string can be passed in as get_data_paths is\n        # called before get_dataset, hence the check\n        if label_files is None or vid_files is None:\n            label_files, vid_files = self.get_data_paths(mode, dataset_params)\n        dataset_params[\"slp_files\"] = label_files\n        dataset_params[\"video_files\"] = vid_files\n        dataset_params[\"data_dirs\"] = self.data_dirs\n        self.data_paths = (mode, vid_files)\n\n        return SleapDataset(**dataset_params)\n\n    elif self.labels_suffix == \".tif\":\n        # for CTC datasets, pass in a list of gt and raw image directories, eaech of which contain tifs\n        ctc_track_meta = None\n        list_dir_path = self.data_dirs  # don't modify self.data_dirs\n        if not isinstance(list_dir_path, list):\n            list_dir_path = [list_dir_path]\n        if label_files is None or vid_files is None:\n            label_files, vid_files, ctc_track_meta = self.get_ctc_paths(\n                list_dir_path\n            )\n        dataset_params[\"data_dirs\"] = self.data_dirs\n        # extract filepaths of all raw images and gt images (i.e. labelled masks)\n        dataset_params[\"gt_list\"] = label_files\n        dataset_params[\"raw_img_list\"] = vid_files\n        dataset_params[\"ctc_track_meta\"] = ctc_track_meta\n\n        return CellTrackingDataset(**dataset_params)\n\n    else:\n        raise ValueError(\n            \"Could not resolve dataset type from Config! Only .slp (SLEAP) and .tif (Cell Tracking Challenge) data formats are supported.\"\n        )\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_early_stopping","title":"<code>get_early_stopping()</code>","text":"<p>Getter for lightning early stopping callback.</p> <p>Returns:</p> Type Description <code>EarlyStopping</code> <p>A lightning early stopping callback with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_early_stopping(self) -&gt; pl.callbacks.EarlyStopping:\n    \"\"\"Getter for lightning early stopping callback.\n\n    Returns:\n        A lightning early stopping callback with specified params\n    \"\"\"\n    early_stopping_params = self.get(\"early_stopping\", None)\n\n    if early_stopping_params is None:\n        logger.warning(\n            \"`early_stopping` was not found in cfg or was `null`. Early stopping will not be used!\"\n        )\n        return None\n    elif len(early_stopping_params) == 0:\n        logger.warning(\"`early_stopping` cfg is empty! Using defaults\")\n    return pl.callbacks.EarlyStopping(**early_stopping_params)\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_gtr_runner","title":"<code>get_gtr_runner(ckpt_path=None)</code>","text":"<p>Get lightning module for training, validation, and inference.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_path</code> <code>str | None</code> <p>path to checkpoint for override</p> <code>None</code> <p>Returns:</p> Type Description <code>'GTRRunner'</code> <p>a gtr runner model</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_gtr_runner(self, ckpt_path: str | None = None) -&gt; \"GTRRunner\":\n    \"\"\"Get lightning module for training, validation, and inference.\n\n    Args:\n        ckpt_path: path to checkpoint for override\n\n    Returns:\n        a gtr runner model\n    \"\"\"\n    from dreem.models import GTRRunner\n\n    keys = [\"tracker\", \"optimizer\", \"scheduler\", \"loss\", \"runner\", \"model\"]\n    args = [key + \"_cfg\" if key != \"runner\" else key for key in keys]\n\n    params = {}\n    for key, arg in zip(keys, args):\n        sub_params = self.get(key, {})\n\n        if len(sub_params) == 0:\n            logger.warning(\n                f\"`{key}` not found in config or is empty. Using defaults for {arg}!\"\n            )\n\n        if key == \"runner\":\n            runner_params = sub_params\n            for k, v in runner_params.items():\n                params[k] = v\n        else:\n            params[arg] = sub_params\n\n    ckpt_path = params[\"model_cfg\"].pop(\"ckpt_path\", None)\n\n    if ckpt_path is not None and ckpt_path != \"\":\n        model = GTRRunner.load_from_checkpoint(\n            ckpt_path, tracker_cfg=params[\"tracker_cfg\"], **runner_params\n        )\n\n    else:\n        model = GTRRunner(**params)\n\n    return model\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_logger","title":"<code>get_logger()</code>","text":"<p>Getter for logging callback.</p> <p>Returns:</p> Type Description <code>Logger</code> <p>A Logger with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_logger(self) -&gt; pl.loggers.Logger:\n    \"\"\"Getter for logging callback.\n\n    Returns:\n        A Logger with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_logger\n\n    logger_params = self.get(\"logging\", {})\n    if len(logger_params) == 0:\n        logger.warning(\n            \"`logging` key not found in cfg. No logger will be configured!\"\n        )\n\n    return init_logger(\n        logger_params, OmegaConf.to_container(self.cfg, resolve=True)\n    )\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_loss","title":"<code>get_loss()</code>","text":"<p>Getter for loss functions.</p> <p>Returns:</p> Type Description <code>'dreem.training.losses.AssoLoss'</code> <p>An AssoLoss with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_loss(self) -&gt; \"dreem.training.losses.AssoLoss\":\n    \"\"\"Getter for loss functions.\n\n    Returns:\n        An AssoLoss with specified params\n    \"\"\"\n    from dreem.training.losses import AssoLoss\n\n    loss_params = self.get(\"loss\", {})\n\n    if len(loss_params) == 0:\n        logger.warning(\n            \"`loss` key not found in cfg. Using default params for `AssoLoss`\"\n        )\n\n    return AssoLoss(**loss_params)\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_model","title":"<code>get_model()</code>","text":"<p>Getter for gtr model.</p> <p>Returns:</p> Type Description <code>'GlobalTrackingTransformer'</code> <p>A global tracking transformer with parameters indicated by cfg</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_model(self) -&gt; \"GlobalTrackingTransformer\":\n    \"\"\"Getter for gtr model.\n\n    Returns:\n        A global tracking transformer with parameters indicated by cfg\n    \"\"\"\n    from dreem.models import GlobalTrackingTransformer, GTRRunner\n\n    model_params = self.get(\"model\", {})\n\n    ckpt_path = model_params.pop(\"ckpt_path\", None)\n\n    if ckpt_path is not None and len(ckpt_path) &gt; 0:\n        return GTRRunner.load_from_checkpoint(ckpt_path).model\n\n    return GlobalTrackingTransformer(**model_params)\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_optimizer","title":"<code>get_optimizer(params)</code>","text":"<p>Getter for optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable</code> <p>iterable of model parameters to optimize or dicts defining parameter groups</p> required <p>Returns:</p> Type Description <code>Optimizer</code> <p>A torch Optimizer with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_optimizer(self, params: Iterable) -&gt; torch.optim.Optimizer:\n    \"\"\"Getter for optimizer.\n\n    Args:\n        params: iterable of model parameters to optimize or dicts defining\n            parameter groups\n\n    Returns:\n        A torch Optimizer with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_optimizer\n\n    optimizer_params = self.get(\"optimizer\")\n\n    return init_optimizer(params, optimizer_params)\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_scheduler","title":"<code>get_scheduler(optimizer)</code>","text":"<p>Getter for lr scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to wrap the scheduler around</p> required <p>Returns:</p> Type Description <code>LRScheduler | None</code> <p>A torch learning rate scheduler with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_scheduler(\n    self, optimizer: torch.optim.Optimizer\n) -&gt; torch.optim.lr_scheduler.LRScheduler | None:\n    \"\"\"Getter for lr scheduler.\n\n    Args:\n        optimizer: The optimizer to wrap the scheduler around\n\n    Returns:\n        A torch learning rate scheduler with specified params\n    \"\"\"\n    from dreem.models.model_utils import init_scheduler\n\n    lr_scheduler_params = self.get(\"scheduler\")\n\n    if lr_scheduler_params is None:\n        logger.warning(\n            \"`scheduler` key not found in cfg or is empty. No scheduler will be returned!\"\n        )\n        return None\n    return init_scheduler(optimizer, lr_scheduler_params)\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_tracker_cfg","title":"<code>get_tracker_cfg()</code>","text":"<p>Getter for tracker config params.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict containing the init params for <code>Tracker</code>.</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_tracker_cfg(self) -&gt; dict:\n    \"\"\"Getter for tracker config params.\n\n    Returns:\n        A dict containing the init params for `Tracker`.\n    \"\"\"\n    return self.get(\"tracker\", {})\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.get_trainer","title":"<code>get_trainer(callbacks=None, logger=None, devices=1, accelerator='auto')</code>","text":"<p>Getter for the lightning trainer.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list[Callback] | None</code> <p>a list of lightning callbacks preconfigured to be used for training</p> <code>None</code> <code>logger</code> <code>WandbLogger | None</code> <p>the Wandb logger used for logging during training</p> <code>None</code> <code>devices</code> <code>int</code> <p>The number of gpus to be used. 0 means cpu</p> <code>1</code> <code>accelerator</code> <code>str</code> <p>either \"gpu\" or \"cpu\" specifies which device to use</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>Trainer</code> <p>A lightning Trainer with specified params</p> Source code in <code>dreem/io/config.py</code> <pre><code>def get_trainer(\n    self,\n    callbacks: list[pl.callbacks.Callback] | None = None,\n    logger: pl.loggers.WandbLogger | None = None,\n    devices: int = 1,\n    accelerator: str = \"auto\",\n) -&gt; pl.Trainer:\n    \"\"\"Getter for the lightning trainer.\n\n    Args:\n        callbacks: a list of lightning callbacks preconfigured to be used\n            for training\n        logger: the Wandb logger used for logging during training\n        devices: The number of gpus to be used. 0 means cpu\n        accelerator: either \"gpu\" or \"cpu\" specifies which device to use\n\n    Returns:\n        A lightning Trainer with specified params\n    \"\"\"\n    trainer_params = self.get(\"trainer\", {})\n    profiler = trainer_params.pop(\"profiler\", None)\n    if len(trainer_params) == 0:\n        print(\n            \"`trainer` key was not found in cfg or was empty. Using defaults for `pl.Trainer`!\"\n        )\n\n    if \"accelerator\" not in trainer_params:\n        trainer_params[\"accelerator\"] = accelerator\n    if \"devices\" not in trainer_params:\n        trainer_params[\"devices\"] = devices\n\n    map_profiler = {\n        \"advanced\": pl.profilers.AdvancedProfiler,\n        \"simple\": pl.profilers.SimpleProfiler,\n        \"pytorch\": pl.profilers.PyTorchProfiler,\n        \"passthrough\": pl.profilers.PassThroughProfiler,\n        \"xla\": pl.profilers.XLAProfiler,\n    }\n\n    if profiler:\n        if profiler in map_profiler:\n            profiler = map_profiler[profiler](filename=\"profile\")\n        else:\n            raise ValueError(\n                f\"Profiler {profiler} not supported! Please use one of {list(map_profiler.keys())}\"\n            )\n\n    return pl.Trainer(\n        callbacks=callbacks,\n        logger=logger,\n        profiler=profiler,\n        **trainer_params,\n    )\n</code></pre>"},{"location":"reference/dreem/io/config/#dreem.io.config.Config.set_hparams","title":"<code>set_hparams(hparams)</code>","text":"<p>Setter function for overwriting specific hparams.</p> <p>Useful for changing 1 or 2 hyperparameters such as dataset.</p> <p>Parameters:</p> Name Type Description Default <code>hparams</code> <code>dict</code> <p>A dict containing the hyperparameter to be overwritten and the value to be changed</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if config is successfully updated, <code>False</code> otherwise</p> Source code in <code>dreem/io/config.py</code> <pre><code>def set_hparams(self, hparams: dict) -&gt; bool:\n    \"\"\"Setter function for overwriting specific hparams.\n\n    Useful for changing 1 or 2 hyperparameters such as dataset.\n\n    Args:\n        hparams: A dict containing the hyperparameter to be overwritten and\n            the value to be changed\n\n    Returns:\n        `True` if config is successfully updated, `False` otherwise\n    \"\"\"\n    if hparams == {} or hparams is None:\n        logger.warning(\"Nothing to update!\")\n        return False\n    for hparam, val in hparams.items():\n        try:\n            OmegaConf.update(self.cfg, hparam, val)\n        except Exception as e:\n            logger.exception(f\"Failed to update {hparam} to {val} due to {e}\")\n            return False\n    return True\n</code></pre>"},{"location":"reference/dreem/io/frame/","title":"frame","text":""},{"location":"reference/dreem/io/frame/#dreem.io.frame","title":"<code>dreem.io.frame</code>","text":"<p>Module containing data classes such as Instances and Frames.</p> <p>Classes:</p> Name Description <code>Frame</code> <p>Data structure containing metadata for a single frame of a video.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame","title":"<code>Frame</code>","text":"<p>Data structure containing metadata for a single frame of a video.</p> <p>Attributes:</p> Name Type Description <code>video_id</code> <code>Tensor</code> <p>The video index in the dataset.</p> <code>frame_id</code> <code>Tensor</code> <p>The index of the frame in a video.</p> <code>vid_file</code> <code>Tensor</code> <p>The path to the video the frame is from.</p> <code>img_shape</code> <code>Tensor</code> <p>The shape of the original frame (not the crop).</p> <code>instances</code> <code>list['Instance']</code> <p>A list of Instance objects that appear in the frame.</p> <code>asso_output</code> <code>'AssociationMatrix'</code> <p>The association matrix between instances output directly from the transformer.</p> <code>matches</code> <code>tuple</code> <p>matches from LSA algorithm between the instances and available trajectories during tracking.</p> <code>traj_score</code> <code>tuple</code> <p>Either a dict containing the association matrix between instances and trajectories along postprocessing pipeline or a single association matrix.</p> <code>device</code> <code>str</code> <p>The device the frame should be moved to.</p> <p>Methods:</p> Name Description <code>__attrs_post_init__</code> <p>Handle more intricate default initializations and moving to device.</p> <code>__repr__</code> <p>Return String representation of the Frame.</p> <code>add_traj_score</code> <p>Add trajectory score to dictionary.</p> <code>from_slp</code> <p>Convert <code>sio.LabeledFrame</code> to <code>dreem.io.Frame</code>.</p> <code>get_anchors</code> <p>Get the anchor names of instances in the frame.</p> <code>get_bboxes</code> <p>Get the bounding boxes of all instances in the frame.</p> <code>get_centroids</code> <p>Get the centroids around which each instance's crop was formed.</p> <code>get_crops</code> <p>Get the crops of all instances in the frame.</p> <code>get_features</code> <p>Get the reid feature vectors of all instances in the frame.</p> <code>get_gt_track_ids</code> <p>Get the gt track ids of all instances in the frame.</p> <code>get_pred_track_ids</code> <p>Get the pred track ids of all instances in the frame.</p> <code>get_traj_score</code> <p>Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.</p> <code>has_asso_output</code> <p>Determine whether the frame has an association matrix computed.</p> <code>has_bboxes</code> <p>Check if any of frames instances has a bounding box.</p> <code>has_crops</code> <p>Check if any of frames instances has a crop.</p> <code>has_features</code> <p>Check if any of frames instances has reid features already computed.</p> <code>has_gt_track_ids</code> <p>Check if any of frames instances has a gt track id.</p> <code>has_instances</code> <p>Determine whether there are instances in the frame.</p> <code>has_matches</code> <p>Check whether or not matches have been computed for frame.</p> <code>has_pred_track_ids</code> <p>Check if any of frames instances has a pred track id.</p> <code>has_traj_score</code> <p>Check if any trajectory association matrix has been saved.</p> <code>to</code> <p>Move frame to different device or dtype (See <code>torch.to</code> for more info).</p> <code>to_h5</code> <p>Convert frame to h5py group.</p> <code>to_slp</code> <p>Convert Frame to sleap_io.LabeledFrame object.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>@attrs.define(eq=False)\nclass Frame:\n    \"\"\"Data structure containing metadata for a single frame of a video.\n\n    Attributes:\n        video_id: The video index in the dataset.\n        frame_id: The index of the frame in a video.\n        vid_file: The path to the video the frame is from.\n        img_shape: The shape of the original frame (not the crop).\n        instances: A list of Instance objects that appear in the frame.\n        asso_output: The association matrix between instances\n            output directly from the transformer.\n        matches: matches from LSA algorithm between the instances and\n            available trajectories during tracking.\n        traj_score: Either a dict containing the association matrix\n            between instances and trajectories along postprocessing pipeline\n            or a single association matrix.\n        device: The device the frame should be moved to.\n    \"\"\"\n\n    _video_id: int = attrs.field(alias=\"video_id\", converter=_to_tensor)\n    _frame_id: int = attrs.field(alias=\"frame_id\", converter=_to_tensor)\n    _video: str = attrs.field(alias=\"vid_file\", default=\"\")\n    _img_shape: ArrayLike = attrs.field(\n        alias=\"img_shape\", converter=_to_tensor, factory=list\n    )\n\n    _instances: list[\"Instance\"] = attrs.field(alias=\"instances\", factory=list)\n    _asso_output: \"AssociationMatrix\" | None = attrs.field(\n        alias=\"asso_output\", default=None\n    )\n    _matches: tuple = attrs.field(alias=\"matches\", factory=tuple)\n    _traj_score: dict = attrs.field(alias=\"traj_score\", factory=dict)\n    _device: str | torch.device | None = attrs.field(alias=\"device\", default=None)\n\n    def __attrs_post_init__(self) -&gt; None:\n        \"\"\"Handle more intricate default initializations and moving to device.\"\"\"\n        if len(self.img_shape) == 0:\n            self.img_shape = torch.tensor([0, 0, 0])\n\n        for instance in self.instances:\n            instance.frame = self\n\n        self.to(self.device)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return String representation of the Frame.\n\n        Returns:\n            The string representation of the frame.\n        \"\"\"\n        return (\n            \"Frame(\"\n            f\"video={self._video.filename if isinstance(self._video, sio.Video) else self._video}, \"\n            f\"video_id={self._video_id.item()}, \"\n            f\"frame_id={self._frame_id.item()}, \"\n            f\"img_shape={self._img_shape}, \"\n            f\"num_detected={self.num_detected}, \"\n            f\"asso_output={self._asso_output}, \"\n            f\"traj_score={self._traj_score}, \"\n            f\"matches={self._matches}, \"\n            f\"instances={self._instances}, \"\n            f\"device={self._device}\"\n            \")\"\n        )\n\n    def to(self, map_location: str | torch.device) -&gt; Self:\n        \"\"\"Move frame to different device or dtype (See `torch.to` for more info).\n\n        Args:\n            map_location: A string representing the device to move to.\n\n        Returns:\n            The frame moved to a different device/dtype.\n        \"\"\"\n        self._video_id = self._video_id.to(map_location)\n        self._frame_id = self._frame_id.to(map_location)\n        self._img_shape = self._img_shape.to(map_location)\n\n        if isinstance(self._asso_output, torch.Tensor):\n            self._asso_output = self._asso_output.to(map_location)\n\n        if isinstance(self._matches, torch.Tensor):\n            self._matches = self._matches.to(map_location)\n\n        for key, val in self._traj_score.items():\n            if isinstance(val, torch.Tensor):\n                self._traj_score[key] = val.to(map_location)\n        for instance in self.instances:\n            instance = instance.to(map_location)\n\n        if isinstance(map_location, (str, torch.device)):\n            self._device = map_location\n\n        return self\n\n    @classmethod\n    def from_slp(\n        cls,\n        lf: sio.LabeledFrame,\n        video_id: int = 0,\n        device: str | None = None,\n        **kwargs,\n    ) -&gt; Self:\n        \"\"\"Convert `sio.LabeledFrame` to `dreem.io.Frame`.\n\n        Args:\n            lf: A sio.LabeledFrame object\n\n        Returns:\n            A dreem.io.Frame object\n        \"\"\"\n        from dreem.io.instance import Instance\n\n        img_shape = lf.image.shape\n        if len(img_shape) == 2:\n            img_shape = (1, *img_shape)\n        elif len(img_shape) &gt; 2 and img_shape[-1] &lt;= 3:\n            img_shape = (lf.image.shape[-1], lf.image.shape[0], lf.image.shape[1])\n        return cls(\n            video_id=video_id,\n            frame_id=(\n                lf.frame_idx.astype(np.int32)\n                if isinstance(lf.frame_idx, np.number)\n                else lf.frame_idx\n            ),\n            vid_file=lf.video.filename,\n            img_shape=img_shape,\n            instances=[Instance.from_slp(instance, **kwargs) for instance in lf],\n            device=device,\n        )\n\n    def to_slp(\n        self,\n        track_lookup: dict[int, sio.Track] | None = None,\n        video: sio.Video | None = None,\n    ) -&gt; tuple[sio.LabeledFrame, dict[int, sio.Track]]:\n        \"\"\"Convert Frame to sleap_io.LabeledFrame object.\n\n        Args:\n            track_lookup: A lookup dictionary containing the track_id and sio.Track for persistence\n            video: An sio.Video object used for overriding.\n\n        Returns: A tuple containing a LabeledFrame object with necessary metadata and\n        a lookup dictionary containing the track_id and sio.Track for persistence\n        \"\"\"\n        if track_lookup is None:\n            track_lookup = {}\n\n        slp_instances = []\n        for instance in self.instances:\n            slp_instance, track_lookup = instance.to_slp(track_lookup=track_lookup)\n            slp_instances.append(slp_instance)\n\n        if video is None:\n            video = (\n                self.video\n                if isinstance(self.video, sio.Video)\n                else sio.load_video(self.video)\n            )\n\n        return (\n            sio.LabeledFrame(\n                video=video,\n                frame_idx=self.frame_id.item(),\n                instances=slp_instances,\n            ),\n            track_lookup,\n        )\n\n    def to_h5(\n        self,\n        clip_group: h5py.Group,\n        instance_labels: list | None = None,\n        save: dict[str, bool] | None = None,\n    ) -&gt; h5py.Group:\n        \"\"\"Convert frame to h5py group.\n\n        Args:\n            clip_group: the h5py group representing the clip (e.g batch/video) the frame belongs to\n            instance_labels: the labels used to create instance group names\n            save: whether to save crops, features and embeddings for the instance\n        Returns:\n            An h5py group containing the frame\n        \"\"\"\n        if save is None:\n            save = {\"crop\": False, \"features\": False, \"embeddings\": False}\n        frame_group = clip_group.require_group(f\"frame_{self.frame_id.item()}\")\n        frame_group.attrs.create(\"frame_id\", self.frame_id.item())\n        frame_group.attrs.create(\"vid_id\", self.video_id.item())\n        frame_group.attrs.create(\"vid_name\", self.vid_name)\n\n        frame_group.create_dataset(\n            \"asso_matrix\",\n            data=self.asso_output.numpy() if self.asso_output is not None else [],\n        )\n        asso_group = frame_group.require_group(\"traj_scores\")\n        for key, value in self.get_traj_score().items():\n            asso_group.create_dataset(\n                key, data=value.to_numpy() if value is not None else []\n            )\n\n        if instance_labels is None:\n            instance_labels = self.get_gt_track_ids.cpu().numpy()\n        for instance_label, instance in zip(instance_labels, self.instances):\n            kwargs = {}\n            if save.get(\"crop\", False):\n                kwargs[\"crop\"] = instance.crop.cpu().numpy()\n            if save.get(\"features\", False):\n                kwargs[\"features\"] = instance.features.cpu().numpy()\n            if save.get(\"embeddings\", False):\n                for key, val in instance.get_embedding().items():\n                    kwargs[f\"{key}_emb\"] = val.cpu().numpy()\n            _ = instance.to_h5(frame_group, f\"instance_{instance_label}\", **kwargs)\n\n        return frame_group\n\n    @property\n    def device(self) -&gt; str:\n        \"\"\"The device the frame is on.\n\n        Returns:\n            The string representation of the device the frame is on.\n        \"\"\"\n        return self._device\n\n    @device.setter\n    def device(self, device: str) -&gt; None:\n        \"\"\"Set the device.\n\n        Note: Do not set `frame.device = device` normally. Use `frame.to(device)` instead.\n\n        Args:\n            device: the device the function should be on.\n        \"\"\"\n        self._device = device\n\n    @property\n    def video_id(self) -&gt; torch.Tensor:\n        \"\"\"The index of the video the frame comes from.\n\n        Returns:\n            A tensor containing the video index.\n        \"\"\"\n        return self._video_id\n\n    @video_id.setter\n    def video_id(self, video_id: int) -&gt; None:\n        \"\"\"Set the video index.\n\n        Note: Generally the video_id should be immutable after initialization.\n\n        Args:\n            video_id: an int representing the index of the video that the frame came from.\n        \"\"\"\n        self._video_id = torch.tensor([video_id])\n\n    @property\n    def frame_id(self) -&gt; torch.Tensor:\n        \"\"\"The index of the frame in a full video.\n\n        Returns:\n            A torch tensor containing the index of the frame in the video.\n        \"\"\"\n        return self._frame_id\n\n    @frame_id.setter\n    def frame_id(self, frame_id: int) -&gt; None:\n        \"\"\"Set the frame index of the frame.\n\n        Note: The frame_id should generally be immutable after initialization.\n\n        Args:\n            frame_id: The int index of the frame in the full video.\n        \"\"\"\n        self._frame_id = torch.tensor([frame_id])\n\n    @property\n    def video(self) -&gt; sio.Video | str:\n        \"\"\"Get the video associated with the frame.\n\n        Returns: An sio.Video object representing the video or a placeholder string\n        if it is not possible to create the sio.Video\n        \"\"\"\n        return self._video\n\n    @video.setter\n    def video(self, video: sio.Video | str) -&gt; None:\n        \"\"\"Set the video associated with the frame.\n\n        Note: we try to store the video in an sio.Video object.\n        However, if this is not possible (e.g. incompatible format or missing filepath)\n        then we simply store the string.\n\n        Args:\n            video: sio.Video containing the vid reader or string path to video_file\n        \"\"\"\n        if isinstance(video, sio.Video):\n            self._video = video\n        else:\n            try:\n                self._video = sio.load_video(video)\n            except ValueError:\n                self._video = video\n\n    @property\n    def vid_name(self) -&gt; str:\n        \"\"\"Get the path to the video corresponding to this frame.\n\n        Returns: A str file path corresponding to the frame.\n        \"\"\"\n        if isinstance(self.video, str):\n            return self.video\n        else:\n            return self.video.name\n\n    @property\n    def img_shape(self) -&gt; torch.Tensor:\n        \"\"\"The shape of the pre-cropped frame.\n\n        Returns:\n            A torch tensor containing the shape of the frame. Should generally be (c, h, w)\n        \"\"\"\n        return self._img_shape\n\n    @img_shape.setter\n    def img_shape(self, img_shape: ArrayLike) -&gt; None:\n        \"\"\"Set the shape of the frame image.\n\n        Note: the img_shape should generally be immutable after initialization.\n\n        Args:\n            img_shape: an ArrayLike object containing the shape of the frame image.\n        \"\"\"\n        self._img_shape = _to_tensor(img_shape)\n\n    @property\n    def instances(self) -&gt; list[\"Instance\"]:\n        \"\"\"A list of instances in the frame.\n\n        Returns:\n            The list of instances that appear in the frame.\n        \"\"\"\n        return self._instances\n\n    @instances.setter\n    def instances(self, instances: list[\"Instance\"]) -&gt; None:\n        \"\"\"Set the frame's instance.\n\n        Args:\n            instances: A list of Instances that appear in the frame.\n        \"\"\"\n        for instance in instances:\n            instance.frame = self\n        self._instances = instances\n\n    def has_instances(self) -&gt; bool:\n        \"\"\"Determine whether there are instances in the frame.\n\n        Returns:\n            True if there are instances in the frame, otherwise False.\n        \"\"\"\n        if self.num_detected == 0:\n            return False\n        return True\n\n    @property\n    def num_detected(self) -&gt; int:\n        \"\"\"The number of instances in the frame.\n\n        Returns:\n            the number of instances in the frame.\n        \"\"\"\n        return len(self.instances)\n\n    @property\n    def asso_output(self) -&gt; \"AssociationMatrix\":\n        \"\"\"The association matrix between instances outputted directly by transformer.\n\n        Returns:\n            An arraylike (n_query, n_nonquery) association matrix between instances.\n        \"\"\"\n        return self._asso_output\n\n    def has_asso_output(self) -&gt; bool:\n        \"\"\"Determine whether the frame has an association matrix computed.\n\n        Returns:\n            True if the frame has an association matrix otherwise, False.\n        \"\"\"\n        if self._asso_output is None or len(self._asso_output.matrix) == 0:\n            return False\n        return True\n\n    @asso_output.setter\n    def asso_output(self, asso_output: \"AssociationMatrix\") -&gt; None:\n        \"\"\"Set the association matrix of a frame.\n\n        Args:\n            asso_output: An arraylike (n_query, n_nonquery) association matrix between instances.\n        \"\"\"\n        self._asso_output = asso_output\n\n    @property\n    def matches(self) -&gt; tuple:\n        \"\"\"Matches between frame instances and available trajectories.\n\n        Returns:\n            A tuple containing the instance idx and trajectory idx for the matched instance.\n        \"\"\"\n        return self._matches\n\n    @matches.setter\n    def matches(self, matches: tuple) -&gt; None:\n        \"\"\"Set the frame matches.\n\n        Args:\n            matches: A tuple containing the instance idx and trajectory idx for the matched instance.\n        \"\"\"\n        self._matches = matches\n\n    def has_matches(self) -&gt; bool:\n        \"\"\"Check whether or not matches have been computed for frame.\n\n        Returns:\n            True if frame contains matches otherwise False.\n        \"\"\"\n        if self._matches is not None and len(self._matches) &gt; 0:\n            return True\n        return False\n\n    def get_traj_score(self, key: str | None = None) -&gt; dict | ArrayLike | None:\n        \"\"\"Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.\n\n        Args:\n            key: The key of the trajectory score to be accessed.\n                Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}\n\n        Returns:\n            - dictionary containing all trajectory scores if key is None\n            - trajectory score associated with key\n            - None if the key is not found\n        \"\"\"\n        if key is None:\n            return self._traj_score\n        else:\n            try:\n                return self._traj_score[key]\n            except KeyError as e:\n                logger.exception(f\"Could not access {key} traj_score due to {e}\")\n                return None\n\n    def add_traj_score(self, key: str, traj_score: ArrayLike) -&gt; None:\n        \"\"\"Add trajectory score to dictionary.\n\n        Args:\n            key: key associated with traj score to be used in dictionary\n            traj_score: association matrix between instances and trajectories\n        \"\"\"\n        self._traj_score[key] = traj_score\n\n    def has_traj_score(self) -&gt; bool:\n        \"\"\"Check if any trajectory association matrix has been saved.\n\n        Returns:\n            True there is at least one association matrix otherwise, false.\n        \"\"\"\n        if len(self._traj_score) == 0:\n            return False\n        return True\n\n    def has_gt_track_ids(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a gt track id.\n\n        Returns:\n            True if at least 1 instance has a gt track id otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_gt_track_id() for instance in self.instances])\n        return False\n\n    def get_gt_track_ids(self) -&gt; torch.Tensor:\n        \"\"\"Get the gt track ids of all instances in the frame.\n\n        Returns:\n            an (N,) shaped tensor with the gt track ids of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.gt_track_id for instance in self.instances])\n\n    def has_pred_track_ids(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a pred track id.\n\n        Returns:\n            True if at least 1 instance has a pred track id otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_pred_track_id() for instance in self.instances])\n        return False\n\n    def get_pred_track_ids(self) -&gt; torch.Tensor:\n        \"\"\"Get the pred track ids of all instances in the frame.\n\n        Returns:\n            an (N,) shaped tensor with the pred track ids of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.pred_track_id for instance in self.instances])\n\n    def has_bboxes(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a bounding box.\n\n        Returns:\n            True if at least 1 instance has a bounding box otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_bboxes() for instance in self.instances])\n        return False\n\n    def get_bboxes(self) -&gt; torch.Tensor:\n        \"\"\"Get the bounding boxes of all instances in the frame.\n\n        Returns:\n            an (N,4) shaped tensor with bounding boxes of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.empty(0, 4)\n        return torch.cat([instance.bbox for instance in self.instances], dim=0)\n\n    def has_crops(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has a crop.\n\n        Returns:\n            True if at least 1 instance has a crop otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_crop() for instance in self.instances])\n        return False\n\n    def get_crops(self) -&gt; torch.Tensor:\n        \"\"\"Get the crops of all instances in the frame.\n\n        Returns:\n            an (N, C, H, W) shaped tensor with crops of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n\n        return torch.cat([instance.crop for instance in self.instances], dim=0)\n\n    def has_features(self) -&gt; bool:\n        \"\"\"Check if any of frames instances has reid features already computed.\n\n        Returns:\n            True if at least 1 instance have reid features otherwise False.\n        \"\"\"\n        if self.has_instances():\n            return any([instance.has_features() for instance in self.instances])\n        return False\n\n    def get_features(self) -&gt; torch.Tensor:\n        \"\"\"Get the reid feature vectors of all instances in the frame.\n\n        Returns:\n            an (N, D) shaped tensor with reid feature vectors of each instance in the frame.\n        \"\"\"\n        if not self.has_instances():\n            return torch.tensor([])\n        return torch.cat([instance.features for instance in self.instances], dim=0)\n\n    def get_anchors(self) -&gt; list[str]:\n        \"\"\"Get the anchor names of instances in the frame.\n\n        Returns:\n            A list of anchor names used by the instances to get the crop.\n        \"\"\"\n        return [instance.anchor for instance in self.instances]\n\n    def get_centroids(self) -&gt; tuple[list[str], ArrayLike]:\n        \"\"\"Get the centroids around which each instance's crop was formed.\n\n        Returns:\n            anchors: the node names for the corresponding point\n            points: an n_instances x 2 array containing the centroids\n        \"\"\"\n        anchors = [\n            anchor for instance in self.instances for anchor in instance.centroid.keys()\n        ]\n\n        points = np.array(\n            [\n                point\n                for instance in self.instances\n                for point in instance.centroid.values()\n            ]\n        )\n\n        return (anchors, points)\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.asso_output","title":"<code>asso_output</code>  <code>property</code> <code>writable</code>","text":"<p>The association matrix between instances outputted directly by transformer.</p> <p>Returns:</p> Type Description <code>'AssociationMatrix'</code> <p>An arraylike (n_query, n_nonquery) association matrix between instances.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.device","title":"<code>device</code>  <code>property</code> <code>writable</code>","text":"<p>The device the frame is on.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the device the frame is on.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.frame_id","title":"<code>frame_id</code>  <code>property</code> <code>writable</code>","text":"<p>The index of the frame in a full video.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor containing the index of the frame in the video.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.img_shape","title":"<code>img_shape</code>  <code>property</code> <code>writable</code>","text":"<p>The shape of the pre-cropped frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor containing the shape of the frame. Should generally be (c, h, w)</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.instances","title":"<code>instances</code>  <code>property</code> <code>writable</code>","text":"<p>A list of instances in the frame.</p> <p>Returns:</p> Type Description <code>list['Instance']</code> <p>The list of instances that appear in the frame.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.matches","title":"<code>matches</code>  <code>property</code> <code>writable</code>","text":"<p>Matches between frame instances and available trajectories.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the instance idx and trajectory idx for the matched instance.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.num_detected","title":"<code>num_detected</code>  <code>property</code>","text":"<p>The number of instances in the frame.</p> <p>Returns:</p> Type Description <code>int</code> <p>the number of instances in the frame.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.vid_name","title":"<code>vid_name</code>  <code>property</code>","text":"<p>Get the path to the video corresponding to this frame.</p> <p>Returns: A str file path corresponding to the frame.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.video","title":"<code>video</code>  <code>property</code> <code>writable</code>","text":"<p>Get the video associated with the frame.</p> <p>Returns: An sio.Video object representing the video or a placeholder string if it is not possible to create the sio.Video</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.video_id","title":"<code>video_id</code>  <code>property</code> <code>writable</code>","text":"<p>The index of the video the frame comes from.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the video index.</p>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.__attrs_post_init__","title":"<code>__attrs_post_init__()</code>","text":"<p>Handle more intricate default initializations and moving to device.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def __attrs_post_init__(self) -&gt; None:\n    \"\"\"Handle more intricate default initializations and moving to device.\"\"\"\n    if len(self.img_shape) == 0:\n        self.img_shape = torch.tensor([0, 0, 0])\n\n    for instance in self.instances:\n        instance.frame = self\n\n    self.to(self.device)\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.__repr__","title":"<code>__repr__()</code>","text":"<p>Return String representation of the Frame.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return String representation of the Frame.\n\n    Returns:\n        The string representation of the frame.\n    \"\"\"\n    return (\n        \"Frame(\"\n        f\"video={self._video.filename if isinstance(self._video, sio.Video) else self._video}, \"\n        f\"video_id={self._video_id.item()}, \"\n        f\"frame_id={self._frame_id.item()}, \"\n        f\"img_shape={self._img_shape}, \"\n        f\"num_detected={self.num_detected}, \"\n        f\"asso_output={self._asso_output}, \"\n        f\"traj_score={self._traj_score}, \"\n        f\"matches={self._matches}, \"\n        f\"instances={self._instances}, \"\n        f\"device={self._device}\"\n        \")\"\n    )\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.add_traj_score","title":"<code>add_traj_score(key, traj_score)</code>","text":"<p>Add trajectory score to dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key associated with traj score to be used in dictionary</p> required <code>traj_score</code> <code>ArrayLike</code> <p>association matrix between instances and trajectories</p> required Source code in <code>dreem/io/frame.py</code> <pre><code>def add_traj_score(self, key: str, traj_score: ArrayLike) -&gt; None:\n    \"\"\"Add trajectory score to dictionary.\n\n    Args:\n        key: key associated with traj score to be used in dictionary\n        traj_score: association matrix between instances and trajectories\n    \"\"\"\n    self._traj_score[key] = traj_score\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.from_slp","title":"<code>from_slp(lf, video_id=0, device=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Convert <code>sio.LabeledFrame</code> to <code>dreem.io.Frame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>lf</code> <code>LabeledFrame</code> <p>A sio.LabeledFrame object</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A dreem.io.Frame object</p> Source code in <code>dreem/io/frame.py</code> <pre><code>@classmethod\ndef from_slp(\n    cls,\n    lf: sio.LabeledFrame,\n    video_id: int = 0,\n    device: str | None = None,\n    **kwargs,\n) -&gt; Self:\n    \"\"\"Convert `sio.LabeledFrame` to `dreem.io.Frame`.\n\n    Args:\n        lf: A sio.LabeledFrame object\n\n    Returns:\n        A dreem.io.Frame object\n    \"\"\"\n    from dreem.io.instance import Instance\n\n    img_shape = lf.image.shape\n    if len(img_shape) == 2:\n        img_shape = (1, *img_shape)\n    elif len(img_shape) &gt; 2 and img_shape[-1] &lt;= 3:\n        img_shape = (lf.image.shape[-1], lf.image.shape[0], lf.image.shape[1])\n    return cls(\n        video_id=video_id,\n        frame_id=(\n            lf.frame_idx.astype(np.int32)\n            if isinstance(lf.frame_idx, np.number)\n            else lf.frame_idx\n        ),\n        vid_file=lf.video.filename,\n        img_shape=img_shape,\n        instances=[Instance.from_slp(instance, **kwargs) for instance in lf],\n        device=device,\n    )\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.get_anchors","title":"<code>get_anchors()</code>","text":"<p>Get the anchor names of instances in the frame.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of anchor names used by the instances to get the crop.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_anchors(self) -&gt; list[str]:\n    \"\"\"Get the anchor names of instances in the frame.\n\n    Returns:\n        A list of anchor names used by the instances to get the crop.\n    \"\"\"\n    return [instance.anchor for instance in self.instances]\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.get_bboxes","title":"<code>get_bboxes()</code>","text":"<p>Get the bounding boxes of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,4) shaped tensor with bounding boxes of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_bboxes(self) -&gt; torch.Tensor:\n    \"\"\"Get the bounding boxes of all instances in the frame.\n\n    Returns:\n        an (N,4) shaped tensor with bounding boxes of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.empty(0, 4)\n    return torch.cat([instance.bbox for instance in self.instances], dim=0)\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.get_centroids","title":"<code>get_centroids()</code>","text":"<p>Get the centroids around which each instance's crop was formed.</p> <p>Returns:</p> Name Type Description <code>anchors</code> <code>tuple[list[str], ArrayLike]</code> <p>the node names for the corresponding point points: an n_instances x 2 array containing the centroids</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_centroids(self) -&gt; tuple[list[str], ArrayLike]:\n    \"\"\"Get the centroids around which each instance's crop was formed.\n\n    Returns:\n        anchors: the node names for the corresponding point\n        points: an n_instances x 2 array containing the centroids\n    \"\"\"\n    anchors = [\n        anchor for instance in self.instances for anchor in instance.centroid.keys()\n    ]\n\n    points = np.array(\n        [\n            point\n            for instance in self.instances\n            for point in instance.centroid.values()\n        ]\n    )\n\n    return (anchors, points)\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.get_crops","title":"<code>get_crops()</code>","text":"<p>Get the crops of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N, C, H, W) shaped tensor with crops of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_crops(self) -&gt; torch.Tensor:\n    \"\"\"Get the crops of all instances in the frame.\n\n    Returns:\n        an (N, C, H, W) shaped tensor with crops of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n\n    return torch.cat([instance.crop for instance in self.instances], dim=0)\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.get_features","title":"<code>get_features()</code>","text":"<p>Get the reid feature vectors of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N, D) shaped tensor with reid feature vectors of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_features(self) -&gt; torch.Tensor:\n    \"\"\"Get the reid feature vectors of all instances in the frame.\n\n    Returns:\n        an (N, D) shaped tensor with reid feature vectors of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.features for instance in self.instances], dim=0)\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.get_gt_track_ids","title":"<code>get_gt_track_ids()</code>","text":"<p>Get the gt track ids of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,) shaped tensor with the gt track ids of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_gt_track_ids(self) -&gt; torch.Tensor:\n    \"\"\"Get the gt track ids of all instances in the frame.\n\n    Returns:\n        an (N,) shaped tensor with the gt track ids of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.gt_track_id for instance in self.instances])\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.get_pred_track_ids","title":"<code>get_pred_track_ids()</code>","text":"<p>Get the pred track ids of all instances in the frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>an (N,) shaped tensor with the pred track ids of each instance in the frame.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_pred_track_ids(self) -&gt; torch.Tensor:\n    \"\"\"Get the pred track ids of all instances in the frame.\n\n    Returns:\n        an (N,) shaped tensor with the pred track ids of each instance in the frame.\n    \"\"\"\n    if not self.has_instances():\n        return torch.tensor([])\n    return torch.cat([instance.pred_track_id for instance in self.instances])\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.get_traj_score","title":"<code>get_traj_score(key=None)</code>","text":"<p>Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str | None</code> <p>The key of the trajectory score to be accessed. Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}</p> <code>None</code> <p>Returns:</p> Type Description <code>dict | ArrayLike | None</code> <ul> <li>dictionary containing all trajectory scores if key is None</li> <li>trajectory score associated with key</li> <li>None if the key is not found</li> </ul> Source code in <code>dreem/io/frame.py</code> <pre><code>def get_traj_score(self, key: str | None = None) -&gt; dict | ArrayLike | None:\n    \"\"\"Get dictionary containing association matrix between instances and trajectories along postprocessing pipeline.\n\n    Args:\n        key: The key of the trajectory score to be accessed.\n            Can be one of {None, 'initial', 'decay_time', 'max_center_dist', 'iou', 'final'}\n\n    Returns:\n        - dictionary containing all trajectory scores if key is None\n        - trajectory score associated with key\n        - None if the key is not found\n    \"\"\"\n    if key is None:\n        return self._traj_score\n    else:\n        try:\n            return self._traj_score[key]\n        except KeyError as e:\n            logger.exception(f\"Could not access {key} traj_score due to {e}\")\n            return None\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_asso_output","title":"<code>has_asso_output()</code>","text":"<p>Determine whether the frame has an association matrix computed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the frame has an association matrix otherwise, False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_asso_output(self) -&gt; bool:\n    \"\"\"Determine whether the frame has an association matrix computed.\n\n    Returns:\n        True if the frame has an association matrix otherwise, False.\n    \"\"\"\n    if self._asso_output is None or len(self._asso_output.matrix) == 0:\n        return False\n    return True\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_bboxes","title":"<code>has_bboxes()</code>","text":"<p>Check if any of frames instances has a bounding box.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a bounding box otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_bboxes(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a bounding box.\n\n    Returns:\n        True if at least 1 instance has a bounding box otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_bboxes() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_crops","title":"<code>has_crops()</code>","text":"<p>Check if any of frames instances has a crop.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a crop otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_crops(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a crop.\n\n    Returns:\n        True if at least 1 instance has a crop otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_crop() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_features","title":"<code>has_features()</code>","text":"<p>Check if any of frames instances has reid features already computed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance have reid features otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_features(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has reid features already computed.\n\n    Returns:\n        True if at least 1 instance have reid features otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_features() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_gt_track_ids","title":"<code>has_gt_track_ids()</code>","text":"<p>Check if any of frames instances has a gt track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a gt track id otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_gt_track_ids(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a gt track id.\n\n    Returns:\n        True if at least 1 instance has a gt track id otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_gt_track_id() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_instances","title":"<code>has_instances()</code>","text":"<p>Determine whether there are instances in the frame.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if there are instances in the frame, otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_instances(self) -&gt; bool:\n    \"\"\"Determine whether there are instances in the frame.\n\n    Returns:\n        True if there are instances in the frame, otherwise False.\n    \"\"\"\n    if self.num_detected == 0:\n        return False\n    return True\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_matches","title":"<code>has_matches()</code>","text":"<p>Check whether or not matches have been computed for frame.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if frame contains matches otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_matches(self) -&gt; bool:\n    \"\"\"Check whether or not matches have been computed for frame.\n\n    Returns:\n        True if frame contains matches otherwise False.\n    \"\"\"\n    if self._matches is not None and len(self._matches) &gt; 0:\n        return True\n    return False\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_pred_track_ids","title":"<code>has_pred_track_ids()</code>","text":"<p>Check if any of frames instances has a pred track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if at least 1 instance has a pred track id otherwise False.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_pred_track_ids(self) -&gt; bool:\n    \"\"\"Check if any of frames instances has a pred track id.\n\n    Returns:\n        True if at least 1 instance has a pred track id otherwise False.\n    \"\"\"\n    if self.has_instances():\n        return any([instance.has_pred_track_id() for instance in self.instances])\n    return False\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.has_traj_score","title":"<code>has_traj_score()</code>","text":"<p>Check if any trajectory association matrix has been saved.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True there is at least one association matrix otherwise, false.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def has_traj_score(self) -&gt; bool:\n    \"\"\"Check if any trajectory association matrix has been saved.\n\n    Returns:\n        True there is at least one association matrix otherwise, false.\n    \"\"\"\n    if len(self._traj_score) == 0:\n        return False\n    return True\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.to","title":"<code>to(map_location)</code>","text":"<p>Move frame to different device or dtype (See <code>torch.to</code> for more info).</p> <p>Parameters:</p> Name Type Description Default <code>map_location</code> <code>str | device</code> <p>A string representing the device to move to.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The frame moved to a different device/dtype.</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to(self, map_location: str | torch.device) -&gt; Self:\n    \"\"\"Move frame to different device or dtype (See `torch.to` for more info).\n\n    Args:\n        map_location: A string representing the device to move to.\n\n    Returns:\n        The frame moved to a different device/dtype.\n    \"\"\"\n    self._video_id = self._video_id.to(map_location)\n    self._frame_id = self._frame_id.to(map_location)\n    self._img_shape = self._img_shape.to(map_location)\n\n    if isinstance(self._asso_output, torch.Tensor):\n        self._asso_output = self._asso_output.to(map_location)\n\n    if isinstance(self._matches, torch.Tensor):\n        self._matches = self._matches.to(map_location)\n\n    for key, val in self._traj_score.items():\n        if isinstance(val, torch.Tensor):\n            self._traj_score[key] = val.to(map_location)\n    for instance in self.instances:\n        instance = instance.to(map_location)\n\n    if isinstance(map_location, (str, torch.device)):\n        self._device = map_location\n\n    return self\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.to_h5","title":"<code>to_h5(clip_group, instance_labels=None, save=None)</code>","text":"<p>Convert frame to h5py group.</p> <p>Parameters:</p> Name Type Description Default <code>clip_group</code> <code>Group</code> <p>the h5py group representing the clip (e.g batch/video) the frame belongs to</p> required <code>instance_labels</code> <code>list | None</code> <p>the labels used to create instance group names</p> <code>None</code> <code>save</code> <code>dict[str, bool] | None</code> <p>whether to save crops, features and embeddings for the instance</p> <code>None</code> <p>Returns:     An h5py group containing the frame</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to_h5(\n    self,\n    clip_group: h5py.Group,\n    instance_labels: list | None = None,\n    save: dict[str, bool] | None = None,\n) -&gt; h5py.Group:\n    \"\"\"Convert frame to h5py group.\n\n    Args:\n        clip_group: the h5py group representing the clip (e.g batch/video) the frame belongs to\n        instance_labels: the labels used to create instance group names\n        save: whether to save crops, features and embeddings for the instance\n    Returns:\n        An h5py group containing the frame\n    \"\"\"\n    if save is None:\n        save = {\"crop\": False, \"features\": False, \"embeddings\": False}\n    frame_group = clip_group.require_group(f\"frame_{self.frame_id.item()}\")\n    frame_group.attrs.create(\"frame_id\", self.frame_id.item())\n    frame_group.attrs.create(\"vid_id\", self.video_id.item())\n    frame_group.attrs.create(\"vid_name\", self.vid_name)\n\n    frame_group.create_dataset(\n        \"asso_matrix\",\n        data=self.asso_output.numpy() if self.asso_output is not None else [],\n    )\n    asso_group = frame_group.require_group(\"traj_scores\")\n    for key, value in self.get_traj_score().items():\n        asso_group.create_dataset(\n            key, data=value.to_numpy() if value is not None else []\n        )\n\n    if instance_labels is None:\n        instance_labels = self.get_gt_track_ids.cpu().numpy()\n    for instance_label, instance in zip(instance_labels, self.instances):\n        kwargs = {}\n        if save.get(\"crop\", False):\n            kwargs[\"crop\"] = instance.crop.cpu().numpy()\n        if save.get(\"features\", False):\n            kwargs[\"features\"] = instance.features.cpu().numpy()\n        if save.get(\"embeddings\", False):\n            for key, val in instance.get_embedding().items():\n                kwargs[f\"{key}_emb\"] = val.cpu().numpy()\n        _ = instance.to_h5(frame_group, f\"instance_{instance_label}\", **kwargs)\n\n    return frame_group\n</code></pre>"},{"location":"reference/dreem/io/frame/#dreem.io.frame.Frame.to_slp","title":"<code>to_slp(track_lookup=None, video=None)</code>","text":"<p>Convert Frame to sleap_io.LabeledFrame object.</p> <p>Parameters:</p> Name Type Description Default <code>track_lookup</code> <code>dict[int, Track] | None</code> <p>A lookup dictionary containing the track_id and sio.Track for persistence</p> <code>None</code> <code>video</code> <code>Video | None</code> <p>An sio.Video object used for overriding.</p> <code>None</code> <p>Returns: A tuple containing a LabeledFrame object with necessary metadata and a lookup dictionary containing the track_id and sio.Track for persistence</p> Source code in <code>dreem/io/frame.py</code> <pre><code>def to_slp(\n    self,\n    track_lookup: dict[int, sio.Track] | None = None,\n    video: sio.Video | None = None,\n) -&gt; tuple[sio.LabeledFrame, dict[int, sio.Track]]:\n    \"\"\"Convert Frame to sleap_io.LabeledFrame object.\n\n    Args:\n        track_lookup: A lookup dictionary containing the track_id and sio.Track for persistence\n        video: An sio.Video object used for overriding.\n\n    Returns: A tuple containing a LabeledFrame object with necessary metadata and\n    a lookup dictionary containing the track_id and sio.Track for persistence\n    \"\"\"\n    if track_lookup is None:\n        track_lookup = {}\n\n    slp_instances = []\n    for instance in self.instances:\n        slp_instance, track_lookup = instance.to_slp(track_lookup=track_lookup)\n        slp_instances.append(slp_instance)\n\n    if video is None:\n        video = (\n            self.video\n            if isinstance(self.video, sio.Video)\n            else sio.load_video(self.video)\n        )\n\n    return (\n        sio.LabeledFrame(\n            video=video,\n            frame_idx=self.frame_id.item(),\n            instances=slp_instances,\n        ),\n        track_lookup,\n    )\n</code></pre>"},{"location":"reference/dreem/io/instance/","title":"instance","text":""},{"location":"reference/dreem/io/instance/#dreem.io.instance","title":"<code>dreem.io.instance</code>","text":"<p>Module containing data class for storing detections.</p> <p>Classes:</p> Name Description <code>Instance</code> <p>Class representing a single instance to be tracked.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance","title":"<code>Instance</code>","text":"<p>Class representing a single instance to be tracked.</p> <p>Attributes:</p> Name Type Description <code>gt_track_id</code> <code>Tensor</code> <p>Ground truth track id - only used for train/eval.</p> <code>pred_track_id</code> <code>Tensor</code> <p>Predicted track id. Untracked instance is represented by -1.</p> <code>bbox</code> <code>Tensor</code> <p>The bounding box coordinate of the instance. Defaults to an empty tensor.</p> <code>crop</code> <code>Tensor</code> <p>The crop of the instance.</p> <code>centroid</code> <code>dict[str, ArrayLike]</code> <p>the centroid around which the bbox was cropped.</p> <code>features</code> <code>Tensor</code> <p>The reid features extracted from the CNN backbone used in the transformer.</p> <code>track_score</code> <code>float</code> <p>The track score output from the association matrix.</p> <code>point_scores</code> <code>ArrayLike</code> <p>The point scores from sleap.</p> <code>instance_score</code> <code>float</code> <p>The instance scores from sleap.</p> <code>skeleton</code> <code>Skeleton</code> <p>The sleap skeleton used for the instance.</p> <code>pose</code> <code>dict[str, ArrayLike]</code> <p>A dictionary containing the node name and corresponding point.</p> <code>device</code> <code>str</code> <p>String representation of the device the instance should be on.</p> <p>Methods:</p> Name Description <code>__attrs_post_init__</code> <p>Handle dimensionality and more intricate default initializations post-init.</p> <code>__repr__</code> <p>Return string representation of the Instance.</p> <code>add_embedding</code> <p>Save embedding to instance embedding dictionary.</p> <code>from_slp</code> <p>Convert a slp instance to a dreem instance.</p> <code>get_embedding</code> <p>Retrieve instance's spatial/temporal embedding.</p> <code>has_bbox</code> <p>Determine if the instance has a bbox.</p> <code>has_crop</code> <p>Determine if the instance has a crop.</p> <code>has_embedding</code> <p>Determine if the instance has embedding type requested.</p> <code>has_features</code> <p>Determine if the instance has computed reid features.</p> <code>has_gt_track_id</code> <p>Determine if instance has a gt track assignment.</p> <code>has_pose</code> <p>Check if the instance has a pose.</p> <code>has_pred_track_id</code> <p>Determine whether instance has predicted track id.</p> <code>to</code> <p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <code>to_h5</code> <p>Convert instance to an h5 group\".</p> <code>to_slp</code> <p>Convert instance to sleap_io.PredictedInstance object.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>@attrs.define(eq=False)\nclass Instance:\n    \"\"\"Class representing a single instance to be tracked.\n\n    Attributes:\n        gt_track_id: Ground truth track id - only used for train/eval.\n        pred_track_id: Predicted track id. Untracked instance is represented by -1.\n        bbox: The bounding box coordinate of the instance. Defaults to an empty tensor.\n        crop: The crop of the instance.\n        centroid: the centroid around which the bbox was cropped.\n        features: The reid features extracted from the CNN backbone used in the transformer.\n        track_score: The track score output from the association matrix.\n        point_scores: The point scores from sleap.\n        instance_score: The instance scores from sleap.\n        skeleton: The sleap skeleton used for the instance.\n        pose: A dictionary containing the node name and corresponding point.\n        device: String representation of the device the instance should be on.\n    \"\"\"\n\n    _gt_track_id: int = attrs.field(\n        alias=\"gt_track_id\", default=-1, converter=_to_tensor\n    )\n    _pred_track_id: int = attrs.field(\n        alias=\"pred_track_id\", default=-1, converter=_to_tensor\n    )\n    _bbox: ArrayLike = attrs.field(alias=\"bbox\", factory=list, converter=_to_tensor)\n    _crop: ArrayLike = attrs.field(alias=\"crop\", factory=list, converter=_to_tensor)\n    _centroid: dict[str, ArrayLike] = attrs.field(alias=\"centroid\", factory=dict)\n    _features: ArrayLike = attrs.field(\n        alias=\"features\", factory=list, converter=_to_tensor\n    )\n    _embeddings: dict = attrs.field(alias=\"embeddings\", factory=dict)\n    _track_score: float = attrs.field(alias=\"track_score\", default=-1.0)\n    _instance_score: float = attrs.field(alias=\"instance_score\", default=-1.0)\n    _point_scores: ArrayLike | None = attrs.field(alias=\"point_scores\", default=None)\n    _skeleton: sio.Skeleton | None = attrs.field(alias=\"skeleton\", default=None)\n    _mask: ArrayLike | None = attrs.field(\n        alias=\"mask\", converter=_to_tensor, default=None\n    )\n    _pose: dict[str, ArrayLike] = attrs.field(alias=\"pose\", factory=dict)\n    _device: str | torch.device | None = attrs.field(alias=\"device\", default=None)\n    _frame: \"Frame\" = None\n\n    def __attrs_post_init__(self) -&gt; None:\n        \"\"\"Handle dimensionality and more intricate default initializations post-init.\"\"\"\n        self.bbox = _expand_to_rank(self.bbox, 3)\n        self.crop = _expand_to_rank(self.crop, 4)\n        self.features = _expand_to_rank(self.features, 2)\n\n        if self.skeleton is None:\n            self.skeleton = sio.Skeleton([\"centroid\"])\n\n        if self.bbox.shape[-1] == 0:\n            self.bbox = torch.empty([1, 0, 4])\n\n        if self.crop.shape[-1] == 0 and self.bbox.shape[1] != 0:\n            y1, x1, y2, x2 = self.bbox.squeeze(dim=0).nanmean(dim=0)\n            self.centroid = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        if len(self.pose) == 0 and self.bbox.shape[1]:\n            y1, x1, y2, x2 = self.bbox.squeeze(dim=0).mean(dim=0)\n            self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        if self.point_scores is None and len(self.pose) != 0:\n            self._point_scores = np.zeros((len(self.pose), 2))\n\n        self.to(self.device)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation of the Instance.\"\"\"\n        return (\n            \"Instance(\"\n            f\"gt_track_id={self._gt_track_id.item()}, \"\n            f\"pred_track_id={self._pred_track_id.item()}, \"\n            f\"bbox={self._bbox}, \"\n            f\"centroid={self._centroid}, \"\n            f\"crop={self._crop.shape}, \"\n            f\"features={self._features.shape}, \"\n            f\"device={self._device}\"\n            \")\"\n        )\n\n    def to(self, map_location: str | torch.device) -&gt; Self:\n        \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n        Args:\n            map_location: Either the device or dtype for the instance to be moved.\n\n        Returns:\n            self: reference to the instance moved to correct device/dtype.\n        \"\"\"\n        if map_location is not None and map_location != \"\":\n            self._gt_track_id = self._gt_track_id.to(map_location)\n            self._pred_track_id = self._pred_track_id.to(map_location)\n            self._bbox = self._bbox.to(map_location)\n            self._crop = self._crop.to(map_location)\n            self._features = self._features.to(map_location)\n            if isinstance(map_location, (str, torch.device)):\n                self.device = map_location\n\n        return self\n\n    @classmethod\n    def from_slp(\n        cls,\n        slp_instance: sio.PredictedInstance | sio.Instance,\n        bbox_size: int | tuple[int, int] = 64,\n        crop: ArrayLike | None = None,\n        device: str | None = None,\n    ) -&gt; Self:\n        \"\"\"Convert a slp instance to a dreem instance.\n\n        Args:\n            slp_instance: A `sleap_io.Instance` object representing a detection\n            bbox_size: size of the pose-centered bbox to form.\n            crop: The corresponding crop of the bbox\n            device: which device to keep the instance on\n        Returns:\n            A dreem.Instance object with a pose-centered bbox and no crop.\n        \"\"\"\n        try:\n            track_id = int(slp_instance.track.name)\n        except ValueError:\n            track_id = int(\n                \"\".join([str(ord(c)) for c in slp_instance.track.name])\n            )  # better way to handle this?\n        if isinstance(bbox_size, int):\n            bbox_size = (bbox_size, bbox_size)\n\n        track_score = -1.0\n        point_scores = np.full(len(slp_instance.points), -1)\n        instance_score = -1\n        if isinstance(slp_instance, sio.PredictedInstance):\n            track_score = slp_instance.tracking_score\n            point_scores = slp_instance.numpy()[:, -1]\n            instance_score = slp_instance.score\n\n        centroid = np.nanmean(slp_instance.numpy(), axis=1)\n        bbox = [\n            centroid[1] - bbox_size[1],\n            centroid[0] - bbox_size[0],\n            centroid[1] + bbox_size[1],\n            centroid[0] + bbox_size[0],\n        ]\n        return cls(\n            gt_track_id=track_id,\n            bbox=bbox,\n            crop=crop,\n            centroid={\"centroid\": centroid},\n            track_score=track_score,\n            point_scores=point_scores,\n            instance_score=instance_score,\n            skeleton=slp_instance.skeleton,\n            pose={\n                node.name: point.numpy() for node, point in slp_instance.points.items()\n            },\n            device=device,\n        )\n\n    def to_slp(\n        self, track_lookup: dict[int, sio.Track] = {}\n    ) -&gt; tuple[sio.PredictedInstance, dict[int, sio.Track]]:\n        \"\"\"Convert instance to sleap_io.PredictedInstance object.\n\n        Args:\n            track_lookup: A track look up dictionary containing track_id:sio.Track.\n        Returns: A sleap_io.PredictedInstance with necessary metadata\n            and a track_lookup dictionary to persist tracks.\n        \"\"\"\n        try:\n            track_id = self.pred_track_id.item()\n            if track_id not in track_lookup:\n                track_lookup[track_id] = sio.Track(name=self.pred_track_id.item())\n\n            track = track_lookup[track_id]\n\n            return (\n                sio.PredictedInstance.from_numpy(\n                    points_data=np.array(list(self.pose.values())),\n                    skeleton=self.skeleton,\n                    point_scores=self.point_scores,\n                    score=self.instance_score,\n                    tracking_score=self.track_score,\n                    track=track,\n                ),\n                track_lookup,\n            )\n        except Exception as e:\n            logger.exception(\n                f\"Pose: {np.array(list(self.pose.values())).shape}, Pose score shape {self.point_scores.shape}\"\n            )\n            raise RuntimeError(f\"Failed to convert to sio.PredictedInstance: {e}\")\n\n    def to_h5(\n        self, frame_group: h5py.Group, label: Any = None, **kwargs: dict\n    ) -&gt; h5py.Group:\n        \"\"\"Convert instance to an h5 group\".\n\n        By default we always save:\n            - the gt/pred track id\n            - bbox\n            - centroid\n            - pose\n            - instance/traj/points score\n        Larger arrays (crops/features/embeddings) can be saved by passing as kwargs\n\n        Args:\n            frame_group: the h5py group representing the frame the instance appears on\n            label: the name of the instance group that will be created\n            **kwargs: additional key:value pairs to be saved as datasets.\n\n        Returns:\n            The h5 group representing this instance.\n        \"\"\"\n        if label is None:\n            if pred_track_id != -1:\n                label = f\"instance_{self.pred_track_id.item()}\"\n            else:\n                label = f\"instance_{self.gt_track_id.item()}\"\n        instance_group = frame_group.create_group(label)\n        instance_group.attrs.create(\"gt_track_id\", self.gt_track_id.item())\n        instance_group.attrs.create(\"pred_track_id\", self.pred_track_id.item())\n        instance_group.attrs.create(\"track_score\", self.track_score)\n        instance_group.attrs.create(\"instance_score\", self.instance_score)\n\n        instance_group.create_dataset(\"bbox\", data=self.bbox.cpu().numpy())\n\n        pose_group = instance_group.create_group(\"pose\")\n        pose_group.create_dataset(\"points\", data=np.array(list(self.pose.values())))\n        pose_group.attrs.create(\"nodes\", list(self.pose.keys()))\n        pose_group.create_dataset(\"scores\", data=self.point_scores)\n\n        for key, value in kwargs.items():\n            if \"emb\" in key:\n                emb_group = instance_group.require_group(\"emb\")\n                emb_group.create_dataset(key, data=value)\n            else:\n                instance_group.create_dataset(key, data=value)\n\n        return instance_group\n\n    @property\n    def device(self) -&gt; str:\n        \"\"\"The device the instance is on.\n\n        Returns:\n            The str representation of the device the gpu is on.\n        \"\"\"\n        return self._device\n\n    @device.setter\n    def device(self, device) -&gt; None:\n        \"\"\"Set for the device property.\n\n        Args:\n            device: The str representation of the device.\n        \"\"\"\n        self._device = device\n\n    @property\n    def gt_track_id(self) -&gt; torch.Tensor:\n        \"\"\"The ground truth track id of the instance.\n\n        Returns:\n            A tensor containing the ground truth track id\n        \"\"\"\n        return self._gt_track_id\n\n    @gt_track_id.setter\n    def gt_track_id(self, track: int):\n        \"\"\"Set the instance ground-truth track id.\n\n        Args:\n           track: An int representing the ground-truth track id.\n        \"\"\"\n        if track is not None:\n            self._gt_track_id = torch.tensor([track])\n        else:\n            self._gt_track_id = torch.tensor([])\n\n    def has_gt_track_id(self) -&gt; bool:\n        \"\"\"Determine if instance has a gt track assignment.\n\n        Returns:\n            True if the gt track id is set, otherwise False.\n        \"\"\"\n        if self._gt_track_id.shape[0] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def pred_track_id(self) -&gt; torch.Tensor:\n        \"\"\"The track id predicted by the tracker using asso_output from model.\n\n        Returns:\n            A tensor containing the predicted track id.\n        \"\"\"\n        return self._pred_track_id\n\n    @pred_track_id.setter\n    def pred_track_id(self, track: int) -&gt; None:\n        \"\"\"Set predicted track id.\n\n        Args:\n            track: an int representing the predicted track id.\n        \"\"\"\n        if track is not None:\n            self._pred_track_id = torch.tensor([track])\n        else:\n            self._pred_track_id = torch.tensor([])\n\n    def has_pred_track_id(self) -&gt; bool:\n        \"\"\"Determine whether instance has predicted track id.\n\n        Returns:\n            True if instance has a pred track id, False otherwise.\n        \"\"\"\n        if self._pred_track_id.item() == -1 or self._pred_track_id.shape[0] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def bbox(self) -&gt; torch.Tensor:\n        \"\"\"The bounding box coordinates of the instance in the original frame.\n\n        Returns:\n            A (1,4) tensor containing the bounding box coordinates.\n        \"\"\"\n        return self._bbox\n\n    @bbox.setter\n    def bbox(self, bbox: ArrayLike) -&gt; None:\n        \"\"\"Set the instance bounding box.\n\n        Args:\n            bbox: an arraylike object containing the bounding box coordinates.\n        \"\"\"\n        if bbox is None or len(bbox) == 0:\n            self._bbox = torch.empty((0, 4))\n        else:\n            if not isinstance(bbox, torch.Tensor):\n                self._bbox = torch.tensor(bbox)\n            else:\n                self._bbox = bbox\n\n        if self._bbox.shape[0] and len(self._bbox.shape) == 1:\n            self._bbox = self._bbox.unsqueeze(0)\n        if self._bbox.shape[1] and len(self._bbox.shape) == 2:\n            self._bbox = self._bbox.unsqueeze(0)\n\n    def has_bbox(self) -&gt; bool:\n        \"\"\"Determine if the instance has a bbox.\n\n        Returns:\n            True if the instance has a bounding box, false otherwise.\n        \"\"\"\n        if self._bbox.shape[1] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def centroid(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"The centroid around which the crop was formed.\n\n        Returns:\n            A dict containing the anchor name and the x, y bbox midpoint.\n        \"\"\"\n        return self._centroid\n\n    @centroid.setter\n    def centroid(self, centroid: dict[str, ArrayLike]) -&gt; None:\n        \"\"\"Set the centroid of the instance.\n\n        Args:\n            centroid: A dict containing the anchor name and points.\n        \"\"\"\n        self._centroid = centroid\n\n    @property\n    def anchor(self) -&gt; list[str]:\n        \"\"\"The anchor node name around which the crop was formed.\n\n        Returns:\n            the list of anchors around which each crop was formed\n            the list of anchors around which each crop was formed\n        \"\"\"\n        if self.centroid:\n            return list(self.centroid.keys())\n        return \"\"\n\n    @property\n    def mask(self) -&gt; torch.Tensor:\n        \"\"\"The mask of the instance.\n\n        Returns:\n            A (h, w) tensor containing the mask of the instance.\n        \"\"\"\n        return self._mask\n\n    @mask.setter\n    def mask(self, mask: ArrayLike) -&gt; None:\n        \"\"\"Set the mask of the instance.\n\n        Args:\n            mask: an arraylike object containing the mask of the instance.\n        \"\"\"\n        if mask is None or len(mask) == 0:\n            self._mask = torch.tensor([])\n        else:\n            if not isinstance(mask, torch.Tensor):\n                self._mask = torch.tensor(mask)\n            else:\n                self._mask = mask\n\n    @property\n    def crop(self) -&gt; torch.Tensor:\n        \"\"\"The crop of the instance.\n\n        Returns:\n            A (1, c, h , w) tensor containing the cropped image centered around the instance.\n        \"\"\"\n        return self._crop\n\n    @crop.setter\n    def crop(self, crop: ArrayLike) -&gt; None:\n        \"\"\"Set the crop of the instance.\n\n        Args:\n            crop: an arraylike object containing the cropped image of the centered instance.\n        \"\"\"\n        if crop is None or len(crop) == 0:\n            self._crop = torch.tensor([])\n        else:\n            if not isinstance(crop, torch.Tensor):\n                self._crop = torch.tensor(crop)\n            else:\n                self._crop = crop\n\n        if len(self._crop.shape) == 2:\n            self._crop = self._crop.unsqueeze(0)\n        if len(self._crop.shape) == 3:\n            self._crop = self._crop.unsqueeze(0)\n\n    def has_crop(self) -&gt; bool:\n        \"\"\"Determine if the instance has a crop.\n\n        Returns:\n            True if the instance has an image otherwise False.\n        \"\"\"\n        if self._crop.shape[-1] == 0:\n            return False\n        else:\n            return True\n\n    @property\n    def features(self) -&gt; torch.Tensor:\n        \"\"\"Re-ID feature vector from backbone model to be used as input to transformer.\n\n        Returns:\n            a (1, d) tensor containing the reid feature vector.\n        \"\"\"\n        return self._features\n\n    @features.setter\n    def features(self, features: ArrayLike) -&gt; None:\n        \"\"\"Set the reid feature vector of the instance.\n\n        Args:\n            features: a (1,d) array like object containing the reid features for the instance.\n        \"\"\"\n        if features is None or len(features) == 0:\n            self._features = torch.tensor([])\n\n        elif not isinstance(features, torch.Tensor):\n            self._features = torch.tensor(features)\n        else:\n            self._features = features\n\n        if self._features.shape[0] and len(self._features.shape) == 1:\n            self._features = self._features.unsqueeze(0)\n\n    def has_features(self) -&gt; bool:\n        \"\"\"Determine if the instance has computed reid features.\n\n        Returns:\n            True if the instance has reid features, False otherwise.\n        \"\"\"\n        if self._features.shape[-1] == 0:\n            return False\n        else:\n            return True\n\n    def has_embedding(self, emb_type: str | None = None) -&gt; bool:\n        \"\"\"Determine if the instance has embedding type requested.\n\n        Args:\n            emb_type: The key to check in the embedding dictionary.\n\n        Returns:\n            True if `emb_type` in embedding_dict else false\n        \"\"\"\n        return emb_type in self._embeddings\n\n    def get_embedding(\n        self, emb_type: str = \"all\"\n    ) -&gt; dict[str, torch.Tensor] | torch.Tensor | None:\n        \"\"\"Retrieve instance's spatial/temporal embedding.\n\n        Args:\n            emb_type: The string key of the embedding to retrieve. Should be \"pos\", \"temp\"\n\n        Returns:\n            * A torch tensor representing the spatial/temporal location of the instance.\n            * None if the embedding is not stored\n        \"\"\"\n        if emb_type.lower() == \"all\":\n            return self._embeddings\n        else:\n            try:\n                return self._embeddings[emb_type]\n            except KeyError:\n                logger.exception(\n                    f\"{emb_type} not saved! Only {list(self._embeddings.keys())} are available\"\n                )\n        return None\n\n    def add_embedding(self, emb_type: str, embedding: torch.Tensor) -&gt; None:\n        \"\"\"Save embedding to instance embedding dictionary.\n\n        Args:\n            emb_type: Key/embedding type to be saved to dictionary\n            embedding: The actual torch tensor embedding.\n        \"\"\"\n        embedding = _expand_to_rank(embedding, 2)\n        self._embeddings[emb_type] = embedding\n\n    @property\n    def frame(self) -&gt; \"Frame\":\n        \"\"\"Get the frame the instance belongs to.\n\n        Returns:\n            The back reference to the `Frame` that this `Instance` belongs to.\n        \"\"\"\n        return self._frame\n\n    @frame.setter\n    def frame(self, frame: \"Frame\") -&gt; None:\n        \"\"\"Set the back reference to the `Frame` that this `Instance` belongs to.\n\n        This field is set when instances are added to `Frame` object.\n\n        Args:\n            frame: A `Frame` object containing the metadata for the frame that the instance belongs to\n        \"\"\"\n        self._frame = frame\n\n    @property\n    def pose(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"Get the pose of the instance.\n\n        Returns:\n            A dictionary containing the node and corresponding x,y points\n        \"\"\"\n        return self._pose\n\n    @pose.setter\n    def pose(self, pose: dict[str, ArrayLike]) -&gt; None:\n        \"\"\"Set the pose of the instance.\n\n        Args:\n            pose: A nodes x 2 array containing the pose coordinates.\n        \"\"\"\n        if pose is not None:\n            self._pose = pose\n\n        elif self.bbox.shape[0]:\n            y1, x1, y2, x2 = self.bbox.squeeze()\n            self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n        else:\n            self._pose = {}\n\n    def has_pose(self) -&gt; bool:\n        \"\"\"Check if the instance has a pose.\n\n        Returns True if the instance has a pose.\n        \"\"\"\n        if len(self.pose):\n            return True\n        return False\n\n    @property\n    def shown_pose(self) -&gt; dict[str, ArrayLike]:\n        \"\"\"Get the pose with shown nodes only.\n\n        Returns: A dictionary filtered by nodes that are shown (points are not nan).\n        \"\"\"\n        pose = self.pose\n        return {node: point for node, point in pose.items() if not np.isna(point).any()}\n\n    @property\n    def skeleton(self) -&gt; sio.Skeleton:\n        \"\"\"Get the skeleton associated with the instance.\n\n        Returns: The sio.Skeleton associated with the instance.\n        \"\"\"\n        return self._skeleton\n\n    @skeleton.setter\n    def skeleton(self, skeleton: sio.Skeleton) -&gt; None:\n        \"\"\"Set the skeleton associated with the instance.\n\n        Args:\n            skeleton: The sio.Skeleton associated with the instance.\n        \"\"\"\n        self._skeleton = skeleton\n\n    @property\n    def point_scores(self) -&gt; ArrayLike:\n        \"\"\"Get the point scores associated with the pose prediction.\n\n        Returns: a vector of shape n containing the point scores outputted from sleap associated with pose predictions.\n        \"\"\"\n        return self._point_scores\n\n    @point_scores.setter\n    def point_scores(self, point_scores: ArrayLike) -&gt; None:\n        \"\"\"Set the point scores associated with the pose prediction.\n\n        Args:\n            point_scores: a vector of shape n containing the point scores\n            outputted from sleap associated with pose predictions.\n        \"\"\"\n        self._point_scores = point_scores\n\n    @property\n    def instance_score(self) -&gt; float:\n        \"\"\"Get the pose prediction score associated with the instance.\n\n        Returns: a float from 0-1 representing an instance_score.\n        \"\"\"\n        return self._instance_score\n\n    @instance_score.setter\n    def instance_score(self, instance_score: float) -&gt; None:\n        \"\"\"Set the pose prediction score associated with the instance.\n\n        Args:\n            instance_score: a float from 0-1 representing an instance_score.\n        \"\"\"\n        self._instance_score = instance_score\n\n    @property\n    def track_score(self) -&gt; float:\n        \"\"\"Get the track_score of the instance.\n\n        Returns: A float from 0-1 representing the output used in the tracker for assignment.\n        \"\"\"\n        return self._track_score\n\n    @track_score.setter\n    def track_score(self, track_score: float) -&gt; None:\n        \"\"\"Set the track_score of the instance.\n\n        Args:\n            track_score: A float from 0-1 representing the output used in the tracker for assignment.\n        \"\"\"\n        self._track_score = track_score\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.anchor","title":"<code>anchor</code>  <code>property</code>","text":"<p>The anchor node name around which the crop was formed.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>the list of anchors around which each crop was formed the list of anchors around which each crop was formed</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.bbox","title":"<code>bbox</code>  <code>property</code> <code>writable</code>","text":"<p>The bounding box coordinates of the instance in the original frame.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A (1,4) tensor containing the bounding box coordinates.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.centroid","title":"<code>centroid</code>  <code>property</code> <code>writable</code>","text":"<p>The centroid around which the crop was formed.</p> <p>Returns:</p> Type Description <code>dict[str, ArrayLike]</code> <p>A dict containing the anchor name and the x, y bbox midpoint.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.crop","title":"<code>crop</code>  <code>property</code> <code>writable</code>","text":"<p>The crop of the instance.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A (1, c, h , w) tensor containing the cropped image centered around the instance.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.device","title":"<code>device</code>  <code>property</code> <code>writable</code>","text":"<p>The device the instance is on.</p> <p>Returns:</p> Type Description <code>str</code> <p>The str representation of the device the gpu is on.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.features","title":"<code>features</code>  <code>property</code> <code>writable</code>","text":"<p>Re-ID feature vector from backbone model to be used as input to transformer.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>a (1, d) tensor containing the reid feature vector.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.frame","title":"<code>frame</code>  <code>property</code> <code>writable</code>","text":"<p>Get the frame the instance belongs to.</p> <p>Returns:</p> Type Description <code>Frame</code> <p>The back reference to the <code>Frame</code> that this <code>Instance</code> belongs to.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.gt_track_id","title":"<code>gt_track_id</code>  <code>property</code> <code>writable</code>","text":"<p>The ground truth track id of the instance.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the ground truth track id</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.instance_score","title":"<code>instance_score</code>  <code>property</code> <code>writable</code>","text":"<p>Get the pose prediction score associated with the instance.</p> <p>Returns: a float from 0-1 representing an instance_score.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.mask","title":"<code>mask</code>  <code>property</code> <code>writable</code>","text":"<p>The mask of the instance.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A (h, w) tensor containing the mask of the instance.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.point_scores","title":"<code>point_scores</code>  <code>property</code> <code>writable</code>","text":"<p>Get the point scores associated with the pose prediction.</p> <p>Returns: a vector of shape n containing the point scores outputted from sleap associated with pose predictions.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.pose","title":"<code>pose</code>  <code>property</code> <code>writable</code>","text":"<p>Get the pose of the instance.</p> <p>Returns:</p> Type Description <code>dict[str, ArrayLike]</code> <p>A dictionary containing the node and corresponding x,y points</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.pred_track_id","title":"<code>pred_track_id</code>  <code>property</code> <code>writable</code>","text":"<p>The track id predicted by the tracker using asso_output from model.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the predicted track id.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.shown_pose","title":"<code>shown_pose</code>  <code>property</code>","text":"<p>Get the pose with shown nodes only.</p> <p>Returns: A dictionary filtered by nodes that are shown (points are not nan).</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.skeleton","title":"<code>skeleton</code>  <code>property</code> <code>writable</code>","text":"<p>Get the skeleton associated with the instance.</p> <p>Returns: The sio.Skeleton associated with the instance.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.track_score","title":"<code>track_score</code>  <code>property</code> <code>writable</code>","text":"<p>Get the track_score of the instance.</p> <p>Returns: A float from 0-1 representing the output used in the tracker for assignment.</p>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.__attrs_post_init__","title":"<code>__attrs_post_init__()</code>","text":"<p>Handle dimensionality and more intricate default initializations post-init.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def __attrs_post_init__(self) -&gt; None:\n    \"\"\"Handle dimensionality and more intricate default initializations post-init.\"\"\"\n    self.bbox = _expand_to_rank(self.bbox, 3)\n    self.crop = _expand_to_rank(self.crop, 4)\n    self.features = _expand_to_rank(self.features, 2)\n\n    if self.skeleton is None:\n        self.skeleton = sio.Skeleton([\"centroid\"])\n\n    if self.bbox.shape[-1] == 0:\n        self.bbox = torch.empty([1, 0, 4])\n\n    if self.crop.shape[-1] == 0 and self.bbox.shape[1] != 0:\n        y1, x1, y2, x2 = self.bbox.squeeze(dim=0).nanmean(dim=0)\n        self.centroid = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n    if len(self.pose) == 0 and self.bbox.shape[1]:\n        y1, x1, y2, x2 = self.bbox.squeeze(dim=0).mean(dim=0)\n        self._pose = {\"centroid\": np.array([(x1 + x2) / 2, (y1 + y2) / 2])}\n\n    if self.point_scores is None and len(self.pose) != 0:\n        self._point_scores = np.zeros((len(self.pose), 2))\n\n    self.to(self.device)\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of the Instance.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation of the Instance.\"\"\"\n    return (\n        \"Instance(\"\n        f\"gt_track_id={self._gt_track_id.item()}, \"\n        f\"pred_track_id={self._pred_track_id.item()}, \"\n        f\"bbox={self._bbox}, \"\n        f\"centroid={self._centroid}, \"\n        f\"crop={self._crop.shape}, \"\n        f\"features={self._features.shape}, \"\n        f\"device={self._device}\"\n        \")\"\n    )\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.add_embedding","title":"<code>add_embedding(emb_type, embedding)</code>","text":"<p>Save embedding to instance embedding dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>Key/embedding type to be saved to dictionary</p> required <code>embedding</code> <code>Tensor</code> <p>The actual torch tensor embedding.</p> required Source code in <code>dreem/io/instance.py</code> <pre><code>def add_embedding(self, emb_type: str, embedding: torch.Tensor) -&gt; None:\n    \"\"\"Save embedding to instance embedding dictionary.\n\n    Args:\n        emb_type: Key/embedding type to be saved to dictionary\n        embedding: The actual torch tensor embedding.\n    \"\"\"\n    embedding = _expand_to_rank(embedding, 2)\n    self._embeddings[emb_type] = embedding\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.from_slp","title":"<code>from_slp(slp_instance, bbox_size=64, crop=None, device=None)</code>  <code>classmethod</code>","text":"<p>Convert a slp instance to a dreem instance.</p> <p>Parameters:</p> Name Type Description Default <code>slp_instance</code> <code>PredictedInstance | Instance</code> <p>A <code>sleap_io.Instance</code> object representing a detection</p> required <code>bbox_size</code> <code>int | tuple[int, int]</code> <p>size of the pose-centered bbox to form.</p> <code>64</code> <code>crop</code> <code>ArrayLike | None</code> <p>The corresponding crop of the bbox</p> <code>None</code> <code>device</code> <code>str | None</code> <p>which device to keep the instance on</p> <code>None</code> <p>Returns:     A dreem.Instance object with a pose-centered bbox and no crop.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>@classmethod\ndef from_slp(\n    cls,\n    slp_instance: sio.PredictedInstance | sio.Instance,\n    bbox_size: int | tuple[int, int] = 64,\n    crop: ArrayLike | None = None,\n    device: str | None = None,\n) -&gt; Self:\n    \"\"\"Convert a slp instance to a dreem instance.\n\n    Args:\n        slp_instance: A `sleap_io.Instance` object representing a detection\n        bbox_size: size of the pose-centered bbox to form.\n        crop: The corresponding crop of the bbox\n        device: which device to keep the instance on\n    Returns:\n        A dreem.Instance object with a pose-centered bbox and no crop.\n    \"\"\"\n    try:\n        track_id = int(slp_instance.track.name)\n    except ValueError:\n        track_id = int(\n            \"\".join([str(ord(c)) for c in slp_instance.track.name])\n        )  # better way to handle this?\n    if isinstance(bbox_size, int):\n        bbox_size = (bbox_size, bbox_size)\n\n    track_score = -1.0\n    point_scores = np.full(len(slp_instance.points), -1)\n    instance_score = -1\n    if isinstance(slp_instance, sio.PredictedInstance):\n        track_score = slp_instance.tracking_score\n        point_scores = slp_instance.numpy()[:, -1]\n        instance_score = slp_instance.score\n\n    centroid = np.nanmean(slp_instance.numpy(), axis=1)\n    bbox = [\n        centroid[1] - bbox_size[1],\n        centroid[0] - bbox_size[0],\n        centroid[1] + bbox_size[1],\n        centroid[0] + bbox_size[0],\n    ]\n    return cls(\n        gt_track_id=track_id,\n        bbox=bbox,\n        crop=crop,\n        centroid={\"centroid\": centroid},\n        track_score=track_score,\n        point_scores=point_scores,\n        instance_score=instance_score,\n        skeleton=slp_instance.skeleton,\n        pose={\n            node.name: point.numpy() for node, point in slp_instance.points.items()\n        },\n        device=device,\n    )\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.get_embedding","title":"<code>get_embedding(emb_type='all')</code>","text":"<p>Retrieve instance's spatial/temporal embedding.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>The string key of the embedding to retrieve. Should be \"pos\", \"temp\"</p> <code>'all'</code> <p>Returns:</p> Type Description <code>dict[str, Tensor] | Tensor | None</code> <ul> <li>A torch tensor representing the spatial/temporal location of the instance.</li> <li>None if the embedding is not stored</li> </ul> Source code in <code>dreem/io/instance.py</code> <pre><code>def get_embedding(\n    self, emb_type: str = \"all\"\n) -&gt; dict[str, torch.Tensor] | torch.Tensor | None:\n    \"\"\"Retrieve instance's spatial/temporal embedding.\n\n    Args:\n        emb_type: The string key of the embedding to retrieve. Should be \"pos\", \"temp\"\n\n    Returns:\n        * A torch tensor representing the spatial/temporal location of the instance.\n        * None if the embedding is not stored\n    \"\"\"\n    if emb_type.lower() == \"all\":\n        return self._embeddings\n    else:\n        try:\n            return self._embeddings[emb_type]\n        except KeyError:\n            logger.exception(\n                f\"{emb_type} not saved! Only {list(self._embeddings.keys())} are available\"\n            )\n    return None\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.has_bbox","title":"<code>has_bbox()</code>","text":"<p>Determine if the instance has a bbox.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has a bounding box, false otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_bbox(self) -&gt; bool:\n    \"\"\"Determine if the instance has a bbox.\n\n    Returns:\n        True if the instance has a bounding box, false otherwise.\n    \"\"\"\n    if self._bbox.shape[1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.has_crop","title":"<code>has_crop()</code>","text":"<p>Determine if the instance has a crop.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has an image otherwise False.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_crop(self) -&gt; bool:\n    \"\"\"Determine if the instance has a crop.\n\n    Returns:\n        True if the instance has an image otherwise False.\n    \"\"\"\n    if self._crop.shape[-1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.has_embedding","title":"<code>has_embedding(emb_type=None)</code>","text":"<p>Determine if the instance has embedding type requested.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str | None</code> <p>The key to check in the embedding dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if <code>emb_type</code> in embedding_dict else false</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_embedding(self, emb_type: str | None = None) -&gt; bool:\n    \"\"\"Determine if the instance has embedding type requested.\n\n    Args:\n        emb_type: The key to check in the embedding dictionary.\n\n    Returns:\n        True if `emb_type` in embedding_dict else false\n    \"\"\"\n    return emb_type in self._embeddings\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.has_features","title":"<code>has_features()</code>","text":"<p>Determine if the instance has computed reid features.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the instance has reid features, False otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_features(self) -&gt; bool:\n    \"\"\"Determine if the instance has computed reid features.\n\n    Returns:\n        True if the instance has reid features, False otherwise.\n    \"\"\"\n    if self._features.shape[-1] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.has_gt_track_id","title":"<code>has_gt_track_id()</code>","text":"<p>Determine if instance has a gt track assignment.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the gt track id is set, otherwise False.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_gt_track_id(self) -&gt; bool:\n    \"\"\"Determine if instance has a gt track assignment.\n\n    Returns:\n        True if the gt track id is set, otherwise False.\n    \"\"\"\n    if self._gt_track_id.shape[0] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.has_pose","title":"<code>has_pose()</code>","text":"<p>Check if the instance has a pose.</p> <p>Returns True if the instance has a pose.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_pose(self) -&gt; bool:\n    \"\"\"Check if the instance has a pose.\n\n    Returns True if the instance has a pose.\n    \"\"\"\n    if len(self.pose):\n        return True\n    return False\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.has_pred_track_id","title":"<code>has_pred_track_id()</code>","text":"<p>Determine whether instance has predicted track id.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if instance has a pred track id, False otherwise.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def has_pred_track_id(self) -&gt; bool:\n    \"\"\"Determine whether instance has predicted track id.\n\n    Returns:\n        True if instance has a pred track id, False otherwise.\n    \"\"\"\n    if self._pred_track_id.item() == -1 or self._pred_track_id.shape[0] == 0:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.to","title":"<code>to(map_location)</code>","text":"<p>Move instance to different device or change dtype. (See <code>torch.to</code> for more info).</p> <p>Parameters:</p> Name Type Description Default <code>map_location</code> <code>str | device</code> <p>Either the device or dtype for the instance to be moved.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>Self</code> <p>reference to the instance moved to correct device/dtype.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to(self, map_location: str | torch.device) -&gt; Self:\n    \"\"\"Move instance to different device or change dtype. (See `torch.to` for more info).\n\n    Args:\n        map_location: Either the device or dtype for the instance to be moved.\n\n    Returns:\n        self: reference to the instance moved to correct device/dtype.\n    \"\"\"\n    if map_location is not None and map_location != \"\":\n        self._gt_track_id = self._gt_track_id.to(map_location)\n        self._pred_track_id = self._pred_track_id.to(map_location)\n        self._bbox = self._bbox.to(map_location)\n        self._crop = self._crop.to(map_location)\n        self._features = self._features.to(map_location)\n        if isinstance(map_location, (str, torch.device)):\n            self.device = map_location\n\n    return self\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.to_h5","title":"<code>to_h5(frame_group, label=None, **kwargs)</code>","text":"<p>Convert instance to an h5 group\".</p> By default we always save <ul> <li>the gt/pred track id</li> <li>bbox</li> <li>centroid</li> <li>pose</li> <li>instance/traj/points score</li> </ul> <p>Larger arrays (crops/features/embeddings) can be saved by passing as kwargs</p> <p>Parameters:</p> Name Type Description Default <code>frame_group</code> <code>Group</code> <p>the h5py group representing the frame the instance appears on</p> required <code>label</code> <code>Any</code> <p>the name of the instance group that will be created</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>additional key:value pairs to be saved as datasets.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Group</code> <p>The h5 group representing this instance.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to_h5(\n    self, frame_group: h5py.Group, label: Any = None, **kwargs: dict\n) -&gt; h5py.Group:\n    \"\"\"Convert instance to an h5 group\".\n\n    By default we always save:\n        - the gt/pred track id\n        - bbox\n        - centroid\n        - pose\n        - instance/traj/points score\n    Larger arrays (crops/features/embeddings) can be saved by passing as kwargs\n\n    Args:\n        frame_group: the h5py group representing the frame the instance appears on\n        label: the name of the instance group that will be created\n        **kwargs: additional key:value pairs to be saved as datasets.\n\n    Returns:\n        The h5 group representing this instance.\n    \"\"\"\n    if label is None:\n        if pred_track_id != -1:\n            label = f\"instance_{self.pred_track_id.item()}\"\n        else:\n            label = f\"instance_{self.gt_track_id.item()}\"\n    instance_group = frame_group.create_group(label)\n    instance_group.attrs.create(\"gt_track_id\", self.gt_track_id.item())\n    instance_group.attrs.create(\"pred_track_id\", self.pred_track_id.item())\n    instance_group.attrs.create(\"track_score\", self.track_score)\n    instance_group.attrs.create(\"instance_score\", self.instance_score)\n\n    instance_group.create_dataset(\"bbox\", data=self.bbox.cpu().numpy())\n\n    pose_group = instance_group.create_group(\"pose\")\n    pose_group.create_dataset(\"points\", data=np.array(list(self.pose.values())))\n    pose_group.attrs.create(\"nodes\", list(self.pose.keys()))\n    pose_group.create_dataset(\"scores\", data=self.point_scores)\n\n    for key, value in kwargs.items():\n        if \"emb\" in key:\n            emb_group = instance_group.require_group(\"emb\")\n            emb_group.create_dataset(key, data=value)\n        else:\n            instance_group.create_dataset(key, data=value)\n\n    return instance_group\n</code></pre>"},{"location":"reference/dreem/io/instance/#dreem.io.instance.Instance.to_slp","title":"<code>to_slp(track_lookup={})</code>","text":"<p>Convert instance to sleap_io.PredictedInstance object.</p> <p>Parameters:</p> Name Type Description Default <code>track_lookup</code> <code>dict[int, Track]</code> <p>A track look up dictionary containing track_id:sio.Track.</p> <code>{}</code> <p>Returns: A sleap_io.PredictedInstance with necessary metadata     and a track_lookup dictionary to persist tracks.</p> Source code in <code>dreem/io/instance.py</code> <pre><code>def to_slp(\n    self, track_lookup: dict[int, sio.Track] = {}\n) -&gt; tuple[sio.PredictedInstance, dict[int, sio.Track]]:\n    \"\"\"Convert instance to sleap_io.PredictedInstance object.\n\n    Args:\n        track_lookup: A track look up dictionary containing track_id:sio.Track.\n    Returns: A sleap_io.PredictedInstance with necessary metadata\n        and a track_lookup dictionary to persist tracks.\n    \"\"\"\n    try:\n        track_id = self.pred_track_id.item()\n        if track_id not in track_lookup:\n            track_lookup[track_id] = sio.Track(name=self.pred_track_id.item())\n\n        track = track_lookup[track_id]\n\n        return (\n            sio.PredictedInstance.from_numpy(\n                points_data=np.array(list(self.pose.values())),\n                skeleton=self.skeleton,\n                point_scores=self.point_scores,\n                score=self.instance_score,\n                tracking_score=self.track_score,\n                track=track,\n            ),\n            track_lookup,\n        )\n    except Exception as e:\n        logger.exception(\n            f\"Pose: {np.array(list(self.pose.values())).shape}, Pose score shape {self.point_scores.shape}\"\n        )\n        raise RuntimeError(f\"Failed to convert to sio.PredictedInstance: {e}\")\n</code></pre>"},{"location":"reference/dreem/io/track/","title":"track","text":""},{"location":"reference/dreem/io/track/#dreem.io.track","title":"<code>dreem.io.track</code>","text":"<p>Module containing data structures for storing instances of the same Track.</p> <p>Classes:</p> Name Description <code>Track</code> <p>Object for storing instances of the same track.</p>"},{"location":"reference/dreem/io/track/#dreem.io.track.Track","title":"<code>Track</code>","text":"<p>Object for storing instances of the same track.</p> <p>Attributes:</p> Name Type Description <code>id</code> <p>the track label.</p> <code>instances</code> <code>list['Instances']</code> <p>A list of instances belonging to the track.</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Get an instance from the track.</p> <code>__len__</code> <p>Get the length of the track.</p> <code>__repr__</code> <p>Get the string representation of the track.</p> Source code in <code>dreem/io/track.py</code> <pre><code>@attrs.define(eq=False)\nclass Track:\n    \"\"\"Object for storing instances of the same track.\n\n    Attributes:\n        id: the track label.\n        instances: A list of instances belonging to the track.\n    \"\"\"\n\n    _id: int = attrs.field(alias=\"id\")\n    _instances: list[\"Instance\"] = attrs.field(alias=\"instances\", factory=list)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get the string representation of the track.\n\n        Returns:\n            the string representation of the Track.\n        \"\"\"\n        return f\"Track(id={self.id}, len={len(self)})\"\n\n    @property\n    def track_id(self) -&gt; int:\n        \"\"\"Get the id of the track.\n\n        Returns:\n            The integer id of the track.\n        \"\"\"\n        return self._id\n\n    @track_id.setter\n    def track_id(self, track_id: int) -&gt; None:\n        \"\"\"Set the id of the track.\n\n        Args:\n            track_id: the int id of the track.\n        \"\"\"\n        self._id = track_id\n\n    @property\n    def instances(self) -&gt; list[\"Instances\"]:\n        \"\"\"Get the instances belonging to this track.\n\n        Returns:\n            A list of instances with this track id.\n        \"\"\"\n        return self._instances\n\n    @instances.setter\n    def instances(self, instances) -&gt; None:\n        \"\"\"Set the instances belonging to this track.\n\n        Args:\n            instances: A list of instances that belong to the same track.\n        \"\"\"\n        self._instances = instances\n\n    @property\n    def frames(self) -&gt; set[\"Frame\"]:\n        \"\"\"Get the frames where this track appears.\n\n        Returns:\n            A set of `Frame` objects where this track appears.\n        \"\"\"\n        return set([instance.frame for instance in self.instances])\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the length of the track.\n\n        Returns:\n            The number of instances/frames in the track.\n        \"\"\"\n        return len(self.instances)\n\n    def __getitem__(self, ind: int | list[int]) -&gt; \"Instance\" | list[\"Instance\"]:\n        \"\"\"Get an instance from the track.\n\n        Args:\n            ind: Either a single int or list of int indices.\n\n        Returns:\n            the instance at that index of the track.instances.\n        \"\"\"\n        if isinstance(ind, int):\n            return self.instances[ind]\n        elif isinstance(ind, list):\n            return [self.instances[i] for i in ind]\n        else:\n            raise ValueError(f\"Ind must be an int or list of ints, found {type(ind)}\")\n</code></pre>"},{"location":"reference/dreem/io/track/#dreem.io.track.Track.frames","title":"<code>frames</code>  <code>property</code>","text":"<p>Get the frames where this track appears.</p> <p>Returns:</p> Type Description <code>set['Frame']</code> <p>A set of <code>Frame</code> objects where this track appears.</p>"},{"location":"reference/dreem/io/track/#dreem.io.track.Track.instances","title":"<code>instances</code>  <code>property</code> <code>writable</code>","text":"<p>Get the instances belonging to this track.</p> <p>Returns:</p> Type Description <code>list['Instances']</code> <p>A list of instances with this track id.</p>"},{"location":"reference/dreem/io/track/#dreem.io.track.Track.track_id","title":"<code>track_id</code>  <code>property</code> <code>writable</code>","text":"<p>Get the id of the track.</p> <p>Returns:</p> Type Description <code>int</code> <p>The integer id of the track.</p>"},{"location":"reference/dreem/io/track/#dreem.io.track.Track.__getitem__","title":"<code>__getitem__(ind)</code>","text":"<p>Get an instance from the track.</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> <code>int | list[int]</code> <p>Either a single int or list of int indices.</p> required <p>Returns:</p> Type Description <code>'Instance' | list['Instance']</code> <p>the instance at that index of the track.instances.</p> Source code in <code>dreem/io/track.py</code> <pre><code>def __getitem__(self, ind: int | list[int]) -&gt; \"Instance\" | list[\"Instance\"]:\n    \"\"\"Get an instance from the track.\n\n    Args:\n        ind: Either a single int or list of int indices.\n\n    Returns:\n        the instance at that index of the track.instances.\n    \"\"\"\n    if isinstance(ind, int):\n        return self.instances[ind]\n    elif isinstance(ind, list):\n        return [self.instances[i] for i in ind]\n    else:\n        raise ValueError(f\"Ind must be an int or list of ints, found {type(ind)}\")\n</code></pre>"},{"location":"reference/dreem/io/track/#dreem.io.track.Track.__len__","title":"<code>__len__()</code>","text":"<p>Get the length of the track.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of instances/frames in the track.</p> Source code in <code>dreem/io/track.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the length of the track.\n\n    Returns:\n        The number of instances/frames in the track.\n    \"\"\"\n    return len(self.instances)\n</code></pre>"},{"location":"reference/dreem/io/track/#dreem.io.track.Track.__repr__","title":"<code>__repr__()</code>","text":"<p>Get the string representation of the track.</p> <p>Returns:</p> Type Description <code>str</code> <p>the string representation of the Track.</p> Source code in <code>dreem/io/track.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get the string representation of the track.\n\n    Returns:\n        the string representation of the Track.\n    \"\"\"\n    return f\"Track(id={self.id}, len={len(self)})\"\n</code></pre>"},{"location":"reference/dreem/io/visualize/","title":"visualize","text":""},{"location":"reference/dreem/io/visualize/#dreem.io.visualize","title":"<code>dreem.io.visualize</code>","text":"<p>Helper functions for visualizing tracking.</p> <p>Functions:</p> Name Description <code>annotate_video</code> <p>Annotate video frames with labels.</p> <code>bold</code> <p>Bold value if it is over a threshold.</p> <code>color</code> <p>Highlight value in dataframe if it is over a threshold.</p> <code>fill_missing</code> <p>Fill missing values independently along each dimension after the first.</p> <code>main</code> <p>Take in a path to a video + labels file, annotates a video and saves it to the specified path.</p> <code>save_vid</code> <p>Save video to file.</p>"},{"location":"reference/dreem/io/visualize/#dreem.io.visualize.annotate_video","title":"<code>annotate_video(video, labels, key, color_palette=palette, trails=2, boxes=(64, 64), names=True, track_scores=0.5, centroids=4, poses=False, save_path='debug_animal.mp4', fps=30, alpha=0.2)</code>","text":"<p>Annotate video frames with labels.</p> <p>Labels video with bboxes, centroids, trajectory trails, and/or poses.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>Reader</code> <p>The video to be annotated in an ndarray</p> required <code>labels</code> <code>DataFrame</code> <p>The pandas dataframe containing the centroid and/or pose locations of the instances</p> required <code>key</code> <code>str</code> <p>The key where labels are stored in the dataframe - mostly used for choosing whether to annotate based on pred or gt labels</p> required <code>color_palette</code> <code>list | str</code> <p>The matplotlib colorpalette to use for annotating the video. Defaults to <code>tab10</code></p> <code>palette</code> <code>trails</code> <code>int</code> <p>The size of the trajectory trail. If trails size &lt;= 0 or None then it is not added</p> <code>2</code> <code>boxes</code> <code>int</code> <p>The size of the bbox. If bbox size &lt;= 0 or None then it is not added</p> <code>(64, 64)</code> <code>names</code> <code>bool</code> <p>Whether or not to annotate with name</p> <code>True</code> <code>centroids</code> <code>int</code> <p>The size of the centroid. If centroid size &lt;= 0 or None then it is not added</p> <code>4</code> <code>poses</code> <code>bool</code> <p>Whether or not to annotate with poses</p> <code>False</code> <code>fps</code> <code>int</code> <p>The frame rate of the generated video</p> <code>30</code> <code>alpha</code> <code>float</code> <p>The opacity of the annotations.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of annotated video frames</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def annotate_video(\n    video: \"imageio.core.format.Reader\",\n    labels: pd.DataFrame,\n    key: str,\n    color_palette: list | str = palette,\n    trails: int = 2,\n    boxes: int = (64, 64),\n    names: bool = True,\n    track_scores=0.5,\n    centroids: int = 4,\n    poses: bool = False,\n    save_path: str = \"debug_animal.mp4\",\n    fps: int = 30,\n    alpha: float = 0.2,\n) -&gt; list:\n    \"\"\"Annotate video frames with labels.\n\n    Labels video with bboxes, centroids, trajectory trails, and/or poses.\n\n    Args:\n        video: The video to be annotated in an ndarray\n        labels: The pandas dataframe containing the centroid and/or pose locations of the instances\n        key: The key where labels are stored in the dataframe - mostly used for choosing whether to annotate based on pred or gt labels\n        color_palette: The matplotlib colorpalette to use for annotating the video. Defaults to `tab10`\n        trails: The size of the trajectory trail. If trails size &lt;= 0 or None then it is not added\n        boxes: The size of the bbox. If bbox size &lt;= 0 or None then it is not added\n        names: Whether or not to annotate with name\n        centroids: The size of the centroid. If centroid size &lt;= 0 or None then it is not added\n        poses: Whether or not to annotate with poses\n        fps: The frame rate of the generated video\n        alpha: The opacity of the annotations.\n\n    Returns:\n        A list of annotated video frames\n    \"\"\"\n    writer = imageio.get_writer(save_path, fps=fps)\n    color_palette = (\n        sns.color_palette(color_palette)\n        if isinstance(color_palette, str)\n        else deepcopy(color_palette)\n    )\n\n    if trails:\n        track_trails = {}\n    try:\n        for i in tqdm(sorted(labels[\"Frame\"].unique()), desc=\"Frame\", unit=\"Frame\"):\n            frame = video.get_data(i)\n            if frame.shape[0] == 1 or frame.shape[-1] == 1:\n                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n            # else:\n            #     frame = frame.copy()\n\n            lf = labels[labels[\"Frame\"] == i]\n            for idx, instance in lf.iterrows():\n                if not trails:\n                    track_trails = {}\n\n                if poses:\n                    # TODO figure out best way to store poses (maybe pass a slp labels file too?)\n                    trails = False\n                    centroids = False\n                    for idx, (pose, edge) in enumerate(\n                        zip(instance[\"poses\"], instance[\"edges\"])\n                    ):\n                        pose = fill_missing(pose.numpy())\n\n                        pred_track_id = instance[key][idx].numpy().tolist()\n\n                        # Add midpt to track trail.\n                        if pred_track_id not in list(track_trails.keys()):\n                            track_trails[pred_track_id] = []\n\n                        # Select a color based on track_id.\n                        track_color_idx = pred_track_id % len(color_palette)\n                        track_color = (\n                            (np.array(color_palette[track_color_idx]) * 255)\n                            .astype(np.uint8)\n                            .tolist()[::-1]\n                        )\n\n                        for p in pose:\n                            # try:\n                            #    p = tuple([int(i) for i in p.numpy()][::-1])\n                            # except:\n                            #    continue\n\n                            p = tuple(int(i) for i in p)[::-1]\n\n                            track_trails[pred_track_id].append(p)\n\n                            frame = cv2.circle(\n                                frame, p, radius=2, color=track_color, thickness=-1\n                            )\n\n                        for e in edge:\n                            source = tuple(int(i) for i in pose[int(e[0])])[::-1]\n                            target = tuple(int(i) for i in pose[int(e[1])])[::-1]\n\n                            frame = cv2.line(frame, source, target, track_color, 1)\n\n                if (boxes) or centroids:\n                    # Get coordinates for detected objects in the current frame.\n                    if isinstance(boxes, int):\n                        boxes = (boxes, boxes)\n\n                    box_w, box_h = boxes\n                    x = instance[\"X\"]\n                    y = instance[\"Y\"]\n                    min_x, min_y, max_x, max_y = (\n                        int(x - box_w / 2),\n                        int(y - box_h / 2),\n                        int(x + box_w / 2),\n                        int(y + box_h / 2),\n                    )\n                    midpt = (int(x), int(y))\n\n                    pred_track_id = instance[key]\n\n                    if \"Track_score\" in instance.index:\n                        track_score = instance[\"Track_score\"]\n                    else:\n                        track_scores = 0\n\n                    # Add midpt to track trail.\n                    if pred_track_id not in list(track_trails.keys()):\n                        track_trails[pred_track_id] = []\n                    track_trails[pred_track_id].append(midpt)\n\n                    # Select a color based on track_id.\n                    track_color_idx = int(pred_track_id) % len(color_palette)\n                    track_color = (\n                        (np.array(color_palette[track_color_idx]) * 255)\n                        .astype(np.uint8)\n                        .tolist()[::-1]\n                    )\n\n                    # Bbox.\n                    if boxes is not None:\n                        frame = cv2.rectangle(\n                            frame,\n                            (min_x, min_y),\n                            (max_x, max_y),\n                            color=track_color,\n                            thickness=2,\n                        )\n\n                    # Track trail.\n                    if centroids:\n                        frame = cv2.circle(\n                            frame,\n                            midpt,\n                            radius=centroids,\n                            color=track_color,\n                            thickness=-1,\n                        )\n                        for i in range(0, len(track_trails[pred_track_id]) - 1):\n                            frame = cv2.addWeighted(\n                                cv2.circle(\n                                    frame,  # .copy(),\n                                    track_trails[pred_track_id][i],\n                                    radius=4,\n                                    color=track_color,\n                                    thickness=-1,\n                                ),\n                                alpha,\n                                frame,\n                                1 - alpha,\n                                0,\n                            )\n                            if trails:\n                                frame = cv2.line(\n                                    frame,\n                                    track_trails[pred_track_id][i],\n                                    track_trails[pred_track_id][i + 1],\n                                    color=track_color,\n                                    thickness=trails,\n                                )\n\n                # Track name.\n                name_str = \"\"\n\n                if names:\n                    name_str += f\"track_{pred_track_id}\"\n                if names and track_scores:\n                    name_str += \" | \"\n                if track_scores:\n                    name_str += f\"score: {track_score:0.3f}\"\n\n                if len(name_str) &gt; 0:\n                    frame = cv2.putText(\n                        frame,\n                        # f\"idx:{idx} | track_{pred_track_id}\",\n                        name_str,\n                        org=(int(min_x), max(0, int(min_y) - 10)),\n                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n                        fontScale=0.9,\n                        color=track_color,\n                        thickness=2,\n                    )\n            writer.append_data(frame)\n            # if i % fps == 0:\n            #     gc.collect()\n\n    except Exception as e:\n        writer.close()\n        logger.exception(e)\n        return False\n\n    writer.close()\n    return True\n</code></pre>"},{"location":"reference/dreem/io/visualize/#dreem.io.visualize.bold","title":"<code>bold(val, thresh=0.01)</code>","text":"<p>Bold value if it is over a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>float</code> <p>The value to bold or not</p> required <code>thresh</code> <code>float</code> <p>The threshold the value has to exceed to be bolded</p> <code>0.01</code> <p>Returns:</p> Type Description <code>str</code> <p>A string indicating how to bold the item.</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def bold(val: float, thresh: float = 0.01) -&gt; str:\n    \"\"\"Bold value if it is over a threshold.\n\n    Args:\n        val: The value to bold or not\n        thresh: The threshold the value has to exceed to be bolded\n\n    Returns:\n        A string indicating how to bold the item.\n    \"\"\"\n    bold = \"bold\" if float(val) &gt; thresh else \"\"\n    return f\"font-weight: {bold}\"\n</code></pre>"},{"location":"reference/dreem/io/visualize/#dreem.io.visualize.color","title":"<code>color(val, thresh=0.01)</code>","text":"<p>Highlight value in dataframe if it is over a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>float</code> <p>The value to color</p> required <code>thresh</code> <code>float</code> <p>The threshold for which to color</p> <code>0.01</code> <p>Returns:</p> Type Description <code>str</code> <p>A string containing how to highlight the value</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def color(val: float, thresh: float = 0.01) -&gt; str:\n    \"\"\"Highlight value in dataframe if it is over a threshold.\n\n    Args:\n        val: The value to color\n        thresh: The threshold for which to color\n\n    Returns:\n        A string containing how to highlight the value\n    \"\"\"\n    color = \"lightblue\" if float(val) &gt; thresh else \"\"\n    return f\"background-color: {color}\"\n</code></pre>"},{"location":"reference/dreem/io/visualize/#dreem.io.visualize.fill_missing","title":"<code>fill_missing(data, kind='linear')</code>","text":"<p>Fill missing values independently along each dimension after the first.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>the array for which to fill missing value</p> required <code>kind</code> <code>str</code> <p>How to interpolate missing values using <code>scipy.interpolate.interp1d</code></p> <code>'linear'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The array with missing values filled in</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>def fill_missing(data: np.ndarray, kind: str = \"linear\") -&gt; np.ndarray:\n    \"\"\"Fill missing values independently along each dimension after the first.\n\n    Args:\n        data: the array for which to fill missing value\n        kind: How to interpolate missing values using `scipy.interpolate.interp1d`\n\n    Returns:\n        The array with missing values filled in\n    \"\"\"\n    # Store initial shape.\n    initial_shape = data.shape\n\n    # Flatten after first dim.\n    data = data.reshape((initial_shape[0], -1))\n\n    # Interpolate along each slice.\n    for i in range(data.shape[-1]):\n        y = data[:, i]\n\n        # Build interpolant.\n        x = np.flatnonzero(~np.isnan(y))\n        f = interp1d(x, y[x], kind=kind, fill_value=np.nan, bounds_error=False)\n\n        # Fill missing\n        xq = np.flatnonzero(np.isnan(y))\n        y[xq] = f(xq)\n\n        # Fill leading or trailing NaNs with the nearest non-NaN values\n        mask = np.isnan(y)\n        y[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), y[~mask])\n\n        # Save slice\n        data[:, i] = y\n\n    # Restore to initial shape.\n    data = data.reshape(initial_shape)\n\n    return data\n</code></pre>"},{"location":"reference/dreem/io/visualize/#dreem.io.visualize.main","title":"<code>main(cfg)</code>","text":"<p>Take in a path to a video + labels file, annotates a video and saves it to the specified path.</p> Source code in <code>dreem/io/visualize.py</code> <pre><code>@hydra.main(config_path=None, config_name=None, version_base=None)\ndef main(cfg: DictConfig):\n    \"\"\"Take in a path to a video + labels file, annotates a video and saves it to the specified path.\"\"\"\n    labels = pd.read_csv(cfg.labels_path)\n    video = imageio.get_reader(cfg.vid_path, \"ffmpeg\")\n    frames_annotated = annotate_video(\n        video, labels, save_path=cfg.save_path, **cfg.annotate\n    )\n\n    if frames_annotated:\n        logger.info(\"Video saved to {cfg.save_path}!\")\n    else:\n        logger.error(\"Failed to annotate video!\")\n</code></pre>"},{"location":"reference/dreem/io/visualize/#dreem.io.visualize.save_vid","title":"<code>save_vid(annotated_frames, save_path='debug_animal', fps=30)</code>","text":"<p>Save video to file.</p> <p>Parameters:</p> Name Type Description Default <code>annotated_frames</code> <code>list</code> <p>a list of frames annotated by <code>annotate_frames</code></p> required <code>save_path</code> <code>str</code> <p>The path of the annotated file.</p> <code>'debug_animal'</code> <code>fps</code> <code>int</code> <p>The frame rate in frames per second of the annotated video</p> <code>30</code> Source code in <code>dreem/io/visualize.py</code> <pre><code>def save_vid(\n    annotated_frames: list,\n    save_path: str = \"debug_animal\",\n    fps: int = 30,\n):\n    \"\"\"Save video to file.\n\n    Args:\n        annotated_frames: a list of frames annotated by `annotate_frames`\n        save_path: The path of the annotated file.\n        fps: The frame rate in frames per second of the annotated video\n    \"\"\"\n    for idx, (ds_name, data) in enumerate([(save_path, annotated_frames)]):\n        imageio.mimwrite(f\"{ds_name}.mp4\", data, fps=fps, macro_block_size=1)\n</code></pre>"},{"location":"reference/dreem/models/","title":"models","text":""},{"location":"reference/dreem/models/#dreem.models","title":"<code>dreem.models</code>","text":"<p>Model architectures and layers.</p> <p>Modules:</p> Name Description <code>attention_head</code> <p>Module containing different components of multi-head attention heads.</p> <code>embedding</code> <p>Module containing different position and temporal embeddings.</p> <code>global_tracking_transformer</code> <p>Module containing GTR model used for training.</p> <code>gtr_runner</code> <p>Module containing training, validation and inference logic.</p> <code>mlp</code> <p>Multi-Layer Perceptron (MLP) module.</p> <code>model_utils</code> <p>Module containing model helper functions.</p> <code>transformer</code> <p>DETR Transformer class.</p> <code>visual_encoder</code> <p>Module for different visual feature extractors.</p> <p>Classes:</p> Name Description <code>DescriptorVisualEncoder</code> <p>Visual Encoder based on image descriptors.</p> <code>Embedding</code> <p>Class that wraps around different embedding types.</p> <code>FourierPositionalEmbeddings</code> <p>Fourier positional embeddings.</p> <code>GTRRunner</code> <p>A lightning wrapper around GTR model.</p> <code>GlobalTrackingTransformer</code> <p>Modular GTR model composed of visual encoder + transformer used for tracking.</p> <code>Transformer</code> <p>Transformer class.</p> <code>VisualEncoder</code> <p>Class wrapping around a visual feature extractor backbone.</p> <p>Functions:</p> Name Description <code>create_visual_encoder</code> <p>Create a visual encoder based on the specified type.</p> <code>register_encoder</code> <p>Register a new encoder type.</p>"},{"location":"reference/dreem/models/#dreem.models.DescriptorVisualEncoder","title":"<code>DescriptorVisualEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Visual Encoder based on image descriptors.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize Descriptor Visual Encoder.</p> <code>compute_hu_moments</code> <p>Compute Hu moments.</p> <code>compute_inertia_tensor</code> <p>Compute inertia tensor.</p> <code>forward</code> <p>Forward pass of feature extractor to get feature vector.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>class DescriptorVisualEncoder(torch.nn.Module):\n    \"\"\"Visual Encoder based on image descriptors.\"\"\"\n\n    def __init__(self, use_hu_moments: bool = False, **kwargs):\n        \"\"\"Initialize Descriptor Visual Encoder.\n\n        Args:\n            use_hu_moments: Whether to use Hu moments.\n        \"\"\"\n        super().__init__()\n        self.use_hu_moments = use_hu_moments\n\n    def compute_hu_moments(self, img):\n        \"\"\"Compute Hu moments.\"\"\"\n        mu = measure.moments_central(img)\n        nu = measure.moments_normalized(mu)\n        hu = measure.moments_hu(nu)\n        # log transform hu moments for scale differences; switched off; numerically unstable\n        # hu_log = -np.sign(hu) * np.log(np.abs(hu))\n\n        return hu\n\n    def compute_inertia_tensor(self, img):\n        \"\"\"Compute inertia tensor.\"\"\"\n        return measure.inertia_tensor(img)\n\n    @torch.no_grad()\n    def forward(self, img: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of feature extractor to get feature vector.\"\"\"\n        descriptors = []\n\n        for im in img:\n            im = im[0].cpu().numpy()\n\n            inertia_tensor = self.compute_inertia_tensor(im)\n            mean_intensity = im.mean()\n            if self.use_hu_moments:\n                hu_moments = self.compute_hu_moments(im)\n\n            # Flatten inertia tensor\n            inertia_tensor_flat = inertia_tensor.flatten()\n\n            # Combine all features into a single descriptor\n            descriptor = np.concatenate(\n                [\n                    inertia_tensor_flat,\n                    [mean_intensity],\n                    hu_moments if self.use_hu_moments else [],\n                ]\n            )\n\n            descriptors.append(torch.tensor(descriptor, dtype=torch.float32))\n\n        return torch.stack(descriptors)\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.DescriptorVisualEncoder.__init__","title":"<code>__init__(use_hu_moments=False, **kwargs)</code>","text":"<p>Initialize Descriptor Visual Encoder.</p> <p>Parameters:</p> Name Type Description Default <code>use_hu_moments</code> <code>bool</code> <p>Whether to use Hu moments.</p> <code>False</code> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def __init__(self, use_hu_moments: bool = False, **kwargs):\n    \"\"\"Initialize Descriptor Visual Encoder.\n\n    Args:\n        use_hu_moments: Whether to use Hu moments.\n    \"\"\"\n    super().__init__()\n    self.use_hu_moments = use_hu_moments\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.DescriptorVisualEncoder.compute_hu_moments","title":"<code>compute_hu_moments(img)</code>","text":"<p>Compute Hu moments.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def compute_hu_moments(self, img):\n    \"\"\"Compute Hu moments.\"\"\"\n    mu = measure.moments_central(img)\n    nu = measure.moments_normalized(mu)\n    hu = measure.moments_hu(nu)\n    # log transform hu moments for scale differences; switched off; numerically unstable\n    # hu_log = -np.sign(hu) * np.log(np.abs(hu))\n\n    return hu\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.DescriptorVisualEncoder.compute_inertia_tensor","title":"<code>compute_inertia_tensor(img)</code>","text":"<p>Compute inertia tensor.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def compute_inertia_tensor(self, img):\n    \"\"\"Compute inertia tensor.\"\"\"\n    return measure.inertia_tensor(img)\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.DescriptorVisualEncoder.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of feature extractor to get feature vector.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>@torch.no_grad()\ndef forward(self, img: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of feature extractor to get feature vector.\"\"\"\n    descriptors = []\n\n    for im in img:\n        im = im[0].cpu().numpy()\n\n        inertia_tensor = self.compute_inertia_tensor(im)\n        mean_intensity = im.mean()\n        if self.use_hu_moments:\n            hu_moments = self.compute_hu_moments(im)\n\n        # Flatten inertia tensor\n        inertia_tensor_flat = inertia_tensor.flatten()\n\n        # Combine all features into a single descriptor\n        descriptor = np.concatenate(\n            [\n                inertia_tensor_flat,\n                [mean_intensity],\n                hu_moments if self.use_hu_moments else [],\n            ]\n        )\n\n        descriptors.append(torch.tensor(descriptor, dtype=torch.float32))\n\n    return torch.stack(descriptors)\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.Embedding","title":"<code>Embedding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Class that wraps around different embedding types.</p> <p>Used for both learned and fixed embeddings.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize embeddings.</p> <code>forward</code> <p>Get the sequence positional embeddings.</p> Source code in <code>dreem/models/embedding.py</code> <pre><code>class Embedding(torch.nn.Module):\n    \"\"\"Class that wraps around different embedding types.\n\n    Used for both learned and fixed embeddings.\n    \"\"\"\n\n    EMB_TYPES = {\n        \"temp\": {},\n        \"pos\": {\"over_boxes\"},\n        \"off\": {},\n        None: {},\n    }  # dict of valid args:keyword params\n    EMB_MODES = {\n        \"fixed\": {\"temperature\", \"scale\", \"normalize\"},\n        \"learned\": {\"emb_num\"},\n        \"off\": {},\n    }  # dict of valid args:keyword params\n\n    def __init__(\n        self,\n        emb_type: str,\n        mode: str,\n        features: int,\n        n_points: int = 1,\n        emb_num: int = 16,\n        over_boxes: bool = True,\n        temperature: int = 10000,\n        normalize: bool = False,\n        scale: float | None = None,\n        mlp_cfg: dict | None = None,\n    ):\n        \"\"\"Initialize embeddings.\n\n        Args:\n            emb_type: The type of embedding to compute.\n                Must be one of `{\"temp\", \"pos\", \"off\"}`\n            mode: The mode or function used to map positions to vector embeddings.\n                  Must be one of `{\"fixed\", \"learned\", \"off\"}`\n            features: The embedding dimensions. Must match the dimension of the\n                      input vectors for the transformer model.\n            n_points: the number of points that will be embedded.\n            emb_num: the number of embeddings in the `self.lookup` table\n                (Only used in learned embeddings).\n            over_boxes: Whether to compute the position embedding for each bbox\n                coordinate (y1x1y2x2) or the centroid + bbox size (yxwh).\n            temperature: the temperature constant to be used when computing\n                the sinusoidal position embedding\n            normalize: whether or not to normalize the positions\n                (Only used in fixed embeddings).\n            scale: factor by which to scale the positions after normalizing\n                (Only used in fixed embeddings).\n            mlp_cfg: A dictionary of mlp hyperparameters for projecting\n                embedding to correct space.\n                    Example: {\"hidden_dims\": 256, \"num_layers\":3, \"dropout\": 0.3}\n        \"\"\"\n        self._check_init_args(emb_type, mode)\n\n        super().__init__()\n\n        self.emb_type = emb_type\n        self.mode = mode\n        self.features = features\n        self.emb_num = emb_num\n        self.over_boxes = over_boxes\n        self.temperature = temperature\n        self.normalize = normalize\n        self.scale = scale\n        self.n_points = n_points\n\n        if self.normalize and self.scale is None:\n            self.scale = 2 * math.pi\n\n        if self.emb_type == \"pos\" and mlp_cfg is not None and mlp_cfg[\"num_layers\"] &gt; 0:\n            if self.mode == \"fixed\":\n                self.mlp = MLP(\n                    input_dim=n_points * self.features,\n                    output_dim=self.features,\n                    **mlp_cfg,\n                )\n            else:\n                in_dim = (self.features // (4 * n_points)) * (4 * n_points)\n                self.mlp = MLP(\n                    input_dim=in_dim,\n                    output_dim=self.features,\n                    **mlp_cfg,\n                )\n        else:\n            self.mlp = torch.nn.Identity()\n\n        self._emb_func = lambda tensor: torch.zeros(\n            (tensor.shape[0], self.features), dtype=tensor.dtype, device=tensor.device\n        )  # turn off embedding by returning zeros\n\n        self.lookup = None\n\n        if self.mode == \"learned\":\n            if self.emb_type == \"pos\":\n                self.lookup = torch.nn.Embedding(\n                    self.emb_num * 4 * self.n_points, self.features // (4 * n_points)\n                )\n                self._emb_func = self._learned_pos_embedding\n            elif self.emb_type == \"temp\":\n                self.lookup = torch.nn.Embedding(self.emb_num, self.features)\n                self._emb_func = self._learned_temp_embedding\n\n        elif self.mode == \"fixed\":\n            if self.emb_type == \"pos\":\n                self._emb_func = self._sine_box_embedding\n            elif self.emb_type == \"temp\":\n                self._emb_func = self._sine_temp_embedding\n\n    def _check_init_args(self, emb_type: str, mode: str):\n        \"\"\"Check whether the correct arguments were passed to initialization.\n\n        Args:\n            emb_type: The type of embedding to compute. Must be one of `{\"temp\", \"pos\", \"\"}`\n            mode: The mode or function used to map positions to vector embeddings.\n                Must be one of `{\"fixed\", \"learned\"}`\n\n        Raises:\n            ValueError:\n              * if the incorrect `emb_type` or `mode` string are passed\n            NotImplementedError: if `emb_type` is `temp` and `mode` is `fixed`.\n        \"\"\"\n        if emb_type.lower() not in self.EMB_TYPES:\n            raise ValueError(\n                f\"Embedding `emb_type` must be one of {self.EMB_TYPES} not {emb_type}\"\n            )\n\n        if mode.lower() not in self.EMB_MODES:\n            raise ValueError(\n                f\"Embedding `mode` must be one of {self.EMB_MODES} not {mode}\"\n            )\n\n    def forward(self, seq_positions: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Get the sequence positional embeddings.\n\n        Args:\n            seq_positions:\n                * An (`N`, 1) tensor where seq_positions[i] represents the temporal position of instance_i in the sequence.\n                * An (`N`, n_anchors x 4) tensor where seq_positions[i, j, :] represents the [y1, x1, y2, x2] spatial locations of jth point of instance_i in the sequence.\n\n        Returns:\n            An `N` x `self.features` tensor representing the corresponding spatial or temporal embedding.\n        \"\"\"\n        emb = self._emb_func(seq_positions)\n\n        if emb.shape[-1] != self.features:\n            raise RuntimeError(\n                (\n                    f\"Output embedding dimension is {emb.shape[-1]} but requested {self.features} dimensions! \\n\"\n                    f\"hint: Try turning the MLP on by passing `mlp_cfg` to the constructor to project to the correct embedding dimensions.\"\n                )\n            )\n        return emb\n\n    def _torch_int_div(\n        self, tensor1: torch.Tensor, tensor2: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Perform integer division of two tensors.\n\n        Args:\n            tensor1: dividend tensor.\n            tensor2: divisor tensor.\n\n        Returns:\n            torch.Tensor, resulting tensor.\n        \"\"\"\n        return torch.div(tensor1, tensor2, rounding_mode=\"floor\")\n\n    def _sine_box_embedding(self, boxes: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute sine positional embeddings for boxes using given parameters.\n\n        Args:\n             boxes: the input boxes of shape N, n_anchors, 4 or B, N, n_anchors, 4\n                    where the last dimension is the bbox coords in [y1, x1, y2, x2].\n                    (Note currently `B=batch_size=1`).\n\n        Returns:\n             torch.Tensor, the sine positional embeddings\n             (embedding[:, 4i] = sin(x)\n              embedding[:, 4i+1] = cos(x)\n              embedding[:, 4i+2] = sin(y)\n              embedding[:, 4i+3] = cos(y)\n              )\n        \"\"\"\n        if self.scale is not None and self.normalize is False:\n            raise ValueError(\"normalize should be True if scale is passed\")\n\n        if len(boxes.size()) == 3:\n            boxes = boxes.unsqueeze(0)\n\n        if self.normalize:\n            boxes = boxes / (boxes[:, :, -1:] + 1e-6) * self.scale\n\n        dim_t = torch.arange(self.features // 4, dtype=torch.float32)\n\n        dim_t = self.temperature ** (\n            2 * self._torch_int_div(dim_t, 2) / (self.features // 4)\n        )\n\n        # (b, n_t, n_anchors, 4, D//4)\n        pos_emb = boxes[:, :, :, :, None] / dim_t.to(boxes.device)\n\n        pos_emb = torch.stack(\n            (pos_emb[:, :, :, :, 0::2].sin(), pos_emb[:, :, :, :, 1::2].cos()), dim=4\n        )\n        pos_emb = pos_emb.flatten(2).squeeze(0)  # (N_t, n_anchors * D)\n\n        pos_emb = self.mlp(pos_emb)\n\n        pos_emb = pos_emb.view(boxes.shape[1], self.features)\n\n        return pos_emb\n\n    def _sine_temp_embedding(self, times: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute fixed sine temporal embeddings.\n\n        Args:\n            times: the input times of shape (N,) or (N,1) where N = (sum(instances_per_frame))\n            which is the frame index of the instance relative\n            to the batch size\n            (e.g. `torch.tensor([0, 0, ..., 0, 1, 1, ..., 1, 2, 2, ..., 2,..., B, B, ...B])`).\n\n        Returns:\n            an n_instances x D embedding representing the temporal embedding.\n        \"\"\"\n        T = times.int().max().item() + 1\n        d = self.features\n        n = self.temperature\n\n        positions = torch.arange(0, T).unsqueeze(1)\n        temp_lookup = torch.zeros(T, d, device=times.device)\n\n        denominators = torch.pow(\n            n, 2 * torch.arange(0, d // 2) / d\n        )  # 10000^(2i/d_model), i is the index of embedding\n        temp_lookup[:, 0::2] = torch.sin(\n            positions / denominators\n        )  # sin(pos/10000^(2i/d_model))\n        temp_lookup[:, 1::2] = torch.cos(\n            positions / denominators\n        )  # cos(pos/10000^(2i/d_model))\n\n        temp_emb = temp_lookup[times.int()]\n        return temp_emb  # .view(len(times), self.features)\n\n    def _learned_pos_embedding(self, boxes: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute learned positional embeddings for boxes using given parameters.\n\n        Args:\n            boxes: the input boxes of shape N x 4 or B x N x 4\n                   where the last dimension is the bbox coords in [y1, x1, y2, x2].\n                   (Note currently `B=batch_size=1`).\n\n        Returns:\n            torch.Tensor, the learned positional embeddings.\n        \"\"\"\n        pos_lookup = self.lookup\n\n        N, n_anchors, _ = boxes.shape\n        boxes = boxes.view(N, n_anchors, 4)\n\n        if self.over_boxes:\n            xywh = boxes\n        else:\n            xywh = torch.cat(\n                [\n                    (boxes[:, :, 2:] + boxes[:, :, :2]) / 2,\n                    (boxes[:, :, 2:] - boxes[:, :, :2]),\n                ],\n                dim=1,\n            )\n\n        left_ind, right_ind, left_weight, right_weight = self._compute_weights(xywh)\n        f = pos_lookup.weight.shape[1]  # self.features // 4\n\n        try:\n            pos_emb_table = pos_lookup.weight.view(\n                self.emb_num, n_anchors, 4, f\n            )  # T x 4 x (D * 4)\n        except RuntimeError as e:\n            logger.exception(\n                f\"Hint: `n_points` ({self.n_points}) may be set incorrectly!\"\n            )\n            logger.exception(e)\n            raise (e)\n\n        left_emb = pos_emb_table.gather(\n            0,\n            left_ind[:, :, :, None].to(pos_emb_table.device).expand(N, n_anchors, 4, f),\n        )  # N x 4 x d\n        right_emb = pos_emb_table.gather(\n            0,\n            right_ind[:, :, :, None]\n            .to(pos_emb_table.device)\n            .expand(N, n_anchors, 4, f),\n        )  # N x 4 x d\n        pos_emb = left_weight[:, :, :, None] * right_emb.to(\n            left_weight.device\n        ) + right_weight[:, :, :, None] * left_emb.to(right_weight.device)\n\n        pos_emb = pos_emb.flatten(1)\n        pos_emb = self.mlp(pos_emb)\n\n        return pos_emb.view(N, self.features)\n\n    def _learned_temp_embedding(self, times: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute learned temporal embeddings for times using given parameters.\n\n        Args:\n            times: the input times of shape (N,) or (N,1) where N = (sum(instances_per_frame))\n            which is the frame index of the instance relative\n            to the batch size\n            (e.g. `torch.tensor([0, 0, ..., 0, 1, 1, ..., 1, 2, 2, ..., 2,..., B, B, ...B])`).\n\n        Returns:\n            torch.Tensor, the learned temporal embeddings.\n        \"\"\"\n        temp_lookup = self.lookup\n        N = times.shape[0]\n\n        left_ind, right_ind, left_weight, right_weight = self._compute_weights(times)\n\n        left_emb = temp_lookup.weight[\n            left_ind.to(temp_lookup.weight.device)\n        ]  # T x D --&gt; N x D\n        right_emb = temp_lookup.weight[right_ind.to(temp_lookup.weight.device)]\n\n        temp_emb = left_weight[:, None] * right_emb.to(\n            left_weight.device\n        ) + right_weight[:, None] * left_emb.to(right_weight.device)\n\n        return temp_emb.view(N, self.features)\n\n    def _compute_weights(self, data: torch.Tensor) -&gt; tuple[torch.Tensor, ...]:\n        \"\"\"Compute left and right learned embedding weights.\n\n        Args:\n            data: the input data (e.g boxes or times).\n\n        Returns:\n            A torch.Tensor for each of the left/right indices and weights, respectively\n        \"\"\"\n        data = data * self.emb_num\n\n        left_ind = data.clamp(min=0, max=self.emb_num - 1).long()  # N x 4\n        right_ind = (left_ind + 1).clamp(min=0, max=self.emb_num - 1).long()  # N x 4\n\n        left_weight = data - left_ind.float()  # N x 4\n\n        right_weight = 1.0 - left_weight\n\n        return left_ind, right_ind, left_weight, right_weight\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.Embedding.__init__","title":"<code>__init__(emb_type, mode, features, n_points=1, emb_num=16, over_boxes=True, temperature=10000, normalize=False, scale=None, mlp_cfg=None)</code>","text":"<p>Initialize embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>The type of embedding to compute. Must be one of <code>{\"temp\", \"pos\", \"off\"}</code></p> required <code>mode</code> <code>str</code> <p>The mode or function used to map positions to vector embeddings.   Must be one of <code>{\"fixed\", \"learned\", \"off\"}</code></p> required <code>features</code> <code>int</code> <p>The embedding dimensions. Must match the dimension of the       input vectors for the transformer model.</p> required <code>n_points</code> <code>int</code> <p>the number of points that will be embedded.</p> <code>1</code> <code>emb_num</code> <code>int</code> <p>the number of embeddings in the <code>self.lookup</code> table (Only used in learned embeddings).</p> <code>16</code> <code>over_boxes</code> <code>bool</code> <p>Whether to compute the position embedding for each bbox coordinate (y1x1y2x2) or the centroid + bbox size (yxwh).</p> <code>True</code> <code>temperature</code> <code>int</code> <p>the temperature constant to be used when computing the sinusoidal position embedding</p> <code>10000</code> <code>normalize</code> <code>bool</code> <p>whether or not to normalize the positions (Only used in fixed embeddings).</p> <code>False</code> <code>scale</code> <code>float | None</code> <p>factor by which to scale the positions after normalizing (Only used in fixed embeddings).</p> <code>None</code> <code>mlp_cfg</code> <code>dict | None</code> <p>A dictionary of mlp hyperparameters for projecting embedding to correct space.     Example: {\"hidden_dims\": 256, \"num_layers\":3, \"dropout\": 0.3}</p> <code>None</code> Source code in <code>dreem/models/embedding.py</code> <pre><code>def __init__(\n    self,\n    emb_type: str,\n    mode: str,\n    features: int,\n    n_points: int = 1,\n    emb_num: int = 16,\n    over_boxes: bool = True,\n    temperature: int = 10000,\n    normalize: bool = False,\n    scale: float | None = None,\n    mlp_cfg: dict | None = None,\n):\n    \"\"\"Initialize embeddings.\n\n    Args:\n        emb_type: The type of embedding to compute.\n            Must be one of `{\"temp\", \"pos\", \"off\"}`\n        mode: The mode or function used to map positions to vector embeddings.\n              Must be one of `{\"fixed\", \"learned\", \"off\"}`\n        features: The embedding dimensions. Must match the dimension of the\n                  input vectors for the transformer model.\n        n_points: the number of points that will be embedded.\n        emb_num: the number of embeddings in the `self.lookup` table\n            (Only used in learned embeddings).\n        over_boxes: Whether to compute the position embedding for each bbox\n            coordinate (y1x1y2x2) or the centroid + bbox size (yxwh).\n        temperature: the temperature constant to be used when computing\n            the sinusoidal position embedding\n        normalize: whether or not to normalize the positions\n            (Only used in fixed embeddings).\n        scale: factor by which to scale the positions after normalizing\n            (Only used in fixed embeddings).\n        mlp_cfg: A dictionary of mlp hyperparameters for projecting\n            embedding to correct space.\n                Example: {\"hidden_dims\": 256, \"num_layers\":3, \"dropout\": 0.3}\n    \"\"\"\n    self._check_init_args(emb_type, mode)\n\n    super().__init__()\n\n    self.emb_type = emb_type\n    self.mode = mode\n    self.features = features\n    self.emb_num = emb_num\n    self.over_boxes = over_boxes\n    self.temperature = temperature\n    self.normalize = normalize\n    self.scale = scale\n    self.n_points = n_points\n\n    if self.normalize and self.scale is None:\n        self.scale = 2 * math.pi\n\n    if self.emb_type == \"pos\" and mlp_cfg is not None and mlp_cfg[\"num_layers\"] &gt; 0:\n        if self.mode == \"fixed\":\n            self.mlp = MLP(\n                input_dim=n_points * self.features,\n                output_dim=self.features,\n                **mlp_cfg,\n            )\n        else:\n            in_dim = (self.features // (4 * n_points)) * (4 * n_points)\n            self.mlp = MLP(\n                input_dim=in_dim,\n                output_dim=self.features,\n                **mlp_cfg,\n            )\n    else:\n        self.mlp = torch.nn.Identity()\n\n    self._emb_func = lambda tensor: torch.zeros(\n        (tensor.shape[0], self.features), dtype=tensor.dtype, device=tensor.device\n    )  # turn off embedding by returning zeros\n\n    self.lookup = None\n\n    if self.mode == \"learned\":\n        if self.emb_type == \"pos\":\n            self.lookup = torch.nn.Embedding(\n                self.emb_num * 4 * self.n_points, self.features // (4 * n_points)\n            )\n            self._emb_func = self._learned_pos_embedding\n        elif self.emb_type == \"temp\":\n            self.lookup = torch.nn.Embedding(self.emb_num, self.features)\n            self._emb_func = self._learned_temp_embedding\n\n    elif self.mode == \"fixed\":\n        if self.emb_type == \"pos\":\n            self._emb_func = self._sine_box_embedding\n        elif self.emb_type == \"temp\":\n            self._emb_func = self._sine_temp_embedding\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.Embedding.forward","title":"<code>forward(seq_positions)</code>","text":"<p>Get the sequence positional embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>seq_positions</code> <code>Tensor</code> <ul> <li>An (<code>N</code>, 1) tensor where seq_positions[i] represents the temporal position of instance_i in the sequence.</li> <li>An (<code>N</code>, n_anchors x 4) tensor where seq_positions[i, j, :] represents the [y1, x1, y2, x2] spatial locations of jth point of instance_i in the sequence.</li> </ul> required <p>Returns:</p> Type Description <code>Tensor</code> <p>An <code>N</code> x <code>self.features</code> tensor representing the corresponding spatial or temporal embedding.</p> Source code in <code>dreem/models/embedding.py</code> <pre><code>def forward(self, seq_positions: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Get the sequence positional embeddings.\n\n    Args:\n        seq_positions:\n            * An (`N`, 1) tensor where seq_positions[i] represents the temporal position of instance_i in the sequence.\n            * An (`N`, n_anchors x 4) tensor where seq_positions[i, j, :] represents the [y1, x1, y2, x2] spatial locations of jth point of instance_i in the sequence.\n\n    Returns:\n        An `N` x `self.features` tensor representing the corresponding spatial or temporal embedding.\n    \"\"\"\n    emb = self._emb_func(seq_positions)\n\n    if emb.shape[-1] != self.features:\n        raise RuntimeError(\n            (\n                f\"Output embedding dimension is {emb.shape[-1]} but requested {self.features} dimensions! \\n\"\n                f\"hint: Try turning the MLP on by passing `mlp_cfg` to the constructor to project to the correct embedding dimensions.\"\n            )\n        )\n    return emb\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.FourierPositionalEmbeddings","title":"<code>FourierPositionalEmbeddings</code>","text":"<p>               Bases: <code>Module</code></p> <p>Fourier positional embeddings.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize Fourier positional embeddings.</p> <code>forward</code> <p>Compute learnable fourier coefficients for each spatial/temporal position.</p> Source code in <code>dreem/models/embedding.py</code> <pre><code>class FourierPositionalEmbeddings(torch.nn.Module):\n    \"\"\"Fourier positional embeddings.\"\"\"\n\n    def __init__(\n        self,\n        n_components: int,\n        d_model: int,\n    ):\n        \"\"\"Initialize Fourier positional embeddings.\n\n        Args:\n            n_components: Number of frequencies for each dimension.\n            d_model: Model dimension.\n        \"\"\"\n        super().__init__()\n        self.d_model = d_model\n        self.n_components = n_components\n        self.freq = torch.nn.Parameter(\n            _pos_embed_fourier1d_init(self.d_model, n_components)\n        )\n\n    def forward(self, seq_positions: torch.Tensor):\n        \"\"\"Compute learnable fourier coefficients for each spatial/temporal position.\n\n        Args:\n            seq_positions: tensor of shape (num_queries,)\n\n        Returns:\n            tensor of shape (num_queries, embed_dim)\n        \"\"\"\n        freq = self.freq.to(seq_positions.device)\n        # seq_positions is of shape (num_queries,) but needs to be (1,num_queries,1)\n        embed = torch.cat(\n            (\n                torch.sin(\n                    0.5 * math.pi * seq_positions.unsqueeze(-1).unsqueeze(0) * freq\n                ),\n                torch.cos(\n                    0.5 * math.pi * seq_positions.unsqueeze(-1).unsqueeze(0) * freq\n                ),\n            ),\n            axis=-1,\n        ) / math.sqrt(len(freq))  # (B,N,2*n_components)\n\n        if self.d_model % self.n_components != 0:\n            raise ValueError(\n                f\"d_model ({self.d_model}) must be divisible by number of Fourier components n_components ({self.n_components})\"\n            )\n\n        # tile until shape is (B,N,embed_dim) to multiply with input queries/keys\n        embed = embed.repeat(\n            1, 1, self.d_model // (2 * self.n_components)\n        )  # 2*n_components to account for sin/cos\n\n        return embed\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.FourierPositionalEmbeddings.__init__","title":"<code>__init__(n_components, d_model)</code>","text":"<p>Initialize Fourier positional embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of frequencies for each dimension.</p> required <code>d_model</code> <code>int</code> <p>Model dimension.</p> required Source code in <code>dreem/models/embedding.py</code> <pre><code>def __init__(\n    self,\n    n_components: int,\n    d_model: int,\n):\n    \"\"\"Initialize Fourier positional embeddings.\n\n    Args:\n        n_components: Number of frequencies for each dimension.\n        d_model: Model dimension.\n    \"\"\"\n    super().__init__()\n    self.d_model = d_model\n    self.n_components = n_components\n    self.freq = torch.nn.Parameter(\n        _pos_embed_fourier1d_init(self.d_model, n_components)\n    )\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.FourierPositionalEmbeddings.forward","title":"<code>forward(seq_positions)</code>","text":"<p>Compute learnable fourier coefficients for each spatial/temporal position.</p> <p>Parameters:</p> Name Type Description Default <code>seq_positions</code> <code>Tensor</code> <p>tensor of shape (num_queries,)</p> required <p>Returns:</p> Type Description <p>tensor of shape (num_queries, embed_dim)</p> Source code in <code>dreem/models/embedding.py</code> <pre><code>def forward(self, seq_positions: torch.Tensor):\n    \"\"\"Compute learnable fourier coefficients for each spatial/temporal position.\n\n    Args:\n        seq_positions: tensor of shape (num_queries,)\n\n    Returns:\n        tensor of shape (num_queries, embed_dim)\n    \"\"\"\n    freq = self.freq.to(seq_positions.device)\n    # seq_positions is of shape (num_queries,) but needs to be (1,num_queries,1)\n    embed = torch.cat(\n        (\n            torch.sin(\n                0.5 * math.pi * seq_positions.unsqueeze(-1).unsqueeze(0) * freq\n            ),\n            torch.cos(\n                0.5 * math.pi * seq_positions.unsqueeze(-1).unsqueeze(0) * freq\n            ),\n        ),\n        axis=-1,\n    ) / math.sqrt(len(freq))  # (B,N,2*n_components)\n\n    if self.d_model % self.n_components != 0:\n        raise ValueError(\n            f\"d_model ({self.d_model}) must be divisible by number of Fourier components n_components ({self.n_components})\"\n        )\n\n    # tile until shape is (B,N,embed_dim) to multiply with input queries/keys\n    embed = embed.repeat(\n        1, 1, self.d_model // (2 * self.n_components)\n    )  # 2*n_components to account for sin/cos\n\n    return embed\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.GTRRunner","title":"<code>GTRRunner</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A lightning wrapper around GTR model.</p> <p>Used for training, validation and inference.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize a lightning module for GTR.</p> <code>configure_optimizers</code> <p>Get optimizers and schedulers for training.</p> <code>forward</code> <p>Execute forward pass of the lightning module.</p> <code>log_metrics</code> <p>Log metrics computed during evaluation.</p> <code>on_test_end</code> <p>Run inference and metrics pipeline to compute metrics for test set.</p> <code>on_validation_epoch_end</code> <p>Execute hook for validation end.</p> <code>predict_step</code> <p>Run inference for model.</p> <code>test_step</code> <p>Execute single test step for model.</p> <code>training_step</code> <p>Execute single training step for model.</p> <code>validation_step</code> <p>Execute single val step for model.</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>class GTRRunner(LightningModule):\n    \"\"\"A lightning wrapper around GTR model.\n\n    Used for training, validation and inference.\n    \"\"\"\n\n    DEFAULT_METRICS = {\n        \"train\": [],\n        \"val\": [],\n        \"test\": [\"num_switches\", \"global_tracking_accuracy\"],\n    }\n    DEFAULT_TRACKING = {\n        \"train\": False,\n        \"val\": False,\n        \"test\": True,\n    }\n    DEFAULT_SAVE = {\"train\": False, \"val\": False, \"test\": False}\n\n    def __init__(\n        self,\n        model_cfg: dict | None = None,\n        tracker_cfg: dict | None = None,\n        loss_cfg: dict | None = None,\n        optimizer_cfg: dict | None = None,\n        scheduler_cfg: dict | None = None,\n        metrics: dict[str, list[str]] | None = None,\n        persistent_tracking: dict[str, bool] | None = None,\n        test_save_path: str = \"./test_results.h5\",\n    ):\n        \"\"\"Initialize a lightning module for GTR.\n\n        Args:\n            model_cfg: hyperparameters for GlobalTrackingTransformer\n            tracker_cfg: The parameters used for the tracker post-processing\n            loss_cfg: hyperparameters for AssoLoss\n            optimizer_cfg: hyper parameters used for optimizer.\n                       Only used to overwrite `configure_optimizer`\n            scheduler_cfg: hyperparameters for lr_scheduler used to overwrite `configure_optimizer\n            metrics: a dict containing the metrics to be computed during train, val, and test.\n            persistent_tracking: a dict containing whether to use persistent tracking during train, val and test inference.\n            test_save_path: path to a directory to save the eval and tracking results to\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.model_cfg = model_cfg if model_cfg else {}\n        self.loss_cfg = loss_cfg if loss_cfg else {}\n        self.tracker_cfg = tracker_cfg if tracker_cfg else {}\n\n        self.model = GlobalTrackingTransformer(**self.model_cfg)\n        self.loss = AssoLoss(**self.loss_cfg)\n        if self.tracker_cfg.get(\"tracker_type\", \"standard\") == \"batch\":\n            from dreem.inference.batch_tracker import BatchTracker\n\n            self.tracker = BatchTracker(**self.tracker_cfg)\n        else:\n            from dreem.inference.tracker import Tracker\n\n            self.tracker = Tracker(**self.tracker_cfg)\n        self.optimizer_cfg = optimizer_cfg\n        self.scheduler_cfg = scheduler_cfg\n\n        self.metrics = metrics if metrics is not None else self.DEFAULT_METRICS\n        self.persistent_tracking = (\n            persistent_tracking\n            if persistent_tracking is not None\n            else self.DEFAULT_TRACKING\n        )\n        self.test_results = {\"preds\": [], \"save_path\": test_save_path}\n\n    def forward(\n        self,\n        ref_instances: list[\"dreem.io.Instance\"],\n        query_instances: list[\"dreem.io.Instance\"] | None = None,\n    ) -&gt; list[\"AssociationMatrix\"]:\n        \"\"\"Execute forward pass of the lightning module.\n\n        Args:\n            ref_instances: a list of `Instance` objects containing crops and other data needed for transformer model\n            query_instances: a list of `Instance` objects used as queries in the decoder. Mostly used for inference.\n\n        Returns:\n            An association matrix between objects\n        \"\"\"\n        asso_preds = self.model(ref_instances, query_instances)\n        return asso_preds\n\n    def training_step(\n        self, train_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single training step for model.\n\n        Args:\n            train_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the train loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(train_batch[0], mode=\"train\")\n        self.log_metrics(result, len(train_batch[0]), \"train\")\n\n        return result\n\n    def validation_step(\n        self, val_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single val step for model.\n\n        Args:\n            val_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the val loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(val_batch[0], mode=\"val\")\n        self.log_metrics(result, len(val_batch[0]), \"val\")\n\n        return result\n\n    def test_step(\n        self, test_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single test step for model.\n\n        Args:\n            test_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the val loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(test_batch[0], mode=\"test\")\n        self.log_metrics(result, len(test_batch[0]), \"test\")\n\n        return result\n\n    def predict_step(\n        self, batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; list[\"dreem.io.Frame\"]:\n        \"\"\"Run inference for model.\n\n        Computes association + assignment.\n\n        Args:\n            batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A list of dicts where each dict is a frame containing the predicted track ids\n        \"\"\"\n        frames_pred = self.tracker(self.model, batch[0])\n        return frames_pred\n\n    def _shared_eval_step(\n        self, frames: list[\"dreem.io.Frame\"], mode: str\n    ) -&gt; dict[str, float]:\n        \"\"\"Run evaluation used by train, test, and val steps.\n\n        Args:\n            frames: A list of dicts where each dict is a frame containing gt data\n            mode: which metrics to compute and whether to use persistent tracking or not\n\n        Returns:\n            a dict containing the loss and any other metrics specified by `eval_metrics`\n        \"\"\"\n        try:\n            instances = [instance for frame in frames for instance in frame.instances]\n\n            if len(instances) == 0:\n                return None\n\n            # eval_metrics = self.metrics[mode]  # Currently unused but available for future metric computation\n\n            logits = self(instances)\n            logits = [asso.matrix for asso in logits]\n            loss = self.loss(logits, frames)\n\n            return_metrics = {\"loss\": loss}\n            if mode == \"test\":\n                self.tracker.persistent_tracking = True\n                frames_pred = self.tracker(self.model, frames)\n                self.test_results[\"preds\"].extend(\n                    [frame.to(\"cpu\") for frame in frames_pred]\n                )\n            return_metrics[\"batch_size\"] = len(frames)\n        except Exception as e:\n            logger.exception(\n                f\"Failed on frame {frames[0].frame_id} of video {frames[0].video_id}\"\n            )\n            logger.exception(e)\n            raise (e)\n\n        return return_metrics\n\n    def configure_optimizers(self) -&gt; dict:\n        \"\"\"Get optimizers and schedulers for training.\n\n        Is overridden by config but defaults to Adam + ReduceLROnPlateau.\n\n        Returns:\n            an optimizer config dict containing the optimizer, scheduler, and scheduler params\n        \"\"\"\n        # todo: init from config\n        if self.optimizer_cfg is None:\n            optimizer = torch.optim.Adam(self.parameters(), lr=1e-4, betas=(0.9, 0.999))\n        else:\n            optimizer = init_optimizer(self.parameters(), self.optimizer_cfg)\n\n        if self.scheduler_cfg is None:\n            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer, \"min\", 0.5, 10\n            )\n        else:\n            scheduler = init_scheduler(optimizer, self.scheduler_cfg)\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n                \"interval\": \"epoch\",\n                \"frequency\": 1,\n            },\n        }\n\n    def log_metrics(self, result: dict, batch_size: int, mode: str) -&gt; None:\n        \"\"\"Log metrics computed during evaluation.\n\n        Args:\n            result: A dict containing metrics to be logged.\n            batch_size: the size of the batch used to compute the metrics\n            mode: One of {'train', 'test' or 'val'}. Used as prefix while logging.\n        \"\"\"\n        if result:\n            batch_size = result.pop(\"batch_size\")\n            for metric, val in result.items():\n                if isinstance(val, torch.Tensor):\n                    val = val.item()\n                self.log(f\"{mode}_{metric}\", val, batch_size=batch_size)\n\n    def on_validation_epoch_end(self):\n        \"\"\"Execute hook for validation end.\n\n        Currently, we simply clear the gpu cache and do garbage collection.\n        \"\"\"\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def on_test_end(self):\n        \"\"\"Run inference and metrics pipeline to compute metrics for test set.\n\n        Args:\n            test_results: dict containing predictions and metrics to be filled out in metrics.evaluate\n            metrics: list of metrics to compute\n        \"\"\"\n        # input validation\n        metrics_to_compute = self.metrics[\n            \"test\"\n        ]  # list of metrics to compute, or \"all\"\n        if metrics_to_compute == \"all\":\n            metrics_to_compute = [\"motmetrics\", \"global_tracking_accuracy\"]\n        if isinstance(metrics_to_compute, str):\n            metrics_to_compute = [metrics_to_compute]\n        for metric in metrics_to_compute:\n            if metric not in [\"motmetrics\", \"global_tracking_accuracy\"]:\n                raise ValueError(\n                    f\"Metric {metric} not supported. Please select from 'motmetrics' or 'global_tracking_accuracy'\"\n                )\n\n        preds = self.test_results[\"preds\"]\n\n        # results is a dict with key being the metric name, and value being the metric value computed\n        results = metrics.evaluate(preds, metrics_to_compute)\n\n        # save metrics and frame metadata to hdf5\n\n        # Get the video name from the first frame\n        vid_name = Path(preds[0].vid_name).stem\n        # save the results to an hdf5 file\n        fname = os.path.join(\n            self.test_results[\"save_path\"], f\"{vid_name}.dreem_metrics.h5\"\n        )\n        logger.info(f\"Saving metrics to {fname}\")\n        # Check if the h5 file exists and add a suffix to prevent name collision\n        suffix_counter = 0\n        original_fname = fname\n        while os.path.exists(fname):\n            suffix_counter += 1\n            fname = original_fname.replace(\n                \".dreem_metrics.h5\", f\"_{suffix_counter}.dreem_metrics.h5\"\n            )\n\n        if suffix_counter &gt; 0:\n            logger.info(f\"File already exists. Saving to {fname} instead\")\n\n        with h5py.File(fname, \"a\") as results_file:\n            # Create a group for this video\n            vid_group = results_file.require_group(vid_name)\n            # Save each metric\n            for metric_name, value in results.items():\n                if metric_name == \"motmetrics\":\n                    # For num_switches, save mot_summary and mot_events separately\n                    mot_summary = value[0]\n                    mot_events = value[1]\n                    frame_switch_map = value[2]\n                    mot_summary_group = vid_group.require_group(\"mot_summary\")\n                    # Loop through each row in mot_summary and save as attributes\n                    for _, row in mot_summary.iterrows():\n                        mot_summary_group.attrs[row.name] = row[\"acc\"]\n                    # save extra metadata for frames in which there is a switch\n                    for frame_id, switch in frame_switch_map.items():\n                        frame = preds[frame_id]\n                        frame = frame.to(\"cpu\")\n                        if switch:\n                            _ = frame.to_h5(\n                                vid_group,\n                                frame.get_gt_track_ids().cpu().numpy(),\n                                save={\n                                    \"crop\": True,\n                                    \"features\": True,\n                                    \"embeddings\": True,\n                                },\n                            )\n                        else:\n                            _ = frame.to_h5(\n                                vid_group, frame.get_gt_track_ids().cpu().numpy()\n                            )\n                    # save motevents log to csv\n                    motevents_path = os.path.join(\n                        self.test_results[\"save_path\"], f\"{vid_name}.motevents.csv\"\n                    )\n                    logger.info(f\"Saving motevents log to {motevents_path}\")\n                    mot_events.to_csv(motevents_path, index=False)\n\n                elif metric_name == \"global_tracking_accuracy\":\n                    gta_by_gt_track = value\n                    gta_group = vid_group.require_group(\"global_tracking_accuracy\")\n                    # save as a key value pair with gt track id: gta\n                    for gt_track_id, gta in gta_by_gt_track.items():\n                        gta_group.attrs[f\"track_{gt_track_id}\"] = gta\n\n        # save the tracking results to a slp/labelled masks file\n        if isinstance(self.trainer.test_dataloaders.dataset, CellTrackingDataset):\n            outpath = os.path.join(\n                self.test_results[\"save_path\"],\n                f\"{vid_name}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.tif\",\n            )\n            pred_imgs = []\n            for frame in preds:\n                frame_masks = []\n                for instance in frame.instances:\n                    # centroid = instance.centroid[\"centroid\"]  # Currently unused but available if needed\n                    mask = instance.mask.cpu().numpy()\n                    track_id = instance.pred_track_id.cpu().numpy().item()\n                    mask = mask.astype(np.uint8)\n                    mask[mask != 0] = track_id  # label the mask with the track id\n                    frame_masks.append(mask)\n                frame_mask = np.max(frame_masks, axis=0)\n                pred_imgs.append(frame_mask)\n            pred_imgs = np.stack(pred_imgs)\n            tifffile.imwrite(outpath, pred_imgs.astype(np.uint16))\n        else:\n            outpath = os.path.join(\n                self.test_results[\"save_path\"],\n                f\"{vid_name}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.slp\",\n            )\n            pred_slp = []\n\n            logger.info(f\"Saving inference results to {outpath}\")\n            # save the tracking results to a slp file\n            tracks = {}\n            for frame in preds:\n                if frame.frame_id.item() == 0:\n                    video = (\n                        sio.Video(frame.video)\n                        if isinstance(frame.video, str)\n                        else sio.Video\n                    )\n                lf, tracks = frame.to_slp(tracks, video=video)\n                pred_slp.append(lf)\n            pred_slp = sio.Labels(pred_slp)\n\n            pred_slp.save(outpath)\n\n        # clear the preds\n        self.test_results[\"preds\"] = []\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.GTRRunner.__init__","title":"<code>__init__(model_cfg=None, tracker_cfg=None, loss_cfg=None, optimizer_cfg=None, scheduler_cfg=None, metrics=None, persistent_tracking=None, test_save_path='./test_results.h5')</code>","text":"<p>Initialize a lightning module for GTR.</p> <p>Parameters:</p> Name Type Description Default <code>model_cfg</code> <code>dict | None</code> <p>hyperparameters for GlobalTrackingTransformer</p> <code>None</code> <code>tracker_cfg</code> <code>dict | None</code> <p>The parameters used for the tracker post-processing</p> <code>None</code> <code>loss_cfg</code> <code>dict | None</code> <p>hyperparameters for AssoLoss</p> <code>None</code> <code>optimizer_cfg</code> <code>dict | None</code> <p>hyper parameters used for optimizer.        Only used to overwrite <code>configure_optimizer</code></p> <code>None</code> <code>scheduler_cfg</code> <code>dict | None</code> <p>hyperparameters for lr_scheduler used to overwrite `configure_optimizer</p> <code>None</code> <code>metrics</code> <code>dict[str, list[str]] | None</code> <p>a dict containing the metrics to be computed during train, val, and test.</p> <code>None</code> <code>persistent_tracking</code> <code>dict[str, bool] | None</code> <p>a dict containing whether to use persistent tracking during train, val and test inference.</p> <code>None</code> <code>test_save_path</code> <code>str</code> <p>path to a directory to save the eval and tracking results to</p> <code>'./test_results.h5'</code> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def __init__(\n    self,\n    model_cfg: dict | None = None,\n    tracker_cfg: dict | None = None,\n    loss_cfg: dict | None = None,\n    optimizer_cfg: dict | None = None,\n    scheduler_cfg: dict | None = None,\n    metrics: dict[str, list[str]] | None = None,\n    persistent_tracking: dict[str, bool] | None = None,\n    test_save_path: str = \"./test_results.h5\",\n):\n    \"\"\"Initialize a lightning module for GTR.\n\n    Args:\n        model_cfg: hyperparameters for GlobalTrackingTransformer\n        tracker_cfg: The parameters used for the tracker post-processing\n        loss_cfg: hyperparameters for AssoLoss\n        optimizer_cfg: hyper parameters used for optimizer.\n                   Only used to overwrite `configure_optimizer`\n        scheduler_cfg: hyperparameters for lr_scheduler used to overwrite `configure_optimizer\n        metrics: a dict containing the metrics to be computed during train, val, and test.\n        persistent_tracking: a dict containing whether to use persistent tracking during train, val and test inference.\n        test_save_path: path to a directory to save the eval and tracking results to\n    \"\"\"\n    super().__init__()\n    self.save_hyperparameters()\n\n    self.model_cfg = model_cfg if model_cfg else {}\n    self.loss_cfg = loss_cfg if loss_cfg else {}\n    self.tracker_cfg = tracker_cfg if tracker_cfg else {}\n\n    self.model = GlobalTrackingTransformer(**self.model_cfg)\n    self.loss = AssoLoss(**self.loss_cfg)\n    if self.tracker_cfg.get(\"tracker_type\", \"standard\") == \"batch\":\n        from dreem.inference.batch_tracker import BatchTracker\n\n        self.tracker = BatchTracker(**self.tracker_cfg)\n    else:\n        from dreem.inference.tracker import Tracker\n\n        self.tracker = Tracker(**self.tracker_cfg)\n    self.optimizer_cfg = optimizer_cfg\n    self.scheduler_cfg = scheduler_cfg\n\n    self.metrics = metrics if metrics is not None else self.DEFAULT_METRICS\n    self.persistent_tracking = (\n        persistent_tracking\n        if persistent_tracking is not None\n        else self.DEFAULT_TRACKING\n    )\n    self.test_results = {\"preds\": [], \"save_path\": test_save_path}\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.GTRRunner.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Get optimizers and schedulers for training.</p> <p>Is overridden by config but defaults to Adam + ReduceLROnPlateau.</p> <p>Returns:</p> Type Description <code>dict</code> <p>an optimizer config dict containing the optimizer, scheduler, and scheduler params</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def configure_optimizers(self) -&gt; dict:\n    \"\"\"Get optimizers and schedulers for training.\n\n    Is overridden by config but defaults to Adam + ReduceLROnPlateau.\n\n    Returns:\n        an optimizer config dict containing the optimizer, scheduler, and scheduler params\n    \"\"\"\n    # todo: init from config\n    if self.optimizer_cfg is None:\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4, betas=(0.9, 0.999))\n    else:\n        optimizer = init_optimizer(self.parameters(), self.optimizer_cfg)\n\n    if self.scheduler_cfg is None:\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, \"min\", 0.5, 10\n        )\n    else:\n        scheduler = init_scheduler(optimizer, self.scheduler_cfg)\n\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"monitor\": \"val_loss\",\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        },\n    }\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.GTRRunner.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute forward pass of the lightning module.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>a list of <code>Instance</code> objects containing crops and other data needed for transformer model</p> required <code>query_instances</code> <code>list[Instance] | None</code> <p>a list of <code>Instance</code> objects used as queries in the decoder. Mostly used for inference.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[AssociationMatrix]</code> <p>An association matrix between objects</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def forward(\n    self,\n    ref_instances: list[\"dreem.io.Instance\"],\n    query_instances: list[\"dreem.io.Instance\"] | None = None,\n) -&gt; list[\"AssociationMatrix\"]:\n    \"\"\"Execute forward pass of the lightning module.\n\n    Args:\n        ref_instances: a list of `Instance` objects containing crops and other data needed for transformer model\n        query_instances: a list of `Instance` objects used as queries in the decoder. Mostly used for inference.\n\n    Returns:\n        An association matrix between objects\n    \"\"\"\n    asso_preds = self.model(ref_instances, query_instances)\n    return asso_preds\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.GTRRunner.log_metrics","title":"<code>log_metrics(result, batch_size, mode)</code>","text":"<p>Log metrics computed during evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>dict</code> <p>A dict containing metrics to be logged.</p> required <code>batch_size</code> <code>int</code> <p>the size of the batch used to compute the metrics</p> required <code>mode</code> <code>str</code> <p>One of {'train', 'test' or 'val'}. Used as prefix while logging.</p> required Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def log_metrics(self, result: dict, batch_size: int, mode: str) -&gt; None:\n    \"\"\"Log metrics computed during evaluation.\n\n    Args:\n        result: A dict containing metrics to be logged.\n        batch_size: the size of the batch used to compute the metrics\n        mode: One of {'train', 'test' or 'val'}. Used as prefix while logging.\n    \"\"\"\n    if result:\n        batch_size = result.pop(\"batch_size\")\n        for metric, val in result.items():\n            if isinstance(val, torch.Tensor):\n                val = val.item()\n            self.log(f\"{mode}_{metric}\", val, batch_size=batch_size)\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.GTRRunner.on_test_end","title":"<code>on_test_end()</code>","text":"<p>Run inference and metrics pipeline to compute metrics for test set.</p> <p>Parameters:</p> Name Type Description Default <code>test_results</code> <p>dict containing predictions and metrics to be filled out in metrics.evaluate</p> required <code>metrics</code> <p>list of metrics to compute</p> required Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def on_test_end(self):\n    \"\"\"Run inference and metrics pipeline to compute metrics for test set.\n\n    Args:\n        test_results: dict containing predictions and metrics to be filled out in metrics.evaluate\n        metrics: list of metrics to compute\n    \"\"\"\n    # input validation\n    metrics_to_compute = self.metrics[\n        \"test\"\n    ]  # list of metrics to compute, or \"all\"\n    if metrics_to_compute == \"all\":\n        metrics_to_compute = [\"motmetrics\", \"global_tracking_accuracy\"]\n    if isinstance(metrics_to_compute, str):\n        metrics_to_compute = [metrics_to_compute]\n    for metric in metrics_to_compute:\n        if metric not in [\"motmetrics\", \"global_tracking_accuracy\"]:\n            raise ValueError(\n                f\"Metric {metric} not supported. Please select from 'motmetrics' or 'global_tracking_accuracy'\"\n            )\n\n    preds = self.test_results[\"preds\"]\n\n    # results is a dict with key being the metric name, and value being the metric value computed\n    results = metrics.evaluate(preds, metrics_to_compute)\n\n    # save metrics and frame metadata to hdf5\n\n    # Get the video name from the first frame\n    vid_name = Path(preds[0].vid_name).stem\n    # save the results to an hdf5 file\n    fname = os.path.join(\n        self.test_results[\"save_path\"], f\"{vid_name}.dreem_metrics.h5\"\n    )\n    logger.info(f\"Saving metrics to {fname}\")\n    # Check if the h5 file exists and add a suffix to prevent name collision\n    suffix_counter = 0\n    original_fname = fname\n    while os.path.exists(fname):\n        suffix_counter += 1\n        fname = original_fname.replace(\n            \".dreem_metrics.h5\", f\"_{suffix_counter}.dreem_metrics.h5\"\n        )\n\n    if suffix_counter &gt; 0:\n        logger.info(f\"File already exists. Saving to {fname} instead\")\n\n    with h5py.File(fname, \"a\") as results_file:\n        # Create a group for this video\n        vid_group = results_file.require_group(vid_name)\n        # Save each metric\n        for metric_name, value in results.items():\n            if metric_name == \"motmetrics\":\n                # For num_switches, save mot_summary and mot_events separately\n                mot_summary = value[0]\n                mot_events = value[1]\n                frame_switch_map = value[2]\n                mot_summary_group = vid_group.require_group(\"mot_summary\")\n                # Loop through each row in mot_summary and save as attributes\n                for _, row in mot_summary.iterrows():\n                    mot_summary_group.attrs[row.name] = row[\"acc\"]\n                # save extra metadata for frames in which there is a switch\n                for frame_id, switch in frame_switch_map.items():\n                    frame = preds[frame_id]\n                    frame = frame.to(\"cpu\")\n                    if switch:\n                        _ = frame.to_h5(\n                            vid_group,\n                            frame.get_gt_track_ids().cpu().numpy(),\n                            save={\n                                \"crop\": True,\n                                \"features\": True,\n                                \"embeddings\": True,\n                            },\n                        )\n                    else:\n                        _ = frame.to_h5(\n                            vid_group, frame.get_gt_track_ids().cpu().numpy()\n                        )\n                # save motevents log to csv\n                motevents_path = os.path.join(\n                    self.test_results[\"save_path\"], f\"{vid_name}.motevents.csv\"\n                )\n                logger.info(f\"Saving motevents log to {motevents_path}\")\n                mot_events.to_csv(motevents_path, index=False)\n\n            elif metric_name == \"global_tracking_accuracy\":\n                gta_by_gt_track = value\n                gta_group = vid_group.require_group(\"global_tracking_accuracy\")\n                # save as a key value pair with gt track id: gta\n                for gt_track_id, gta in gta_by_gt_track.items():\n                    gta_group.attrs[f\"track_{gt_track_id}\"] = gta\n\n    # save the tracking results to a slp/labelled masks file\n    if isinstance(self.trainer.test_dataloaders.dataset, CellTrackingDataset):\n        outpath = os.path.join(\n            self.test_results[\"save_path\"],\n            f\"{vid_name}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.tif\",\n        )\n        pred_imgs = []\n        for frame in preds:\n            frame_masks = []\n            for instance in frame.instances:\n                # centroid = instance.centroid[\"centroid\"]  # Currently unused but available if needed\n                mask = instance.mask.cpu().numpy()\n                track_id = instance.pred_track_id.cpu().numpy().item()\n                mask = mask.astype(np.uint8)\n                mask[mask != 0] = track_id  # label the mask with the track id\n                frame_masks.append(mask)\n            frame_mask = np.max(frame_masks, axis=0)\n            pred_imgs.append(frame_mask)\n        pred_imgs = np.stack(pred_imgs)\n        tifffile.imwrite(outpath, pred_imgs.astype(np.uint16))\n    else:\n        outpath = os.path.join(\n            self.test_results[\"save_path\"],\n            f\"{vid_name}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.slp\",\n        )\n        pred_slp = []\n\n        logger.info(f\"Saving inference results to {outpath}\")\n        # save the tracking results to a slp file\n        tracks = {}\n        for frame in preds:\n            if frame.frame_id.item() == 0:\n                video = (\n                    sio.Video(frame.video)\n                    if isinstance(frame.video, str)\n                    else sio.Video\n                )\n            lf, tracks = frame.to_slp(tracks, video=video)\n            pred_slp.append(lf)\n        pred_slp = sio.Labels(pred_slp)\n\n        pred_slp.save(outpath)\n\n    # clear the preds\n    self.test_results[\"preds\"] = []\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.GTRRunner.on_validation_epoch_end","title":"<code>on_validation_epoch_end()</code>","text":"<p>Execute hook for validation end.</p> <p>Currently, we simply clear the gpu cache and do garbage collection.</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Execute hook for validation end.\n\n    Currently, we simply clear the gpu cache and do garbage collection.\n    \"\"\"\n    gc.collect()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.GTRRunner.predict_step","title":"<code>predict_step(batch, batch_idx)</code>","text":"<p>Run inference for model.</p> <p>Computes association + assignment.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects     with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A list of dicts where each dict is a frame containing the predicted track ids</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def predict_step(\n    self, batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; list[\"dreem.io.Frame\"]:\n    \"\"\"Run inference for model.\n\n    Computes association + assignment.\n\n    Args:\n        batch: A single batch from the dataset which is a list of `Frame` objects\n                with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A list of dicts where each dict is a frame containing the predicted track ids\n    \"\"\"\n    frames_pred = self.tracker(self.model, batch[0])\n    return frames_pred\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.GTRRunner.test_step","title":"<code>test_step(test_batch, batch_idx)</code>","text":"<p>Execute single test step for model.</p> <p>Parameters:</p> Name Type Description Default <code>test_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the val loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def test_step(\n    self, test_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single test step for model.\n\n    Args:\n        test_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the val loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(test_batch[0], mode=\"test\")\n    self.log_metrics(result, len(test_batch[0]), \"test\")\n\n    return result\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.GTRRunner.training_step","title":"<code>training_step(train_batch, batch_idx)</code>","text":"<p>Execute single training step for model.</p> <p>Parameters:</p> Name Type Description Default <code>train_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the train loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def training_step(\n    self, train_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single training step for model.\n\n    Args:\n        train_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the train loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(train_batch[0], mode=\"train\")\n    self.log_metrics(result, len(train_batch[0]), \"train\")\n\n    return result\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.GTRRunner.validation_step","title":"<code>validation_step(val_batch, batch_idx)</code>","text":"<p>Execute single val step for model.</p> <p>Parameters:</p> Name Type Description Default <code>val_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the val loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def validation_step(\n    self, val_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single val step for model.\n\n    Args:\n        val_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the val loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(val_batch[0], mode=\"val\")\n    self.log_metrics(result, len(val_batch[0]), \"val\")\n\n    return result\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.GlobalTrackingTransformer","title":"<code>GlobalTrackingTransformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Modular GTR model composed of visual encoder + transformer used for tracking.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize GTR.</p> <code>extract_features</code> <p>Extract features from instances using visual encoder backbone.</p> <code>forward</code> <p>Execute forward pass of GTR Model to get asso matrix.</p> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>class GlobalTrackingTransformer(torch.nn.Module):\n    \"\"\"Modular GTR model composed of visual encoder + transformer used for tracking.\"\"\"\n\n    def __init__(\n        self,\n        encoder_cfg: dict | None = None,\n        d_model: int = 1024,\n        nhead: int = 8,\n        num_encoder_layers: int = 6,\n        num_decoder_layers: int = 6,\n        dropout: int = 0.1,\n        activation: str = \"relu\",\n        return_intermediate_dec: bool = False,\n        norm: bool = False,\n        num_layers_attn_head: int = 2,\n        dropout_attn_head: int = 0.1,\n        embedding_meta: dict | None = None,\n        return_embedding: bool = False,\n        decoder_self_attn: bool = False,\n    ):\n        \"\"\"Initialize GTR.\n\n        Args:\n            encoder_cfg: Dictionary of arguments to pass to the CNN constructor,\n                e.g: `cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}`\n            d_model: The number of features in the encoder/decoder inputs.\n            nhead: The number of heads in the transformer encoder/decoder.\n            num_encoder_layers: The number of encoder-layers in the encoder.\n            num_decoder_layers: The number of decoder-layers in the decoder.\n            dropout: Dropout value applied to the output of transformer layers.\n            activation: Activation function to use.\n            return_intermediate_dec: Return intermediate layers from decoder.\n            norm: If True, normalize output of encoder and decoder.\n            num_layers_attn_head: The number of layers in the attention head.\n            dropout_attn_head: Dropout value for the attention_head.\n            embedding_meta: Metadata for positional embeddings. See below.\n            return_embedding: Whether to return the positional embeddings\n            decoder_self_attn: If True, use decoder self attention.\n\n                More details on `embedding_meta`:\n                    By default this will be an empty dict and indicate\n                    that no positional embeddings should be used. To use the positional embeddings\n                    pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                    `{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},\n                    \"temp\": {'mode': 'learned', 'emb_num': 16}}`. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                    and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n        \"\"\"\n        super().__init__()\n\n        if not encoder_cfg:\n            encoder_cfg = {}\n        self.visual_encoder = create_visual_encoder(d_model=d_model, **encoder_cfg)\n\n        self.transformer = Transformer(\n            d_model=d_model,\n            nhead=nhead,\n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers,\n            dropout=dropout,\n            activation=activation,\n            return_intermediate_dec=return_intermediate_dec,\n            norm=norm,\n            num_layers_attn_head=num_layers_attn_head,\n            dropout_attn_head=dropout_attn_head,\n            embedding_meta=embedding_meta,\n            return_embedding=return_embedding,\n            decoder_self_attn=decoder_self_attn,\n            encoder_cfg=encoder_cfg,\n        )\n\n    def forward(\n        self, ref_instances: list[\"Instance\"], query_instances: list[\"Instance\"] = None\n    ) -&gt; list[\"AssociationMatrix\"]:\n        \"\"\"Execute forward pass of GTR Model to get asso matrix.\n\n        Args:\n            ref_instances: List of instances from chunk containing crops of objects + gt label info\n            query_instances: list of instances used as query in decoder.\n\n        Returns:\n            An N_T x N association matrix\n        \"\"\"\n        # Extract feature representations with pre-trained encoder.\n        self.extract_features(ref_instances)\n\n        if query_instances:\n            self.extract_features(query_instances)\n\n        asso_preds = self.transformer(ref_instances, query_instances)\n\n        return asso_preds\n\n    def extract_features(\n        self, instances: list[\"Instance\"], force_recompute: bool = False\n    ) -&gt; None:\n        \"\"\"Extract features from instances using visual encoder backbone.\n\n        Args:\n            instances: A list of instances to compute features for\n            force_recompute: indicate whether to compute features for all instances regardless of if they have instances\n        \"\"\"\n        if not force_recompute:\n            instances_to_compute = [\n                instance\n                for instance in instances\n                if instance.has_crop() and not instance.has_features()\n            ]\n        else:\n            instances_to_compute = instances\n\n        if len(instances_to_compute) == 0:\n            return\n        elif len(instances_to_compute) == 1:  # handle batch norm error when B=1\n            instances_to_compute = instances\n\n        crops = torch.concatenate([instance.crop for instance in instances_to_compute])\n\n        features = self.visual_encoder(crops)\n        features = features.to(device=instances_to_compute[0].device)\n\n        for i, z_i in enumerate(features):\n            instances_to_compute[i].features = z_i\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.GlobalTrackingTransformer.__init__","title":"<code>__init__(encoder_cfg=None, d_model=1024, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dropout=0.1, activation='relu', return_intermediate_dec=False, norm=False, num_layers_attn_head=2, dropout_attn_head=0.1, embedding_meta=None, return_embedding=False, decoder_self_attn=False)</code>","text":"<p>Initialize GTR.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_cfg</code> <code>dict | None</code> <p>Dictionary of arguments to pass to the CNN constructor, e.g: <code>cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}</code></p> <code>None</code> <code>d_model</code> <code>int</code> <p>The number of features in the encoder/decoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads in the transformer encoder/decoder.</p> <code>8</code> <code>num_encoder_layers</code> <code>int</code> <p>The number of encoder-layers in the encoder.</p> <code>6</code> <code>num_decoder_layers</code> <code>int</code> <p>The number of decoder-layers in the decoder.</p> <code>6</code> <code>dropout</code> <code>int</code> <p>Dropout value applied to the output of transformer layers.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>return_intermediate_dec</code> <code>bool</code> <p>Return intermediate layers from decoder.</p> <code>False</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of encoder and decoder.</p> <code>False</code> <code>num_layers_attn_head</code> <code>int</code> <p>The number of layers in the attention head.</p> <code>2</code> <code>dropout_attn_head</code> <code>int</code> <p>Dropout value for the attention_head.</p> <code>0.1</code> <code>embedding_meta</code> <code>dict | None</code> <p>Metadata for positional embeddings. See below.</p> <code>None</code> <code>return_embedding</code> <code>bool</code> <p>Whether to return the positional embeddings</p> <code>False</code> <code>decoder_self_attn</code> <code>bool</code> <p>If True, use decoder self attention.</p> <p>More details on <code>embedding_meta</code>:     By default this will be an empty dict and indicate     that no positional embeddings should be used. To use the positional embeddings     pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:     <code>{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},     \"temp\": {'mode': 'learned', 'emb_num': 16}}</code>. (see <code>dreem.models.embeddings.Embedding.EMB_TYPES</code>     and <code>dreem.models.embeddings.Embedding.EMB_MODES</code> for embedding parameters).</p> <code>False</code> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def __init__(\n    self,\n    encoder_cfg: dict | None = None,\n    d_model: int = 1024,\n    nhead: int = 8,\n    num_encoder_layers: int = 6,\n    num_decoder_layers: int = 6,\n    dropout: int = 0.1,\n    activation: str = \"relu\",\n    return_intermediate_dec: bool = False,\n    norm: bool = False,\n    num_layers_attn_head: int = 2,\n    dropout_attn_head: int = 0.1,\n    embedding_meta: dict | None = None,\n    return_embedding: bool = False,\n    decoder_self_attn: bool = False,\n):\n    \"\"\"Initialize GTR.\n\n    Args:\n        encoder_cfg: Dictionary of arguments to pass to the CNN constructor,\n            e.g: `cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}`\n        d_model: The number of features in the encoder/decoder inputs.\n        nhead: The number of heads in the transformer encoder/decoder.\n        num_encoder_layers: The number of encoder-layers in the encoder.\n        num_decoder_layers: The number of decoder-layers in the decoder.\n        dropout: Dropout value applied to the output of transformer layers.\n        activation: Activation function to use.\n        return_intermediate_dec: Return intermediate layers from decoder.\n        norm: If True, normalize output of encoder and decoder.\n        num_layers_attn_head: The number of layers in the attention head.\n        dropout_attn_head: Dropout value for the attention_head.\n        embedding_meta: Metadata for positional embeddings. See below.\n        return_embedding: Whether to return the positional embeddings\n        decoder_self_attn: If True, use decoder self attention.\n\n            More details on `embedding_meta`:\n                By default this will be an empty dict and indicate\n                that no positional embeddings should be used. To use the positional embeddings\n                pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                `{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},\n                \"temp\": {'mode': 'learned', 'emb_num': 16}}`. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n    \"\"\"\n    super().__init__()\n\n    if not encoder_cfg:\n        encoder_cfg = {}\n    self.visual_encoder = create_visual_encoder(d_model=d_model, **encoder_cfg)\n\n    self.transformer = Transformer(\n        d_model=d_model,\n        nhead=nhead,\n        num_encoder_layers=num_encoder_layers,\n        num_decoder_layers=num_decoder_layers,\n        dropout=dropout,\n        activation=activation,\n        return_intermediate_dec=return_intermediate_dec,\n        norm=norm,\n        num_layers_attn_head=num_layers_attn_head,\n        dropout_attn_head=dropout_attn_head,\n        embedding_meta=embedding_meta,\n        return_embedding=return_embedding,\n        decoder_self_attn=decoder_self_attn,\n        encoder_cfg=encoder_cfg,\n    )\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.GlobalTrackingTransformer.extract_features","title":"<code>extract_features(instances, force_recompute=False)</code>","text":"<p>Extract features from instances using visual encoder backbone.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[Instance]</code> <p>A list of instances to compute features for</p> required <code>force_recompute</code> <code>bool</code> <p>indicate whether to compute features for all instances regardless of if they have instances</p> <code>False</code> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def extract_features(\n    self, instances: list[\"Instance\"], force_recompute: bool = False\n) -&gt; None:\n    \"\"\"Extract features from instances using visual encoder backbone.\n\n    Args:\n        instances: A list of instances to compute features for\n        force_recompute: indicate whether to compute features for all instances regardless of if they have instances\n    \"\"\"\n    if not force_recompute:\n        instances_to_compute = [\n            instance\n            for instance in instances\n            if instance.has_crop() and not instance.has_features()\n        ]\n    else:\n        instances_to_compute = instances\n\n    if len(instances_to_compute) == 0:\n        return\n    elif len(instances_to_compute) == 1:  # handle batch norm error when B=1\n        instances_to_compute = instances\n\n    crops = torch.concatenate([instance.crop for instance in instances_to_compute])\n\n    features = self.visual_encoder(crops)\n    features = features.to(device=instances_to_compute[0].device)\n\n    for i, z_i in enumerate(features):\n        instances_to_compute[i].features = z_i\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.GlobalTrackingTransformer.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute forward pass of GTR Model to get asso matrix.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>List of instances from chunk containing crops of objects + gt label info</p> required <code>query_instances</code> <code>list[Instance]</code> <p>list of instances used as query in decoder.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[AssociationMatrix]</code> <p>An N_T x N association matrix</p> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def forward(\n    self, ref_instances: list[\"Instance\"], query_instances: list[\"Instance\"] = None\n) -&gt; list[\"AssociationMatrix\"]:\n    \"\"\"Execute forward pass of GTR Model to get asso matrix.\n\n    Args:\n        ref_instances: List of instances from chunk containing crops of objects + gt label info\n        query_instances: list of instances used as query in decoder.\n\n    Returns:\n        An N_T x N association matrix\n    \"\"\"\n    # Extract feature representations with pre-trained encoder.\n    self.extract_features(ref_instances)\n\n    if query_instances:\n        self.extract_features(query_instances)\n\n    asso_preds = self.transformer(ref_instances, query_instances)\n\n    return asso_preds\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.Transformer","title":"<code>Transformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer class.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize Transformer.</p> <code>forward</code> <p>Execute a forward pass through the transformer and attention head.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class Transformer(torch.nn.Module):\n    \"\"\"Transformer class.\"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 1024,\n        nhead: int = 8,\n        num_encoder_layers: int = 6,\n        num_decoder_layers: int = 6,\n        dropout: float = 0.1,\n        activation: str = \"relu\",\n        return_intermediate_dec: bool = False,\n        norm: bool = False,\n        num_layers_attn_head: int = 2,\n        dropout_attn_head: float = 0.1,\n        embedding_meta: dict | None = None,\n        return_embedding: bool = False,\n        decoder_self_attn: bool = False,\n        encoder_cfg: dict | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize Transformer.\n\n        Args:\n            d_model: The number of features in the encoder/decoder inputs.\n            nhead: The number of heads in the transformer encoder/decoder.\n            num_encoder_layers: The number of encoder-layers in the encoder.\n            num_decoder_layers: The number of decoder-layers in the decoder.\n            dropout: Dropout value applied to the output of transformer layers.\n            activation: Activation function to use.\n            return_intermediate_dec: Return intermediate layers from decoder.\n            norm: If True, normalize output of encoder and decoder.\n            num_layers_attn_head: The number of layers in the attention head.\n            dropout_attn_head: Dropout value for the attention_head.\n            embedding_meta: Metadata for positional embeddings. See below.\n            return_embedding: Whether to return the positional embeddings\n            decoder_self_attn: If True, use decoder self attention.\n            encoder_cfg: Encoder configuration.\n\n                More details on `embedding_meta`:\n                    By default this will be an empty dict and indicate\n                    that no positional embeddings should be used. To use the positional embeddings\n                    pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                    {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},\n                    \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                    and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n        \"\"\"\n        super().__init__()\n\n        self.d_model = dim_feedforward = feature_dim_attn_head = d_model\n\n        self.embedding_meta = embedding_meta\n        self.return_embedding = return_embedding\n        self.encoder_cfg = encoder_cfg\n\n        self.pos_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n        self.temp_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n\n        if self.embedding_meta:\n            if \"pos\" in self.embedding_meta:\n                pos_emb_cfg = self.embedding_meta[\"pos\"]\n                if pos_emb_cfg:\n                    self.pos_emb = Embedding(\n                        emb_type=\"pos\", features=self.d_model, **pos_emb_cfg\n                    )\n            if \"temp\" in self.embedding_meta:\n                temp_emb_cfg = self.embedding_meta[\"temp\"]\n                if temp_emb_cfg:\n                    self.temp_emb = Embedding(\n                        emb_type=\"temp\", features=self.d_model, **temp_emb_cfg\n                    )\n\n        self.fourier_embeddings = FourierPositionalEmbeddings(\n            n_components=8, d_model=d_model\n        )\n\n        # Transformer Encoder\n        encoder_layer = TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, activation, norm\n        )\n\n        encoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n        # only used if using descriptor visual encoder; default resnet encoder uses d_model directly\n        if self.encoder_cfg and \"encoder_type\" in self.encoder_cfg:\n            self.visual_feat_dim = (\n                self.encoder_cfg[\"ndim\"] if \"ndim\" in self.encoder_cfg else 5\n            )  # 5 is default for descriptor\n            self.fourier_proj = nn.Linear(self.d_model + self.visual_feat_dim, d_model)\n            self.fourier_norm = nn.LayerNorm(self.d_model)\n\n        self.encoder = TransformerEncoder(\n            encoder_layer, num_encoder_layers, encoder_norm\n        )\n\n        # Transformer Decoder\n        decoder_layer = TransformerDecoderLayer(\n            d_model,\n            nhead,\n            dim_feedforward,\n            dropout,\n            activation,\n            norm,\n            decoder_self_attn,\n        )\n\n        decoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n        self.decoder = TransformerDecoder(\n            decoder_layer, num_decoder_layers, return_intermediate_dec, decoder_norm\n        )\n\n        # Transformer attention head\n        self.attn_head = ATTWeightHead(\n            feature_dim=feature_dim_attn_head,\n            num_layers=num_layers_attn_head,\n            dropout=dropout_attn_head,\n        )\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        \"\"\"Initialize model weights from xavier distribution.\"\"\"\n        for p in self.parameters():\n            if not torch.nn.parameter.is_lazy(p) and p.dim() &gt; 1:\n                try:\n                    nn.init.xavier_uniform_(p)\n                except ValueError as e:\n                    print(f\"Failed Trying to initialize {p}\")\n                    raise (e)\n\n    def forward(\n        self,\n        ref_instances: list[\"dreem.io.Instance\"],\n        query_instances: list[\"dreem.io.Instance\"] | None = None,\n    ) -&gt; list[AssociationMatrix]:\n        \"\"\"Execute a forward pass through the transformer and attention head.\n\n        Args:\n            ref_instances: A list of instance objects (See `dreem.io.Instance` for more info.)\n            query_instances: An set of instances to be used as decoder queries.\n\n        Returns:\n            asso_output: A list of torch.Tensors of shape (L, n_query, total_instances) where:\n                L: number of decoder blocks\n                n_query: number of instances in current query/frame\n                total_instances: number of instances in window\n        \"\"\"\n        ref_features = torch.cat(\n            [instance.features for instance in ref_instances], dim=0\n        ).unsqueeze(0)\n\n        # window_length = len(frames)\n        # instances_per_frame = [frame.num_detected for frame in frames]\n        total_instances = len(ref_instances)\n        embed_dim = self.d_model\n        # print(f'T: {window_length}; N: {total_instances}; N_t: {instances_per_frame} n_reid: {reid_features.shape}')\n        ref_boxes = get_boxes(ref_instances)  # total_instances, 4\n        ref_boxes = torch.nan_to_num(ref_boxes, -1.0)\n        ref_times, query_times = get_times(ref_instances, query_instances)\n\n        # window_length = len(ref_times.unique())  # Currently unused but may be useful for debugging\n\n        ref_temp_emb = self.temp_emb(ref_times)\n\n        ref_pos_emb = self.pos_emb(ref_boxes)\n\n        if self.return_embedding:\n            for i, instance in enumerate(ref_instances):\n                instance.add_embedding(\"pos\", ref_pos_emb[i])\n                instance.add_embedding(\"temp\", ref_temp_emb[i])\n\n        ref_emb = (ref_pos_emb + ref_temp_emb) / 2.0\n\n        ref_emb = ref_emb.view(1, total_instances, embed_dim)\n\n        ref_emb = ref_emb.permute(1, 0, 2)  # (total_instances, batch_size, embed_dim)\n\n        batch_size, total_instances = ref_features.shape[:-1]\n\n        ref_features = ref_features.permute(\n            1, 0, 2\n        )  # (total_instances, batch_size, embed_dim)\n\n        encoder_queries = ref_features\n\n        # apply fourier embeddings if using fourier rope, OR if using descriptor (compact) visual encoder\n        if (\n            self.embedding_meta\n            and \"use_fourier\" in self.embedding_meta\n            and self.embedding_meta[\"use_fourier\"]\n        ) or (\n            self.encoder_cfg\n            and \"encoder_type\" in self.encoder_cfg\n            and self.encoder_cfg[\"encoder_type\"] == \"descriptor\"\n        ):\n            encoder_queries = apply_fourier_embeddings(\n                encoder_queries,\n                ref_times,\n                self.d_model,\n                self.fourier_embeddings,\n                self.fourier_proj,\n                self.fourier_norm,\n            )\n\n        encoder_features = self.encoder(\n            encoder_queries, pos_emb=ref_emb\n        )  # (total_instances, batch_size, embed_dim)\n\n        n_query = total_instances\n\n        query_features = ref_features\n        query_pos_emb = ref_pos_emb\n        query_temp_emb = ref_temp_emb\n        query_emb = ref_emb\n\n        if query_instances is not None:\n            n_query = len(query_instances)\n\n            query_features = torch.cat(\n                [instance.features for instance in query_instances], dim=0\n            ).unsqueeze(0)\n\n            query_features = query_features.permute(\n                1, 0, 2\n            )  # (n_query, batch_size, embed_dim)\n\n            query_boxes = get_boxes(query_instances)\n            query_boxes = torch.nan_to_num(query_boxes, -1.0)\n            query_temp_emb = self.temp_emb(query_times)\n\n            query_pos_emb = self.pos_emb(query_boxes)\n\n            query_emb = (query_pos_emb + query_temp_emb) / 2.0\n            query_emb = query_emb.view(1, n_query, embed_dim)\n            query_emb = query_emb.permute(1, 0, 2)  # (n_query, batch_size, embed_dim)\n\n        else:\n            query_instances = ref_instances\n            query_times = ref_times\n\n        if self.return_embedding:\n            for i, instance in enumerate(query_instances):\n                instance.add_embedding(\"pos\", query_pos_emb[i])\n                instance.add_embedding(\"temp\", query_temp_emb[i])\n\n        # apply fourier embeddings if using fourier rope, OR if using descriptor (compact) visual encoder\n        if (\n            self.embedding_meta\n            and \"use_fourier\" in self.embedding_meta\n            and self.embedding_meta[\"use_fourier\"]\n        ) or (\n            self.encoder_cfg\n            and \"encoder_type\" in self.encoder_cfg\n            and self.encoder_cfg[\"encoder_type\"] == \"descriptor\"\n        ):\n            query_features = apply_fourier_embeddings(\n                query_features,\n                query_times,\n                self.d_model,\n                self.fourier_embeddings,\n                self.fourier_proj,\n                self.fourier_norm,\n            )\n\n        decoder_features = self.decoder(\n            query_features,\n            encoder_features,\n            ref_pos_emb=ref_emb,\n            query_pos_emb=query_emb,\n        )  # (L, n_query, batch_size, embed_dim)\n\n        decoder_features = decoder_features.transpose(\n            1, 2\n        )  # # (L, batch_size, n_query, embed_dim)\n        encoder_features = encoder_features.permute(1, 0, 2).view(\n            batch_size, total_instances, embed_dim\n        )  # (batch_size, total_instances, embed_dim)\n\n        asso_output = []\n        for frame_features in decoder_features:\n            asso_matrix = self.attn_head(frame_features, encoder_features).view(\n                n_query, total_instances\n            )\n            asso_matrix = AssociationMatrix(asso_matrix, ref_instances, query_instances)\n\n            asso_output.append(asso_matrix)\n\n        # (L=1, n_query, total_instances)\n        return asso_output\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.Transformer.__init__","title":"<code>__init__(d_model=1024, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dropout=0.1, activation='relu', return_intermediate_dec=False, norm=False, num_layers_attn_head=2, dropout_attn_head=0.1, embedding_meta=None, return_embedding=False, decoder_self_attn=False, encoder_cfg=None)</code>","text":"<p>Initialize Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of features in the encoder/decoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads in the transformer encoder/decoder.</p> <code>8</code> <code>num_encoder_layers</code> <code>int</code> <p>The number of encoder-layers in the encoder.</p> <code>6</code> <code>num_decoder_layers</code> <code>int</code> <p>The number of decoder-layers in the decoder.</p> <code>6</code> <code>dropout</code> <code>float</code> <p>Dropout value applied to the output of transformer layers.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>return_intermediate_dec</code> <code>bool</code> <p>Return intermediate layers from decoder.</p> <code>False</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of encoder and decoder.</p> <code>False</code> <code>num_layers_attn_head</code> <code>int</code> <p>The number of layers in the attention head.</p> <code>2</code> <code>dropout_attn_head</code> <code>float</code> <p>Dropout value for the attention_head.</p> <code>0.1</code> <code>embedding_meta</code> <code>dict | None</code> <p>Metadata for positional embeddings. See below.</p> <code>None</code> <code>return_embedding</code> <code>bool</code> <p>Whether to return the positional embeddings</p> <code>False</code> <code>decoder_self_attn</code> <code>bool</code> <p>If True, use decoder self attention.</p> <code>False</code> <code>encoder_cfg</code> <code>dict | None</code> <p>Encoder configuration.</p> <p>More details on <code>embedding_meta</code>:     By default this will be an empty dict and indicate     that no positional embeddings should be used. To use the positional embeddings     pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:     {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},     \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see <code>dreem.models.embeddings.Embedding.EMB_TYPES</code>     and <code>dreem.models.embeddings.Embedding.EMB_MODES</code> for embedding parameters).</p> <code>None</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    d_model: int = 1024,\n    nhead: int = 8,\n    num_encoder_layers: int = 6,\n    num_decoder_layers: int = 6,\n    dropout: float = 0.1,\n    activation: str = \"relu\",\n    return_intermediate_dec: bool = False,\n    norm: bool = False,\n    num_layers_attn_head: int = 2,\n    dropout_attn_head: float = 0.1,\n    embedding_meta: dict | None = None,\n    return_embedding: bool = False,\n    decoder_self_attn: bool = False,\n    encoder_cfg: dict | None = None,\n) -&gt; None:\n    \"\"\"Initialize Transformer.\n\n    Args:\n        d_model: The number of features in the encoder/decoder inputs.\n        nhead: The number of heads in the transformer encoder/decoder.\n        num_encoder_layers: The number of encoder-layers in the encoder.\n        num_decoder_layers: The number of decoder-layers in the decoder.\n        dropout: Dropout value applied to the output of transformer layers.\n        activation: Activation function to use.\n        return_intermediate_dec: Return intermediate layers from decoder.\n        norm: If True, normalize output of encoder and decoder.\n        num_layers_attn_head: The number of layers in the attention head.\n        dropout_attn_head: Dropout value for the attention_head.\n        embedding_meta: Metadata for positional embeddings. See below.\n        return_embedding: Whether to return the positional embeddings\n        decoder_self_attn: If True, use decoder self attention.\n        encoder_cfg: Encoder configuration.\n\n            More details on `embedding_meta`:\n                By default this will be an empty dict and indicate\n                that no positional embeddings should be used. To use the positional embeddings\n                pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},\n                \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n    \"\"\"\n    super().__init__()\n\n    self.d_model = dim_feedforward = feature_dim_attn_head = d_model\n\n    self.embedding_meta = embedding_meta\n    self.return_embedding = return_embedding\n    self.encoder_cfg = encoder_cfg\n\n    self.pos_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n    self.temp_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n\n    if self.embedding_meta:\n        if \"pos\" in self.embedding_meta:\n            pos_emb_cfg = self.embedding_meta[\"pos\"]\n            if pos_emb_cfg:\n                self.pos_emb = Embedding(\n                    emb_type=\"pos\", features=self.d_model, **pos_emb_cfg\n                )\n        if \"temp\" in self.embedding_meta:\n            temp_emb_cfg = self.embedding_meta[\"temp\"]\n            if temp_emb_cfg:\n                self.temp_emb = Embedding(\n                    emb_type=\"temp\", features=self.d_model, **temp_emb_cfg\n                )\n\n    self.fourier_embeddings = FourierPositionalEmbeddings(\n        n_components=8, d_model=d_model\n    )\n\n    # Transformer Encoder\n    encoder_layer = TransformerEncoderLayer(\n        d_model, nhead, dim_feedforward, dropout, activation, norm\n    )\n\n    encoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n    # only used if using descriptor visual encoder; default resnet encoder uses d_model directly\n    if self.encoder_cfg and \"encoder_type\" in self.encoder_cfg:\n        self.visual_feat_dim = (\n            self.encoder_cfg[\"ndim\"] if \"ndim\" in self.encoder_cfg else 5\n        )  # 5 is default for descriptor\n        self.fourier_proj = nn.Linear(self.d_model + self.visual_feat_dim, d_model)\n        self.fourier_norm = nn.LayerNorm(self.d_model)\n\n    self.encoder = TransformerEncoder(\n        encoder_layer, num_encoder_layers, encoder_norm\n    )\n\n    # Transformer Decoder\n    decoder_layer = TransformerDecoderLayer(\n        d_model,\n        nhead,\n        dim_feedforward,\n        dropout,\n        activation,\n        norm,\n        decoder_self_attn,\n    )\n\n    decoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n    self.decoder = TransformerDecoder(\n        decoder_layer, num_decoder_layers, return_intermediate_dec, decoder_norm\n    )\n\n    # Transformer attention head\n    self.attn_head = ATTWeightHead(\n        feature_dim=feature_dim_attn_head,\n        num_layers=num_layers_attn_head,\n        dropout=dropout_attn_head,\n    )\n\n    self._reset_parameters()\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.Transformer.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute a forward pass through the transformer and attention head.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>A list of instance objects (See <code>dreem.io.Instance</code> for more info.)</p> required <code>query_instances</code> <code>list[Instance] | None</code> <p>An set of instances to be used as decoder queries.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>asso_output</code> <code>list[AssociationMatrix]</code> <p>A list of torch.Tensors of shape (L, n_query, total_instances) where:     L: number of decoder blocks     n_query: number of instances in current query/frame     total_instances: number of instances in window</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self,\n    ref_instances: list[\"dreem.io.Instance\"],\n    query_instances: list[\"dreem.io.Instance\"] | None = None,\n) -&gt; list[AssociationMatrix]:\n    \"\"\"Execute a forward pass through the transformer and attention head.\n\n    Args:\n        ref_instances: A list of instance objects (See `dreem.io.Instance` for more info.)\n        query_instances: An set of instances to be used as decoder queries.\n\n    Returns:\n        asso_output: A list of torch.Tensors of shape (L, n_query, total_instances) where:\n            L: number of decoder blocks\n            n_query: number of instances in current query/frame\n            total_instances: number of instances in window\n    \"\"\"\n    ref_features = torch.cat(\n        [instance.features for instance in ref_instances], dim=0\n    ).unsqueeze(0)\n\n    # window_length = len(frames)\n    # instances_per_frame = [frame.num_detected for frame in frames]\n    total_instances = len(ref_instances)\n    embed_dim = self.d_model\n    # print(f'T: {window_length}; N: {total_instances}; N_t: {instances_per_frame} n_reid: {reid_features.shape}')\n    ref_boxes = get_boxes(ref_instances)  # total_instances, 4\n    ref_boxes = torch.nan_to_num(ref_boxes, -1.0)\n    ref_times, query_times = get_times(ref_instances, query_instances)\n\n    # window_length = len(ref_times.unique())  # Currently unused but may be useful for debugging\n\n    ref_temp_emb = self.temp_emb(ref_times)\n\n    ref_pos_emb = self.pos_emb(ref_boxes)\n\n    if self.return_embedding:\n        for i, instance in enumerate(ref_instances):\n            instance.add_embedding(\"pos\", ref_pos_emb[i])\n            instance.add_embedding(\"temp\", ref_temp_emb[i])\n\n    ref_emb = (ref_pos_emb + ref_temp_emb) / 2.0\n\n    ref_emb = ref_emb.view(1, total_instances, embed_dim)\n\n    ref_emb = ref_emb.permute(1, 0, 2)  # (total_instances, batch_size, embed_dim)\n\n    batch_size, total_instances = ref_features.shape[:-1]\n\n    ref_features = ref_features.permute(\n        1, 0, 2\n    )  # (total_instances, batch_size, embed_dim)\n\n    encoder_queries = ref_features\n\n    # apply fourier embeddings if using fourier rope, OR if using descriptor (compact) visual encoder\n    if (\n        self.embedding_meta\n        and \"use_fourier\" in self.embedding_meta\n        and self.embedding_meta[\"use_fourier\"]\n    ) or (\n        self.encoder_cfg\n        and \"encoder_type\" in self.encoder_cfg\n        and self.encoder_cfg[\"encoder_type\"] == \"descriptor\"\n    ):\n        encoder_queries = apply_fourier_embeddings(\n            encoder_queries,\n            ref_times,\n            self.d_model,\n            self.fourier_embeddings,\n            self.fourier_proj,\n            self.fourier_norm,\n        )\n\n    encoder_features = self.encoder(\n        encoder_queries, pos_emb=ref_emb\n    )  # (total_instances, batch_size, embed_dim)\n\n    n_query = total_instances\n\n    query_features = ref_features\n    query_pos_emb = ref_pos_emb\n    query_temp_emb = ref_temp_emb\n    query_emb = ref_emb\n\n    if query_instances is not None:\n        n_query = len(query_instances)\n\n        query_features = torch.cat(\n            [instance.features for instance in query_instances], dim=0\n        ).unsqueeze(0)\n\n        query_features = query_features.permute(\n            1, 0, 2\n        )  # (n_query, batch_size, embed_dim)\n\n        query_boxes = get_boxes(query_instances)\n        query_boxes = torch.nan_to_num(query_boxes, -1.0)\n        query_temp_emb = self.temp_emb(query_times)\n\n        query_pos_emb = self.pos_emb(query_boxes)\n\n        query_emb = (query_pos_emb + query_temp_emb) / 2.0\n        query_emb = query_emb.view(1, n_query, embed_dim)\n        query_emb = query_emb.permute(1, 0, 2)  # (n_query, batch_size, embed_dim)\n\n    else:\n        query_instances = ref_instances\n        query_times = ref_times\n\n    if self.return_embedding:\n        for i, instance in enumerate(query_instances):\n            instance.add_embedding(\"pos\", query_pos_emb[i])\n            instance.add_embedding(\"temp\", query_temp_emb[i])\n\n    # apply fourier embeddings if using fourier rope, OR if using descriptor (compact) visual encoder\n    if (\n        self.embedding_meta\n        and \"use_fourier\" in self.embedding_meta\n        and self.embedding_meta[\"use_fourier\"]\n    ) or (\n        self.encoder_cfg\n        and \"encoder_type\" in self.encoder_cfg\n        and self.encoder_cfg[\"encoder_type\"] == \"descriptor\"\n    ):\n        query_features = apply_fourier_embeddings(\n            query_features,\n            query_times,\n            self.d_model,\n            self.fourier_embeddings,\n            self.fourier_proj,\n            self.fourier_norm,\n        )\n\n    decoder_features = self.decoder(\n        query_features,\n        encoder_features,\n        ref_pos_emb=ref_emb,\n        query_pos_emb=query_emb,\n    )  # (L, n_query, batch_size, embed_dim)\n\n    decoder_features = decoder_features.transpose(\n        1, 2\n    )  # # (L, batch_size, n_query, embed_dim)\n    encoder_features = encoder_features.permute(1, 0, 2).view(\n        batch_size, total_instances, embed_dim\n    )  # (batch_size, total_instances, embed_dim)\n\n    asso_output = []\n    for frame_features in decoder_features:\n        asso_matrix = self.attn_head(frame_features, encoder_features).view(\n            n_query, total_instances\n        )\n        asso_matrix = AssociationMatrix(asso_matrix, ref_instances, query_instances)\n\n        asso_output.append(asso_matrix)\n\n    # (L=1, n_query, total_instances)\n    return asso_output\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.VisualEncoder","title":"<code>VisualEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Class wrapping around a visual feature extractor backbone.</p> <p>Currently CNN only.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize Visual Encoder.</p> <code>encoder_dim</code> <p>Compute dummy forward pass of encoder model and get embedding dimension.</p> <code>forward</code> <p>Forward pass of feature extractor to get feature vector.</p> <code>select_feature_extractor</code> <p>Select the appropriate feature extractor based on config.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>class VisualEncoder(torch.nn.Module):\n    \"\"\"Class wrapping around a visual feature extractor backbone.\n\n    Currently CNN only.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"resnet18\",\n        d_model: int = 512,\n        in_chans: int = 3,\n        backend: int = \"timm\",\n        **kwargs: Any | None,\n    ):\n        \"\"\"Initialize Visual Encoder.\n\n        Args:\n            model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n            d_model (int): Output embedding dimension.\n            in_chans: the number of input channels of the image.\n            backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n            kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n        \"\"\"\n        super().__init__()\n\n        self.model_name = model_name.lower()\n        self.d_model = d_model\n        self.backend = backend\n        if in_chans == 1:\n            self.in_chans = 3\n        else:\n            self.in_chans = in_chans\n\n        self.feature_extractor = self.select_feature_extractor(\n            model_name=self.model_name,\n            in_chans=self.in_chans,\n            backend=self.backend,\n            **kwargs,\n        )\n\n        self.out_layer = torch.nn.Linear(\n            self.encoder_dim(self.feature_extractor), self.d_model\n        )\n\n    def select_feature_extractor(\n        self, model_name: str, in_chans: int, backend: str, **kwargs: Any\n    ) -&gt; torch.nn.Module:\n        \"\"\"Select the appropriate feature extractor based on config.\n\n        Args:\n            model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n            in_chans: the number of input channels of the image.\n            backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n            kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n\n        Returns:\n            a CNN encoder based on the config and backend selected.\n        \"\"\"\n        if \"timm\" in backend.lower():\n            feature_extractor = timm.create_model(\n                model_name=self.model_name,\n                in_chans=self.in_chans,\n                num_classes=0,\n                **kwargs,\n            )\n        elif \"torch\" in backend.lower():\n            if model_name.lower() == \"resnet18\":\n                feature_extractor = torchvision.models.resnet18(**kwargs)\n\n            elif model_name.lower() == \"resnet50\":\n                feature_extractor = torchvision.models.resnet50(**kwargs)\n\n            else:\n                raise ValueError(\n                    f\"Only `[resnet18, resnet50]` are available when backend is {backend}. Found {model_name}\"\n                )\n            feature_extractor = torch.nn.Sequential(\n                *list(feature_extractor.children())[:-1]\n            )\n            input_layer = feature_extractor[0]\n            if in_chans != 3:\n                feature_extractor[0] = torch.nn.Conv2d(\n                    in_channels=in_chans,\n                    out_channels=input_layer.out_channels,\n                    kernel_size=input_layer.kernel_size,\n                    stride=input_layer.stride,\n                    padding=input_layer.padding,\n                    dilation=input_layer.dilation,\n                    groups=input_layer.groups,\n                    bias=input_layer.bias,\n                    padding_mode=input_layer.padding_mode,\n                )\n\n        else:\n            raise ValueError(\n                f\"Only ['timm', 'torch'] backends are available! Found {backend}.\"\n            )\n        return feature_extractor\n\n    def encoder_dim(self, model: torch.nn.Module) -&gt; int:\n        \"\"\"Compute dummy forward pass of encoder model and get embedding dimension.\n\n        Args:\n            model: a vision encoder model.\n\n        Returns:\n            The embedding dimension size.\n        \"\"\"\n        _ = model.eval()\n        dummy_output = model(torch.randn(1, self.in_chans, 224, 224)).squeeze()\n        _ = model.train()  # to be safe\n        return dummy_output.shape[-1]\n\n    def forward(self, img: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of feature extractor to get feature vector.\n\n        Args:\n            img: Input image tensor of shape (B, C, H, W).\n\n        Returns:\n            feats: Normalized output tensor of shape (B, d_model).\n        \"\"\"\n        # If grayscale, tile the image to 3 channels.\n        if img.shape[1] == 1:\n            img = img.repeat([1, 3, 1, 1])  # (B, nc=3, H, W)\n\n        b, c, h, w = img.shape\n\n        if c != self.in_chans:\n            raise ValueError(\n                f\"\"\"Found {c} channels in image but model was configured for {self.in_chans} channels! \\n\n                    Hint: have you set the number of anchors in your dataset &gt; 1? \\n\n                    If so, make sure to set `in_chans=3 * n_anchors`\"\"\"\n            )\n        feats = self.feature_extractor(\n            img\n        )  # (B, out_dim, 1, 1) if using resnet18 backbone.\n\n        # Reshape feature vectors\n        feats = feats.reshape([img.shape[0], -1])  # (B, out_dim)\n        # Map feature vectors to output dimension using linear layer.\n        feats = self.out_layer(feats)  # (B, d_model)\n        # Normalize output feature vectors.\n        feats = F.normalize(feats)  # (B, d_model)\n        return feats\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.VisualEncoder.__init__","title":"<code>__init__(model_name='resnet18', d_model=512, in_chans=3, backend='timm', **kwargs)</code>","text":"<p>Initialize Visual Encoder.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").</p> <code>'resnet18'</code> <code>d_model</code> <code>int</code> <p>Output embedding dimension.</p> <code>512</code> <code>in_chans</code> <code>int</code> <p>the number of input channels of the image.</p> <code>3</code> <code>backend</code> <code>int</code> <p>Which model backend to use. One of {\"timm\", \"torchvision\"}</p> <code>'timm'</code> <code>kwargs</code> <code>Any | None</code> <p>see <code>timm.create_model</code> and <code>torchvision.models.resnetX</code> for kwargs.</p> <code>{}</code> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"resnet18\",\n    d_model: int = 512,\n    in_chans: int = 3,\n    backend: int = \"timm\",\n    **kwargs: Any | None,\n):\n    \"\"\"Initialize Visual Encoder.\n\n    Args:\n        model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n        d_model (int): Output embedding dimension.\n        in_chans: the number of input channels of the image.\n        backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n        kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n    \"\"\"\n    super().__init__()\n\n    self.model_name = model_name.lower()\n    self.d_model = d_model\n    self.backend = backend\n    if in_chans == 1:\n        self.in_chans = 3\n    else:\n        self.in_chans = in_chans\n\n    self.feature_extractor = self.select_feature_extractor(\n        model_name=self.model_name,\n        in_chans=self.in_chans,\n        backend=self.backend,\n        **kwargs,\n    )\n\n    self.out_layer = torch.nn.Linear(\n        self.encoder_dim(self.feature_extractor), self.d_model\n    )\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.VisualEncoder.encoder_dim","title":"<code>encoder_dim(model)</code>","text":"<p>Compute dummy forward pass of encoder model and get embedding dimension.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>a vision encoder model.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The embedding dimension size.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def encoder_dim(self, model: torch.nn.Module) -&gt; int:\n    \"\"\"Compute dummy forward pass of encoder model and get embedding dimension.\n\n    Args:\n        model: a vision encoder model.\n\n    Returns:\n        The embedding dimension size.\n    \"\"\"\n    _ = model.eval()\n    dummy_output = model(torch.randn(1, self.in_chans, 224, 224)).squeeze()\n    _ = model.train()  # to be safe\n    return dummy_output.shape[-1]\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.VisualEncoder.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of feature extractor to get feature vector.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor</code> <p>Input image tensor of shape (B, C, H, W).</p> required <p>Returns:</p> Name Type Description <code>feats</code> <code>Tensor</code> <p>Normalized output tensor of shape (B, d_model).</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def forward(self, img: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of feature extractor to get feature vector.\n\n    Args:\n        img: Input image tensor of shape (B, C, H, W).\n\n    Returns:\n        feats: Normalized output tensor of shape (B, d_model).\n    \"\"\"\n    # If grayscale, tile the image to 3 channels.\n    if img.shape[1] == 1:\n        img = img.repeat([1, 3, 1, 1])  # (B, nc=3, H, W)\n\n    b, c, h, w = img.shape\n\n    if c != self.in_chans:\n        raise ValueError(\n            f\"\"\"Found {c} channels in image but model was configured for {self.in_chans} channels! \\n\n                Hint: have you set the number of anchors in your dataset &gt; 1? \\n\n                If so, make sure to set `in_chans=3 * n_anchors`\"\"\"\n        )\n    feats = self.feature_extractor(\n        img\n    )  # (B, out_dim, 1, 1) if using resnet18 backbone.\n\n    # Reshape feature vectors\n    feats = feats.reshape([img.shape[0], -1])  # (B, out_dim)\n    # Map feature vectors to output dimension using linear layer.\n    feats = self.out_layer(feats)  # (B, d_model)\n    # Normalize output feature vectors.\n    feats = F.normalize(feats)  # (B, d_model)\n    return feats\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.VisualEncoder.select_feature_extractor","title":"<code>select_feature_extractor(model_name, in_chans, backend, **kwargs)</code>","text":"<p>Select the appropriate feature extractor based on config.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").</p> required <code>in_chans</code> <code>int</code> <p>the number of input channels of the image.</p> required <code>backend</code> <code>str</code> <p>Which model backend to use. One of {\"timm\", \"torchvision\"}</p> required <code>kwargs</code> <code>Any</code> <p>see <code>timm.create_model</code> and <code>torchvision.models.resnetX</code> for kwargs.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Module</code> <p>a CNN encoder based on the config and backend selected.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def select_feature_extractor(\n    self, model_name: str, in_chans: int, backend: str, **kwargs: Any\n) -&gt; torch.nn.Module:\n    \"\"\"Select the appropriate feature extractor based on config.\n\n    Args:\n        model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n        in_chans: the number of input channels of the image.\n        backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n        kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n\n    Returns:\n        a CNN encoder based on the config and backend selected.\n    \"\"\"\n    if \"timm\" in backend.lower():\n        feature_extractor = timm.create_model(\n            model_name=self.model_name,\n            in_chans=self.in_chans,\n            num_classes=0,\n            **kwargs,\n        )\n    elif \"torch\" in backend.lower():\n        if model_name.lower() == \"resnet18\":\n            feature_extractor = torchvision.models.resnet18(**kwargs)\n\n        elif model_name.lower() == \"resnet50\":\n            feature_extractor = torchvision.models.resnet50(**kwargs)\n\n        else:\n            raise ValueError(\n                f\"Only `[resnet18, resnet50]` are available when backend is {backend}. Found {model_name}\"\n            )\n        feature_extractor = torch.nn.Sequential(\n            *list(feature_extractor.children())[:-1]\n        )\n        input_layer = feature_extractor[0]\n        if in_chans != 3:\n            feature_extractor[0] = torch.nn.Conv2d(\n                in_channels=in_chans,\n                out_channels=input_layer.out_channels,\n                kernel_size=input_layer.kernel_size,\n                stride=input_layer.stride,\n                padding=input_layer.padding,\n                dilation=input_layer.dilation,\n                groups=input_layer.groups,\n                bias=input_layer.bias,\n                padding_mode=input_layer.padding_mode,\n            )\n\n    else:\n        raise ValueError(\n            f\"Only ['timm', 'torch'] backends are available! Found {backend}.\"\n        )\n    return feature_extractor\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.create_visual_encoder","title":"<code>create_visual_encoder(d_model, **encoder_cfg)</code>","text":"<p>Create a visual encoder based on the specified type.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def create_visual_encoder(d_model: int, **encoder_cfg) -&gt; torch.nn.Module:\n    \"\"\"Create a visual encoder based on the specified type.\"\"\"\n    register_encoder(\"resnet\", VisualEncoder)\n    register_encoder(\"descriptor\", DescriptorVisualEncoder)\n    # register any custom encoders here\n\n    # compatibility with configs that don't specify encoder_type; default to resnet\n    if not encoder_cfg or \"encoder_type\" not in encoder_cfg:\n        encoder_type = \"resnet\"\n        return ENCODER_REGISTRY[encoder_type](d_model=d_model, **encoder_cfg)\n    else:\n        encoder_type = encoder_cfg.pop(\"encoder_type\")\n\n    if encoder_type in ENCODER_REGISTRY:\n        # choose the relevant encoder configs based on the encoder_type\n        configs = encoder_cfg[encoder_type]\n        return ENCODER_REGISTRY[encoder_type](d_model=d_model, **configs)\n    else:\n        raise ValueError(\n            f\"Unknown encoder type: {encoder_type}. Please use one of {list(ENCODER_REGISTRY.keys())}\"\n        )\n</code></pre>"},{"location":"reference/dreem/models/#dreem.models.register_encoder","title":"<code>register_encoder(encoder_type, encoder_class)</code>","text":"<p>Register a new encoder type.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def register_encoder(encoder_type: str, encoder_class: Type[torch.nn.Module]):\n    \"\"\"Register a new encoder type.\"\"\"\n    if not issubclass(encoder_class, torch.nn.Module):\n        raise ValueError(f\"{encoder_class} must be a subclass of torch.nn.Module\")\n    ENCODER_REGISTRY[encoder_type] = encoder_class\n</code></pre>"},{"location":"reference/dreem/models/attention_head/","title":"attention_head","text":""},{"location":"reference/dreem/models/attention_head/#dreem.models.attention_head","title":"<code>dreem.models.attention_head</code>","text":"<p>Module containing different components of multi-head attention heads.</p> <p>Classes:</p> Name Description <code>ATTWeightHead</code> <p>Single attention head.</p>"},{"location":"reference/dreem/models/attention_head/#dreem.models.attention_head.ATTWeightHead","title":"<code>ATTWeightHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Single attention head.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize an instance of ATTWeightHead.</p> <code>forward</code> <p>Compute the attention weights of a query tensor using the key tensor.</p> Source code in <code>dreem/models/attention_head.py</code> <pre><code>class ATTWeightHead(torch.nn.Module):\n    \"\"\"Single attention head.\"\"\"\n\n    def __init__(\n        self,\n        feature_dim: int,\n        num_layers: int,\n        dropout: float,\n    ):\n        \"\"\"Initialize an instance of ATTWeightHead.\n\n        Args:\n            feature_dim: The dimensionality of input features.\n            num_layers: The number of hidden layers in the MLP.\n            dropout: Dropout probability.\n        \"\"\"\n        super().__init__()\n\n        self.q_proj = MLP(feature_dim, feature_dim, feature_dim, num_layers, dropout)\n        self.k_proj = MLP(feature_dim, feature_dim, feature_dim, num_layers, dropout)\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the attention weights of a query tensor using the key tensor.\n\n        Args:\n            query: Input tensor of shape (batch_size, num_frame_instances, feature_dim).\n            key: Input tensor of shape (batch_size, num_window_instances, feature_dim).\n\n        Returns:\n            Output tensor of shape\n            (batch_size, num_frame_instances, num_window_instances).\n        \"\"\"\n        k = self.k_proj(key)\n        q = self.q_proj(query)\n        attn_weights = torch.bmm(q, k.transpose(1, 2))\n\n        return attn_weights  # (B, N_t, N)\n</code></pre>"},{"location":"reference/dreem/models/attention_head/#dreem.models.attention_head.ATTWeightHead.__init__","title":"<code>__init__(feature_dim, num_layers, dropout)</code>","text":"<p>Initialize an instance of ATTWeightHead.</p> <p>Parameters:</p> Name Type Description Default <code>feature_dim</code> <code>int</code> <p>The dimensionality of input features.</p> required <code>num_layers</code> <code>int</code> <p>The number of hidden layers in the MLP.</p> required <code>dropout</code> <code>float</code> <p>Dropout probability.</p> required Source code in <code>dreem/models/attention_head.py</code> <pre><code>def __init__(\n    self,\n    feature_dim: int,\n    num_layers: int,\n    dropout: float,\n):\n    \"\"\"Initialize an instance of ATTWeightHead.\n\n    Args:\n        feature_dim: The dimensionality of input features.\n        num_layers: The number of hidden layers in the MLP.\n        dropout: Dropout probability.\n    \"\"\"\n    super().__init__()\n\n    self.q_proj = MLP(feature_dim, feature_dim, feature_dim, num_layers, dropout)\n    self.k_proj = MLP(feature_dim, feature_dim, feature_dim, num_layers, dropout)\n</code></pre>"},{"location":"reference/dreem/models/attention_head/#dreem.models.attention_head.ATTWeightHead.forward","title":"<code>forward(query, key)</code>","text":"<p>Compute the attention weights of a query tensor using the key tensor.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, num_frame_instances, feature_dim).</p> required <code>key</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, num_window_instances, feature_dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, num_frame_instances, num_window_instances).</p> Source code in <code>dreem/models/attention_head.py</code> <pre><code>def forward(\n    self,\n    query: torch.Tensor,\n    key: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the attention weights of a query tensor using the key tensor.\n\n    Args:\n        query: Input tensor of shape (batch_size, num_frame_instances, feature_dim).\n        key: Input tensor of shape (batch_size, num_window_instances, feature_dim).\n\n    Returns:\n        Output tensor of shape\n        (batch_size, num_frame_instances, num_window_instances).\n    \"\"\"\n    k = self.k_proj(key)\n    q = self.q_proj(query)\n    attn_weights = torch.bmm(q, k.transpose(1, 2))\n\n    return attn_weights  # (B, N_t, N)\n</code></pre>"},{"location":"reference/dreem/models/embedding/","title":"embedding","text":""},{"location":"reference/dreem/models/embedding/#dreem.models.embedding","title":"<code>dreem.models.embedding</code>","text":"<p>Module containing different position and temporal embeddings.</p> <p>Classes:</p> Name Description <code>Embedding</code> <p>Class that wraps around different embedding types.</p> <code>FourierPositionalEmbeddings</code> <p>Fourier positional embeddings.</p>"},{"location":"reference/dreem/models/embedding/#dreem.models.embedding.Embedding","title":"<code>Embedding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Class that wraps around different embedding types.</p> <p>Used for both learned and fixed embeddings.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize embeddings.</p> <code>forward</code> <p>Get the sequence positional embeddings.</p> Source code in <code>dreem/models/embedding.py</code> <pre><code>class Embedding(torch.nn.Module):\n    \"\"\"Class that wraps around different embedding types.\n\n    Used for both learned and fixed embeddings.\n    \"\"\"\n\n    EMB_TYPES = {\n        \"temp\": {},\n        \"pos\": {\"over_boxes\"},\n        \"off\": {},\n        None: {},\n    }  # dict of valid args:keyword params\n    EMB_MODES = {\n        \"fixed\": {\"temperature\", \"scale\", \"normalize\"},\n        \"learned\": {\"emb_num\"},\n        \"off\": {},\n    }  # dict of valid args:keyword params\n\n    def __init__(\n        self,\n        emb_type: str,\n        mode: str,\n        features: int,\n        n_points: int = 1,\n        emb_num: int = 16,\n        over_boxes: bool = True,\n        temperature: int = 10000,\n        normalize: bool = False,\n        scale: float | None = None,\n        mlp_cfg: dict | None = None,\n    ):\n        \"\"\"Initialize embeddings.\n\n        Args:\n            emb_type: The type of embedding to compute.\n                Must be one of `{\"temp\", \"pos\", \"off\"}`\n            mode: The mode or function used to map positions to vector embeddings.\n                  Must be one of `{\"fixed\", \"learned\", \"off\"}`\n            features: The embedding dimensions. Must match the dimension of the\n                      input vectors for the transformer model.\n            n_points: the number of points that will be embedded.\n            emb_num: the number of embeddings in the `self.lookup` table\n                (Only used in learned embeddings).\n            over_boxes: Whether to compute the position embedding for each bbox\n                coordinate (y1x1y2x2) or the centroid + bbox size (yxwh).\n            temperature: the temperature constant to be used when computing\n                the sinusoidal position embedding\n            normalize: whether or not to normalize the positions\n                (Only used in fixed embeddings).\n            scale: factor by which to scale the positions after normalizing\n                (Only used in fixed embeddings).\n            mlp_cfg: A dictionary of mlp hyperparameters for projecting\n                embedding to correct space.\n                    Example: {\"hidden_dims\": 256, \"num_layers\":3, \"dropout\": 0.3}\n        \"\"\"\n        self._check_init_args(emb_type, mode)\n\n        super().__init__()\n\n        self.emb_type = emb_type\n        self.mode = mode\n        self.features = features\n        self.emb_num = emb_num\n        self.over_boxes = over_boxes\n        self.temperature = temperature\n        self.normalize = normalize\n        self.scale = scale\n        self.n_points = n_points\n\n        if self.normalize and self.scale is None:\n            self.scale = 2 * math.pi\n\n        if self.emb_type == \"pos\" and mlp_cfg is not None and mlp_cfg[\"num_layers\"] &gt; 0:\n            if self.mode == \"fixed\":\n                self.mlp = MLP(\n                    input_dim=n_points * self.features,\n                    output_dim=self.features,\n                    **mlp_cfg,\n                )\n            else:\n                in_dim = (self.features // (4 * n_points)) * (4 * n_points)\n                self.mlp = MLP(\n                    input_dim=in_dim,\n                    output_dim=self.features,\n                    **mlp_cfg,\n                )\n        else:\n            self.mlp = torch.nn.Identity()\n\n        self._emb_func = lambda tensor: torch.zeros(\n            (tensor.shape[0], self.features), dtype=tensor.dtype, device=tensor.device\n        )  # turn off embedding by returning zeros\n\n        self.lookup = None\n\n        if self.mode == \"learned\":\n            if self.emb_type == \"pos\":\n                self.lookup = torch.nn.Embedding(\n                    self.emb_num * 4 * self.n_points, self.features // (4 * n_points)\n                )\n                self._emb_func = self._learned_pos_embedding\n            elif self.emb_type == \"temp\":\n                self.lookup = torch.nn.Embedding(self.emb_num, self.features)\n                self._emb_func = self._learned_temp_embedding\n\n        elif self.mode == \"fixed\":\n            if self.emb_type == \"pos\":\n                self._emb_func = self._sine_box_embedding\n            elif self.emb_type == \"temp\":\n                self._emb_func = self._sine_temp_embedding\n\n    def _check_init_args(self, emb_type: str, mode: str):\n        \"\"\"Check whether the correct arguments were passed to initialization.\n\n        Args:\n            emb_type: The type of embedding to compute. Must be one of `{\"temp\", \"pos\", \"\"}`\n            mode: The mode or function used to map positions to vector embeddings.\n                Must be one of `{\"fixed\", \"learned\"}`\n\n        Raises:\n            ValueError:\n              * if the incorrect `emb_type` or `mode` string are passed\n            NotImplementedError: if `emb_type` is `temp` and `mode` is `fixed`.\n        \"\"\"\n        if emb_type.lower() not in self.EMB_TYPES:\n            raise ValueError(\n                f\"Embedding `emb_type` must be one of {self.EMB_TYPES} not {emb_type}\"\n            )\n\n        if mode.lower() not in self.EMB_MODES:\n            raise ValueError(\n                f\"Embedding `mode` must be one of {self.EMB_MODES} not {mode}\"\n            )\n\n    def forward(self, seq_positions: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Get the sequence positional embeddings.\n\n        Args:\n            seq_positions:\n                * An (`N`, 1) tensor where seq_positions[i] represents the temporal position of instance_i in the sequence.\n                * An (`N`, n_anchors x 4) tensor where seq_positions[i, j, :] represents the [y1, x1, y2, x2] spatial locations of jth point of instance_i in the sequence.\n\n        Returns:\n            An `N` x `self.features` tensor representing the corresponding spatial or temporal embedding.\n        \"\"\"\n        emb = self._emb_func(seq_positions)\n\n        if emb.shape[-1] != self.features:\n            raise RuntimeError(\n                (\n                    f\"Output embedding dimension is {emb.shape[-1]} but requested {self.features} dimensions! \\n\"\n                    f\"hint: Try turning the MLP on by passing `mlp_cfg` to the constructor to project to the correct embedding dimensions.\"\n                )\n            )\n        return emb\n\n    def _torch_int_div(\n        self, tensor1: torch.Tensor, tensor2: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Perform integer division of two tensors.\n\n        Args:\n            tensor1: dividend tensor.\n            tensor2: divisor tensor.\n\n        Returns:\n            torch.Tensor, resulting tensor.\n        \"\"\"\n        return torch.div(tensor1, tensor2, rounding_mode=\"floor\")\n\n    def _sine_box_embedding(self, boxes: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute sine positional embeddings for boxes using given parameters.\n\n        Args:\n             boxes: the input boxes of shape N, n_anchors, 4 or B, N, n_anchors, 4\n                    where the last dimension is the bbox coords in [y1, x1, y2, x2].\n                    (Note currently `B=batch_size=1`).\n\n        Returns:\n             torch.Tensor, the sine positional embeddings\n             (embedding[:, 4i] = sin(x)\n              embedding[:, 4i+1] = cos(x)\n              embedding[:, 4i+2] = sin(y)\n              embedding[:, 4i+3] = cos(y)\n              )\n        \"\"\"\n        if self.scale is not None and self.normalize is False:\n            raise ValueError(\"normalize should be True if scale is passed\")\n\n        if len(boxes.size()) == 3:\n            boxes = boxes.unsqueeze(0)\n\n        if self.normalize:\n            boxes = boxes / (boxes[:, :, -1:] + 1e-6) * self.scale\n\n        dim_t = torch.arange(self.features // 4, dtype=torch.float32)\n\n        dim_t = self.temperature ** (\n            2 * self._torch_int_div(dim_t, 2) / (self.features // 4)\n        )\n\n        # (b, n_t, n_anchors, 4, D//4)\n        pos_emb = boxes[:, :, :, :, None] / dim_t.to(boxes.device)\n\n        pos_emb = torch.stack(\n            (pos_emb[:, :, :, :, 0::2].sin(), pos_emb[:, :, :, :, 1::2].cos()), dim=4\n        )\n        pos_emb = pos_emb.flatten(2).squeeze(0)  # (N_t, n_anchors * D)\n\n        pos_emb = self.mlp(pos_emb)\n\n        pos_emb = pos_emb.view(boxes.shape[1], self.features)\n\n        return pos_emb\n\n    def _sine_temp_embedding(self, times: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute fixed sine temporal embeddings.\n\n        Args:\n            times: the input times of shape (N,) or (N,1) where N = (sum(instances_per_frame))\n            which is the frame index of the instance relative\n            to the batch size\n            (e.g. `torch.tensor([0, 0, ..., 0, 1, 1, ..., 1, 2, 2, ..., 2,..., B, B, ...B])`).\n\n        Returns:\n            an n_instances x D embedding representing the temporal embedding.\n        \"\"\"\n        T = times.int().max().item() + 1\n        d = self.features\n        n = self.temperature\n\n        positions = torch.arange(0, T).unsqueeze(1)\n        temp_lookup = torch.zeros(T, d, device=times.device)\n\n        denominators = torch.pow(\n            n, 2 * torch.arange(0, d // 2) / d\n        )  # 10000^(2i/d_model), i is the index of embedding\n        temp_lookup[:, 0::2] = torch.sin(\n            positions / denominators\n        )  # sin(pos/10000^(2i/d_model))\n        temp_lookup[:, 1::2] = torch.cos(\n            positions / denominators\n        )  # cos(pos/10000^(2i/d_model))\n\n        temp_emb = temp_lookup[times.int()]\n        return temp_emb  # .view(len(times), self.features)\n\n    def _learned_pos_embedding(self, boxes: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute learned positional embeddings for boxes using given parameters.\n\n        Args:\n            boxes: the input boxes of shape N x 4 or B x N x 4\n                   where the last dimension is the bbox coords in [y1, x1, y2, x2].\n                   (Note currently `B=batch_size=1`).\n\n        Returns:\n            torch.Tensor, the learned positional embeddings.\n        \"\"\"\n        pos_lookup = self.lookup\n\n        N, n_anchors, _ = boxes.shape\n        boxes = boxes.view(N, n_anchors, 4)\n\n        if self.over_boxes:\n            xywh = boxes\n        else:\n            xywh = torch.cat(\n                [\n                    (boxes[:, :, 2:] + boxes[:, :, :2]) / 2,\n                    (boxes[:, :, 2:] - boxes[:, :, :2]),\n                ],\n                dim=1,\n            )\n\n        left_ind, right_ind, left_weight, right_weight = self._compute_weights(xywh)\n        f = pos_lookup.weight.shape[1]  # self.features // 4\n\n        try:\n            pos_emb_table = pos_lookup.weight.view(\n                self.emb_num, n_anchors, 4, f\n            )  # T x 4 x (D * 4)\n        except RuntimeError as e:\n            logger.exception(\n                f\"Hint: `n_points` ({self.n_points}) may be set incorrectly!\"\n            )\n            logger.exception(e)\n            raise (e)\n\n        left_emb = pos_emb_table.gather(\n            0,\n            left_ind[:, :, :, None].to(pos_emb_table.device).expand(N, n_anchors, 4, f),\n        )  # N x 4 x d\n        right_emb = pos_emb_table.gather(\n            0,\n            right_ind[:, :, :, None]\n            .to(pos_emb_table.device)\n            .expand(N, n_anchors, 4, f),\n        )  # N x 4 x d\n        pos_emb = left_weight[:, :, :, None] * right_emb.to(\n            left_weight.device\n        ) + right_weight[:, :, :, None] * left_emb.to(right_weight.device)\n\n        pos_emb = pos_emb.flatten(1)\n        pos_emb = self.mlp(pos_emb)\n\n        return pos_emb.view(N, self.features)\n\n    def _learned_temp_embedding(self, times: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute learned temporal embeddings for times using given parameters.\n\n        Args:\n            times: the input times of shape (N,) or (N,1) where N = (sum(instances_per_frame))\n            which is the frame index of the instance relative\n            to the batch size\n            (e.g. `torch.tensor([0, 0, ..., 0, 1, 1, ..., 1, 2, 2, ..., 2,..., B, B, ...B])`).\n\n        Returns:\n            torch.Tensor, the learned temporal embeddings.\n        \"\"\"\n        temp_lookup = self.lookup\n        N = times.shape[0]\n\n        left_ind, right_ind, left_weight, right_weight = self._compute_weights(times)\n\n        left_emb = temp_lookup.weight[\n            left_ind.to(temp_lookup.weight.device)\n        ]  # T x D --&gt; N x D\n        right_emb = temp_lookup.weight[right_ind.to(temp_lookup.weight.device)]\n\n        temp_emb = left_weight[:, None] * right_emb.to(\n            left_weight.device\n        ) + right_weight[:, None] * left_emb.to(right_weight.device)\n\n        return temp_emb.view(N, self.features)\n\n    def _compute_weights(self, data: torch.Tensor) -&gt; tuple[torch.Tensor, ...]:\n        \"\"\"Compute left and right learned embedding weights.\n\n        Args:\n            data: the input data (e.g boxes or times).\n\n        Returns:\n            A torch.Tensor for each of the left/right indices and weights, respectively\n        \"\"\"\n        data = data * self.emb_num\n\n        left_ind = data.clamp(min=0, max=self.emb_num - 1).long()  # N x 4\n        right_ind = (left_ind + 1).clamp(min=0, max=self.emb_num - 1).long()  # N x 4\n\n        left_weight = data - left_ind.float()  # N x 4\n\n        right_weight = 1.0 - left_weight\n\n        return left_ind, right_ind, left_weight, right_weight\n</code></pre>"},{"location":"reference/dreem/models/embedding/#dreem.models.embedding.Embedding.__init__","title":"<code>__init__(emb_type, mode, features, n_points=1, emb_num=16, over_boxes=True, temperature=10000, normalize=False, scale=None, mlp_cfg=None)</code>","text":"<p>Initialize embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>emb_type</code> <code>str</code> <p>The type of embedding to compute. Must be one of <code>{\"temp\", \"pos\", \"off\"}</code></p> required <code>mode</code> <code>str</code> <p>The mode or function used to map positions to vector embeddings.   Must be one of <code>{\"fixed\", \"learned\", \"off\"}</code></p> required <code>features</code> <code>int</code> <p>The embedding dimensions. Must match the dimension of the       input vectors for the transformer model.</p> required <code>n_points</code> <code>int</code> <p>the number of points that will be embedded.</p> <code>1</code> <code>emb_num</code> <code>int</code> <p>the number of embeddings in the <code>self.lookup</code> table (Only used in learned embeddings).</p> <code>16</code> <code>over_boxes</code> <code>bool</code> <p>Whether to compute the position embedding for each bbox coordinate (y1x1y2x2) or the centroid + bbox size (yxwh).</p> <code>True</code> <code>temperature</code> <code>int</code> <p>the temperature constant to be used when computing the sinusoidal position embedding</p> <code>10000</code> <code>normalize</code> <code>bool</code> <p>whether or not to normalize the positions (Only used in fixed embeddings).</p> <code>False</code> <code>scale</code> <code>float | None</code> <p>factor by which to scale the positions after normalizing (Only used in fixed embeddings).</p> <code>None</code> <code>mlp_cfg</code> <code>dict | None</code> <p>A dictionary of mlp hyperparameters for projecting embedding to correct space.     Example: {\"hidden_dims\": 256, \"num_layers\":3, \"dropout\": 0.3}</p> <code>None</code> Source code in <code>dreem/models/embedding.py</code> <pre><code>def __init__(\n    self,\n    emb_type: str,\n    mode: str,\n    features: int,\n    n_points: int = 1,\n    emb_num: int = 16,\n    over_boxes: bool = True,\n    temperature: int = 10000,\n    normalize: bool = False,\n    scale: float | None = None,\n    mlp_cfg: dict | None = None,\n):\n    \"\"\"Initialize embeddings.\n\n    Args:\n        emb_type: The type of embedding to compute.\n            Must be one of `{\"temp\", \"pos\", \"off\"}`\n        mode: The mode or function used to map positions to vector embeddings.\n              Must be one of `{\"fixed\", \"learned\", \"off\"}`\n        features: The embedding dimensions. Must match the dimension of the\n                  input vectors for the transformer model.\n        n_points: the number of points that will be embedded.\n        emb_num: the number of embeddings in the `self.lookup` table\n            (Only used in learned embeddings).\n        over_boxes: Whether to compute the position embedding for each bbox\n            coordinate (y1x1y2x2) or the centroid + bbox size (yxwh).\n        temperature: the temperature constant to be used when computing\n            the sinusoidal position embedding\n        normalize: whether or not to normalize the positions\n            (Only used in fixed embeddings).\n        scale: factor by which to scale the positions after normalizing\n            (Only used in fixed embeddings).\n        mlp_cfg: A dictionary of mlp hyperparameters for projecting\n            embedding to correct space.\n                Example: {\"hidden_dims\": 256, \"num_layers\":3, \"dropout\": 0.3}\n    \"\"\"\n    self._check_init_args(emb_type, mode)\n\n    super().__init__()\n\n    self.emb_type = emb_type\n    self.mode = mode\n    self.features = features\n    self.emb_num = emb_num\n    self.over_boxes = over_boxes\n    self.temperature = temperature\n    self.normalize = normalize\n    self.scale = scale\n    self.n_points = n_points\n\n    if self.normalize and self.scale is None:\n        self.scale = 2 * math.pi\n\n    if self.emb_type == \"pos\" and mlp_cfg is not None and mlp_cfg[\"num_layers\"] &gt; 0:\n        if self.mode == \"fixed\":\n            self.mlp = MLP(\n                input_dim=n_points * self.features,\n                output_dim=self.features,\n                **mlp_cfg,\n            )\n        else:\n            in_dim = (self.features // (4 * n_points)) * (4 * n_points)\n            self.mlp = MLP(\n                input_dim=in_dim,\n                output_dim=self.features,\n                **mlp_cfg,\n            )\n    else:\n        self.mlp = torch.nn.Identity()\n\n    self._emb_func = lambda tensor: torch.zeros(\n        (tensor.shape[0], self.features), dtype=tensor.dtype, device=tensor.device\n    )  # turn off embedding by returning zeros\n\n    self.lookup = None\n\n    if self.mode == \"learned\":\n        if self.emb_type == \"pos\":\n            self.lookup = torch.nn.Embedding(\n                self.emb_num * 4 * self.n_points, self.features // (4 * n_points)\n            )\n            self._emb_func = self._learned_pos_embedding\n        elif self.emb_type == \"temp\":\n            self.lookup = torch.nn.Embedding(self.emb_num, self.features)\n            self._emb_func = self._learned_temp_embedding\n\n    elif self.mode == \"fixed\":\n        if self.emb_type == \"pos\":\n            self._emb_func = self._sine_box_embedding\n        elif self.emb_type == \"temp\":\n            self._emb_func = self._sine_temp_embedding\n</code></pre>"},{"location":"reference/dreem/models/embedding/#dreem.models.embedding.Embedding.forward","title":"<code>forward(seq_positions)</code>","text":"<p>Get the sequence positional embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>seq_positions</code> <code>Tensor</code> <ul> <li>An (<code>N</code>, 1) tensor where seq_positions[i] represents the temporal position of instance_i in the sequence.</li> <li>An (<code>N</code>, n_anchors x 4) tensor where seq_positions[i, j, :] represents the [y1, x1, y2, x2] spatial locations of jth point of instance_i in the sequence.</li> </ul> required <p>Returns:</p> Type Description <code>Tensor</code> <p>An <code>N</code> x <code>self.features</code> tensor representing the corresponding spatial or temporal embedding.</p> Source code in <code>dreem/models/embedding.py</code> <pre><code>def forward(self, seq_positions: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Get the sequence positional embeddings.\n\n    Args:\n        seq_positions:\n            * An (`N`, 1) tensor where seq_positions[i] represents the temporal position of instance_i in the sequence.\n            * An (`N`, n_anchors x 4) tensor where seq_positions[i, j, :] represents the [y1, x1, y2, x2] spatial locations of jth point of instance_i in the sequence.\n\n    Returns:\n        An `N` x `self.features` tensor representing the corresponding spatial or temporal embedding.\n    \"\"\"\n    emb = self._emb_func(seq_positions)\n\n    if emb.shape[-1] != self.features:\n        raise RuntimeError(\n            (\n                f\"Output embedding dimension is {emb.shape[-1]} but requested {self.features} dimensions! \\n\"\n                f\"hint: Try turning the MLP on by passing `mlp_cfg` to the constructor to project to the correct embedding dimensions.\"\n            )\n        )\n    return emb\n</code></pre>"},{"location":"reference/dreem/models/embedding/#dreem.models.embedding.FourierPositionalEmbeddings","title":"<code>FourierPositionalEmbeddings</code>","text":"<p>               Bases: <code>Module</code></p> <p>Fourier positional embeddings.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize Fourier positional embeddings.</p> <code>forward</code> <p>Compute learnable fourier coefficients for each spatial/temporal position.</p> Source code in <code>dreem/models/embedding.py</code> <pre><code>class FourierPositionalEmbeddings(torch.nn.Module):\n    \"\"\"Fourier positional embeddings.\"\"\"\n\n    def __init__(\n        self,\n        n_components: int,\n        d_model: int,\n    ):\n        \"\"\"Initialize Fourier positional embeddings.\n\n        Args:\n            n_components: Number of frequencies for each dimension.\n            d_model: Model dimension.\n        \"\"\"\n        super().__init__()\n        self.d_model = d_model\n        self.n_components = n_components\n        self.freq = torch.nn.Parameter(\n            _pos_embed_fourier1d_init(self.d_model, n_components)\n        )\n\n    def forward(self, seq_positions: torch.Tensor):\n        \"\"\"Compute learnable fourier coefficients for each spatial/temporal position.\n\n        Args:\n            seq_positions: tensor of shape (num_queries,)\n\n        Returns:\n            tensor of shape (num_queries, embed_dim)\n        \"\"\"\n        freq = self.freq.to(seq_positions.device)\n        # seq_positions is of shape (num_queries,) but needs to be (1,num_queries,1)\n        embed = torch.cat(\n            (\n                torch.sin(\n                    0.5 * math.pi * seq_positions.unsqueeze(-1).unsqueeze(0) * freq\n                ),\n                torch.cos(\n                    0.5 * math.pi * seq_positions.unsqueeze(-1).unsqueeze(0) * freq\n                ),\n            ),\n            axis=-1,\n        ) / math.sqrt(len(freq))  # (B,N,2*n_components)\n\n        if self.d_model % self.n_components != 0:\n            raise ValueError(\n                f\"d_model ({self.d_model}) must be divisible by number of Fourier components n_components ({self.n_components})\"\n            )\n\n        # tile until shape is (B,N,embed_dim) to multiply with input queries/keys\n        embed = embed.repeat(\n            1, 1, self.d_model // (2 * self.n_components)\n        )  # 2*n_components to account for sin/cos\n\n        return embed\n</code></pre>"},{"location":"reference/dreem/models/embedding/#dreem.models.embedding.FourierPositionalEmbeddings.__init__","title":"<code>__init__(n_components, d_model)</code>","text":"<p>Initialize Fourier positional embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of frequencies for each dimension.</p> required <code>d_model</code> <code>int</code> <p>Model dimension.</p> required Source code in <code>dreem/models/embedding.py</code> <pre><code>def __init__(\n    self,\n    n_components: int,\n    d_model: int,\n):\n    \"\"\"Initialize Fourier positional embeddings.\n\n    Args:\n        n_components: Number of frequencies for each dimension.\n        d_model: Model dimension.\n    \"\"\"\n    super().__init__()\n    self.d_model = d_model\n    self.n_components = n_components\n    self.freq = torch.nn.Parameter(\n        _pos_embed_fourier1d_init(self.d_model, n_components)\n    )\n</code></pre>"},{"location":"reference/dreem/models/embedding/#dreem.models.embedding.FourierPositionalEmbeddings.forward","title":"<code>forward(seq_positions)</code>","text":"<p>Compute learnable fourier coefficients for each spatial/temporal position.</p> <p>Parameters:</p> Name Type Description Default <code>seq_positions</code> <code>Tensor</code> <p>tensor of shape (num_queries,)</p> required <p>Returns:</p> Type Description <p>tensor of shape (num_queries, embed_dim)</p> Source code in <code>dreem/models/embedding.py</code> <pre><code>def forward(self, seq_positions: torch.Tensor):\n    \"\"\"Compute learnable fourier coefficients for each spatial/temporal position.\n\n    Args:\n        seq_positions: tensor of shape (num_queries,)\n\n    Returns:\n        tensor of shape (num_queries, embed_dim)\n    \"\"\"\n    freq = self.freq.to(seq_positions.device)\n    # seq_positions is of shape (num_queries,) but needs to be (1,num_queries,1)\n    embed = torch.cat(\n        (\n            torch.sin(\n                0.5 * math.pi * seq_positions.unsqueeze(-1).unsqueeze(0) * freq\n            ),\n            torch.cos(\n                0.5 * math.pi * seq_positions.unsqueeze(-1).unsqueeze(0) * freq\n            ),\n        ),\n        axis=-1,\n    ) / math.sqrt(len(freq))  # (B,N,2*n_components)\n\n    if self.d_model % self.n_components != 0:\n        raise ValueError(\n            f\"d_model ({self.d_model}) must be divisible by number of Fourier components n_components ({self.n_components})\"\n        )\n\n    # tile until shape is (B,N,embed_dim) to multiply with input queries/keys\n    embed = embed.repeat(\n        1, 1, self.d_model // (2 * self.n_components)\n    )  # 2*n_components to account for sin/cos\n\n    return embed\n</code></pre>"},{"location":"reference/dreem/models/global_tracking_transformer/","title":"global_tracking_transformer","text":""},{"location":"reference/dreem/models/global_tracking_transformer/#dreem.models.global_tracking_transformer","title":"<code>dreem.models.global_tracking_transformer</code>","text":"<p>Module containing GTR model used for training.</p> <p>Classes:</p> Name Description <code>GlobalTrackingTransformer</code> <p>Modular GTR model composed of visual encoder + transformer used for tracking.</p>"},{"location":"reference/dreem/models/global_tracking_transformer/#dreem.models.global_tracking_transformer.GlobalTrackingTransformer","title":"<code>GlobalTrackingTransformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Modular GTR model composed of visual encoder + transformer used for tracking.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize GTR.</p> <code>extract_features</code> <p>Extract features from instances using visual encoder backbone.</p> <code>forward</code> <p>Execute forward pass of GTR Model to get asso matrix.</p> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>class GlobalTrackingTransformer(torch.nn.Module):\n    \"\"\"Modular GTR model composed of visual encoder + transformer used for tracking.\"\"\"\n\n    def __init__(\n        self,\n        encoder_cfg: dict | None = None,\n        d_model: int = 1024,\n        nhead: int = 8,\n        num_encoder_layers: int = 6,\n        num_decoder_layers: int = 6,\n        dropout: int = 0.1,\n        activation: str = \"relu\",\n        return_intermediate_dec: bool = False,\n        norm: bool = False,\n        num_layers_attn_head: int = 2,\n        dropout_attn_head: int = 0.1,\n        embedding_meta: dict | None = None,\n        return_embedding: bool = False,\n        decoder_self_attn: bool = False,\n    ):\n        \"\"\"Initialize GTR.\n\n        Args:\n            encoder_cfg: Dictionary of arguments to pass to the CNN constructor,\n                e.g: `cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}`\n            d_model: The number of features in the encoder/decoder inputs.\n            nhead: The number of heads in the transformer encoder/decoder.\n            num_encoder_layers: The number of encoder-layers in the encoder.\n            num_decoder_layers: The number of decoder-layers in the decoder.\n            dropout: Dropout value applied to the output of transformer layers.\n            activation: Activation function to use.\n            return_intermediate_dec: Return intermediate layers from decoder.\n            norm: If True, normalize output of encoder and decoder.\n            num_layers_attn_head: The number of layers in the attention head.\n            dropout_attn_head: Dropout value for the attention_head.\n            embedding_meta: Metadata for positional embeddings. See below.\n            return_embedding: Whether to return the positional embeddings\n            decoder_self_attn: If True, use decoder self attention.\n\n                More details on `embedding_meta`:\n                    By default this will be an empty dict and indicate\n                    that no positional embeddings should be used. To use the positional embeddings\n                    pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                    `{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},\n                    \"temp\": {'mode': 'learned', 'emb_num': 16}}`. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                    and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n        \"\"\"\n        super().__init__()\n\n        if not encoder_cfg:\n            encoder_cfg = {}\n        self.visual_encoder = create_visual_encoder(d_model=d_model, **encoder_cfg)\n\n        self.transformer = Transformer(\n            d_model=d_model,\n            nhead=nhead,\n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers,\n            dropout=dropout,\n            activation=activation,\n            return_intermediate_dec=return_intermediate_dec,\n            norm=norm,\n            num_layers_attn_head=num_layers_attn_head,\n            dropout_attn_head=dropout_attn_head,\n            embedding_meta=embedding_meta,\n            return_embedding=return_embedding,\n            decoder_self_attn=decoder_self_attn,\n            encoder_cfg=encoder_cfg,\n        )\n\n    def forward(\n        self, ref_instances: list[\"Instance\"], query_instances: list[\"Instance\"] = None\n    ) -&gt; list[\"AssociationMatrix\"]:\n        \"\"\"Execute forward pass of GTR Model to get asso matrix.\n\n        Args:\n            ref_instances: List of instances from chunk containing crops of objects + gt label info\n            query_instances: list of instances used as query in decoder.\n\n        Returns:\n            An N_T x N association matrix\n        \"\"\"\n        # Extract feature representations with pre-trained encoder.\n        self.extract_features(ref_instances)\n\n        if query_instances:\n            self.extract_features(query_instances)\n\n        asso_preds = self.transformer(ref_instances, query_instances)\n\n        return asso_preds\n\n    def extract_features(\n        self, instances: list[\"Instance\"], force_recompute: bool = False\n    ) -&gt; None:\n        \"\"\"Extract features from instances using visual encoder backbone.\n\n        Args:\n            instances: A list of instances to compute features for\n            force_recompute: indicate whether to compute features for all instances regardless of if they have instances\n        \"\"\"\n        if not force_recompute:\n            instances_to_compute = [\n                instance\n                for instance in instances\n                if instance.has_crop() and not instance.has_features()\n            ]\n        else:\n            instances_to_compute = instances\n\n        if len(instances_to_compute) == 0:\n            return\n        elif len(instances_to_compute) == 1:  # handle batch norm error when B=1\n            instances_to_compute = instances\n\n        crops = torch.concatenate([instance.crop for instance in instances_to_compute])\n\n        features = self.visual_encoder(crops)\n        features = features.to(device=instances_to_compute[0].device)\n\n        for i, z_i in enumerate(features):\n            instances_to_compute[i].features = z_i\n</code></pre>"},{"location":"reference/dreem/models/global_tracking_transformer/#dreem.models.global_tracking_transformer.GlobalTrackingTransformer.__init__","title":"<code>__init__(encoder_cfg=None, d_model=1024, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dropout=0.1, activation='relu', return_intermediate_dec=False, norm=False, num_layers_attn_head=2, dropout_attn_head=0.1, embedding_meta=None, return_embedding=False, decoder_self_attn=False)</code>","text":"<p>Initialize GTR.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_cfg</code> <code>dict | None</code> <p>Dictionary of arguments to pass to the CNN constructor, e.g: <code>cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}</code></p> <code>None</code> <code>d_model</code> <code>int</code> <p>The number of features in the encoder/decoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads in the transformer encoder/decoder.</p> <code>8</code> <code>num_encoder_layers</code> <code>int</code> <p>The number of encoder-layers in the encoder.</p> <code>6</code> <code>num_decoder_layers</code> <code>int</code> <p>The number of decoder-layers in the decoder.</p> <code>6</code> <code>dropout</code> <code>int</code> <p>Dropout value applied to the output of transformer layers.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>return_intermediate_dec</code> <code>bool</code> <p>Return intermediate layers from decoder.</p> <code>False</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of encoder and decoder.</p> <code>False</code> <code>num_layers_attn_head</code> <code>int</code> <p>The number of layers in the attention head.</p> <code>2</code> <code>dropout_attn_head</code> <code>int</code> <p>Dropout value for the attention_head.</p> <code>0.1</code> <code>embedding_meta</code> <code>dict | None</code> <p>Metadata for positional embeddings. See below.</p> <code>None</code> <code>return_embedding</code> <code>bool</code> <p>Whether to return the positional embeddings</p> <code>False</code> <code>decoder_self_attn</code> <code>bool</code> <p>If True, use decoder self attention.</p> <p>More details on <code>embedding_meta</code>:     By default this will be an empty dict and indicate     that no positional embeddings should be used. To use the positional embeddings     pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:     <code>{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},     \"temp\": {'mode': 'learned', 'emb_num': 16}}</code>. (see <code>dreem.models.embeddings.Embedding.EMB_TYPES</code>     and <code>dreem.models.embeddings.Embedding.EMB_MODES</code> for embedding parameters).</p> <code>False</code> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def __init__(\n    self,\n    encoder_cfg: dict | None = None,\n    d_model: int = 1024,\n    nhead: int = 8,\n    num_encoder_layers: int = 6,\n    num_decoder_layers: int = 6,\n    dropout: int = 0.1,\n    activation: str = \"relu\",\n    return_intermediate_dec: bool = False,\n    norm: bool = False,\n    num_layers_attn_head: int = 2,\n    dropout_attn_head: int = 0.1,\n    embedding_meta: dict | None = None,\n    return_embedding: bool = False,\n    decoder_self_attn: bool = False,\n):\n    \"\"\"Initialize GTR.\n\n    Args:\n        encoder_cfg: Dictionary of arguments to pass to the CNN constructor,\n            e.g: `cfg = {\"model_name\": \"resnet18\", \"pretrained\": False, \"in_chans\": 3}`\n        d_model: The number of features in the encoder/decoder inputs.\n        nhead: The number of heads in the transformer encoder/decoder.\n        num_encoder_layers: The number of encoder-layers in the encoder.\n        num_decoder_layers: The number of decoder-layers in the decoder.\n        dropout: Dropout value applied to the output of transformer layers.\n        activation: Activation function to use.\n        return_intermediate_dec: Return intermediate layers from decoder.\n        norm: If True, normalize output of encoder and decoder.\n        num_layers_attn_head: The number of layers in the attention head.\n        dropout_attn_head: Dropout value for the attention_head.\n        embedding_meta: Metadata for positional embeddings. See below.\n        return_embedding: Whether to return the positional embeddings\n        decoder_self_attn: If True, use decoder self attention.\n\n            More details on `embedding_meta`:\n                By default this will be an empty dict and indicate\n                that no positional embeddings should be used. To use the positional embeddings\n                pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                `{\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: True},\n                \"temp\": {'mode': 'learned', 'emb_num': 16}}`. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n    \"\"\"\n    super().__init__()\n\n    if not encoder_cfg:\n        encoder_cfg = {}\n    self.visual_encoder = create_visual_encoder(d_model=d_model, **encoder_cfg)\n\n    self.transformer = Transformer(\n        d_model=d_model,\n        nhead=nhead,\n        num_encoder_layers=num_encoder_layers,\n        num_decoder_layers=num_decoder_layers,\n        dropout=dropout,\n        activation=activation,\n        return_intermediate_dec=return_intermediate_dec,\n        norm=norm,\n        num_layers_attn_head=num_layers_attn_head,\n        dropout_attn_head=dropout_attn_head,\n        embedding_meta=embedding_meta,\n        return_embedding=return_embedding,\n        decoder_self_attn=decoder_self_attn,\n        encoder_cfg=encoder_cfg,\n    )\n</code></pre>"},{"location":"reference/dreem/models/global_tracking_transformer/#dreem.models.global_tracking_transformer.GlobalTrackingTransformer.extract_features","title":"<code>extract_features(instances, force_recompute=False)</code>","text":"<p>Extract features from instances using visual encoder backbone.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[Instance]</code> <p>A list of instances to compute features for</p> required <code>force_recompute</code> <code>bool</code> <p>indicate whether to compute features for all instances regardless of if they have instances</p> <code>False</code> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def extract_features(\n    self, instances: list[\"Instance\"], force_recompute: bool = False\n) -&gt; None:\n    \"\"\"Extract features from instances using visual encoder backbone.\n\n    Args:\n        instances: A list of instances to compute features for\n        force_recompute: indicate whether to compute features for all instances regardless of if they have instances\n    \"\"\"\n    if not force_recompute:\n        instances_to_compute = [\n            instance\n            for instance in instances\n            if instance.has_crop() and not instance.has_features()\n        ]\n    else:\n        instances_to_compute = instances\n\n    if len(instances_to_compute) == 0:\n        return\n    elif len(instances_to_compute) == 1:  # handle batch norm error when B=1\n        instances_to_compute = instances\n\n    crops = torch.concatenate([instance.crop for instance in instances_to_compute])\n\n    features = self.visual_encoder(crops)\n    features = features.to(device=instances_to_compute[0].device)\n\n    for i, z_i in enumerate(features):\n        instances_to_compute[i].features = z_i\n</code></pre>"},{"location":"reference/dreem/models/global_tracking_transformer/#dreem.models.global_tracking_transformer.GlobalTrackingTransformer.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute forward pass of GTR Model to get asso matrix.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>List of instances from chunk containing crops of objects + gt label info</p> required <code>query_instances</code> <code>list[Instance]</code> <p>list of instances used as query in decoder.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[AssociationMatrix]</code> <p>An N_T x N association matrix</p> Source code in <code>dreem/models/global_tracking_transformer.py</code> <pre><code>def forward(\n    self, ref_instances: list[\"Instance\"], query_instances: list[\"Instance\"] = None\n) -&gt; list[\"AssociationMatrix\"]:\n    \"\"\"Execute forward pass of GTR Model to get asso matrix.\n\n    Args:\n        ref_instances: List of instances from chunk containing crops of objects + gt label info\n        query_instances: list of instances used as query in decoder.\n\n    Returns:\n        An N_T x N association matrix\n    \"\"\"\n    # Extract feature representations with pre-trained encoder.\n    self.extract_features(ref_instances)\n\n    if query_instances:\n        self.extract_features(query_instances)\n\n    asso_preds = self.transformer(ref_instances, query_instances)\n\n    return asso_preds\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/","title":"gtr_runner","text":""},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner","title":"<code>dreem.models.gtr_runner</code>","text":"<p>Module containing training, validation and inference logic.</p> <p>Classes:</p> Name Description <code>GTRRunner</code> <p>A lightning wrapper around GTR model.</p>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner","title":"<code>GTRRunner</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A lightning wrapper around GTR model.</p> <p>Used for training, validation and inference.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize a lightning module for GTR.</p> <code>configure_optimizers</code> <p>Get optimizers and schedulers for training.</p> <code>forward</code> <p>Execute forward pass of the lightning module.</p> <code>log_metrics</code> <p>Log metrics computed during evaluation.</p> <code>on_test_end</code> <p>Run inference and metrics pipeline to compute metrics for test set.</p> <code>on_validation_epoch_end</code> <p>Execute hook for validation end.</p> <code>predict_step</code> <p>Run inference for model.</p> <code>test_step</code> <p>Execute single test step for model.</p> <code>training_step</code> <p>Execute single training step for model.</p> <code>validation_step</code> <p>Execute single val step for model.</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>class GTRRunner(LightningModule):\n    \"\"\"A lightning wrapper around GTR model.\n\n    Used for training, validation and inference.\n    \"\"\"\n\n    DEFAULT_METRICS = {\n        \"train\": [],\n        \"val\": [],\n        \"test\": [\"num_switches\", \"global_tracking_accuracy\"],\n    }\n    DEFAULT_TRACKING = {\n        \"train\": False,\n        \"val\": False,\n        \"test\": True,\n    }\n    DEFAULT_SAVE = {\"train\": False, \"val\": False, \"test\": False}\n\n    def __init__(\n        self,\n        model_cfg: dict | None = None,\n        tracker_cfg: dict | None = None,\n        loss_cfg: dict | None = None,\n        optimizer_cfg: dict | None = None,\n        scheduler_cfg: dict | None = None,\n        metrics: dict[str, list[str]] | None = None,\n        persistent_tracking: dict[str, bool] | None = None,\n        test_save_path: str = \"./test_results.h5\",\n    ):\n        \"\"\"Initialize a lightning module for GTR.\n\n        Args:\n            model_cfg: hyperparameters for GlobalTrackingTransformer\n            tracker_cfg: The parameters used for the tracker post-processing\n            loss_cfg: hyperparameters for AssoLoss\n            optimizer_cfg: hyper parameters used for optimizer.\n                       Only used to overwrite `configure_optimizer`\n            scheduler_cfg: hyperparameters for lr_scheduler used to overwrite `configure_optimizer\n            metrics: a dict containing the metrics to be computed during train, val, and test.\n            persistent_tracking: a dict containing whether to use persistent tracking during train, val and test inference.\n            test_save_path: path to a directory to save the eval and tracking results to\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.model_cfg = model_cfg if model_cfg else {}\n        self.loss_cfg = loss_cfg if loss_cfg else {}\n        self.tracker_cfg = tracker_cfg if tracker_cfg else {}\n\n        self.model = GlobalTrackingTransformer(**self.model_cfg)\n        self.loss = AssoLoss(**self.loss_cfg)\n        if self.tracker_cfg.get(\"tracker_type\", \"standard\") == \"batch\":\n            from dreem.inference.batch_tracker import BatchTracker\n\n            self.tracker = BatchTracker(**self.tracker_cfg)\n        else:\n            from dreem.inference.tracker import Tracker\n\n            self.tracker = Tracker(**self.tracker_cfg)\n        self.optimizer_cfg = optimizer_cfg\n        self.scheduler_cfg = scheduler_cfg\n\n        self.metrics = metrics if metrics is not None else self.DEFAULT_METRICS\n        self.persistent_tracking = (\n            persistent_tracking\n            if persistent_tracking is not None\n            else self.DEFAULT_TRACKING\n        )\n        self.test_results = {\"preds\": [], \"save_path\": test_save_path}\n\n    def forward(\n        self,\n        ref_instances: list[\"dreem.io.Instance\"],\n        query_instances: list[\"dreem.io.Instance\"] | None = None,\n    ) -&gt; list[\"AssociationMatrix\"]:\n        \"\"\"Execute forward pass of the lightning module.\n\n        Args:\n            ref_instances: a list of `Instance` objects containing crops and other data needed for transformer model\n            query_instances: a list of `Instance` objects used as queries in the decoder. Mostly used for inference.\n\n        Returns:\n            An association matrix between objects\n        \"\"\"\n        asso_preds = self.model(ref_instances, query_instances)\n        return asso_preds\n\n    def training_step(\n        self, train_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single training step for model.\n\n        Args:\n            train_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the train loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(train_batch[0], mode=\"train\")\n        self.log_metrics(result, len(train_batch[0]), \"train\")\n\n        return result\n\n    def validation_step(\n        self, val_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single val step for model.\n\n        Args:\n            val_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the val loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(val_batch[0], mode=\"val\")\n        self.log_metrics(result, len(val_batch[0]), \"val\")\n\n        return result\n\n    def test_step(\n        self, test_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; dict[str, float]:\n        \"\"\"Execute single test step for model.\n\n        Args:\n            test_batch: A single batch from the dataset which is a list of `Frame` objects\n                        with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A dict containing the val loss plus any other metrics specified\n        \"\"\"\n        result = self._shared_eval_step(test_batch[0], mode=\"test\")\n        self.log_metrics(result, len(test_batch[0]), \"test\")\n\n        return result\n\n    def predict_step(\n        self, batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n    ) -&gt; list[\"dreem.io.Frame\"]:\n        \"\"\"Run inference for model.\n\n        Computes association + assignment.\n\n        Args:\n            batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n            batch_idx: the batch number used by lightning\n\n        Returns:\n            A list of dicts where each dict is a frame containing the predicted track ids\n        \"\"\"\n        frames_pred = self.tracker(self.model, batch[0])\n        return frames_pred\n\n    def _shared_eval_step(\n        self, frames: list[\"dreem.io.Frame\"], mode: str\n    ) -&gt; dict[str, float]:\n        \"\"\"Run evaluation used by train, test, and val steps.\n\n        Args:\n            frames: A list of dicts where each dict is a frame containing gt data\n            mode: which metrics to compute and whether to use persistent tracking or not\n\n        Returns:\n            a dict containing the loss and any other metrics specified by `eval_metrics`\n        \"\"\"\n        try:\n            instances = [instance for frame in frames for instance in frame.instances]\n\n            if len(instances) == 0:\n                return None\n\n            # eval_metrics = self.metrics[mode]  # Currently unused but available for future metric computation\n\n            logits = self(instances)\n            logits = [asso.matrix for asso in logits]\n            loss = self.loss(logits, frames)\n\n            return_metrics = {\"loss\": loss}\n            if mode == \"test\":\n                self.tracker.persistent_tracking = True\n                frames_pred = self.tracker(self.model, frames)\n                self.test_results[\"preds\"].extend(\n                    [frame.to(\"cpu\") for frame in frames_pred]\n                )\n            return_metrics[\"batch_size\"] = len(frames)\n        except Exception as e:\n            logger.exception(\n                f\"Failed on frame {frames[0].frame_id} of video {frames[0].video_id}\"\n            )\n            logger.exception(e)\n            raise (e)\n\n        return return_metrics\n\n    def configure_optimizers(self) -&gt; dict:\n        \"\"\"Get optimizers and schedulers for training.\n\n        Is overridden by config but defaults to Adam + ReduceLROnPlateau.\n\n        Returns:\n            an optimizer config dict containing the optimizer, scheduler, and scheduler params\n        \"\"\"\n        # todo: init from config\n        if self.optimizer_cfg is None:\n            optimizer = torch.optim.Adam(self.parameters(), lr=1e-4, betas=(0.9, 0.999))\n        else:\n            optimizer = init_optimizer(self.parameters(), self.optimizer_cfg)\n\n        if self.scheduler_cfg is None:\n            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer, \"min\", 0.5, 10\n            )\n        else:\n            scheduler = init_scheduler(optimizer, self.scheduler_cfg)\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n                \"interval\": \"epoch\",\n                \"frequency\": 1,\n            },\n        }\n\n    def log_metrics(self, result: dict, batch_size: int, mode: str) -&gt; None:\n        \"\"\"Log metrics computed during evaluation.\n\n        Args:\n            result: A dict containing metrics to be logged.\n            batch_size: the size of the batch used to compute the metrics\n            mode: One of {'train', 'test' or 'val'}. Used as prefix while logging.\n        \"\"\"\n        if result:\n            batch_size = result.pop(\"batch_size\")\n            for metric, val in result.items():\n                if isinstance(val, torch.Tensor):\n                    val = val.item()\n                self.log(f\"{mode}_{metric}\", val, batch_size=batch_size)\n\n    def on_validation_epoch_end(self):\n        \"\"\"Execute hook for validation end.\n\n        Currently, we simply clear the gpu cache and do garbage collection.\n        \"\"\"\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def on_test_end(self):\n        \"\"\"Run inference and metrics pipeline to compute metrics for test set.\n\n        Args:\n            test_results: dict containing predictions and metrics to be filled out in metrics.evaluate\n            metrics: list of metrics to compute\n        \"\"\"\n        # input validation\n        metrics_to_compute = self.metrics[\n            \"test\"\n        ]  # list of metrics to compute, or \"all\"\n        if metrics_to_compute == \"all\":\n            metrics_to_compute = [\"motmetrics\", \"global_tracking_accuracy\"]\n        if isinstance(metrics_to_compute, str):\n            metrics_to_compute = [metrics_to_compute]\n        for metric in metrics_to_compute:\n            if metric not in [\"motmetrics\", \"global_tracking_accuracy\"]:\n                raise ValueError(\n                    f\"Metric {metric} not supported. Please select from 'motmetrics' or 'global_tracking_accuracy'\"\n                )\n\n        preds = self.test_results[\"preds\"]\n\n        # results is a dict with key being the metric name, and value being the metric value computed\n        results = metrics.evaluate(preds, metrics_to_compute)\n\n        # save metrics and frame metadata to hdf5\n\n        # Get the video name from the first frame\n        vid_name = Path(preds[0].vid_name).stem\n        # save the results to an hdf5 file\n        fname = os.path.join(\n            self.test_results[\"save_path\"], f\"{vid_name}.dreem_metrics.h5\"\n        )\n        logger.info(f\"Saving metrics to {fname}\")\n        # Check if the h5 file exists and add a suffix to prevent name collision\n        suffix_counter = 0\n        original_fname = fname\n        while os.path.exists(fname):\n            suffix_counter += 1\n            fname = original_fname.replace(\n                \".dreem_metrics.h5\", f\"_{suffix_counter}.dreem_metrics.h5\"\n            )\n\n        if suffix_counter &gt; 0:\n            logger.info(f\"File already exists. Saving to {fname} instead\")\n\n        with h5py.File(fname, \"a\") as results_file:\n            # Create a group for this video\n            vid_group = results_file.require_group(vid_name)\n            # Save each metric\n            for metric_name, value in results.items():\n                if metric_name == \"motmetrics\":\n                    # For num_switches, save mot_summary and mot_events separately\n                    mot_summary = value[0]\n                    mot_events = value[1]\n                    frame_switch_map = value[2]\n                    mot_summary_group = vid_group.require_group(\"mot_summary\")\n                    # Loop through each row in mot_summary and save as attributes\n                    for _, row in mot_summary.iterrows():\n                        mot_summary_group.attrs[row.name] = row[\"acc\"]\n                    # save extra metadata for frames in which there is a switch\n                    for frame_id, switch in frame_switch_map.items():\n                        frame = preds[frame_id]\n                        frame = frame.to(\"cpu\")\n                        if switch:\n                            _ = frame.to_h5(\n                                vid_group,\n                                frame.get_gt_track_ids().cpu().numpy(),\n                                save={\n                                    \"crop\": True,\n                                    \"features\": True,\n                                    \"embeddings\": True,\n                                },\n                            )\n                        else:\n                            _ = frame.to_h5(\n                                vid_group, frame.get_gt_track_ids().cpu().numpy()\n                            )\n                    # save motevents log to csv\n                    motevents_path = os.path.join(\n                        self.test_results[\"save_path\"], f\"{vid_name}.motevents.csv\"\n                    )\n                    logger.info(f\"Saving motevents log to {motevents_path}\")\n                    mot_events.to_csv(motevents_path, index=False)\n\n                elif metric_name == \"global_tracking_accuracy\":\n                    gta_by_gt_track = value\n                    gta_group = vid_group.require_group(\"global_tracking_accuracy\")\n                    # save as a key value pair with gt track id: gta\n                    for gt_track_id, gta in gta_by_gt_track.items():\n                        gta_group.attrs[f\"track_{gt_track_id}\"] = gta\n\n        # save the tracking results to a slp/labelled masks file\n        if isinstance(self.trainer.test_dataloaders.dataset, CellTrackingDataset):\n            outpath = os.path.join(\n                self.test_results[\"save_path\"],\n                f\"{vid_name}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.tif\",\n            )\n            pred_imgs = []\n            for frame in preds:\n                frame_masks = []\n                for instance in frame.instances:\n                    # centroid = instance.centroid[\"centroid\"]  # Currently unused but available if needed\n                    mask = instance.mask.cpu().numpy()\n                    track_id = instance.pred_track_id.cpu().numpy().item()\n                    mask = mask.astype(np.uint8)\n                    mask[mask != 0] = track_id  # label the mask with the track id\n                    frame_masks.append(mask)\n                frame_mask = np.max(frame_masks, axis=0)\n                pred_imgs.append(frame_mask)\n            pred_imgs = np.stack(pred_imgs)\n            tifffile.imwrite(outpath, pred_imgs.astype(np.uint16))\n        else:\n            outpath = os.path.join(\n                self.test_results[\"save_path\"],\n                f\"{vid_name}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.slp\",\n            )\n            pred_slp = []\n\n            logger.info(f\"Saving inference results to {outpath}\")\n            # save the tracking results to a slp file\n            tracks = {}\n            for frame in preds:\n                if frame.frame_id.item() == 0:\n                    video = (\n                        sio.Video(frame.video)\n                        if isinstance(frame.video, str)\n                        else sio.Video\n                    )\n                lf, tracks = frame.to_slp(tracks, video=video)\n                pred_slp.append(lf)\n            pred_slp = sio.Labels(pred_slp)\n\n            pred_slp.save(outpath)\n\n        # clear the preds\n        self.test_results[\"preds\"] = []\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.__init__","title":"<code>__init__(model_cfg=None, tracker_cfg=None, loss_cfg=None, optimizer_cfg=None, scheduler_cfg=None, metrics=None, persistent_tracking=None, test_save_path='./test_results.h5')</code>","text":"<p>Initialize a lightning module for GTR.</p> <p>Parameters:</p> Name Type Description Default <code>model_cfg</code> <code>dict | None</code> <p>hyperparameters for GlobalTrackingTransformer</p> <code>None</code> <code>tracker_cfg</code> <code>dict | None</code> <p>The parameters used for the tracker post-processing</p> <code>None</code> <code>loss_cfg</code> <code>dict | None</code> <p>hyperparameters for AssoLoss</p> <code>None</code> <code>optimizer_cfg</code> <code>dict | None</code> <p>hyper parameters used for optimizer.        Only used to overwrite <code>configure_optimizer</code></p> <code>None</code> <code>scheduler_cfg</code> <code>dict | None</code> <p>hyperparameters for lr_scheduler used to overwrite `configure_optimizer</p> <code>None</code> <code>metrics</code> <code>dict[str, list[str]] | None</code> <p>a dict containing the metrics to be computed during train, val, and test.</p> <code>None</code> <code>persistent_tracking</code> <code>dict[str, bool] | None</code> <p>a dict containing whether to use persistent tracking during train, val and test inference.</p> <code>None</code> <code>test_save_path</code> <code>str</code> <p>path to a directory to save the eval and tracking results to</p> <code>'./test_results.h5'</code> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def __init__(\n    self,\n    model_cfg: dict | None = None,\n    tracker_cfg: dict | None = None,\n    loss_cfg: dict | None = None,\n    optimizer_cfg: dict | None = None,\n    scheduler_cfg: dict | None = None,\n    metrics: dict[str, list[str]] | None = None,\n    persistent_tracking: dict[str, bool] | None = None,\n    test_save_path: str = \"./test_results.h5\",\n):\n    \"\"\"Initialize a lightning module for GTR.\n\n    Args:\n        model_cfg: hyperparameters for GlobalTrackingTransformer\n        tracker_cfg: The parameters used for the tracker post-processing\n        loss_cfg: hyperparameters for AssoLoss\n        optimizer_cfg: hyper parameters used for optimizer.\n                   Only used to overwrite `configure_optimizer`\n        scheduler_cfg: hyperparameters for lr_scheduler used to overwrite `configure_optimizer\n        metrics: a dict containing the metrics to be computed during train, val, and test.\n        persistent_tracking: a dict containing whether to use persistent tracking during train, val and test inference.\n        test_save_path: path to a directory to save the eval and tracking results to\n    \"\"\"\n    super().__init__()\n    self.save_hyperparameters()\n\n    self.model_cfg = model_cfg if model_cfg else {}\n    self.loss_cfg = loss_cfg if loss_cfg else {}\n    self.tracker_cfg = tracker_cfg if tracker_cfg else {}\n\n    self.model = GlobalTrackingTransformer(**self.model_cfg)\n    self.loss = AssoLoss(**self.loss_cfg)\n    if self.tracker_cfg.get(\"tracker_type\", \"standard\") == \"batch\":\n        from dreem.inference.batch_tracker import BatchTracker\n\n        self.tracker = BatchTracker(**self.tracker_cfg)\n    else:\n        from dreem.inference.tracker import Tracker\n\n        self.tracker = Tracker(**self.tracker_cfg)\n    self.optimizer_cfg = optimizer_cfg\n    self.scheduler_cfg = scheduler_cfg\n\n    self.metrics = metrics if metrics is not None else self.DEFAULT_METRICS\n    self.persistent_tracking = (\n        persistent_tracking\n        if persistent_tracking is not None\n        else self.DEFAULT_TRACKING\n    )\n    self.test_results = {\"preds\": [], \"save_path\": test_save_path}\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Get optimizers and schedulers for training.</p> <p>Is overridden by config but defaults to Adam + ReduceLROnPlateau.</p> <p>Returns:</p> Type Description <code>dict</code> <p>an optimizer config dict containing the optimizer, scheduler, and scheduler params</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def configure_optimizers(self) -&gt; dict:\n    \"\"\"Get optimizers and schedulers for training.\n\n    Is overridden by config but defaults to Adam + ReduceLROnPlateau.\n\n    Returns:\n        an optimizer config dict containing the optimizer, scheduler, and scheduler params\n    \"\"\"\n    # todo: init from config\n    if self.optimizer_cfg is None:\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4, betas=(0.9, 0.999))\n    else:\n        optimizer = init_optimizer(self.parameters(), self.optimizer_cfg)\n\n    if self.scheduler_cfg is None:\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, \"min\", 0.5, 10\n        )\n    else:\n        scheduler = init_scheduler(optimizer, self.scheduler_cfg)\n\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"monitor\": \"val_loss\",\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        },\n    }\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute forward pass of the lightning module.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>a list of <code>Instance</code> objects containing crops and other data needed for transformer model</p> required <code>query_instances</code> <code>list[Instance] | None</code> <p>a list of <code>Instance</code> objects used as queries in the decoder. Mostly used for inference.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[AssociationMatrix]</code> <p>An association matrix between objects</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def forward(\n    self,\n    ref_instances: list[\"dreem.io.Instance\"],\n    query_instances: list[\"dreem.io.Instance\"] | None = None,\n) -&gt; list[\"AssociationMatrix\"]:\n    \"\"\"Execute forward pass of the lightning module.\n\n    Args:\n        ref_instances: a list of `Instance` objects containing crops and other data needed for transformer model\n        query_instances: a list of `Instance` objects used as queries in the decoder. Mostly used for inference.\n\n    Returns:\n        An association matrix between objects\n    \"\"\"\n    asso_preds = self.model(ref_instances, query_instances)\n    return asso_preds\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.log_metrics","title":"<code>log_metrics(result, batch_size, mode)</code>","text":"<p>Log metrics computed during evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>dict</code> <p>A dict containing metrics to be logged.</p> required <code>batch_size</code> <code>int</code> <p>the size of the batch used to compute the metrics</p> required <code>mode</code> <code>str</code> <p>One of {'train', 'test' or 'val'}. Used as prefix while logging.</p> required Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def log_metrics(self, result: dict, batch_size: int, mode: str) -&gt; None:\n    \"\"\"Log metrics computed during evaluation.\n\n    Args:\n        result: A dict containing metrics to be logged.\n        batch_size: the size of the batch used to compute the metrics\n        mode: One of {'train', 'test' or 'val'}. Used as prefix while logging.\n    \"\"\"\n    if result:\n        batch_size = result.pop(\"batch_size\")\n        for metric, val in result.items():\n            if isinstance(val, torch.Tensor):\n                val = val.item()\n            self.log(f\"{mode}_{metric}\", val, batch_size=batch_size)\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.on_test_end","title":"<code>on_test_end()</code>","text":"<p>Run inference and metrics pipeline to compute metrics for test set.</p> <p>Parameters:</p> Name Type Description Default <code>test_results</code> <p>dict containing predictions and metrics to be filled out in metrics.evaluate</p> required <code>metrics</code> <p>list of metrics to compute</p> required Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def on_test_end(self):\n    \"\"\"Run inference and metrics pipeline to compute metrics for test set.\n\n    Args:\n        test_results: dict containing predictions and metrics to be filled out in metrics.evaluate\n        metrics: list of metrics to compute\n    \"\"\"\n    # input validation\n    metrics_to_compute = self.metrics[\n        \"test\"\n    ]  # list of metrics to compute, or \"all\"\n    if metrics_to_compute == \"all\":\n        metrics_to_compute = [\"motmetrics\", \"global_tracking_accuracy\"]\n    if isinstance(metrics_to_compute, str):\n        metrics_to_compute = [metrics_to_compute]\n    for metric in metrics_to_compute:\n        if metric not in [\"motmetrics\", \"global_tracking_accuracy\"]:\n            raise ValueError(\n                f\"Metric {metric} not supported. Please select from 'motmetrics' or 'global_tracking_accuracy'\"\n            )\n\n    preds = self.test_results[\"preds\"]\n\n    # results is a dict with key being the metric name, and value being the metric value computed\n    results = metrics.evaluate(preds, metrics_to_compute)\n\n    # save metrics and frame metadata to hdf5\n\n    # Get the video name from the first frame\n    vid_name = Path(preds[0].vid_name).stem\n    # save the results to an hdf5 file\n    fname = os.path.join(\n        self.test_results[\"save_path\"], f\"{vid_name}.dreem_metrics.h5\"\n    )\n    logger.info(f\"Saving metrics to {fname}\")\n    # Check if the h5 file exists and add a suffix to prevent name collision\n    suffix_counter = 0\n    original_fname = fname\n    while os.path.exists(fname):\n        suffix_counter += 1\n        fname = original_fname.replace(\n            \".dreem_metrics.h5\", f\"_{suffix_counter}.dreem_metrics.h5\"\n        )\n\n    if suffix_counter &gt; 0:\n        logger.info(f\"File already exists. Saving to {fname} instead\")\n\n    with h5py.File(fname, \"a\") as results_file:\n        # Create a group for this video\n        vid_group = results_file.require_group(vid_name)\n        # Save each metric\n        for metric_name, value in results.items():\n            if metric_name == \"motmetrics\":\n                # For num_switches, save mot_summary and mot_events separately\n                mot_summary = value[0]\n                mot_events = value[1]\n                frame_switch_map = value[2]\n                mot_summary_group = vid_group.require_group(\"mot_summary\")\n                # Loop through each row in mot_summary and save as attributes\n                for _, row in mot_summary.iterrows():\n                    mot_summary_group.attrs[row.name] = row[\"acc\"]\n                # save extra metadata for frames in which there is a switch\n                for frame_id, switch in frame_switch_map.items():\n                    frame = preds[frame_id]\n                    frame = frame.to(\"cpu\")\n                    if switch:\n                        _ = frame.to_h5(\n                            vid_group,\n                            frame.get_gt_track_ids().cpu().numpy(),\n                            save={\n                                \"crop\": True,\n                                \"features\": True,\n                                \"embeddings\": True,\n                            },\n                        )\n                    else:\n                        _ = frame.to_h5(\n                            vid_group, frame.get_gt_track_ids().cpu().numpy()\n                        )\n                # save motevents log to csv\n                motevents_path = os.path.join(\n                    self.test_results[\"save_path\"], f\"{vid_name}.motevents.csv\"\n                )\n                logger.info(f\"Saving motevents log to {motevents_path}\")\n                mot_events.to_csv(motevents_path, index=False)\n\n            elif metric_name == \"global_tracking_accuracy\":\n                gta_by_gt_track = value\n                gta_group = vid_group.require_group(\"global_tracking_accuracy\")\n                # save as a key value pair with gt track id: gta\n                for gt_track_id, gta in gta_by_gt_track.items():\n                    gta_group.attrs[f\"track_{gt_track_id}\"] = gta\n\n    # save the tracking results to a slp/labelled masks file\n    if isinstance(self.trainer.test_dataloaders.dataset, CellTrackingDataset):\n        outpath = os.path.join(\n            self.test_results[\"save_path\"],\n            f\"{vid_name}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.tif\",\n        )\n        pred_imgs = []\n        for frame in preds:\n            frame_masks = []\n            for instance in frame.instances:\n                # centroid = instance.centroid[\"centroid\"]  # Currently unused but available if needed\n                mask = instance.mask.cpu().numpy()\n                track_id = instance.pred_track_id.cpu().numpy().item()\n                mask = mask.astype(np.uint8)\n                mask[mask != 0] = track_id  # label the mask with the track id\n                frame_masks.append(mask)\n            frame_mask = np.max(frame_masks, axis=0)\n            pred_imgs.append(frame_mask)\n        pred_imgs = np.stack(pred_imgs)\n        tifffile.imwrite(outpath, pred_imgs.astype(np.uint16))\n    else:\n        outpath = os.path.join(\n            self.test_results[\"save_path\"],\n            f\"{vid_name}.dreem_inference.{datetime.now().strftime('%m-%d-%Y-%H-%M-%S')}.slp\",\n        )\n        pred_slp = []\n\n        logger.info(f\"Saving inference results to {outpath}\")\n        # save the tracking results to a slp file\n        tracks = {}\n        for frame in preds:\n            if frame.frame_id.item() == 0:\n                video = (\n                    sio.Video(frame.video)\n                    if isinstance(frame.video, str)\n                    else sio.Video\n                )\n            lf, tracks = frame.to_slp(tracks, video=video)\n            pred_slp.append(lf)\n        pred_slp = sio.Labels(pred_slp)\n\n        pred_slp.save(outpath)\n\n    # clear the preds\n    self.test_results[\"preds\"] = []\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.on_validation_epoch_end","title":"<code>on_validation_epoch_end()</code>","text":"<p>Execute hook for validation end.</p> <p>Currently, we simply clear the gpu cache and do garbage collection.</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Execute hook for validation end.\n\n    Currently, we simply clear the gpu cache and do garbage collection.\n    \"\"\"\n    gc.collect()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.predict_step","title":"<code>predict_step(batch, batch_idx)</code>","text":"<p>Run inference for model.</p> <p>Computes association + assignment.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects     with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>list[Frame]</code> <p>A list of dicts where each dict is a frame containing the predicted track ids</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def predict_step(\n    self, batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; list[\"dreem.io.Frame\"]:\n    \"\"\"Run inference for model.\n\n    Computes association + assignment.\n\n    Args:\n        batch: A single batch from the dataset which is a list of `Frame` objects\n                with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A list of dicts where each dict is a frame containing the predicted track ids\n    \"\"\"\n    frames_pred = self.tracker(self.model, batch[0])\n    return frames_pred\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.test_step","title":"<code>test_step(test_batch, batch_idx)</code>","text":"<p>Execute single test step for model.</p> <p>Parameters:</p> Name Type Description Default <code>test_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the val loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def test_step(\n    self, test_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single test step for model.\n\n    Args:\n        test_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the val loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(test_batch[0], mode=\"test\")\n    self.log_metrics(result, len(test_batch[0]), \"test\")\n\n    return result\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.training_step","title":"<code>training_step(train_batch, batch_idx)</code>","text":"<p>Execute single training step for model.</p> <p>Parameters:</p> Name Type Description Default <code>train_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the train loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def training_step(\n    self, train_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single training step for model.\n\n    Args:\n        train_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the train loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(train_batch[0], mode=\"train\")\n    self.log_metrics(result, len(train_batch[0]), \"train\")\n\n    return result\n</code></pre>"},{"location":"reference/dreem/models/gtr_runner/#dreem.models.gtr_runner.GTRRunner.validation_step","title":"<code>validation_step(val_batch, batch_idx)</code>","text":"<p>Execute single val step for model.</p> <p>Parameters:</p> Name Type Description Default <code>val_batch</code> <code>list[list[Frame]]</code> <p>A single batch from the dataset which is a list of <code>Frame</code> objects         with length <code>clip_length</code> containing Instances and other metadata.</p> required <code>batch_idx</code> <code>int</code> <p>the batch number used by lightning</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dict containing the val loss plus any other metrics specified</p> Source code in <code>dreem/models/gtr_runner.py</code> <pre><code>def validation_step(\n    self, val_batch: list[list[\"dreem.io.Frame\"]], batch_idx: int\n) -&gt; dict[str, float]:\n    \"\"\"Execute single val step for model.\n\n    Args:\n        val_batch: A single batch from the dataset which is a list of `Frame` objects\n                    with length `clip_length` containing Instances and other metadata.\n        batch_idx: the batch number used by lightning\n\n    Returns:\n        A dict containing the val loss plus any other metrics specified\n    \"\"\"\n    result = self._shared_eval_step(val_batch[0], mode=\"val\")\n    self.log_metrics(result, len(val_batch[0]), \"val\")\n\n    return result\n</code></pre>"},{"location":"reference/dreem/models/mlp/","title":"mlp","text":""},{"location":"reference/dreem/models/mlp/#dreem.models.mlp","title":"<code>dreem.models.mlp</code>","text":"<p>Multi-Layer Perceptron (MLP) module.</p> <p>Classes:</p> Name Description <code>MLP</code> <p>Multi-Layer Perceptron (MLP) module.</p>"},{"location":"reference/dreem/models/mlp/#dreem.models.mlp.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>Multi-Layer Perceptron (MLP) module.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize MLP.</p> <code>forward</code> <p>Forward pass of the MLP.</p> Source code in <code>dreem/models/mlp.py</code> <pre><code>class MLP(torch.nn.Module):\n    \"\"\"Multi-Layer Perceptron (MLP) module.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        output_dim: int,\n        num_layers: int,\n        dropout: float = 0.0,\n    ):\n        \"\"\"Initialize MLP.\n\n        Args:\n            input_dim: Dimensionality of the input features.\n            hidden_dim: Number of units in the hidden layers.\n            output_dim: Dimensionality of the output features.\n            num_layers: Number of hidden layers.\n            dropout: Dropout probability.\n        \"\"\"\n        super().__init__()\n\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        if self.num_layers &gt; 0:\n            h = [hidden_dim] * (num_layers - 1)\n            self.layers = torch.nn.ModuleList(\n                [\n                    torch.nn.Linear(n, k)\n                    for n, k in zip([input_dim] + h, h + [output_dim])\n                ]\n            )\n            if self.dropout &gt; 0.0:\n                self.dropouts = torch.nn.ModuleList(\n                    [torch.nn.Dropout(dropout) for _ in range(self.num_layers - 1)]\n                )\n        else:\n            self.layers = []\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of the MLP.\n\n        Args:\n            x: Input tensor of shape (batch_size, num_instances, input_dim).\n\n        Returns:\n            Output tensor of shape (batch_size, num_instances, output_dim).\n        \"\"\"\n        for i, layer in enumerate(self.layers):\n            x = F.relu(layer(x)) if i &lt; self.num_layers - 1 else layer(x)\n            if i &lt; self.num_layers - 1 and self.dropout &gt; 0.0:\n                x = self.dropouts[i](x)\n\n        return x\n</code></pre>"},{"location":"reference/dreem/models/mlp/#dreem.models.mlp.MLP.__init__","title":"<code>__init__(input_dim, hidden_dim, output_dim, num_layers, dropout=0.0)</code>","text":"<p>Initialize MLP.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of the input features.</p> required <code>hidden_dim</code> <code>int</code> <p>Number of units in the hidden layers.</p> required <code>output_dim</code> <code>int</code> <p>Dimensionality of the output features.</p> required <code>num_layers</code> <code>int</code> <p>Number of hidden layers.</p> required <code>dropout</code> <code>float</code> <p>Dropout probability.</p> <code>0.0</code> Source code in <code>dreem/models/mlp.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    hidden_dim: int,\n    output_dim: int,\n    num_layers: int,\n    dropout: float = 0.0,\n):\n    \"\"\"Initialize MLP.\n\n    Args:\n        input_dim: Dimensionality of the input features.\n        hidden_dim: Number of units in the hidden layers.\n        output_dim: Dimensionality of the output features.\n        num_layers: Number of hidden layers.\n        dropout: Dropout probability.\n    \"\"\"\n    super().__init__()\n\n    self.num_layers = num_layers\n    self.dropout = dropout\n\n    if self.num_layers &gt; 0:\n        h = [hidden_dim] * (num_layers - 1)\n        self.layers = torch.nn.ModuleList(\n            [\n                torch.nn.Linear(n, k)\n                for n, k in zip([input_dim] + h, h + [output_dim])\n            ]\n        )\n        if self.dropout &gt; 0.0:\n            self.dropouts = torch.nn.ModuleList(\n                [torch.nn.Dropout(dropout) for _ in range(self.num_layers - 1)]\n            )\n    else:\n        self.layers = []\n</code></pre>"},{"location":"reference/dreem/models/mlp/#dreem.models.mlp.MLP.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the MLP.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, num_instances, input_dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, num_instances, output_dim).</p> Source code in <code>dreem/models/mlp.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the MLP.\n\n    Args:\n        x: Input tensor of shape (batch_size, num_instances, input_dim).\n\n    Returns:\n        Output tensor of shape (batch_size, num_instances, output_dim).\n    \"\"\"\n    for i, layer in enumerate(self.layers):\n        x = F.relu(layer(x)) if i &lt; self.num_layers - 1 else layer(x)\n        if i &lt; self.num_layers - 1 and self.dropout &gt; 0.0:\n            x = self.dropouts[i](x)\n\n    return x\n</code></pre>"},{"location":"reference/dreem/models/model_utils/","title":"model_utils","text":""},{"location":"reference/dreem/models/model_utils/#dreem.models.model_utils","title":"<code>dreem.models.model_utils</code>","text":"<p>Module containing model helper functions.</p> <p>Functions:</p> Name Description <code>get_boxes</code> <p>Extract the bounding boxes from the input list of instances.</p> <code>get_device</code> <p>Utility function to get available device.</p> <code>get_times</code> <p>Extract the time indices of each instance relative to the window length.</p> <code>init_logger</code> <p>Initialize logger based on config parameters.</p> <code>init_optimizer</code> <p>Initialize optimizer based on config parameters.</p> <code>init_scheduler</code> <p>Initialize scheduler based on config parameters.</p> <code>softmax_asso</code> <p>Apply the softmax activation function on asso_output.</p>"},{"location":"reference/dreem/models/model_utils/#dreem.models.model_utils.get_boxes","title":"<code>get_boxes(instances)</code>","text":"<p>Extract the bounding boxes from the input list of instances.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>list[Instance]</code> <p>List of Instance objects.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>An (n_instances, n_points, 4) float tensor containing the bounding boxes normalized by the height and width of the image</p> Source code in <code>dreem/models/model_utils.py</code> <pre><code>def get_boxes(instances: list[\"dreem.io.Instance\"]) -&gt; torch.Tensor:\n    \"\"\"Extract the bounding boxes from the input list of instances.\n\n    Args:\n        instances: List of Instance objects.\n\n    Returns:\n        An (n_instances, n_points, 4) float tensor containing the bounding boxes\n        normalized by the height and width of the image\n    \"\"\"\n    boxes = []\n    for i, instance in enumerate(instances):\n        _, h, w = instance.frame.img_shape.flatten()\n        bbox = instance.bbox.clone()\n        bbox[:, :, [0, 2]] /= w\n        bbox[:, :, [1, 3]] /= h\n        boxes.append(bbox)\n\n    boxes = torch.cat(boxes, dim=0)  # N, n_anchors, 4\n\n    return boxes\n</code></pre>"},{"location":"reference/dreem/models/model_utils/#dreem.models.model_utils.get_device","title":"<code>get_device()</code>","text":"<p>Utility function to get available device.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The available device (one of 'cuda', 'mps', or 'cpu').</p> Source code in <code>dreem/models/model_utils.py</code> <pre><code>def get_device() -&gt; str:\n    \"\"\"Utility function to get available device.\n\n    Returns:\n        str: The available device (one of 'cuda', 'mps', or 'cpu').\n    \"\"\"\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif torch.backends.mps.is_available():\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    return device\n</code></pre>"},{"location":"reference/dreem/models/model_utils/#dreem.models.model_utils.get_times","title":"<code>get_times(ref_instances, query_instances=None)</code>","text":"<p>Extract the time indices of each instance relative to the window length.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>Set of instances to query against</p> required <code>query_instances</code> <code>list[Instance] | None</code> <p>Set of query instances to look up using decoder.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Tuple of Corresponding frame indices eg [0, 0, 1, 1, ..., T, T] for ref and query instances.</p> Source code in <code>dreem/models/model_utils.py</code> <pre><code>def get_times(\n    ref_instances: list[\"dreem.io.Instance\"],\n    query_instances: list[\"dreem.io.Instance\"] | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Extract the time indices of each instance relative to the window length.\n\n    Args:\n        ref_instances: Set of instances to query against\n        query_instances: Set of query instances to look up using decoder.\n\n    Returns:\n        Tuple of Corresponding frame indices eg [0, 0, 1, 1, ..., T, T] for ref and query instances.\n    \"\"\"\n    ref_inds = torch.tensor(\n        [instance.frame.frame_id.item() for instance in ref_instances],\n        device=ref_instances[0].device,\n    )\n\n    if query_instances is not None:\n        query_inds = torch.tensor(\n            [instance.frame.frame_id.item() for instance in query_instances],\n            device=ref_inds.device,\n        )\n    else:\n        query_inds = torch.tensor([], device=ref_inds.device)\n\n    frame_inds = torch.concat([ref_inds, query_inds])\n    window_length = len(frame_inds.unique())\n\n    frame_idx_mapping = {frame_inds.unique()[i].item(): i for i in range(window_length)}\n    ref_t = torch.tensor(\n        [frame_idx_mapping[ind.item()] for ind in ref_inds], device=ref_inds.device\n    )\n\n    query_t = torch.tensor(\n        [frame_idx_mapping[ind.item()] for ind in query_inds], device=ref_inds.device\n    )\n\n    return ref_t, query_t\n</code></pre>"},{"location":"reference/dreem/models/model_utils/#dreem.models.model_utils.init_logger","title":"<code>init_logger(logger_params, config=None)</code>","text":"<p>Initialize logger based on config parameters.</p> <p>Allows more flexibility in choosing which logger to use.</p> <p>Parameters:</p> Name Type Description Default <code>logger_params</code> <code>dict</code> <p>logger hyperparameters</p> required <code>config</code> <code>dict | None</code> <p>rest of hyperparameters to log (mostly used for WandB)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>logger</code> <code>Logger</code> <p>A logger with specified params (or None).</p> Source code in <code>dreem/models/model_utils.py</code> <pre><code>def init_logger(logger_params: dict, config: dict | None = None) -&gt; loggers.Logger:\n    \"\"\"Initialize logger based on config parameters.\n\n    Allows more flexibility in choosing which logger to use.\n\n    Args:\n        logger_params: logger hyperparameters\n        config: rest of hyperparameters to log (mostly used for WandB)\n\n    Returns:\n        logger: A logger with specified params (or None).\n    \"\"\"\n    logger_type = logger_params.pop(\"logger_type\", None)\n\n    valid_loggers = [\n        \"CSVLogger\",\n        \"TensorBoardLogger\",\n        \"WandbLogger\",\n    ]\n\n    if logger_type in valid_loggers:\n        logger_class = getattr(loggers, logger_type)\n        if logger_class == loggers.WandbLogger:\n            try:\n                return logger_class(config=config, **logger_params)\n            except Exception as e:\n                print(e, logger_type)\n        else:\n            try:\n                return logger_class(**logger_params)\n            except Exception as e:\n                print(e, logger_type)\n    else:\n        print(\n            f\"{logger_type} not one of {valid_loggers} or set to None, skipping logging\"\n        )\n        return None\n</code></pre>"},{"location":"reference/dreem/models/model_utils/#dreem.models.model_utils.init_optimizer","title":"<code>init_optimizer(params, config)</code>","text":"<p>Initialize optimizer based on config parameters.</p> <p>Allows more flexibility in which optimizer to use</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable</code> <p>model parameters to be optimized</p> required <code>config</code> <code>dict</code> <p>optimizer hyperparameters including optimizer name</p> required <p>Returns:</p> Name Type Description <code>optimizer</code> <code>Optimizer</code> <p>A torch.Optimizer with specified params</p> Source code in <code>dreem/models/model_utils.py</code> <pre><code>def init_optimizer(params: Iterable, config: dict) -&gt; torch.optim.Optimizer:\n    \"\"\"Initialize optimizer based on config parameters.\n\n    Allows more flexibility in which optimizer to use\n\n    Args:\n        params: model parameters to be optimized\n        config: optimizer hyperparameters including optimizer name\n\n    Returns:\n        optimizer: A torch.Optimizer with specified params\n    \"\"\"\n    if config is None:\n        config = {\"name\": \"Adam\"}\n    optimizer = config.get(\"name\", \"Adam\")\n    optimizer_params = {\n        param: val for param, val in config.items() if param.lower() != \"name\"\n    }\n\n    try:\n        optimizer_class = getattr(torch.optim, optimizer)\n    except AttributeError:\n        if optimizer_class is None:\n            print(\n                f\"Couldn't instantiate {optimizer} as given. Trying with capitalization\"\n            )\n            optimizer_class = getattr(torch.optim, optimizer.lower().capitalize())\n        if optimizer_class is None:\n            print(\n                f\"Couldn't instantiate {optimizer} with capitalization, Final attempt with all caps\"\n            )\n            optimizer_class = getattr(torch.optim, optimizer.upper(), None)\n\n    if optimizer_class is None:\n        raise ValueError(f\"Unsupported optimizer type: {optimizer}\")\n\n    return optimizer_class(params, **optimizer_params)\n</code></pre>"},{"location":"reference/dreem/models/model_utils/#dreem.models.model_utils.init_scheduler","title":"<code>init_scheduler(optimizer, config)</code>","text":"<p>Initialize scheduler based on config parameters.</p> <p>Allows more flexibility in choosing which scheduler to use.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>optimizer for which to adjust lr</p> required <code>config</code> <code>dict</code> <p>lr scheduler hyperparameters including scheduler name</p> required <p>Returns:</p> Name Type Description <code>scheduler</code> <code>LRScheduler</code> <p>A scheduler with specified params</p> Source code in <code>dreem/models/model_utils.py</code> <pre><code>def init_scheduler(\n    optimizer: torch.optim.Optimizer, config: dict\n) -&gt; torch.optim.lr_scheduler.LRScheduler:\n    \"\"\"Initialize scheduler based on config parameters.\n\n    Allows more flexibility in choosing which scheduler to use.\n\n    Args:\n        optimizer: optimizer for which to adjust lr\n        config: lr scheduler hyperparameters including scheduler name\n\n    Returns:\n        scheduler: A scheduler with specified params\n    \"\"\"\n    if config is None:\n        return None\n    scheduler = config.get(\"name\")\n\n    if scheduler is None:\n        scheduler = \"ReduceLROnPlateau\"\n\n    scheduler_params = {\n        param: val for param, val in config.items() if param.lower() != \"name\"\n    }\n\n    try:\n        # if a list is provided, apply each one sequentially\n        if isinstance(scheduler, list):\n            schedulers = []\n            milestones = scheduler_params.get(\"milestones\", None)\n            for ix, s in enumerate(scheduler):\n                params = scheduler_params[str(ix)]\n                schedulers.append(\n                    getattr(torch.optim.lr_scheduler, s)(optimizer, **params)\n                )\n            scheduler_class = torch.optim.lr_scheduler.SequentialLR(\n                optimizer, schedulers, milestones\n            )\n            return scheduler_class\n\n        else:\n            scheduler_class = getattr(torch.optim.lr_scheduler, scheduler)\n    except AttributeError:\n        if scheduler_class is None:\n            print(\n                f\"Couldn't instantiate {scheduler} as given. Trying with capitalization\"\n            )\n            scheduler_class = getattr(\n                torch.optim.lr_scheduler, scheduler.lower().capitalize()\n            )\n        if scheduler_class is None:\n            print(\n                f\"Couldn't instantiate {scheduler} with capitalization, Final attempt with all caps\"\n            )\n            scheduler_class = getattr(torch.optim.lr_scheduler, scheduler.upper(), None)\n\n    if scheduler_class is None:\n        raise ValueError(f\"Unsupported optimizer type: {scheduler}\")\n\n    return scheduler_class(optimizer, **scheduler_params)\n</code></pre>"},{"location":"reference/dreem/models/model_utils/#dreem.models.model_utils.softmax_asso","title":"<code>softmax_asso(asso_output)</code>","text":"<p>Apply the softmax activation function on asso_output.</p> <p>Parameters:</p> Name Type Description Default <code>asso_output</code> <code>list[Tensor]</code> <p>Raw logits output of the tracking transformer. A list of torch tensors of shape (T, N_t, N_i) where:     T: the length of the window     N_t: number of instances in current/query frame (rightmost frame         of the window).     N_i: number of detected instances in i-th frame of window.</p> required <p>Returns:</p> Name Type Description <code>asso_output</code> <code>list[Tensor]</code> <p>Probabilities following softmax function, with same shape     as input.</p> Source code in <code>dreem/models/model_utils.py</code> <pre><code>def softmax_asso(asso_output: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n    \"\"\"Apply the softmax activation function on asso_output.\n\n    Args:\n        asso_output: Raw logits output of the tracking transformer. A list of\n            torch tensors of shape (T, N_t, N_i) where:\n                T: the length of the window\n                N_t: number of instances in current/query frame (rightmost frame\n                    of the window).\n                N_i: number of detected instances in i-th frame of window.\n\n    Returns:\n        asso_output: Probabilities following softmax function, with same shape\n            as input.\n    \"\"\"\n    asso_active = []\n    for asso in asso_output:\n        asso = torch.cat([asso, asso.new_zeros((asso.shape[0], 1))], dim=1).softmax(\n            dim=1\n        )[:, :-1]\n        asso_active.append(asso)\n\n    return asso_active\n</code></pre>"},{"location":"reference/dreem/models/transformer/","title":"transformer","text":""},{"location":"reference/dreem/models/transformer/#dreem.models.transformer","title":"<code>dreem.models.transformer</code>","text":"<p>DETR Transformer class.</p> <p>Copyright \u00a9 Facebook, Inc. and its affiliates. All Rights Reserved</p> <ul> <li>Modified from https://github.com/facebookresearch/detr/blob/main/models/transformer.py</li> <li>Modified from https://github.com/xingyizhou/GTR/blob/master/gtr/modeling/roi_heads/transformer.py</li> <li>Modifications:<ul> <li>positional encodings are passed in MHattention</li> <li>extra LN at the end of encoder is removed</li> <li>decoder returns a stack of activations from all decoding layers</li> <li>added fixed embeddings over boxes</li> </ul> </li> </ul> <p>Classes:</p> Name Description <code>Transformer</code> <p>Transformer class.</p> <code>TransformerDecoder</code> <p>Transformer Decoder Block composed of Transformer Decoder Layers.</p> <code>TransformerDecoderLayer</code> <p>A single transformer decoder layer.</p> <code>TransformerEncoder</code> <p>A transformer encoder block composed of encoder layers.</p> <code>TransformerEncoderLayer</code> <p>A single transformer encoder layer.</p> <p>Functions:</p> Name Description <code>apply_fourier_embeddings</code> <p>Apply fourier embeddings to queries.</p>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.Transformer","title":"<code>Transformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer class.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize Transformer.</p> <code>forward</code> <p>Execute a forward pass through the transformer and attention head.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class Transformer(torch.nn.Module):\n    \"\"\"Transformer class.\"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 1024,\n        nhead: int = 8,\n        num_encoder_layers: int = 6,\n        num_decoder_layers: int = 6,\n        dropout: float = 0.1,\n        activation: str = \"relu\",\n        return_intermediate_dec: bool = False,\n        norm: bool = False,\n        num_layers_attn_head: int = 2,\n        dropout_attn_head: float = 0.1,\n        embedding_meta: dict | None = None,\n        return_embedding: bool = False,\n        decoder_self_attn: bool = False,\n        encoder_cfg: dict | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize Transformer.\n\n        Args:\n            d_model: The number of features in the encoder/decoder inputs.\n            nhead: The number of heads in the transformer encoder/decoder.\n            num_encoder_layers: The number of encoder-layers in the encoder.\n            num_decoder_layers: The number of decoder-layers in the decoder.\n            dropout: Dropout value applied to the output of transformer layers.\n            activation: Activation function to use.\n            return_intermediate_dec: Return intermediate layers from decoder.\n            norm: If True, normalize output of encoder and decoder.\n            num_layers_attn_head: The number of layers in the attention head.\n            dropout_attn_head: Dropout value for the attention_head.\n            embedding_meta: Metadata for positional embeddings. See below.\n            return_embedding: Whether to return the positional embeddings\n            decoder_self_attn: If True, use decoder self attention.\n            encoder_cfg: Encoder configuration.\n\n                More details on `embedding_meta`:\n                    By default this will be an empty dict and indicate\n                    that no positional embeddings should be used. To use the positional embeddings\n                    pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                    {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},\n                    \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                    and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n        \"\"\"\n        super().__init__()\n\n        self.d_model = dim_feedforward = feature_dim_attn_head = d_model\n\n        self.embedding_meta = embedding_meta\n        self.return_embedding = return_embedding\n        self.encoder_cfg = encoder_cfg\n\n        self.pos_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n        self.temp_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n\n        if self.embedding_meta:\n            if \"pos\" in self.embedding_meta:\n                pos_emb_cfg = self.embedding_meta[\"pos\"]\n                if pos_emb_cfg:\n                    self.pos_emb = Embedding(\n                        emb_type=\"pos\", features=self.d_model, **pos_emb_cfg\n                    )\n            if \"temp\" in self.embedding_meta:\n                temp_emb_cfg = self.embedding_meta[\"temp\"]\n                if temp_emb_cfg:\n                    self.temp_emb = Embedding(\n                        emb_type=\"temp\", features=self.d_model, **temp_emb_cfg\n                    )\n\n        self.fourier_embeddings = FourierPositionalEmbeddings(\n            n_components=8, d_model=d_model\n        )\n\n        # Transformer Encoder\n        encoder_layer = TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, activation, norm\n        )\n\n        encoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n        # only used if using descriptor visual encoder; default resnet encoder uses d_model directly\n        if self.encoder_cfg and \"encoder_type\" in self.encoder_cfg:\n            self.visual_feat_dim = (\n                self.encoder_cfg[\"ndim\"] if \"ndim\" in self.encoder_cfg else 5\n            )  # 5 is default for descriptor\n            self.fourier_proj = nn.Linear(self.d_model + self.visual_feat_dim, d_model)\n            self.fourier_norm = nn.LayerNorm(self.d_model)\n\n        self.encoder = TransformerEncoder(\n            encoder_layer, num_encoder_layers, encoder_norm\n        )\n\n        # Transformer Decoder\n        decoder_layer = TransformerDecoderLayer(\n            d_model,\n            nhead,\n            dim_feedforward,\n            dropout,\n            activation,\n            norm,\n            decoder_self_attn,\n        )\n\n        decoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n        self.decoder = TransformerDecoder(\n            decoder_layer, num_decoder_layers, return_intermediate_dec, decoder_norm\n        )\n\n        # Transformer attention head\n        self.attn_head = ATTWeightHead(\n            feature_dim=feature_dim_attn_head,\n            num_layers=num_layers_attn_head,\n            dropout=dropout_attn_head,\n        )\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        \"\"\"Initialize model weights from xavier distribution.\"\"\"\n        for p in self.parameters():\n            if not torch.nn.parameter.is_lazy(p) and p.dim() &gt; 1:\n                try:\n                    nn.init.xavier_uniform_(p)\n                except ValueError as e:\n                    print(f\"Failed Trying to initialize {p}\")\n                    raise (e)\n\n    def forward(\n        self,\n        ref_instances: list[\"dreem.io.Instance\"],\n        query_instances: list[\"dreem.io.Instance\"] | None = None,\n    ) -&gt; list[AssociationMatrix]:\n        \"\"\"Execute a forward pass through the transformer and attention head.\n\n        Args:\n            ref_instances: A list of instance objects (See `dreem.io.Instance` for more info.)\n            query_instances: An set of instances to be used as decoder queries.\n\n        Returns:\n            asso_output: A list of torch.Tensors of shape (L, n_query, total_instances) where:\n                L: number of decoder blocks\n                n_query: number of instances in current query/frame\n                total_instances: number of instances in window\n        \"\"\"\n        ref_features = torch.cat(\n            [instance.features for instance in ref_instances], dim=0\n        ).unsqueeze(0)\n\n        # window_length = len(frames)\n        # instances_per_frame = [frame.num_detected for frame in frames]\n        total_instances = len(ref_instances)\n        embed_dim = self.d_model\n        # print(f'T: {window_length}; N: {total_instances}; N_t: {instances_per_frame} n_reid: {reid_features.shape}')\n        ref_boxes = get_boxes(ref_instances)  # total_instances, 4\n        ref_boxes = torch.nan_to_num(ref_boxes, -1.0)\n        ref_times, query_times = get_times(ref_instances, query_instances)\n\n        # window_length = len(ref_times.unique())  # Currently unused but may be useful for debugging\n\n        ref_temp_emb = self.temp_emb(ref_times)\n\n        ref_pos_emb = self.pos_emb(ref_boxes)\n\n        if self.return_embedding:\n            for i, instance in enumerate(ref_instances):\n                instance.add_embedding(\"pos\", ref_pos_emb[i])\n                instance.add_embedding(\"temp\", ref_temp_emb[i])\n\n        ref_emb = (ref_pos_emb + ref_temp_emb) / 2.0\n\n        ref_emb = ref_emb.view(1, total_instances, embed_dim)\n\n        ref_emb = ref_emb.permute(1, 0, 2)  # (total_instances, batch_size, embed_dim)\n\n        batch_size, total_instances = ref_features.shape[:-1]\n\n        ref_features = ref_features.permute(\n            1, 0, 2\n        )  # (total_instances, batch_size, embed_dim)\n\n        encoder_queries = ref_features\n\n        # apply fourier embeddings if using fourier rope, OR if using descriptor (compact) visual encoder\n        if (\n            self.embedding_meta\n            and \"use_fourier\" in self.embedding_meta\n            and self.embedding_meta[\"use_fourier\"]\n        ) or (\n            self.encoder_cfg\n            and \"encoder_type\" in self.encoder_cfg\n            and self.encoder_cfg[\"encoder_type\"] == \"descriptor\"\n        ):\n            encoder_queries = apply_fourier_embeddings(\n                encoder_queries,\n                ref_times,\n                self.d_model,\n                self.fourier_embeddings,\n                self.fourier_proj,\n                self.fourier_norm,\n            )\n\n        encoder_features = self.encoder(\n            encoder_queries, pos_emb=ref_emb\n        )  # (total_instances, batch_size, embed_dim)\n\n        n_query = total_instances\n\n        query_features = ref_features\n        query_pos_emb = ref_pos_emb\n        query_temp_emb = ref_temp_emb\n        query_emb = ref_emb\n\n        if query_instances is not None:\n            n_query = len(query_instances)\n\n            query_features = torch.cat(\n                [instance.features for instance in query_instances], dim=0\n            ).unsqueeze(0)\n\n            query_features = query_features.permute(\n                1, 0, 2\n            )  # (n_query, batch_size, embed_dim)\n\n            query_boxes = get_boxes(query_instances)\n            query_boxes = torch.nan_to_num(query_boxes, -1.0)\n            query_temp_emb = self.temp_emb(query_times)\n\n            query_pos_emb = self.pos_emb(query_boxes)\n\n            query_emb = (query_pos_emb + query_temp_emb) / 2.0\n            query_emb = query_emb.view(1, n_query, embed_dim)\n            query_emb = query_emb.permute(1, 0, 2)  # (n_query, batch_size, embed_dim)\n\n        else:\n            query_instances = ref_instances\n            query_times = ref_times\n\n        if self.return_embedding:\n            for i, instance in enumerate(query_instances):\n                instance.add_embedding(\"pos\", query_pos_emb[i])\n                instance.add_embedding(\"temp\", query_temp_emb[i])\n\n        # apply fourier embeddings if using fourier rope, OR if using descriptor (compact) visual encoder\n        if (\n            self.embedding_meta\n            and \"use_fourier\" in self.embedding_meta\n            and self.embedding_meta[\"use_fourier\"]\n        ) or (\n            self.encoder_cfg\n            and \"encoder_type\" in self.encoder_cfg\n            and self.encoder_cfg[\"encoder_type\"] == \"descriptor\"\n        ):\n            query_features = apply_fourier_embeddings(\n                query_features,\n                query_times,\n                self.d_model,\n                self.fourier_embeddings,\n                self.fourier_proj,\n                self.fourier_norm,\n            )\n\n        decoder_features = self.decoder(\n            query_features,\n            encoder_features,\n            ref_pos_emb=ref_emb,\n            query_pos_emb=query_emb,\n        )  # (L, n_query, batch_size, embed_dim)\n\n        decoder_features = decoder_features.transpose(\n            1, 2\n        )  # # (L, batch_size, n_query, embed_dim)\n        encoder_features = encoder_features.permute(1, 0, 2).view(\n            batch_size, total_instances, embed_dim\n        )  # (batch_size, total_instances, embed_dim)\n\n        asso_output = []\n        for frame_features in decoder_features:\n            asso_matrix = self.attn_head(frame_features, encoder_features).view(\n                n_query, total_instances\n            )\n            asso_matrix = AssociationMatrix(asso_matrix, ref_instances, query_instances)\n\n            asso_output.append(asso_matrix)\n\n        # (L=1, n_query, total_instances)\n        return asso_output\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.Transformer.__init__","title":"<code>__init__(d_model=1024, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dropout=0.1, activation='relu', return_intermediate_dec=False, norm=False, num_layers_attn_head=2, dropout_attn_head=0.1, embedding_meta=None, return_embedding=False, decoder_self_attn=False, encoder_cfg=None)</code>","text":"<p>Initialize Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of features in the encoder/decoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads in the transformer encoder/decoder.</p> <code>8</code> <code>num_encoder_layers</code> <code>int</code> <p>The number of encoder-layers in the encoder.</p> <code>6</code> <code>num_decoder_layers</code> <code>int</code> <p>The number of decoder-layers in the decoder.</p> <code>6</code> <code>dropout</code> <code>float</code> <p>Dropout value applied to the output of transformer layers.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>return_intermediate_dec</code> <code>bool</code> <p>Return intermediate layers from decoder.</p> <code>False</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of encoder and decoder.</p> <code>False</code> <code>num_layers_attn_head</code> <code>int</code> <p>The number of layers in the attention head.</p> <code>2</code> <code>dropout_attn_head</code> <code>float</code> <p>Dropout value for the attention_head.</p> <code>0.1</code> <code>embedding_meta</code> <code>dict | None</code> <p>Metadata for positional embeddings. See below.</p> <code>None</code> <code>return_embedding</code> <code>bool</code> <p>Whether to return the positional embeddings</p> <code>False</code> <code>decoder_self_attn</code> <code>bool</code> <p>If True, use decoder self attention.</p> <code>False</code> <code>encoder_cfg</code> <code>dict | None</code> <p>Encoder configuration.</p> <p>More details on <code>embedding_meta</code>:     By default this will be an empty dict and indicate     that no positional embeddings should be used. To use the positional embeddings     pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:     {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},     \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see <code>dreem.models.embeddings.Embedding.EMB_TYPES</code>     and <code>dreem.models.embeddings.Embedding.EMB_MODES</code> for embedding parameters).</p> <code>None</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    d_model: int = 1024,\n    nhead: int = 8,\n    num_encoder_layers: int = 6,\n    num_decoder_layers: int = 6,\n    dropout: float = 0.1,\n    activation: str = \"relu\",\n    return_intermediate_dec: bool = False,\n    norm: bool = False,\n    num_layers_attn_head: int = 2,\n    dropout_attn_head: float = 0.1,\n    embedding_meta: dict | None = None,\n    return_embedding: bool = False,\n    decoder_self_attn: bool = False,\n    encoder_cfg: dict | None = None,\n) -&gt; None:\n    \"\"\"Initialize Transformer.\n\n    Args:\n        d_model: The number of features in the encoder/decoder inputs.\n        nhead: The number of heads in the transformer encoder/decoder.\n        num_encoder_layers: The number of encoder-layers in the encoder.\n        num_decoder_layers: The number of decoder-layers in the decoder.\n        dropout: Dropout value applied to the output of transformer layers.\n        activation: Activation function to use.\n        return_intermediate_dec: Return intermediate layers from decoder.\n        norm: If True, normalize output of encoder and decoder.\n        num_layers_attn_head: The number of layers in the attention head.\n        dropout_attn_head: Dropout value for the attention_head.\n        embedding_meta: Metadata for positional embeddings. See below.\n        return_embedding: Whether to return the positional embeddings\n        decoder_self_attn: If True, use decoder self attention.\n        encoder_cfg: Encoder configuration.\n\n            More details on `embedding_meta`:\n                By default this will be an empty dict and indicate\n                that no positional embeddings should be used. To use the positional embeddings\n                pass in a dictionary containing a \"pos\" and \"temp\" key with subdictionaries for correct parameters ie:\n                {\"pos\": {'mode': 'learned', 'emb_num': 16, 'over_boxes: 'True'},\n                \"temp\": {'mode': 'learned', 'emb_num': 16}}. (see `dreem.models.embeddings.Embedding.EMB_TYPES`\n                and `dreem.models.embeddings.Embedding.EMB_MODES` for embedding parameters).\n    \"\"\"\n    super().__init__()\n\n    self.d_model = dim_feedforward = feature_dim_attn_head = d_model\n\n    self.embedding_meta = embedding_meta\n    self.return_embedding = return_embedding\n    self.encoder_cfg = encoder_cfg\n\n    self.pos_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n    self.temp_emb = Embedding(emb_type=\"off\", mode=\"off\", features=self.d_model)\n\n    if self.embedding_meta:\n        if \"pos\" in self.embedding_meta:\n            pos_emb_cfg = self.embedding_meta[\"pos\"]\n            if pos_emb_cfg:\n                self.pos_emb = Embedding(\n                    emb_type=\"pos\", features=self.d_model, **pos_emb_cfg\n                )\n        if \"temp\" in self.embedding_meta:\n            temp_emb_cfg = self.embedding_meta[\"temp\"]\n            if temp_emb_cfg:\n                self.temp_emb = Embedding(\n                    emb_type=\"temp\", features=self.d_model, **temp_emb_cfg\n                )\n\n    self.fourier_embeddings = FourierPositionalEmbeddings(\n        n_components=8, d_model=d_model\n    )\n\n    # Transformer Encoder\n    encoder_layer = TransformerEncoderLayer(\n        d_model, nhead, dim_feedforward, dropout, activation, norm\n    )\n\n    encoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n    # only used if using descriptor visual encoder; default resnet encoder uses d_model directly\n    if self.encoder_cfg and \"encoder_type\" in self.encoder_cfg:\n        self.visual_feat_dim = (\n            self.encoder_cfg[\"ndim\"] if \"ndim\" in self.encoder_cfg else 5\n        )  # 5 is default for descriptor\n        self.fourier_proj = nn.Linear(self.d_model + self.visual_feat_dim, d_model)\n        self.fourier_norm = nn.LayerNorm(self.d_model)\n\n    self.encoder = TransformerEncoder(\n        encoder_layer, num_encoder_layers, encoder_norm\n    )\n\n    # Transformer Decoder\n    decoder_layer = TransformerDecoderLayer(\n        d_model,\n        nhead,\n        dim_feedforward,\n        dropout,\n        activation,\n        norm,\n        decoder_self_attn,\n    )\n\n    decoder_norm = nn.LayerNorm(d_model) if (norm) else None\n\n    self.decoder = TransformerDecoder(\n        decoder_layer, num_decoder_layers, return_intermediate_dec, decoder_norm\n    )\n\n    # Transformer attention head\n    self.attn_head = ATTWeightHead(\n        feature_dim=feature_dim_attn_head,\n        num_layers=num_layers_attn_head,\n        dropout=dropout_attn_head,\n    )\n\n    self._reset_parameters()\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.Transformer.forward","title":"<code>forward(ref_instances, query_instances=None)</code>","text":"<p>Execute a forward pass through the transformer and attention head.</p> <p>Parameters:</p> Name Type Description Default <code>ref_instances</code> <code>list[Instance]</code> <p>A list of instance objects (See <code>dreem.io.Instance</code> for more info.)</p> required <code>query_instances</code> <code>list[Instance] | None</code> <p>An set of instances to be used as decoder queries.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>asso_output</code> <code>list[AssociationMatrix]</code> <p>A list of torch.Tensors of shape (L, n_query, total_instances) where:     L: number of decoder blocks     n_query: number of instances in current query/frame     total_instances: number of instances in window</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self,\n    ref_instances: list[\"dreem.io.Instance\"],\n    query_instances: list[\"dreem.io.Instance\"] | None = None,\n) -&gt; list[AssociationMatrix]:\n    \"\"\"Execute a forward pass through the transformer and attention head.\n\n    Args:\n        ref_instances: A list of instance objects (See `dreem.io.Instance` for more info.)\n        query_instances: An set of instances to be used as decoder queries.\n\n    Returns:\n        asso_output: A list of torch.Tensors of shape (L, n_query, total_instances) where:\n            L: number of decoder blocks\n            n_query: number of instances in current query/frame\n            total_instances: number of instances in window\n    \"\"\"\n    ref_features = torch.cat(\n        [instance.features for instance in ref_instances], dim=0\n    ).unsqueeze(0)\n\n    # window_length = len(frames)\n    # instances_per_frame = [frame.num_detected for frame in frames]\n    total_instances = len(ref_instances)\n    embed_dim = self.d_model\n    # print(f'T: {window_length}; N: {total_instances}; N_t: {instances_per_frame} n_reid: {reid_features.shape}')\n    ref_boxes = get_boxes(ref_instances)  # total_instances, 4\n    ref_boxes = torch.nan_to_num(ref_boxes, -1.0)\n    ref_times, query_times = get_times(ref_instances, query_instances)\n\n    # window_length = len(ref_times.unique())  # Currently unused but may be useful for debugging\n\n    ref_temp_emb = self.temp_emb(ref_times)\n\n    ref_pos_emb = self.pos_emb(ref_boxes)\n\n    if self.return_embedding:\n        for i, instance in enumerate(ref_instances):\n            instance.add_embedding(\"pos\", ref_pos_emb[i])\n            instance.add_embedding(\"temp\", ref_temp_emb[i])\n\n    ref_emb = (ref_pos_emb + ref_temp_emb) / 2.0\n\n    ref_emb = ref_emb.view(1, total_instances, embed_dim)\n\n    ref_emb = ref_emb.permute(1, 0, 2)  # (total_instances, batch_size, embed_dim)\n\n    batch_size, total_instances = ref_features.shape[:-1]\n\n    ref_features = ref_features.permute(\n        1, 0, 2\n    )  # (total_instances, batch_size, embed_dim)\n\n    encoder_queries = ref_features\n\n    # apply fourier embeddings if using fourier rope, OR if using descriptor (compact) visual encoder\n    if (\n        self.embedding_meta\n        and \"use_fourier\" in self.embedding_meta\n        and self.embedding_meta[\"use_fourier\"]\n    ) or (\n        self.encoder_cfg\n        and \"encoder_type\" in self.encoder_cfg\n        and self.encoder_cfg[\"encoder_type\"] == \"descriptor\"\n    ):\n        encoder_queries = apply_fourier_embeddings(\n            encoder_queries,\n            ref_times,\n            self.d_model,\n            self.fourier_embeddings,\n            self.fourier_proj,\n            self.fourier_norm,\n        )\n\n    encoder_features = self.encoder(\n        encoder_queries, pos_emb=ref_emb\n    )  # (total_instances, batch_size, embed_dim)\n\n    n_query = total_instances\n\n    query_features = ref_features\n    query_pos_emb = ref_pos_emb\n    query_temp_emb = ref_temp_emb\n    query_emb = ref_emb\n\n    if query_instances is not None:\n        n_query = len(query_instances)\n\n        query_features = torch.cat(\n            [instance.features for instance in query_instances], dim=0\n        ).unsqueeze(0)\n\n        query_features = query_features.permute(\n            1, 0, 2\n        )  # (n_query, batch_size, embed_dim)\n\n        query_boxes = get_boxes(query_instances)\n        query_boxes = torch.nan_to_num(query_boxes, -1.0)\n        query_temp_emb = self.temp_emb(query_times)\n\n        query_pos_emb = self.pos_emb(query_boxes)\n\n        query_emb = (query_pos_emb + query_temp_emb) / 2.0\n        query_emb = query_emb.view(1, n_query, embed_dim)\n        query_emb = query_emb.permute(1, 0, 2)  # (n_query, batch_size, embed_dim)\n\n    else:\n        query_instances = ref_instances\n        query_times = ref_times\n\n    if self.return_embedding:\n        for i, instance in enumerate(query_instances):\n            instance.add_embedding(\"pos\", query_pos_emb[i])\n            instance.add_embedding(\"temp\", query_temp_emb[i])\n\n    # apply fourier embeddings if using fourier rope, OR if using descriptor (compact) visual encoder\n    if (\n        self.embedding_meta\n        and \"use_fourier\" in self.embedding_meta\n        and self.embedding_meta[\"use_fourier\"]\n    ) or (\n        self.encoder_cfg\n        and \"encoder_type\" in self.encoder_cfg\n        and self.encoder_cfg[\"encoder_type\"] == \"descriptor\"\n    ):\n        query_features = apply_fourier_embeddings(\n            query_features,\n            query_times,\n            self.d_model,\n            self.fourier_embeddings,\n            self.fourier_proj,\n            self.fourier_norm,\n        )\n\n    decoder_features = self.decoder(\n        query_features,\n        encoder_features,\n        ref_pos_emb=ref_emb,\n        query_pos_emb=query_emb,\n    )  # (L, n_query, batch_size, embed_dim)\n\n    decoder_features = decoder_features.transpose(\n        1, 2\n    )  # # (L, batch_size, n_query, embed_dim)\n    encoder_features = encoder_features.permute(1, 0, 2).view(\n        batch_size, total_instances, embed_dim\n    )  # (batch_size, total_instances, embed_dim)\n\n    asso_output = []\n    for frame_features in decoder_features:\n        asso_matrix = self.attn_head(frame_features, encoder_features).view(\n            n_query, total_instances\n        )\n        asso_matrix = AssociationMatrix(asso_matrix, ref_instances, query_instances)\n\n        asso_output.append(asso_matrix)\n\n    # (L=1, n_query, total_instances)\n    return asso_output\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerDecoder","title":"<code>TransformerDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer Decoder Block composed of Transformer Decoder Layers.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize transformer decoder block.</p> <code>forward</code> <p>Execute a forward pass of the decoder block.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class TransformerDecoder(nn.Module):\n    \"\"\"Transformer Decoder Block composed of Transformer Decoder Layers.\"\"\"\n\n    def __init__(\n        self,\n        decoder_layer: TransformerDecoderLayer,\n        num_layers: int,\n        return_intermediate: bool = False,\n        norm: nn.Module | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize transformer decoder block.\n\n        Args:\n            decoder_layer: An instance of TransformerDecoderLayer.\n            num_layers: The number of decoder layers to be stacked.\n            return_intermediate: Return intermediate layers from decoder.\n            norm: The normalization layer to be applied.\n        \"\"\"\n        super().__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.return_intermediate = return_intermediate\n        self.norm = norm if norm is not None else nn.Identity()\n\n    def forward(\n        self,\n        decoder_queries: torch.Tensor,\n        encoder_features: torch.Tensor,\n        ref_pos_emb: torch.Tensor | None = None,\n        query_pos_emb: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute a forward pass of the decoder block.\n\n        Args:\n            decoder_queries: Query sequence for decoder to generate (n_query, batch_size, embed_dim).\n            encoder_features: Output from encoder, that decoder uses to attend to relevant\n                parts of input sequence (total_instances, batch_size, embed_dim)\n            ref_pos_emb: The input positional embedding tensor of shape (total_instances, batch_size, embed_dim).\n            query_pos_emb: The query positional embedding of shape (n_query, batch_size, embed_dim)\n\n        Returns:\n            The output tensor of shape (L, n_query, batch_size, embed_dim).\n        \"\"\"\n        decoder_features = decoder_queries\n\n        intermediate = []\n\n        for layer in self.layers:\n            decoder_features = layer(\n                decoder_features,\n                encoder_features,\n                ref_pos_emb=ref_pos_emb,\n                query_pos_emb=query_pos_emb,\n            )\n            if self.return_intermediate:\n                intermediate.append(self.norm(decoder_features))\n\n        decoder_features = self.norm(decoder_features)\n        if self.return_intermediate:\n            intermediate.pop()\n            intermediate.append(decoder_features)\n\n            return torch.stack(intermediate)\n\n        return decoder_features.unsqueeze(0)\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerDecoder.__init__","title":"<code>__init__(decoder_layer, num_layers, return_intermediate=False, norm=None)</code>","text":"<p>Initialize transformer decoder block.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_layer</code> <code>TransformerDecoderLayer</code> <p>An instance of TransformerDecoderLayer.</p> required <code>num_layers</code> <code>int</code> <p>The number of decoder layers to be stacked.</p> required <code>return_intermediate</code> <code>bool</code> <p>Return intermediate layers from decoder.</p> <code>False</code> <code>norm</code> <code>Module | None</code> <p>The normalization layer to be applied.</p> <code>None</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    decoder_layer: TransformerDecoderLayer,\n    num_layers: int,\n    return_intermediate: bool = False,\n    norm: nn.Module | None = None,\n) -&gt; None:\n    \"\"\"Initialize transformer decoder block.\n\n    Args:\n        decoder_layer: An instance of TransformerDecoderLayer.\n        num_layers: The number of decoder layers to be stacked.\n        return_intermediate: Return intermediate layers from decoder.\n        norm: The normalization layer to be applied.\n    \"\"\"\n    super().__init__()\n    self.layers = _get_clones(decoder_layer, num_layers)\n    self.num_layers = num_layers\n    self.return_intermediate = return_intermediate\n    self.norm = norm if norm is not None else nn.Identity()\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerDecoder.forward","title":"<code>forward(decoder_queries, encoder_features, ref_pos_emb=None, query_pos_emb=None)</code>","text":"<p>Execute a forward pass of the decoder block.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_queries</code> <code>Tensor</code> <p>Query sequence for decoder to generate (n_query, batch_size, embed_dim).</p> required <code>encoder_features</code> <code>Tensor</code> <p>Output from encoder, that decoder uses to attend to relevant parts of input sequence (total_instances, batch_size, embed_dim)</p> required <code>ref_pos_emb</code> <code>Tensor | None</code> <p>The input positional embedding tensor of shape (total_instances, batch_size, embed_dim).</p> <code>None</code> <code>query_pos_emb</code> <code>Tensor | None</code> <p>The query positional embedding of shape (n_query, batch_size, embed_dim)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (L, n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self,\n    decoder_queries: torch.Tensor,\n    encoder_features: torch.Tensor,\n    ref_pos_emb: torch.Tensor | None = None,\n    query_pos_emb: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Execute a forward pass of the decoder block.\n\n    Args:\n        decoder_queries: Query sequence for decoder to generate (n_query, batch_size, embed_dim).\n        encoder_features: Output from encoder, that decoder uses to attend to relevant\n            parts of input sequence (total_instances, batch_size, embed_dim)\n        ref_pos_emb: The input positional embedding tensor of shape (total_instances, batch_size, embed_dim).\n        query_pos_emb: The query positional embedding of shape (n_query, batch_size, embed_dim)\n\n    Returns:\n        The output tensor of shape (L, n_query, batch_size, embed_dim).\n    \"\"\"\n    decoder_features = decoder_queries\n\n    intermediate = []\n\n    for layer in self.layers:\n        decoder_features = layer(\n            decoder_features,\n            encoder_features,\n            ref_pos_emb=ref_pos_emb,\n            query_pos_emb=query_pos_emb,\n        )\n        if self.return_intermediate:\n            intermediate.append(self.norm(decoder_features))\n\n    decoder_features = self.norm(decoder_features)\n    if self.return_intermediate:\n        intermediate.pop()\n        intermediate.append(decoder_features)\n\n        return torch.stack(intermediate)\n\n    return decoder_features.unsqueeze(0)\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerDecoderLayer","title":"<code>TransformerDecoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A single transformer decoder layer.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize transformer decoder layer.</p> <code>forward</code> <p>Execute forward pass of decoder layer.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class TransformerDecoderLayer(nn.Module):\n    \"\"\"A single transformer decoder layer.\"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 1024,\n        nhead: int = 6,\n        dim_feedforward: int = 1024,\n        dropout: float = 0.1,\n        activation: str = \"relu\",\n        norm: bool = False,\n        decoder_self_attn: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize transformer decoder layer.\n\n        Args:\n            d_model: The number of features in the decoder inputs.\n            nhead: The number of heads for the decoder.\n            dim_feedforward: Dimension of the feedforward layers of decoder.\n            dropout: Dropout value applied to the output of decoder.\n            activation: Activation function to use.\n            norm: If True, normalize output of decoder.\n            decoder_self_attn: If True, use decoder self attention\n        \"\"\"\n        super().__init__()\n\n        self.decoder_self_attn = decoder_self_attn\n\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        if self.decoder_self_attn:\n            self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n\n        self.norm1 = nn.LayerNorm(d_model) if norm else nn.Identity()\n        self.norm2 = nn.LayerNorm(d_model) if norm else nn.Identity()\n        self.norm3 = nn.LayerNorm(d_model) if norm else nn.Identity()\n\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n\n    def forward(\n        self,\n        decoder_queries: torch.Tensor,\n        encoder_features: torch.Tensor,\n        ref_pos_emb: torch.Tensor | None = None,\n        query_pos_emb: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute forward pass of decoder layer.\n\n        Args:\n            decoder_queries: Target sequence for decoder to generate (n_query, batch_size, embed_dim).\n            encoder_features: Output from encoder, that decoder uses to attend to relevant\n                parts of input sequence (total_instances, batch_size, embed_dim)\n            ref_pos_emb: The input positional embedding tensor of shape (n_query, embed_dim).\n            query_pos_emb: The target positional embedding of shape (n_query, embed_dim)\n\n        Returns:\n            The output tensor of shape (n_query, batch_size, embed_dim).\n        \"\"\"\n        if query_pos_emb is None:\n            query_pos_emb = torch.zeros_like(decoder_queries)\n        if ref_pos_emb is None:\n            ref_pos_emb = torch.zeros_like(encoder_features)\n\n        decoder_queries = decoder_queries + query_pos_emb\n        encoder_features = encoder_features + ref_pos_emb\n\n        if self.decoder_self_attn:\n            self_attn_features = self.self_attn(\n                query=decoder_queries, key=decoder_queries, value=decoder_queries\n            )[0]\n            decoder_queries = decoder_queries + self.dropout1(self_attn_features)\n            decoder_queries = self.norm1(decoder_queries)\n\n        x_attn_features = self.multihead_attn(\n            query=decoder_queries,  # (n_query, batch_size, embed_dim)\n            key=encoder_features,  # (total_instances, batch_size, embed_dim)\n            value=encoder_features,  # (total_instances, batch_size, embed_dim)\n        )[0]  # (n_query, batch_size, embed_dim)\n\n        decoder_queries = decoder_queries + self.dropout2(\n            x_attn_features\n        )  # (n_query, batch_size, embed_dim)\n        decoder_queries = self.norm2(\n            decoder_queries\n        )  # (n_query, batch_size, embed_dim)\n        projection = self.linear2(\n            self.dropout(self.activation(self.linear1(decoder_queries)))\n        )  # (n_query, batch_size, embed_dim)\n        decoder_queries = decoder_queries + self.dropout3(\n            projection\n        )  # (n_query, batch_size, embed_dim)\n        decoder_features = self.norm3(decoder_queries)\n\n        return decoder_features  # (n_query, batch_size, embed_dim)\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerDecoderLayer.__init__","title":"<code>__init__(d_model=1024, nhead=6, dim_feedforward=1024, dropout=0.1, activation='relu', norm=False, decoder_self_attn=False)</code>","text":"<p>Initialize transformer decoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of features in the decoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads for the decoder.</p> <code>6</code> <code>dim_feedforward</code> <code>int</code> <p>Dimension of the feedforward layers of decoder.</p> <code>1024</code> <code>dropout</code> <code>float</code> <p>Dropout value applied to the output of decoder.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of decoder.</p> <code>False</code> <code>decoder_self_attn</code> <code>bool</code> <p>If True, use decoder self attention</p> <code>False</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    d_model: int = 1024,\n    nhead: int = 6,\n    dim_feedforward: int = 1024,\n    dropout: float = 0.1,\n    activation: str = \"relu\",\n    norm: bool = False,\n    decoder_self_attn: bool = False,\n) -&gt; None:\n    \"\"\"Initialize transformer decoder layer.\n\n    Args:\n        d_model: The number of features in the decoder inputs.\n        nhead: The number of heads for the decoder.\n        dim_feedforward: Dimension of the feedforward layers of decoder.\n        dropout: Dropout value applied to the output of decoder.\n        activation: Activation function to use.\n        norm: If True, normalize output of decoder.\n        decoder_self_attn: If True, use decoder self attention\n    \"\"\"\n    super().__init__()\n\n    self.decoder_self_attn = decoder_self_attn\n\n    self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n    self.linear1 = nn.Linear(d_model, dim_feedforward)\n    self.dropout = nn.Dropout(dropout)\n    self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n    if self.decoder_self_attn:\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n\n    self.norm1 = nn.LayerNorm(d_model) if norm else nn.Identity()\n    self.norm2 = nn.LayerNorm(d_model) if norm else nn.Identity()\n    self.norm3 = nn.LayerNorm(d_model) if norm else nn.Identity()\n\n    self.dropout1 = nn.Dropout(dropout)\n    self.dropout2 = nn.Dropout(dropout)\n    self.dropout3 = nn.Dropout(dropout)\n\n    self.activation = _get_activation_fn(activation)\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerDecoderLayer.forward","title":"<code>forward(decoder_queries, encoder_features, ref_pos_emb=None, query_pos_emb=None)</code>","text":"<p>Execute forward pass of decoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_queries</code> <code>Tensor</code> <p>Target sequence for decoder to generate (n_query, batch_size, embed_dim).</p> required <code>encoder_features</code> <code>Tensor</code> <p>Output from encoder, that decoder uses to attend to relevant parts of input sequence (total_instances, batch_size, embed_dim)</p> required <code>ref_pos_emb</code> <code>Tensor | None</code> <p>The input positional embedding tensor of shape (n_query, embed_dim).</p> <code>None</code> <code>query_pos_emb</code> <code>Tensor | None</code> <p>The target positional embedding of shape (n_query, embed_dim)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self,\n    decoder_queries: torch.Tensor,\n    encoder_features: torch.Tensor,\n    ref_pos_emb: torch.Tensor | None = None,\n    query_pos_emb: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Execute forward pass of decoder layer.\n\n    Args:\n        decoder_queries: Target sequence for decoder to generate (n_query, batch_size, embed_dim).\n        encoder_features: Output from encoder, that decoder uses to attend to relevant\n            parts of input sequence (total_instances, batch_size, embed_dim)\n        ref_pos_emb: The input positional embedding tensor of shape (n_query, embed_dim).\n        query_pos_emb: The target positional embedding of shape (n_query, embed_dim)\n\n    Returns:\n        The output tensor of shape (n_query, batch_size, embed_dim).\n    \"\"\"\n    if query_pos_emb is None:\n        query_pos_emb = torch.zeros_like(decoder_queries)\n    if ref_pos_emb is None:\n        ref_pos_emb = torch.zeros_like(encoder_features)\n\n    decoder_queries = decoder_queries + query_pos_emb\n    encoder_features = encoder_features + ref_pos_emb\n\n    if self.decoder_self_attn:\n        self_attn_features = self.self_attn(\n            query=decoder_queries, key=decoder_queries, value=decoder_queries\n        )[0]\n        decoder_queries = decoder_queries + self.dropout1(self_attn_features)\n        decoder_queries = self.norm1(decoder_queries)\n\n    x_attn_features = self.multihead_attn(\n        query=decoder_queries,  # (n_query, batch_size, embed_dim)\n        key=encoder_features,  # (total_instances, batch_size, embed_dim)\n        value=encoder_features,  # (total_instances, batch_size, embed_dim)\n    )[0]  # (n_query, batch_size, embed_dim)\n\n    decoder_queries = decoder_queries + self.dropout2(\n        x_attn_features\n    )  # (n_query, batch_size, embed_dim)\n    decoder_queries = self.norm2(\n        decoder_queries\n    )  # (n_query, batch_size, embed_dim)\n    projection = self.linear2(\n        self.dropout(self.activation(self.linear1(decoder_queries)))\n    )  # (n_query, batch_size, embed_dim)\n    decoder_queries = decoder_queries + self.dropout3(\n        projection\n    )  # (n_query, batch_size, embed_dim)\n    decoder_features = self.norm3(decoder_queries)\n\n    return decoder_features  # (n_query, batch_size, embed_dim)\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerEncoder","title":"<code>TransformerEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>A transformer encoder block composed of encoder layers.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize transformer encoder.</p> <code>forward</code> <p>Execute a forward pass of encoder layer.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class TransformerEncoder(nn.Module):\n    \"\"\"A transformer encoder block composed of encoder layers.\"\"\"\n\n    def __init__(\n        self,\n        encoder_layer: TransformerEncoderLayer,\n        num_layers: int,\n        norm: nn.Module | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize transformer encoder.\n\n        Args:\n            encoder_layer: An instance of the TransformerEncoderLayer.\n            num_layers: The number of encoder layers to be stacked.\n            norm: The normalization layer to be applied.\n        \"\"\"\n        super().__init__()\n\n        self.layers = _get_clones(encoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm if norm is not None else nn.Identity()\n\n    def forward(\n        self, queries: torch.Tensor, pos_emb: torch.Tensor = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute a forward pass of encoder layer.\n\n        Args:\n            queries: The input tensor of shape (n_query, batch_size, embed_dim).\n            pos_emb: The positional embedding tensor of shape (n_query, embed_dim).\n\n        Returns:\n            The output tensor of shape (n_query, batch_size, embed_dim).\n        \"\"\"\n        for layer in self.layers:\n            queries = layer(queries, pos_emb=pos_emb)\n\n        encoder_features = self.norm(queries)\n        return encoder_features\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerEncoder.__init__","title":"<code>__init__(encoder_layer, num_layers, norm=None)</code>","text":"<p>Initialize transformer encoder.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_layer</code> <code>TransformerEncoderLayer</code> <p>An instance of the TransformerEncoderLayer.</p> required <code>num_layers</code> <code>int</code> <p>The number of encoder layers to be stacked.</p> required <code>norm</code> <code>Module | None</code> <p>The normalization layer to be applied.</p> <code>None</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    encoder_layer: TransformerEncoderLayer,\n    num_layers: int,\n    norm: nn.Module | None = None,\n) -&gt; None:\n    \"\"\"Initialize transformer encoder.\n\n    Args:\n        encoder_layer: An instance of the TransformerEncoderLayer.\n        num_layers: The number of encoder layers to be stacked.\n        norm: The normalization layer to be applied.\n    \"\"\"\n    super().__init__()\n\n    self.layers = _get_clones(encoder_layer, num_layers)\n    self.num_layers = num_layers\n    self.norm = norm if norm is not None else nn.Identity()\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerEncoder.forward","title":"<code>forward(queries, pos_emb=None)</code>","text":"<p>Execute a forward pass of encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Tensor</code> <p>The input tensor of shape (n_query, batch_size, embed_dim).</p> required <code>pos_emb</code> <code>Tensor</code> <p>The positional embedding tensor of shape (n_query, embed_dim).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self, queries: torch.Tensor, pos_emb: torch.Tensor = None\n) -&gt; torch.Tensor:\n    \"\"\"Execute a forward pass of encoder layer.\n\n    Args:\n        queries: The input tensor of shape (n_query, batch_size, embed_dim).\n        pos_emb: The positional embedding tensor of shape (n_query, embed_dim).\n\n    Returns:\n        The output tensor of shape (n_query, batch_size, embed_dim).\n    \"\"\"\n    for layer in self.layers:\n        queries = layer(queries, pos_emb=pos_emb)\n\n    encoder_features = self.norm(queries)\n    return encoder_features\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerEncoderLayer","title":"<code>TransformerEncoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A single transformer encoder layer.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize a transformer encoder layer.</p> <code>forward</code> <p>Execute a forward pass of the encoder layer.</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>class TransformerEncoderLayer(nn.Module):\n    \"\"\"A single transformer encoder layer.\"\"\"\n\n    def __init__(\n        self,\n        d_model: int = 1024,\n        nhead: int = 6,\n        dim_feedforward: int = 1024,\n        dropout: float = 0.1,\n        activation: str = \"relu\",\n        norm: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize a transformer encoder layer.\n\n        Args:\n            d_model: The number of features in the encoder inputs.\n            nhead: The number of heads for the encoder.\n            dim_feedforward: Dimension of the feedforward layers of encoder.\n            dropout: Dropout value applied to the output of encoder.\n            activation: Activation function to use.\n            norm: If True, normalize output of encoder.\n        \"\"\"\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model) if norm else nn.Identity()\n        self.norm2 = nn.LayerNorm(d_model) if norm else nn.Identity()\n\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n\n    def forward(\n        self, queries: torch.Tensor, pos_emb: torch.Tensor = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Execute a forward pass of the encoder layer.\n\n        Args:\n            queries: Input sequence for encoder (n_query, batch_size, embed_dim).\n            pos_emb: Position embedding, if provided is added to src\n\n        Returns:\n            The output tensor of shape (n_query, batch_size, embed_dim).\n        \"\"\"\n        if pos_emb is None:\n            pos_emb = torch.zeros_like(queries)\n\n        queries = queries + pos_emb\n\n        # q = k = src\n\n        attn_features = self.self_attn(\n            query=queries,\n            key=queries,\n            value=queries,\n        )[0]\n\n        queries = queries + self.dropout1(attn_features)\n        queries = self.norm1(queries)\n        projection = self.linear2(self.dropout(self.activation(self.linear1(queries))))\n        queries = queries + self.dropout2(projection)\n        encoder_features = self.norm2(queries)\n\n        return encoder_features\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerEncoderLayer.__init__","title":"<code>__init__(d_model=1024, nhead=6, dim_feedforward=1024, dropout=0.1, activation='relu', norm=False)</code>","text":"<p>Initialize a transformer encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The number of features in the encoder inputs.</p> <code>1024</code> <code>nhead</code> <code>int</code> <p>The number of heads for the encoder.</p> <code>6</code> <code>dim_feedforward</code> <code>int</code> <p>Dimension of the feedforward layers of encoder.</p> <code>1024</code> <code>dropout</code> <code>float</code> <p>Dropout value applied to the output of encoder.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'relu'</code> <code>norm</code> <code>bool</code> <p>If True, normalize output of encoder.</p> <code>False</code> Source code in <code>dreem/models/transformer.py</code> <pre><code>def __init__(\n    self,\n    d_model: int = 1024,\n    nhead: int = 6,\n    dim_feedforward: int = 1024,\n    dropout: float = 0.1,\n    activation: str = \"relu\",\n    norm: bool = False,\n) -&gt; None:\n    \"\"\"Initialize a transformer encoder layer.\n\n    Args:\n        d_model: The number of features in the encoder inputs.\n        nhead: The number of heads for the encoder.\n        dim_feedforward: Dimension of the feedforward layers of encoder.\n        dropout: Dropout value applied to the output of encoder.\n        activation: Activation function to use.\n        norm: If True, normalize output of encoder.\n    \"\"\"\n    super().__init__()\n    self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n    self.linear1 = nn.Linear(d_model, dim_feedforward)\n    self.dropout = nn.Dropout(dropout)\n    self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n    self.norm1 = nn.LayerNorm(d_model) if norm else nn.Identity()\n    self.norm2 = nn.LayerNorm(d_model) if norm else nn.Identity()\n\n    self.dropout1 = nn.Dropout(dropout)\n    self.dropout2 = nn.Dropout(dropout)\n\n    self.activation = _get_activation_fn(activation)\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.TransformerEncoderLayer.forward","title":"<code>forward(queries, pos_emb=None)</code>","text":"<p>Execute a forward pass of the encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Tensor</code> <p>Input sequence for encoder (n_query, batch_size, embed_dim).</p> required <code>pos_emb</code> <code>Tensor</code> <p>Position embedding, if provided is added to src</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def forward(\n    self, queries: torch.Tensor, pos_emb: torch.Tensor = None\n) -&gt; torch.Tensor:\n    \"\"\"Execute a forward pass of the encoder layer.\n\n    Args:\n        queries: Input sequence for encoder (n_query, batch_size, embed_dim).\n        pos_emb: Position embedding, if provided is added to src\n\n    Returns:\n        The output tensor of shape (n_query, batch_size, embed_dim).\n    \"\"\"\n    if pos_emb is None:\n        pos_emb = torch.zeros_like(queries)\n\n    queries = queries + pos_emb\n\n    # q = k = src\n\n    attn_features = self.self_attn(\n        query=queries,\n        key=queries,\n        value=queries,\n    )[0]\n\n    queries = queries + self.dropout1(attn_features)\n    queries = self.norm1(queries)\n    projection = self.linear2(self.dropout(self.activation(self.linear1(queries))))\n    queries = queries + self.dropout2(projection)\n    encoder_features = self.norm2(queries)\n\n    return encoder_features\n</code></pre>"},{"location":"reference/dreem/models/transformer/#dreem.models.transformer.apply_fourier_embeddings","title":"<code>apply_fourier_embeddings(queries, times, d_model, fourier_embeddings, proj, norm)</code>","text":"<p>Apply fourier embeddings to queries.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Tensor</code> <p>The input tensor of shape (n_query, batch_size, embed_dim).</p> required <code>times</code> <code>Tensor</code> <p>The times index tensor of shape (n_query,).</p> required <code>d_model</code> <code>int</code> <p>Model dimension.</p> required <code>fourier_embeddings</code> <code>FourierPositionalEmbeddings</code> <p>The Fourier positional embeddings object.</p> required <code>proj</code> <code>Linear</code> <p>Linear projection layer that projects concatenated feature vector to model dimension.</p> required <code>norm</code> <code>LayerNorm</code> <p>The normalization layer.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output queries of shape (n_query, batch_size, embed_dim).</p> Source code in <code>dreem/models/transformer.py</code> <pre><code>def apply_fourier_embeddings(\n    queries: torch.Tensor,\n    times: torch.Tensor,\n    d_model: int,\n    fourier_embeddings: FourierPositionalEmbeddings,\n    proj: nn.Linear,\n    norm: nn.LayerNorm,\n) -&gt; torch.Tensor:\n    \"\"\"Apply fourier embeddings to queries.\n\n    Args:\n        queries: The input tensor of shape (n_query, batch_size, embed_dim).\n        times: The times index tensor of shape (n_query,).\n        d_model: Model dimension.\n        fourier_embeddings: The Fourier positional embeddings object.\n        proj: Linear projection layer that projects concatenated feature vector to model dimension.\n        norm: The normalization layer.\n\n    Returns:\n        The output queries of shape (n_query, batch_size, embed_dim).\n    \"\"\"\n    embs = fourier_embeddings(times).permute(1, 0, 2)\n    cat_queries = torch.cat([queries, embs], dim=-1)\n    # project to d_model\n    cat_queries = proj(cat_queries)\n    cat_queries = norm(cat_queries)\n\n    return cat_queries\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/","title":"visual_encoder","text":""},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder","title":"<code>dreem.models.visual_encoder</code>","text":"<p>Module for different visual feature extractors.</p> <p>Classes:</p> Name Description <code>DescriptorVisualEncoder</code> <p>Visual Encoder based on image descriptors.</p> <code>VisualEncoder</code> <p>Class wrapping around a visual feature extractor backbone.</p> <p>Functions:</p> Name Description <code>create_visual_encoder</code> <p>Create a visual encoder based on the specified type.</p> <code>register_encoder</code> <p>Register a new encoder type.</p>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.DescriptorVisualEncoder","title":"<code>DescriptorVisualEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Visual Encoder based on image descriptors.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize Descriptor Visual Encoder.</p> <code>compute_hu_moments</code> <p>Compute Hu moments.</p> <code>compute_inertia_tensor</code> <p>Compute inertia tensor.</p> <code>forward</code> <p>Forward pass of feature extractor to get feature vector.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>class DescriptorVisualEncoder(torch.nn.Module):\n    \"\"\"Visual Encoder based on image descriptors.\"\"\"\n\n    def __init__(self, use_hu_moments: bool = False, **kwargs):\n        \"\"\"Initialize Descriptor Visual Encoder.\n\n        Args:\n            use_hu_moments: Whether to use Hu moments.\n        \"\"\"\n        super().__init__()\n        self.use_hu_moments = use_hu_moments\n\n    def compute_hu_moments(self, img):\n        \"\"\"Compute Hu moments.\"\"\"\n        mu = measure.moments_central(img)\n        nu = measure.moments_normalized(mu)\n        hu = measure.moments_hu(nu)\n        # log transform hu moments for scale differences; switched off; numerically unstable\n        # hu_log = -np.sign(hu) * np.log(np.abs(hu))\n\n        return hu\n\n    def compute_inertia_tensor(self, img):\n        \"\"\"Compute inertia tensor.\"\"\"\n        return measure.inertia_tensor(img)\n\n    @torch.no_grad()\n    def forward(self, img: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of feature extractor to get feature vector.\"\"\"\n        descriptors = []\n\n        for im in img:\n            im = im[0].cpu().numpy()\n\n            inertia_tensor = self.compute_inertia_tensor(im)\n            mean_intensity = im.mean()\n            if self.use_hu_moments:\n                hu_moments = self.compute_hu_moments(im)\n\n            # Flatten inertia tensor\n            inertia_tensor_flat = inertia_tensor.flatten()\n\n            # Combine all features into a single descriptor\n            descriptor = np.concatenate(\n                [\n                    inertia_tensor_flat,\n                    [mean_intensity],\n                    hu_moments if self.use_hu_moments else [],\n                ]\n            )\n\n            descriptors.append(torch.tensor(descriptor, dtype=torch.float32))\n\n        return torch.stack(descriptors)\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.DescriptorVisualEncoder.__init__","title":"<code>__init__(use_hu_moments=False, **kwargs)</code>","text":"<p>Initialize Descriptor Visual Encoder.</p> <p>Parameters:</p> Name Type Description Default <code>use_hu_moments</code> <code>bool</code> <p>Whether to use Hu moments.</p> <code>False</code> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def __init__(self, use_hu_moments: bool = False, **kwargs):\n    \"\"\"Initialize Descriptor Visual Encoder.\n\n    Args:\n        use_hu_moments: Whether to use Hu moments.\n    \"\"\"\n    super().__init__()\n    self.use_hu_moments = use_hu_moments\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.DescriptorVisualEncoder.compute_hu_moments","title":"<code>compute_hu_moments(img)</code>","text":"<p>Compute Hu moments.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def compute_hu_moments(self, img):\n    \"\"\"Compute Hu moments.\"\"\"\n    mu = measure.moments_central(img)\n    nu = measure.moments_normalized(mu)\n    hu = measure.moments_hu(nu)\n    # log transform hu moments for scale differences; switched off; numerically unstable\n    # hu_log = -np.sign(hu) * np.log(np.abs(hu))\n\n    return hu\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.DescriptorVisualEncoder.compute_inertia_tensor","title":"<code>compute_inertia_tensor(img)</code>","text":"<p>Compute inertia tensor.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def compute_inertia_tensor(self, img):\n    \"\"\"Compute inertia tensor.\"\"\"\n    return measure.inertia_tensor(img)\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.DescriptorVisualEncoder.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of feature extractor to get feature vector.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>@torch.no_grad()\ndef forward(self, img: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of feature extractor to get feature vector.\"\"\"\n    descriptors = []\n\n    for im in img:\n        im = im[0].cpu().numpy()\n\n        inertia_tensor = self.compute_inertia_tensor(im)\n        mean_intensity = im.mean()\n        if self.use_hu_moments:\n            hu_moments = self.compute_hu_moments(im)\n\n        # Flatten inertia tensor\n        inertia_tensor_flat = inertia_tensor.flatten()\n\n        # Combine all features into a single descriptor\n        descriptor = np.concatenate(\n            [\n                inertia_tensor_flat,\n                [mean_intensity],\n                hu_moments if self.use_hu_moments else [],\n            ]\n        )\n\n        descriptors.append(torch.tensor(descriptor, dtype=torch.float32))\n\n    return torch.stack(descriptors)\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.VisualEncoder","title":"<code>VisualEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Class wrapping around a visual feature extractor backbone.</p> <p>Currently CNN only.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize Visual Encoder.</p> <code>encoder_dim</code> <p>Compute dummy forward pass of encoder model and get embedding dimension.</p> <code>forward</code> <p>Forward pass of feature extractor to get feature vector.</p> <code>select_feature_extractor</code> <p>Select the appropriate feature extractor based on config.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>class VisualEncoder(torch.nn.Module):\n    \"\"\"Class wrapping around a visual feature extractor backbone.\n\n    Currently CNN only.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"resnet18\",\n        d_model: int = 512,\n        in_chans: int = 3,\n        backend: int = \"timm\",\n        **kwargs: Any | None,\n    ):\n        \"\"\"Initialize Visual Encoder.\n\n        Args:\n            model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n            d_model (int): Output embedding dimension.\n            in_chans: the number of input channels of the image.\n            backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n            kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n        \"\"\"\n        super().__init__()\n\n        self.model_name = model_name.lower()\n        self.d_model = d_model\n        self.backend = backend\n        if in_chans == 1:\n            self.in_chans = 3\n        else:\n            self.in_chans = in_chans\n\n        self.feature_extractor = self.select_feature_extractor(\n            model_name=self.model_name,\n            in_chans=self.in_chans,\n            backend=self.backend,\n            **kwargs,\n        )\n\n        self.out_layer = torch.nn.Linear(\n            self.encoder_dim(self.feature_extractor), self.d_model\n        )\n\n    def select_feature_extractor(\n        self, model_name: str, in_chans: int, backend: str, **kwargs: Any\n    ) -&gt; torch.nn.Module:\n        \"\"\"Select the appropriate feature extractor based on config.\n\n        Args:\n            model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n            in_chans: the number of input channels of the image.\n            backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n            kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n\n        Returns:\n            a CNN encoder based on the config and backend selected.\n        \"\"\"\n        if \"timm\" in backend.lower():\n            feature_extractor = timm.create_model(\n                model_name=self.model_name,\n                in_chans=self.in_chans,\n                num_classes=0,\n                **kwargs,\n            )\n        elif \"torch\" in backend.lower():\n            if model_name.lower() == \"resnet18\":\n                feature_extractor = torchvision.models.resnet18(**kwargs)\n\n            elif model_name.lower() == \"resnet50\":\n                feature_extractor = torchvision.models.resnet50(**kwargs)\n\n            else:\n                raise ValueError(\n                    f\"Only `[resnet18, resnet50]` are available when backend is {backend}. Found {model_name}\"\n                )\n            feature_extractor = torch.nn.Sequential(\n                *list(feature_extractor.children())[:-1]\n            )\n            input_layer = feature_extractor[0]\n            if in_chans != 3:\n                feature_extractor[0] = torch.nn.Conv2d(\n                    in_channels=in_chans,\n                    out_channels=input_layer.out_channels,\n                    kernel_size=input_layer.kernel_size,\n                    stride=input_layer.stride,\n                    padding=input_layer.padding,\n                    dilation=input_layer.dilation,\n                    groups=input_layer.groups,\n                    bias=input_layer.bias,\n                    padding_mode=input_layer.padding_mode,\n                )\n\n        else:\n            raise ValueError(\n                f\"Only ['timm', 'torch'] backends are available! Found {backend}.\"\n            )\n        return feature_extractor\n\n    def encoder_dim(self, model: torch.nn.Module) -&gt; int:\n        \"\"\"Compute dummy forward pass of encoder model and get embedding dimension.\n\n        Args:\n            model: a vision encoder model.\n\n        Returns:\n            The embedding dimension size.\n        \"\"\"\n        _ = model.eval()\n        dummy_output = model(torch.randn(1, self.in_chans, 224, 224)).squeeze()\n        _ = model.train()  # to be safe\n        return dummy_output.shape[-1]\n\n    def forward(self, img: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of feature extractor to get feature vector.\n\n        Args:\n            img: Input image tensor of shape (B, C, H, W).\n\n        Returns:\n            feats: Normalized output tensor of shape (B, d_model).\n        \"\"\"\n        # If grayscale, tile the image to 3 channels.\n        if img.shape[1] == 1:\n            img = img.repeat([1, 3, 1, 1])  # (B, nc=3, H, W)\n\n        b, c, h, w = img.shape\n\n        if c != self.in_chans:\n            raise ValueError(\n                f\"\"\"Found {c} channels in image but model was configured for {self.in_chans} channels! \\n\n                    Hint: have you set the number of anchors in your dataset &gt; 1? \\n\n                    If so, make sure to set `in_chans=3 * n_anchors`\"\"\"\n            )\n        feats = self.feature_extractor(\n            img\n        )  # (B, out_dim, 1, 1) if using resnet18 backbone.\n\n        # Reshape feature vectors\n        feats = feats.reshape([img.shape[0], -1])  # (B, out_dim)\n        # Map feature vectors to output dimension using linear layer.\n        feats = self.out_layer(feats)  # (B, d_model)\n        # Normalize output feature vectors.\n        feats = F.normalize(feats)  # (B, d_model)\n        return feats\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.VisualEncoder.__init__","title":"<code>__init__(model_name='resnet18', d_model=512, in_chans=3, backend='timm', **kwargs)</code>","text":"<p>Initialize Visual Encoder.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").</p> <code>'resnet18'</code> <code>d_model</code> <code>int</code> <p>Output embedding dimension.</p> <code>512</code> <code>in_chans</code> <code>int</code> <p>the number of input channels of the image.</p> <code>3</code> <code>backend</code> <code>int</code> <p>Which model backend to use. One of {\"timm\", \"torchvision\"}</p> <code>'timm'</code> <code>kwargs</code> <code>Any | None</code> <p>see <code>timm.create_model</code> and <code>torchvision.models.resnetX</code> for kwargs.</p> <code>{}</code> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"resnet18\",\n    d_model: int = 512,\n    in_chans: int = 3,\n    backend: int = \"timm\",\n    **kwargs: Any | None,\n):\n    \"\"\"Initialize Visual Encoder.\n\n    Args:\n        model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n        d_model (int): Output embedding dimension.\n        in_chans: the number of input channels of the image.\n        backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n        kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n    \"\"\"\n    super().__init__()\n\n    self.model_name = model_name.lower()\n    self.d_model = d_model\n    self.backend = backend\n    if in_chans == 1:\n        self.in_chans = 3\n    else:\n        self.in_chans = in_chans\n\n    self.feature_extractor = self.select_feature_extractor(\n        model_name=self.model_name,\n        in_chans=self.in_chans,\n        backend=self.backend,\n        **kwargs,\n    )\n\n    self.out_layer = torch.nn.Linear(\n        self.encoder_dim(self.feature_extractor), self.d_model\n    )\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.VisualEncoder.encoder_dim","title":"<code>encoder_dim(model)</code>","text":"<p>Compute dummy forward pass of encoder model and get embedding dimension.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>a vision encoder model.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The embedding dimension size.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def encoder_dim(self, model: torch.nn.Module) -&gt; int:\n    \"\"\"Compute dummy forward pass of encoder model and get embedding dimension.\n\n    Args:\n        model: a vision encoder model.\n\n    Returns:\n        The embedding dimension size.\n    \"\"\"\n    _ = model.eval()\n    dummy_output = model(torch.randn(1, self.in_chans, 224, 224)).squeeze()\n    _ = model.train()  # to be safe\n    return dummy_output.shape[-1]\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.VisualEncoder.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of feature extractor to get feature vector.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor</code> <p>Input image tensor of shape (B, C, H, W).</p> required <p>Returns:</p> Name Type Description <code>feats</code> <code>Tensor</code> <p>Normalized output tensor of shape (B, d_model).</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def forward(self, img: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of feature extractor to get feature vector.\n\n    Args:\n        img: Input image tensor of shape (B, C, H, W).\n\n    Returns:\n        feats: Normalized output tensor of shape (B, d_model).\n    \"\"\"\n    # If grayscale, tile the image to 3 channels.\n    if img.shape[1] == 1:\n        img = img.repeat([1, 3, 1, 1])  # (B, nc=3, H, W)\n\n    b, c, h, w = img.shape\n\n    if c != self.in_chans:\n        raise ValueError(\n            f\"\"\"Found {c} channels in image but model was configured for {self.in_chans} channels! \\n\n                Hint: have you set the number of anchors in your dataset &gt; 1? \\n\n                If so, make sure to set `in_chans=3 * n_anchors`\"\"\"\n        )\n    feats = self.feature_extractor(\n        img\n    )  # (B, out_dim, 1, 1) if using resnet18 backbone.\n\n    # Reshape feature vectors\n    feats = feats.reshape([img.shape[0], -1])  # (B, out_dim)\n    # Map feature vectors to output dimension using linear layer.\n    feats = self.out_layer(feats)  # (B, d_model)\n    # Normalize output feature vectors.\n    feats = F.normalize(feats)  # (B, d_model)\n    return feats\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.VisualEncoder.select_feature_extractor","title":"<code>select_feature_extractor(model_name, in_chans, backend, **kwargs)</code>","text":"<p>Select the appropriate feature extractor based on config.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").</p> required <code>in_chans</code> <code>int</code> <p>the number of input channels of the image.</p> required <code>backend</code> <code>str</code> <p>Which model backend to use. One of {\"timm\", \"torchvision\"}</p> required <code>kwargs</code> <code>Any</code> <p>see <code>timm.create_model</code> and <code>torchvision.models.resnetX</code> for kwargs.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Module</code> <p>a CNN encoder based on the config and backend selected.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def select_feature_extractor(\n    self, model_name: str, in_chans: int, backend: str, **kwargs: Any\n) -&gt; torch.nn.Module:\n    \"\"\"Select the appropriate feature extractor based on config.\n\n    Args:\n        model_name (str): Name of the CNN architecture to use (e.g. \"resnet18\", \"resnet50\").\n        in_chans: the number of input channels of the image.\n        backend: Which model backend to use. One of {\"timm\", \"torchvision\"}\n        kwargs: see `timm.create_model` and `torchvision.models.resnetX` for kwargs.\n\n    Returns:\n        a CNN encoder based on the config and backend selected.\n    \"\"\"\n    if \"timm\" in backend.lower():\n        feature_extractor = timm.create_model(\n            model_name=self.model_name,\n            in_chans=self.in_chans,\n            num_classes=0,\n            **kwargs,\n        )\n    elif \"torch\" in backend.lower():\n        if model_name.lower() == \"resnet18\":\n            feature_extractor = torchvision.models.resnet18(**kwargs)\n\n        elif model_name.lower() == \"resnet50\":\n            feature_extractor = torchvision.models.resnet50(**kwargs)\n\n        else:\n            raise ValueError(\n                f\"Only `[resnet18, resnet50]` are available when backend is {backend}. Found {model_name}\"\n            )\n        feature_extractor = torch.nn.Sequential(\n            *list(feature_extractor.children())[:-1]\n        )\n        input_layer = feature_extractor[0]\n        if in_chans != 3:\n            feature_extractor[0] = torch.nn.Conv2d(\n                in_channels=in_chans,\n                out_channels=input_layer.out_channels,\n                kernel_size=input_layer.kernel_size,\n                stride=input_layer.stride,\n                padding=input_layer.padding,\n                dilation=input_layer.dilation,\n                groups=input_layer.groups,\n                bias=input_layer.bias,\n                padding_mode=input_layer.padding_mode,\n            )\n\n    else:\n        raise ValueError(\n            f\"Only ['timm', 'torch'] backends are available! Found {backend}.\"\n        )\n    return feature_extractor\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.create_visual_encoder","title":"<code>create_visual_encoder(d_model, **encoder_cfg)</code>","text":"<p>Create a visual encoder based on the specified type.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def create_visual_encoder(d_model: int, **encoder_cfg) -&gt; torch.nn.Module:\n    \"\"\"Create a visual encoder based on the specified type.\"\"\"\n    register_encoder(\"resnet\", VisualEncoder)\n    register_encoder(\"descriptor\", DescriptorVisualEncoder)\n    # register any custom encoders here\n\n    # compatibility with configs that don't specify encoder_type; default to resnet\n    if not encoder_cfg or \"encoder_type\" not in encoder_cfg:\n        encoder_type = \"resnet\"\n        return ENCODER_REGISTRY[encoder_type](d_model=d_model, **encoder_cfg)\n    else:\n        encoder_type = encoder_cfg.pop(\"encoder_type\")\n\n    if encoder_type in ENCODER_REGISTRY:\n        # choose the relevant encoder configs based on the encoder_type\n        configs = encoder_cfg[encoder_type]\n        return ENCODER_REGISTRY[encoder_type](d_model=d_model, **configs)\n    else:\n        raise ValueError(\n            f\"Unknown encoder type: {encoder_type}. Please use one of {list(ENCODER_REGISTRY.keys())}\"\n        )\n</code></pre>"},{"location":"reference/dreem/models/visual_encoder/#dreem.models.visual_encoder.register_encoder","title":"<code>register_encoder(encoder_type, encoder_class)</code>","text":"<p>Register a new encoder type.</p> Source code in <code>dreem/models/visual_encoder.py</code> <pre><code>def register_encoder(encoder_type: str, encoder_class: Type[torch.nn.Module]):\n    \"\"\"Register a new encoder type.\"\"\"\n    if not issubclass(encoder_class, torch.nn.Module):\n        raise ValueError(f\"{encoder_class} must be a subclass of torch.nn.Module\")\n    ENCODER_REGISTRY[encoder_type] = encoder_class\n</code></pre>"},{"location":"reference/dreem/training/","title":"training","text":""},{"location":"reference/dreem/training/#dreem.training","title":"<code>dreem.training</code>","text":"<p>Initialize training module.</p> <p>Modules:</p> Name Description <code>losses</code> <p>Module containing different loss functions to be optimized.</p> <code>train</code> <p>Training script for training model.</p>"},{"location":"reference/dreem/training/losses/","title":"losses","text":""},{"location":"reference/dreem/training/losses/#dreem.training.losses","title":"<code>dreem.training.losses</code>","text":"<p>Module containing different loss functions to be optimized.</p> <p>Classes:</p> Name Description <code>AssoLoss</code> <p>Default association loss used for training GTR model.</p>"},{"location":"reference/dreem/training/losses/#dreem.training.losses.AssoLoss","title":"<code>AssoLoss</code>","text":"<p>               Bases: <code>Module</code></p> <p>Default association loss used for training GTR model.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize Loss function.</p> <code>detr_asso_loss</code> <p>Calculate association loss between predicted and gt boxes.</p> <code>forward</code> <p>Calculate association loss.</p> Source code in <code>dreem/training/losses.py</code> <pre><code>class AssoLoss(nn.Module):\n    \"\"\"Default association loss used for training GTR model.\"\"\"\n\n    def __init__(\n        self,\n        neg_unmatched: bool = False,\n        epsilon: float = 1e-4,\n        asso_weight: float = 1.0,\n    ):\n        \"\"\"Initialize Loss function.\n\n        Args:\n            neg_unmatched: Whether or not to set unmatched objects to background\n            epsilon: small number used for numeric precision to prevent dividing by zero\n            asso_weight: How much to weight the association loss by\n        \"\"\"\n        super().__init__()\n\n        self.neg_unmatched = neg_unmatched\n        self.epsilon = epsilon\n        self.asso_weight = asso_weight\n\n    def forward(\n        self, asso_preds: list[torch.Tensor], frames: list[\"Frame\"]\n    ) -&gt; torch.Tensor:\n        \"\"\"Calculate association loss.\n\n        Args:\n            asso_preds: a list containing the association matrix at each frame\n            frames: a list of Frames containing gt labels.\n\n        Returns:\n            the association loss between predicted association and actual\n        \"\"\"\n        # get number of detected objects and ground truth ids\n        n_t = [frame.num_detected for frame in frames]\n        target_inst_id = torch.cat(\n            [frame.get_gt_track_ids().to(asso_preds[-1].device) for frame in frames]\n        )\n        instances = [instance for frame in frames for instance in frame.instances]\n\n        # for now set equal since detections are fixed\n        pred_box = get_boxes(instances)\n        pred_time, _ = get_times(instances)\n        pred_box = torch.nanmean(pred_box, axis=1)\n        target_box, target_time = pred_box, pred_time\n\n        # todo: we should maybe reconsider how we label gt instances. The second\n        # criterion will return true on a single instance video, for example.\n        # For now we can ignore this since we train on dense labels.\n\n        \"\"\"\n            # Return a 0 loss if any of the 2 criteria are met\n            # 1. the video doesn\u2019t have gt bboxes\n            # 2. the maximum id is zero\n\n        sum_instance_lengths = sum(len(x) for x in instances)\n        max_instance_lengths = max(\n            x[\"gt_track_ids\"].max().item() for x in instances if len(x) &gt; 0\n        )\n\n        if sum_instance_lengths == 0 or max_instance_lengths == 0:\n            print(\"No bounding boxes detected, returning zero loss\")\n            print(f\"Sum instance lengths: {sum_instance_lengths}\")\n            print(f\"Max instance lengths: {max_instance_lengths}\")\n            loss = asso_preds[0].new_zeros((1,), dtype=torch.float32)[0]\n            return loss\n        \"\"\"\n\n        asso_gt, match_cues = self._get_asso_gt(\n            pred_box, pred_time, target_box, target_time, target_inst_id, n_t\n        )\n\n        loss = sum(\n            [\n                self.detr_asso_loss(asso_pred, asso_gt, match_cues, n_t)\n                for asso_pred in asso_preds\n            ]\n        )\n\n        loss *= self.asso_weight\n\n        return loss\n\n    def _get_asso_gt(\n        self,\n        pred_box: torch.Tensor,\n        pred_time: torch.Tensor,\n        target_box: torch.Tensor,\n        target_time: torch.Tensor,\n        target_inst_id: torch.Tensor,\n        n_t: torch.Tensor,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Compute the association ground truth for a batch.\n\n        Args:\n            pred_box: predicted bounding boxes (N x 4)\n            pred_time: predicted time intervals (N,)\n            target_box: target bounding boxes (N x 4)\n            target_time: target time intervals (N,)\n            target_inst_id: target instance IDs (N,)\n            n_t: number of ground truth instances (N,)\n\n        Returns:\n            A tuple containing:\n                asso_gt: Ground truth association matrix (K x N) denoting ground\n                    truth instances over time\n                match_cues: Tensor indicating which instance is assigned to each gt\n                    detection (K x 3) or (N,)\n        \"\"\"\n        # compute ious over bboxes, ignore pairs with different time stamps\n        ious = torchvision.ops.box_iou(pred_box, target_box)\n        ious[pred_time[:, None] != target_time[None, :]] = -1.0\n\n        # get unique instance ids\n        inst_ids = torch.unique(target_inst_id[target_inst_id &gt; -1])\n\n        # initialize tensors\n        K, N = len(inst_ids), len(pred_box)\n        match_cues = pred_box.new_full((N,), -1, dtype=torch.long)\n        T = len(n_t)\n        asso_gt = pred_box.new_zeros((K, T), dtype=torch.long)\n\n        # split ious by frames\n        ious_per_frame = ious.split(n_t, dim=0)\n\n        for k, inst_id in enumerate(inst_ids):\n            # get ground truth indices, init index\n            target_inds = target_inst_id == inst_id\n            base_ind = 0\n\n            for t in range(T):\n                # get relevant ious\n                iou_t = ious_per_frame[t][:, target_inds]\n\n                # if there are no detections, asso_gt = # gt instances at time step\n                if iou_t.numel() == 0:\n                    asso_gt[k, t] = n_t[t]\n                else:\n                    # get max iou and index, select positive ious\n                    val, inds = iou_t.max(dim=0)\n                    ind = inds[val &gt; 0.0]\n\n                    # make sure there is at most one detection\n                    assert len(ind) &lt;= 1, f\"{target_inst_id} {n_t}\"\n\n                    # if there is one detection with pos IOU, select it\n                    if len(ind) == 1:\n                        obj_ind = ind[0].item()\n                        asso_gt[k, t] = obj_ind\n                        match_cues[base_ind + obj_ind] = k\n\n                    # otherwise asso_gt = # gt instances at time step\n                    else:\n                        asso_gt[k, t] = n_t[t]\n\n                base_ind += n_t[t]\n\n        return asso_gt, match_cues\n\n    def detr_asso_loss(\n        self,\n        asso_pred: torch.Tensor,\n        asso_gt: torch.Tensor,\n        match_cues: torch.Tensor,\n        n_t: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Calculate association loss between predicted and gt boxes.\n\n        Args:\n            asso_pred: Association matrix output from the transformer forward\n                pass denoting predicted instances over time (M x N)\n            asso_gt: Ground truth association matrix (K x N) denoting ground\n                truth instances over time\n            match_cues: Tensor indicating which instance is assigned to each gt\n                detection (K x 3) or (N,)\n            n_t: number of ground truth instances (N,)\n\n        Returns:\n            loss: association loss normalized by number of objects\n        \"\"\"\n        # get matches between preds and gt\n        src_inds, target_inds = self._match(asso_pred, asso_gt, match_cues, n_t)\n\n        loss = 0\n        num_objs = 0\n\n        zero = asso_pred.new_zeros((asso_pred.shape[0], 1))  # M x 1\n        asso_pred_image = asso_pred.split(n_t, dim=1)  # T x [M x n_t]\n\n        for t in range(len(n_t)):\n            # add background class\n            asso_pred_with_bg = torch.cat(\n                [asso_pred_image[t], zero], dim=1\n            )  # M x (n_t + 1)\n\n            if self.neg_unmatched:\n                # set unmatched preds to background\n                asso_gt_t = asso_gt.new_full((asso_pred.shape[0],), float(n_t[t]))  # M\n                asso_gt_t[src_inds] = asso_gt[target_inds, t]  # M\n            else:\n                # keep only unmatched preds\n                asso_pred_with_bg = asso_pred_with_bg[src_inds]  # K x (n_t + 1)\n                asso_gt_t = asso_gt[target_inds, t]  # K\n\n            num_objs += (asso_gt_t != n_t[t]).float().sum()\n\n            loss += F.cross_entropy(asso_pred_with_bg, asso_gt_t, reduction=\"none\")\n\n        return loss.sum() / (num_objs + self.epsilon)\n\n    @torch.no_grad()\n    def _match(\n        self,\n        asso_pred: torch.Tensor,\n        asso_gt: torch.Tensor,\n        match_cues: torch.Tensor,\n        n_t: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Match predicted scores to gt scores using match cues.\n\n        Args:\n            asso_pred: Association matrix output from the transformer forward\n                pass denoting predicted instances over time (M x N)\n            asso_gt: Ground truth association matrix (K x N) denoting ground\n                truth instances over time\n            match_cues: Tensor indicating which instance is assigned to each gt\n                detection (K x 3) or (N,)\n            n_t: number of ground truth instances (N,)\n\n        Returns:\n            src_inds: Matched source indices (N,)\n            target_inds: Matched target indices (N,)\n        \"\"\"\n        src_inds = torch.where(match_cues &gt;= 0)[0]\n        target_inds = match_cues[src_inds]\n\n        return (src_inds, target_inds)\n</code></pre>"},{"location":"reference/dreem/training/losses/#dreem.training.losses.AssoLoss.__init__","title":"<code>__init__(neg_unmatched=False, epsilon=0.0001, asso_weight=1.0)</code>","text":"<p>Initialize Loss function.</p> <p>Parameters:</p> Name Type Description Default <code>neg_unmatched</code> <code>bool</code> <p>Whether or not to set unmatched objects to background</p> <code>False</code> <code>epsilon</code> <code>float</code> <p>small number used for numeric precision to prevent dividing by zero</p> <code>0.0001</code> <code>asso_weight</code> <code>float</code> <p>How much to weight the association loss by</p> <code>1.0</code> Source code in <code>dreem/training/losses.py</code> <pre><code>def __init__(\n    self,\n    neg_unmatched: bool = False,\n    epsilon: float = 1e-4,\n    asso_weight: float = 1.0,\n):\n    \"\"\"Initialize Loss function.\n\n    Args:\n        neg_unmatched: Whether or not to set unmatched objects to background\n        epsilon: small number used for numeric precision to prevent dividing by zero\n        asso_weight: How much to weight the association loss by\n    \"\"\"\n    super().__init__()\n\n    self.neg_unmatched = neg_unmatched\n    self.epsilon = epsilon\n    self.asso_weight = asso_weight\n</code></pre>"},{"location":"reference/dreem/training/losses/#dreem.training.losses.AssoLoss.detr_asso_loss","title":"<code>detr_asso_loss(asso_pred, asso_gt, match_cues, n_t)</code>","text":"<p>Calculate association loss between predicted and gt boxes.</p> <p>Parameters:</p> Name Type Description Default <code>asso_pred</code> <code>Tensor</code> <p>Association matrix output from the transformer forward pass denoting predicted instances over time (M x N)</p> required <code>asso_gt</code> <code>Tensor</code> <p>Ground truth association matrix (K x N) denoting ground truth instances over time</p> required <code>match_cues</code> <code>Tensor</code> <p>Tensor indicating which instance is assigned to each gt detection (K x 3) or (N,)</p> required <code>n_t</code> <code>Tensor</code> <p>number of ground truth instances (N,)</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>Tensor</code> <p>association loss normalized by number of objects</p> Source code in <code>dreem/training/losses.py</code> <pre><code>def detr_asso_loss(\n    self,\n    asso_pred: torch.Tensor,\n    asso_gt: torch.Tensor,\n    match_cues: torch.Tensor,\n    n_t: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Calculate association loss between predicted and gt boxes.\n\n    Args:\n        asso_pred: Association matrix output from the transformer forward\n            pass denoting predicted instances over time (M x N)\n        asso_gt: Ground truth association matrix (K x N) denoting ground\n            truth instances over time\n        match_cues: Tensor indicating which instance is assigned to each gt\n            detection (K x 3) or (N,)\n        n_t: number of ground truth instances (N,)\n\n    Returns:\n        loss: association loss normalized by number of objects\n    \"\"\"\n    # get matches between preds and gt\n    src_inds, target_inds = self._match(asso_pred, asso_gt, match_cues, n_t)\n\n    loss = 0\n    num_objs = 0\n\n    zero = asso_pred.new_zeros((asso_pred.shape[0], 1))  # M x 1\n    asso_pred_image = asso_pred.split(n_t, dim=1)  # T x [M x n_t]\n\n    for t in range(len(n_t)):\n        # add background class\n        asso_pred_with_bg = torch.cat(\n            [asso_pred_image[t], zero], dim=1\n        )  # M x (n_t + 1)\n\n        if self.neg_unmatched:\n            # set unmatched preds to background\n            asso_gt_t = asso_gt.new_full((asso_pred.shape[0],), float(n_t[t]))  # M\n            asso_gt_t[src_inds] = asso_gt[target_inds, t]  # M\n        else:\n            # keep only unmatched preds\n            asso_pred_with_bg = asso_pred_with_bg[src_inds]  # K x (n_t + 1)\n            asso_gt_t = asso_gt[target_inds, t]  # K\n\n        num_objs += (asso_gt_t != n_t[t]).float().sum()\n\n        loss += F.cross_entropy(asso_pred_with_bg, asso_gt_t, reduction=\"none\")\n\n    return loss.sum() / (num_objs + self.epsilon)\n</code></pre>"},{"location":"reference/dreem/training/losses/#dreem.training.losses.AssoLoss.forward","title":"<code>forward(asso_preds, frames)</code>","text":"<p>Calculate association loss.</p> <p>Parameters:</p> Name Type Description Default <code>asso_preds</code> <code>list[Tensor]</code> <p>a list containing the association matrix at each frame</p> required <code>frames</code> <code>list[Frame]</code> <p>a list of Frames containing gt labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>the association loss between predicted association and actual</p> Source code in <code>dreem/training/losses.py</code> <pre><code>def forward(\n    self, asso_preds: list[torch.Tensor], frames: list[\"Frame\"]\n) -&gt; torch.Tensor:\n    \"\"\"Calculate association loss.\n\n    Args:\n        asso_preds: a list containing the association matrix at each frame\n        frames: a list of Frames containing gt labels.\n\n    Returns:\n        the association loss between predicted association and actual\n    \"\"\"\n    # get number of detected objects and ground truth ids\n    n_t = [frame.num_detected for frame in frames]\n    target_inst_id = torch.cat(\n        [frame.get_gt_track_ids().to(asso_preds[-1].device) for frame in frames]\n    )\n    instances = [instance for frame in frames for instance in frame.instances]\n\n    # for now set equal since detections are fixed\n    pred_box = get_boxes(instances)\n    pred_time, _ = get_times(instances)\n    pred_box = torch.nanmean(pred_box, axis=1)\n    target_box, target_time = pred_box, pred_time\n\n    # todo: we should maybe reconsider how we label gt instances. The second\n    # criterion will return true on a single instance video, for example.\n    # For now we can ignore this since we train on dense labels.\n\n    \"\"\"\n        # Return a 0 loss if any of the 2 criteria are met\n        # 1. the video doesn\u2019t have gt bboxes\n        # 2. the maximum id is zero\n\n    sum_instance_lengths = sum(len(x) for x in instances)\n    max_instance_lengths = max(\n        x[\"gt_track_ids\"].max().item() for x in instances if len(x) &gt; 0\n    )\n\n    if sum_instance_lengths == 0 or max_instance_lengths == 0:\n        print(\"No bounding boxes detected, returning zero loss\")\n        print(f\"Sum instance lengths: {sum_instance_lengths}\")\n        print(f\"Max instance lengths: {max_instance_lengths}\")\n        loss = asso_preds[0].new_zeros((1,), dtype=torch.float32)[0]\n        return loss\n    \"\"\"\n\n    asso_gt, match_cues = self._get_asso_gt(\n        pred_box, pred_time, target_box, target_time, target_inst_id, n_t\n    )\n\n    loss = sum(\n        [\n            self.detr_asso_loss(asso_pred, asso_gt, match_cues, n_t)\n            for asso_pred in asso_preds\n        ]\n    )\n\n    loss *= self.asso_weight\n\n    return loss\n</code></pre>"},{"location":"reference/dreem/training/train/","title":"train","text":""},{"location":"reference/dreem/training/train/#dreem.training.train","title":"<code>dreem.training.train</code>","text":"<p>Training script for training model.</p> <p>Used for training a single model or deploying a batch train job on RUNAI CLI</p> <p>Functions:</p> Name Description <code>run</code> <p>Train model based on config.</p>"},{"location":"reference/dreem/training/train/#dreem.training.train.run","title":"<code>run(cfg)</code>","text":"<p>Train model based on config.</p> <p>Handles all config parsing and initialization then calls <code>trainer.train()</code>. If <code>batch_config</code> is included then run will be assumed to be a batch job.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>The config dict parsed by <code>hydra</code></p> required Source code in <code>dreem/training/train.py</code> <pre><code>@hydra.main(config_path=None, config_name=None, version_base=None)\ndef run(cfg: DictConfig):\n    \"\"\"Train model based on config.\n\n    Handles all config parsing and initialization then calls `trainer.train()`.\n    If `batch_config` is included then run will be assumed to be a batch job.\n\n    Args:\n        cfg: The config dict parsed by `hydra`\n    \"\"\"\n    torch.set_float32_matmul_precision(\"medium\")\n    train_cfg = Config(cfg)\n\n    # update with parameters for batch train job\n    if \"batch_config\" in cfg.keys():\n        try:\n            index = int(os.environ[\"POD_INDEX\"])\n        except KeyError as e:\n            index = int(\n                input(f\"{e}. Assuming single run!\\nPlease input task index to run:\")\n            )\n\n        hparams_df = pd.read_csv(cfg.batch_config)\n        hparams = hparams_df.iloc[index].to_dict()\n\n        if train_cfg.set_hparams(hparams):\n            logger.debug(\"Updated the following hparams to the following values\")\n            logger.debug(hparams)\n    else:\n        hparams = {}\n    logging.getLogger().setLevel(level=cfg.get(\"log_level\", \"INFO\").upper())\n    logger.info(f\"Final train config: {train_cfg}\")\n\n    model = train_cfg.get_model()\n\n    train_dataset = train_cfg.get_dataset(mode=\"train\")\n    train_dataloader = train_cfg.get_dataloader(train_dataset, mode=\"train\")\n\n    val_dataset = train_cfg.get_dataset(mode=\"val\")\n    val_dataloader = train_cfg.get_dataloader(val_dataset, mode=\"val\")\n\n    dataset = TrackingDataset(train_dl=train_dataloader, val_dl=val_dataloader)\n\n    if cfg.view_batch.enable:\n        instances = next(iter(train_dataset))\n        view_training_batch(instances, num_frames=cfg.view_batch.num_frames)\n\n        if cfg.view_batch.no_train:\n            return\n\n    model = train_cfg.get_gtr_runner()  # TODO see if we can use torch.compile()\n\n    run_logger = train_cfg.get_logger()\n\n    if run_logger is not None and isinstance(run_logger, pl.loggers.wandb.WandbLogger):\n        data_paths = train_cfg.data_paths\n        flattened_paths = [\n            [item] for sublist in data_paths.values() for item in sublist\n        ]\n        run_logger.log_text(\n            \"training_files\", columns=[\"data_paths\"], data=flattened_paths\n        )\n\n    callbacks = []\n    _ = callbacks.extend(train_cfg.get_checkpointing())\n    _ = callbacks.append(pl.callbacks.LearningRateMonitor())\n\n    early_stopping = train_cfg.get_early_stopping()\n    if early_stopping is not None:\n        callbacks.append(early_stopping)\n\n    accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n    devices = torch.cuda.device_count() if torch.cuda.is_available() else cpu_count()\n\n    trainer = train_cfg.get_trainer(\n        callbacks,\n        run_logger,\n        accelerator=accelerator,\n        devices=devices,\n    )\n\n    trainer.fit(model, dataset)\n</code></pre>"}]}